<!DOCTYPE html>
<html lang="en">

<head>
    <title>WangRongsheng Arxiv</title>
    <meta charset="utf-8"/>
    <meta http-equiv="X-UA-Compatible" content="IE=edge"/>
    <meta name="robots" content="noindex, nofollow"/>
    <meta name="viewport" content="width=device-width, initial-scale=1"/>
    <link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
    <link href="index.css" rel="stylesheet"/>
    <link href="https://cdn.jsdelivr.net/npm/remixicon@2.5.0/fonts/remixicon.css" rel="stylesheet">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css"
          integrity="sha384-R4558gYOUz8mP9YWpZJjofhk+zx0AS11p36HnD2ZKj/6JR5z27gSSULCNHIRReVs" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css"
          integrity="sha384-R4558gYOUz8mP9YWpZJjofhk+zx0AS11p36HnD2ZKj/6JR5z27gSSULCNHIRReVs" crossorigin="anonymous">
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js"
            integrity="sha384-z1fJDqw8ZApjGO3/unPWUPsIymfsJmyrDVWC8Tv/a1HeOtGmkwNd/7xUS0Xcnvsx"
            crossorigin="anonymous"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/contrib/auto-render.min.js"
            integrity="sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR"
            crossorigin="anonymous"></script>
    <script>
        document.addEventListener("DOMContentLoaded", function () {
            renderMathInElement(document.body, {
                // customised options
                // • auto-render specific keys, e.g.:
                delimiters: [
                    {left: '$$', right: '$$', display: true},
                    {left: '$', right: '$', display: false},
                    {left: '\\(', right: '\\)', display: false},
                    {left: '\\[', right: '\\]', display: true},
                    {left: "\\begin{equation}", right: "\\end{equation}", display: true},
                    {left: "\\begin{align}", right: "\\end{align}", display: true},
                    {left: "\\begin{alignat}", right: "\\end{alignat}", display: true},
                    {left: "\\begin{gather}", right: "\\end{gather}", display: true},
                    {left: "\\begin{CD}", right: "\\end{CD}", display: true},
                ],
                // • rendering keys, e.g.:
                throwOnError: false
            });
        });
    </script>
</head>

<body>
<section class="header-container">
    <div style="display:flex; justify-content:space-between; align-items:flex-end;">
        <div>
            <div class="header-title">
                MyArxiv
            </div>
        </div>

        <div class=icons>
            <label class="theme-switch" for="checkbox">
                <input type="checkbox" id="checkbox"/>
                <i id="theme-icon" class="ri-moon-line" style="font-size: 32px" rel="noopener noreferrer"></i>
            </label>
        </div>
    </div>
</section>

    <section class="day-container">
        <div class="date">
            <time datetime="2025-03-03T00:00:00Z">2025-03-03</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Computation and Language
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Forecasting Frontier Language Model Agent Capabilities 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.15850v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.15850v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Govind Pimpale, Axel Højmark, Jérémy Scheurer, Marius Hobbhahn
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As Language Models (LMs) increasingly operate as autonomous agents,
accurately forecasting their capabilities becomes crucial for societal
preparedness. We evaluate six forecasting methods that predict downstream
capabilities of LM agents. We use "one-step" approaches that predict benchmark
scores from input metrics like compute or model release date directly or
"two-step" approaches that first predict an intermediate metric like the
principal component of cross-benchmark performance (PC-1) and human-evaluated
competitive Elo ratings. We evaluate our forecasting methods by backtesting
them on a dataset of 38 LMs from the OpenLLM 2 leaderboard. We then use the
validated two-step approach (Release Date$\to$Elo$\to$Benchmark) to predict LM
agent performance for frontier models on three benchmarks: SWE-Bench Verified
(software development), Cybench (cybersecurity assessment), and RE-Bench (ML
research engineering). Our forecast predicts that by the beginning of 2026,
non-specialized LM agents with low capability elicitation will reach a success
rate of 54% on SWE-Bench Verified, while state-of-the-art LM agents will reach
an 87% success rate. Our approach does not account for recent advances in
inference-compute scaling and might thus be too conservative.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Chain of Draft: Thinking Faster by Writing Less 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.18600v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.18600v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Silei Xu, Wenhao Xie, Lingxiao Zhao, Pengcheng He
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have demonstrated remarkable performance in
solving complex reasoning tasks through mechanisms like Chain-of-Thought (CoT)
prompting, which emphasizes verbose, step-by-step reasoning. However, humans
typically employ a more efficient strategy: drafting concise intermediate
thoughts that capture only essential information. In this work, we propose
Chain of Draft (CoD), a novel paradigm inspired by human cognitive processes,
where LLMs generate minimalistic yet informative intermediate reasoning outputs
while solving tasks. By reducing verbosity and focusing on critical insights,
CoD matches or surpasses CoT in accuracy while using as little as only 7.6% of
the tokens, significantly reducing cost and latency across various reasoning
tasks. Our code and data are available at
https://github.com/sileix/chain-of-draft.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SensorQA: A Question Answering Benchmark for Daily-Life Monitoring 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.04974v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.04974v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Benjamin Reichman, Xiaofan Yu, Lanxiang Hu, Jack Truxal, Atishay Jain, Rushil Chandrupatla, Tajana Šimunić Rosing, Larry Heck
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the rapid growth in sensor data, effectively interpreting and
interfacing with these data in a human-understandable way has become crucial.
While existing research primarily focuses on learning classification models,
fewer studies have explored how end users can actively extract useful insights
from sensor data, often hindered by the lack of a proper dataset. To address
this gap, we introduce SensorQA, the first human-created question-answering
(QA) dataset for long-term time-series sensor data for daily life monitoring.
SensorQA is created by human workers and includes 5.6K diverse and practical
queries that reflect genuine human interests, paired with accurate answers
derived from sensor data. We further establish benchmarks for state-of-the-art
AI models on this dataset and evaluate their performance on typical edge
devices. Our results reveal a gap between current models and optimal QA
performance and efficiency, highlighting the need for new contributions. The
dataset and code are available at:
https://github.com/benjamin-reichman/SensorQA.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ On Memory Construction and Retrieval for Personalized Conversational
  Agents 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.05589v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.05589v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhuoshi Pan, Qianhui Wu, Huiqiang Jiang, Xufang Luo, Hao Cheng, Dongsheng Li, Yuqing Yang, Chin-Yew Lin, H. Vicky Zhao, Lili Qiu, Jianfeng Gao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To deliver coherent and personalized experiences in long-term conversations,
existing approaches typically perform retrieval augmented response generation
by constructing memory banks from conversation history at either the
turn-level, session-level, or through summarization techniques.In this paper,
we present two key findings: (1) The granularity of memory unit matters:
turn-level, session-level, and summarization-based methods each exhibit
limitations in both memory retrieval accuracy and the semantic quality of the
retrieved content. (2) Prompt compression methods, such as LLMLingua-2, can
effectively serve as a denoising mechanism, enhancing memory retrieval accuracy
across different granularities. Building on these insights, we propose SeCom, a
method that constructs the memory bank at segment level by introducing a
conversation segmentation model that partitions long-term conversations into
topically coherent segments, while applying compression based denoising on
memory units to enhance memory retrieval. Experimental results show that SeCom
exhibits a significant performance advantage over baselines on long-term
conversation benchmarks LOCOMO and Long-MT-Bench+. Additionally, the proposed
conversation segmentation method demonstrates superior performance on dialogue
segmentation datasets such as DialSeg711, TIAGE, and SuperDialSeg.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 5 figures, conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ R1-T1: Fully Incentivizing Translation Capability in LLMs via Reasoning
  Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.19735v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.19735v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Minggui He, Yilun Liu, Shimin Tao, Yuanchang Luo, Hongyong Zeng, Chang Su, Li Zhang, Hongxia Ma, Daimeng Wei, Weibin Meng, Hao Yang, Boxing Chen, Osamu Yoshie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite recent breakthroughs in reasoning-enhanced large language models
(LLMs) like DeepSeek-R1, incorporating inference-time reasoning into machine
translation (MT), where human translators naturally employ structured,
multi-layered reasoning chain-of-thoughts (CoTs), is yet underexplored.
Existing methods either design a fixed CoT tailored for a specific MT sub-task
(e.g., literature translation), or rely on synthesizing CoTs unaligned with
humans, limiting their adaptability to diverse translation scenarios. This
paper introduces R1-Translator (R1-T1), a novel framework to achieve
inference-time reasoning for general MT via reinforcement learning (RL) with
human-aligned CoTs comprising six common patterns. Our approach pioneers three
innovations: (1) extending reasoning-based translation beyond MT sub-tasks to
six languages and diverse tasks (e.g., legal/medical domain adaptation, idiom
resolution); (2) formalizing six expert-curated CoT templates that mirror
hybrid human strategies like context-aware paraphrasing and back translation;
and (3) enabling self-evolving CoT discovery through RL. Experimental results
indicate a steady translation performance improvement in 11 languages and 40
translation directions on Flores-101 test set, especially on the languages
unseen from training.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ InductionBench: LLMs Fail in the Simplest Complexity Class 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.15823v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.15823v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenyue Hua, Tyler Wong, Sun Fei, Liangming Pan, Adam Jardine, William Yang Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have shown remarkable improvements in reasoning
and many existing benchmarks have been addressed by models such as o1 and o3
either fully or partially. However, a majority of these benchmarks emphasize
deductive reasoning, including mathematical and coding tasks in which rules
such as mathematical axioms or programming syntax are clearly defined, based on
which LLMs can plan and apply these rules to arrive at a solution. In contrast,
inductive reasoning, where one infers the underlying rules from observed data,
remains less explored. Such inductive processes lie at the heart of scientific
discovery, as they enable researchers to extract general principles from
empirical observations. To assess whether LLMs possess this capacity, we
introduce InductionBench, a new benchmark designed to evaluate the inductive
reasoning ability of LLMs. Our experimental findings reveal that even the most
advanced models available struggle to master the simplest complexity classes
within the subregular hierarchy of functions, highlighting a notable deficiency
in current LLMs' inductive reasoning capabilities. Coda and data are available
https://github.com/Wenyueh/inductive_reasoning_benchmark.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>24 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Can Knowledge Editing Really Correct Hallucinations? <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.16251v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.16251v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Baixiang Huang, Canyu Chen, Xiongxiao Xu, Ali Payani, Kai Shu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) suffer from hallucinations, referring to the
non-factual information in generated content, despite their superior capacities
across tasks. Meanwhile, knowledge editing has been developed as a new popular
paradigm to correct erroneous factual knowledge encoded in LLMs with the
advantage of avoiding retraining from scratch. However, a common issue of
existing evaluation datasets for knowledge editing is that they do not ensure
that LLMs actually generate hallucinated answers to the evaluation questions
before editing. When LLMs are evaluated on such datasets after being edited by
different techniques, it is hard to directly adopt the performance to assess
the effectiveness of different knowledge editing methods in correcting
hallucinations. Thus, the fundamental question remains insufficiently
validated: Can knowledge editing really correct hallucinations in LLMs? We
proposed HalluEditBench to holistically benchmark knowledge editing methods in
correcting real-world hallucinations. First, we rigorously construct a massive
hallucination dataset with 9 domains, 26 topics and more than 6,000
hallucinations. Then, we assess the performance of knowledge editing methods in
a holistic way on five dimensions including Efficacy, Generalization,
Portability, Locality, and Robustness. Through HalluEditBench, we have provided
new insights into the potentials and limitations of different knowledge editing
methods in correcting hallucinations, which could inspire future improvements
and facilitate progress in the field of knowledge editing.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2025. Main paper: 10 pages; total: 34 pages (including
  appendix). The first two authors contributed equally to this work. Code,
  data, results, and additional resources are available on the project website:
  https://llm-editing.github.io</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Revisiting the Test-Time Scaling of o1-like Models: Do they Truly
  Possess Test-Time Scaling Capabilities? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.12215v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.12215v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhiyuan Zeng, Qinyuan Cheng, Zhangyue Yin, Yunhua Zhou, Xipeng Qiu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The advent of test-time scaling in large language models (LLMs), exemplified
by OpenAI's o1 series, has advanced reasoning capabilities by scaling
computational resource allocation during inference. While successors like QwQ,
Deepseek-R1 (R1) and LIMO replicate these advancements, whether these models
truly possess test-time scaling capabilities remains underexplored. This study
found that longer CoTs of these o1-like models do not consistently enhance
accuracy; in fact, correct solutions are often shorter than incorrect ones for
the same questions. Further investigation shows this phenomenon is closely
related to models' self-revision capabilities - longer CoTs contain more
self-revisions, which often lead to performance degradation. We then compare
sequential and parallel scaling strategies on QwQ, R1 and LIMO, finding that
parallel scaling achieves better coverage and scalability. Based on these
insights, we propose Shortest Majority Vote, a method that combines parallel
scaling strategies with CoT length characteristics, significantly improving
models' test-time scalability compared to conventional majority voting
approaches.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Add the github link</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ First-Person Fairness in Chatbots <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.19803v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.19803v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tyna Eloundou, Alex Beutel, David G. Robinson, Keren Gu-Lemberg, Anna-Luisa Brakman, Pamela Mishkin, Meghan Shah, Johannes Heidecke, Lilian Weng, Adam Tauman Kalai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Evaluating chatbot fairness is crucial given their rapid proliferation, yet
typical chatbot tasks (e.g., resume writing, entertainment) diverge from the
institutional decision-making tasks (e.g., resume screening) which have
traditionally been central to discussion of algorithmic fairness. The
open-ended nature and diverse use-cases of chatbots necessitate novel methods
for bias assessment. This paper addresses these challenges by introducing a
scalable counterfactual approach to evaluate "first-person fairness," meaning
fairness toward chatbot users based on demographic characteristics. Our method
employs a Language Model as a Research Assistant (LMRA) to yield quantitative
measures of harmful stereotypes and qualitative analyses of demographic
differences in chatbot responses. We apply this approach to assess biases in
six of our language models across millions of interactions, covering sixty-six
tasks in nine domains and spanning two genders and four races. Independent
human annotations corroborate the LMRA-generated bias evaluations. This study
represents the first large-scale fairness evaluation based on real-world chat
data. We highlight that post-training reinforcement learning techniques
significantly mitigate these biases. This evaluation provides a practical
methodology for ongoing bias monitoring and mitigation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>In ICLR 2025, 59 pages, 27 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CNsum:Automatic Summarization for Chinese News Text 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.19723v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.19723v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yu Zhao, Songping Huang, Dongsheng Zhou, Zhaoyun Ding, Fei Wang, Aixin Nian
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Obtaining valuable information from massive data efficiently has become our
research goal in the era of Big Data. Text summarization technology has been
continuously developed to meet this demand. Recent work has also shown that
transformer-based pre-trained language models have achieved great success on
various tasks in Natural Language Processing (NLP). Aiming at the problem of
Chinese news text summary generation and the application of Transformer
structure on Chinese, this paper proposes a Chinese news text summarization
model (CNsum) based on Transformer structure, and tests it on Chinese datasets
such as THUCNews. The results of the conducted experiments show that CNsum
achieves better ROUGE score than the baseline models, which verifies the
outperformance of the model.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This withdrawal is due to the lack of authorization from all
  co-authors for the publication of this version</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Gumbel Counterfactual Generation From Language Models <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07180v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07180v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shauli Ravfogel, Anej Svete, Vésteinn Snæbjarnarson, Ryan Cotterell
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Understanding and manipulating the causal generation mechanisms in language
models is essential for controlling their behavior. Previous work has primarily
relied on techniques such as representation surgery -- e.g., model ablations or
manipulation of linear subspaces tied to specific concepts -- to
\emph{intervene} on these models. To understand the impact of interventions
precisely, it is useful to examine \emph{counterfactuals} -- e.g., how a given
sentence would have appeared had it been generated by the model following a
specific intervention. We highlight that counterfactual reasoning is
conceptually distinct from interventions, as articulated in Pearl's causal
hierarchy. Based on this observation, we propose a framework for generating
true string counterfactuals by reformulating language models as a structural
equation model using the Gumbel-max trick, which we called Gumbel
counterfactual generation. This reformulation allows us to model the joint
distribution over original strings and their counterfactuals resulting from the
same instantiation of the sampling noise. We develop an algorithm based on
hindsight Gumbel sampling that allows us to infer the latent noise variables
and generate counterfactuals of observed strings. Our experiments demonstrate
that the approach produces meaningful counterfactuals while at the same time
showing that commonly used intervention techniques have considerable undesired
side effects.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in ICLR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ From Tokens to Words: On the Inner Lexicon of LLMs <span class="chip">ICLR</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05864v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05864v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guy Kaplan, Matanel Oren, Yuval Reif, Roy Schwartz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Natural language is composed of words, but modern large language models
(LLMs) process sub-words as input. A natural question raised by this
discrepancy is whether LLMs encode words internally, and if so how. We present
evidence that LLMs engage in an intrinsic detokenization process, where
sub-word sequences are combined into coherent whole-word representations at
their last token. Our experiments show that this process primarily takes place
within the early and middle layers of the model. We further demonstrate its
robustness to arbitrary splits (e.g., "cats" to "ca" and "ts"), typos, and
importantly-to out-of-vocabulary words: when feeding the last token internal
representations of such words to the model as input, it can "understand" them
as the complete word despite never seeing such representations as input during
training. Our findings suggest that LLMs maintain a latent vocabulary beyond
the tokenizer's scope. These insights provide a practical, finetuning-free
application for expanding the vocabulary of pre-trained models. By enabling the
addition of new vocabulary words, we reduce input length and inference
iterations, which reduces both space and model latency, with little to no loss
in model accuracy.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to the International Conference on Learning Representations
  (ICLR) 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Naturally Occurring Feedback is Common, Extractable and Useful 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.10944v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.10944v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shachar Don-Yehiya, Leshem Choshen, Omri Abend
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Human feedback data is a critical component in developing language models.
However, collecting this feedback is costly and ultimately not scalable.
Inspired by the way human interlocutors provide spontaneous unsolicited
feedback to each other, we propose to extract feedback that users naturally
include when interacting with chat models. We manually annotated conversations
to confirm the presence of naturally occurring feedback in a standard corpus,
finding that as much as 30% of the chats include explicit feedback. Comparing
to older datasets, we find that naturally occurring feedback is more prevalent
in recent conversation datasets, suggesting that more than ever, naturally
occurring feedback can serve as a valuable resource for feedback data. We
propose a method for automatically extracting this feedback, and apply it to
over 1M conversations to obtain hundreds of thousands of feedback samples. The
extracted feedback shows promise: training with it improves over baseline
models and enhances model alignment to human preferences.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Evaluating Intelligence via Trial and Error 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.18858v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.18858v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jingtao Zhan, Jiahao Zhao, Jiayu Li, Yiqun Liu, Bo Zhang, Qingyao Ai, Jiaxin Mao, Hongning Wang, Min Zhang, Shaoping Ma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Intelligence is a crucial trait for species to find solutions within a
limited number of trial-and-error attempts. Building on this idea, we introduce
Survival Game as a framework to evaluate intelligence based on the number of
failed attempts in a trial-and-error process. Fewer failures indicate higher
intelligence. When the expectation and variance of failure counts are both
finite, it signals the ability to consistently find solutions to new
challenges, which we define as the Autonomous Level of intelligence. Using
Survival Game, we comprehensively evaluate existing AI systems. Our results
show that while AI systems achieve the Autonomous Level in simple tasks, they
are still far from it in more complex tasks, such as vision, search,
recommendation, and language. While scaling current AI technologies might help,
this would come at an astronomical cost. Projections suggest that achieving the
Autonomous Level for general tasks would require $10^{26}$ parameters. To put
this into perspective, loading such a massive model requires so many H100 GPUs
that their total value is $10^{7}$ times that of Apple Inc.'s market value.
Even with Moore's Law, supporting such a parameter scale would take $70$ years.
This staggering cost highlights the complexity of human tasks and the
inadequacies of current AI technologies. To further investigate this
phenomenon, we conduct a theoretical analysis of Survival Game and its
experimental results. Our findings suggest that human tasks possess a
criticality property. As a result, Autonomous Level requires a deep
understanding of the task's underlying mechanisms. Current AI systems, however,
do not fully grasp these mechanisms and instead rely on superficial mimicry,
making it difficult for them to reach an autonomous level. We believe Survival
Game can not only guide the future development of AI but also offer profound
insights into human intelligence.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Optimize Incompatible Parameters through Compatibility-aware Knowledge
  Integration <span class="chip">AAAI'25</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07596v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07596v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zheqi Lv, Keming Ye, Zishu Wei, Qi Tian, Shengyu Zhang, Wenqiao Zhang, Wenjie Wang, Kun Kuang, Tat-Seng Chua, Fei Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep neural networks have become foundational to advancements in multiple
domains, including recommendation systems, natural language processing, and so
on. Despite their successes, these models often contain incompatible parameters
that can be underutilized or detrimental to model performance, particularly
when faced with specific, varying data distributions. Existing research excels
in removing such parameters or merging the outputs of multiple different
pretrained models. However, the former focuses on efficiency rather than
performance, while the latter requires several times more computing and storage
resources to support inference. In this paper, we set the goal to explicitly
improve these incompatible parameters by leveraging the complementary strengths
of different models, thereby directly enhancing the models without any
additional parameters. Specifically, we propose Compatibility-aware Knowledge
Integration (CKI), which consists of Parameter Compatibility Assessment and
Parameter Splicing, which are used to evaluate the knowledge content of
multiple models and integrate the knowledge into one model, respectively. The
integrated model can be used directly for inference or for further fine-tuning.
We conduct extensive experiments on various datasets for recommendation and
language tasks, and the results show that Compatibility-aware Knowledge
Integration can effectively optimize incompatible parameters under multiple
tasks and settings to break through the training limit of the original model
without increasing the inference cost.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published on AAAI'25(Oral): The Annual AAAI Conference on Artificial
  Intelligence</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Towards Better Chain-of-Thought: A Reflection on Effectiveness and
  Faithfulness 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.18915v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.18915v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiachun Li, Pengfei Cao, Yubo Chen, Jiexin Xu, Huaijun Li, Xiaojian Jiang, Kang Liu, Jun Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Chain-of-thought (CoT) prompting demonstrates varying performance under
different reasoning tasks. Previous work attempts to evaluate it but falls
short in providing an in-depth analysis of patterns that influence the CoT. In
this paper, we study the CoT performance from the perspective of effectiveness
and faithfulness. For the former, we identify key factors that influence CoT
effectiveness on performance improvement, including problem difficulty,
information gain, and information flow. For the latter, we interpret the
unfaithful CoT issue by conducting a joint analysis of the information
interaction among the question, CoT, and answer. The result demonstrates that,
when the LLM predicts answers, it can recall correct information missing in the
CoT from the question, leading to the problem. Finally, we propose a novel
algorithm to mitigate this issue, in which we recall extra information from the
question to enhance the CoT generation and evaluate CoTs based on their
information gain. Extensive experiments demonstrate that our approach enhances
both the faithfulness and effectiveness of CoT.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages, under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The ShareLM Collection and Plugin: Contributing Human-Model Chats for
  the Benefit of the Community 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.08291v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.08291v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shachar Don-Yehiya, Leshem Choshen, Omri Abend
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Human-model conversations provide a window into users' real-world scenarios,
behavior, and needs, and thus are a valuable resource for model development and
research. While for-profit companies collect user data through the APIs of
their models, using it internally to improve their own models, the open source
and research community lags behind.
  We introduce the ShareLM collection, a unified set of human conversations
with large language models, and its accompanying plugin, a Web extension for
voluntarily contributing user-model conversations. Where few platforms share
their chats, the ShareLM plugin adds this functionality, thus, allowing users
to share conversations from most platforms. The plugin allows the user to rate
their conversations, both at the conversation and the response levels, and
delete conversations they prefer to keep private before they ever leave the
user's local storage. We release the plugin conversations as part of the
ShareLM collection, and call for more community effort in the field of open
human-model data.
  The code, plugin, and data are available.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MOOSE-Chem: Large Language Models for Rediscovering Unseen Chemistry
  Scientific Hypotheses <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.07076v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.07076v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zonglin Yang, Wanhao Liu, Ben Gao, Tong Xie, Yuqiang Li, Wanli Ouyang, Soujanya Poria, Erik Cambria, Dongzhan Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Scientific discovery contributes largely to human society's prosperity, and
recent progress shows that LLMs could potentially catalyze this process.
However, it is still unclear whether LLMs can discover novel and valid
hypotheses in chemistry. In this work, we investigate this central research
question: Can LLMs automatically discover novel and valid chemistry research
hypotheses given only a chemistry research background (consisting of a research
question and/or a background survey), without limitation on the domain of the
research question? After extensive discussions with chemistry experts, we
propose an assumption that a majority of chemistry hypotheses can be resulted
from a research background and several inspirations. With this key insight, we
break the central question into three smaller fundamental questions. In brief,
they are: (1) given a background question, whether LLMs can retrieve good
inspirations; (2) with background and inspirations, whether LLMs can lead to
hypothesis; and (3) whether LLMs can identify good hypotheses to rank them
higher. To investigate these questions, we construct a benchmark consisting of
51 chemistry papers published in Nature, Science, or a similar level in 2024
(all papers are only available online since 2024). Every paper is divided by
chemistry PhD students into three components: background, inspirations, and
hypothesis. The goal is to rediscover the hypothesis, given only the background
and a large randomly selected chemistry literature corpus consisting the ground
truth inspiration papers, with LLMs trained with data up to 2023. We also
develop an LLM-based multi-agent framework that leverages the assumption,
consisting of three stages reflecting the three smaller questions. The proposed
method can rediscover many hypotheses with very high similarity with the ground
truth ones, covering the main innovations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICLR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ NavRAG: Generating User Demand Instructions for Embodied Navigation
  through Retrieval-Augmented LLM 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11142v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11142v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zihan Wang, Yaohui Zhu, Gim Hee Lee, Yachun Fan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision-and-Language Navigation (VLN) is an essential skill for embodied
agents, allowing them to navigate in 3D environments following natural language
instructions. High-performance navigation models require a large amount of
training data, the high cost of manually annotating data has seriously hindered
this field. Therefore, some previous methods translate trajectory videos into
step-by-step instructions for expanding data, but such instructions do not
match well with users' communication styles that briefly describe destinations
or state specific needs. Moreover, local navigation trajectories overlook
global context and high-level task planning. To address these issues, we
propose NavRAG, a retrieval-augmented generation (RAG) framework that generates
user demand instructions for VLN. NavRAG leverages LLM to build a hierarchical
scene description tree for 3D scene understanding from global layout to local
details, then simulates various user roles with specific demands to retrieve
from the scene tree, generating diverse instructions with LLM. We annotate over
2 million navigation instructions across 861 scenes and evaluate the data
quality and navigation performance of trained models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Speculative Decoding and Beyond: An In-Depth <span class="highlight-title">Survey</span> of Techniques 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.19732v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.19732v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yunhai Hu, Zining Liu, Zhenyuan Dong, Tianfan Peng, Bradley McDanel, Sai Qian Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sequential dependencies present a fundamental bottleneck in deploying
large-scale autoregressive models, particularly for real-time applications.
While traditional optimization approaches like pruning and quantization often
compromise model quality, recent advances in generation-refinement frameworks
demonstrate that this trade-off can be significantly mitigated.
  This survey presents a comprehensive taxonomy of generation-refinement
frameworks, analyzing methods across autoregressive sequence tasks. We
categorize methods based on their generation strategies (from simple n-gram
prediction to sophisticated draft models) and refinement mechanisms (including
single-pass verification and iterative approaches). Through systematic analysis
of both algorithmic innovations and system-level implementations, we examine
deployment strategies across computing environments and explore applications
spanning text, images, and speech generation. This systematic examination of
both theoretical frameworks and practical implementations provides a foundation
for future research in efficient autoregressive decoding.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Variational Best-of-N Alignment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.06057v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.06057v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Afra Amini, Tim Vieira, Elliott Ash, Ryan Cotterell
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Best-of-N (BoN) is a popular and effective algorithm for aligning language
models to human preferences. The algorithm works as follows: at inference time,
N samples are drawn from the language model, and the sample with the highest
reward, as judged by a reward model, is returned as the output. Despite its
effectiveness, BoN is computationally expensive; it reduces sampling throughput
by a factor of N. To make BoN more efficient at inference time, one strategy is
to fine-tune the language model to mimic what BoN does during inference. To
achieve this, we derive the distribution induced by the BoN algorithm. We then
propose to fine-tune the language model to minimize backward KL divergence to
the BoN distribution. Our approach is analogous to mean-field variational
inference and, thus, we term it variational BoN (vBoN). To the extent this
fine-tuning is successful and we end up with a good approximation, we have
reduced the inference cost by a factor of N. Our experiments on controlled
generation and summarization tasks show that BoN is the most effective
alignment method, and our variational approximation to BoN achieves the closest
performance to BoN and surpasses models fine-tuned using the standard
KL-constrained RL objective. In the controlled generation task, vBoN appears
more frequently on the Pareto frontier of reward and KL divergence compared to
other alignment methods. In the summarization task, vBoN achieves high reward
values across various sampling temperatures.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Exploring Iterative Controllable Summarization with Large Language
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.12460v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.12460v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sangwon Ryu, Heejin Do, Daehee Kim, Hwanjo Yu, Dongwoo Kim, Yunsu Kim, Gary Geunbae Lee, Jungseul Ok
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have demonstrated remarkable performance in
abstractive summarization tasks. However, their ability to precisely control
summary attributes (e.g., length or topic) remains underexplored, limiting
their adaptability to specific user preferences. In this paper, we
systematically explore the controllability of LLMs. To this end, we revisit
summary attribute measurements and introduce iterative evaluation metrics,
failure rate and average iteration count to precisely evaluate controllability
of LLMs, rather than merely assessing errors. Our findings show that LLMs
struggle more with numerical attributes than with linguistic attributes. To
address this challenge, we propose a guide-to-explain framework (GTE) for
controllable summarization. Our GTE framework enables the model to identify
misaligned attributes in the initial draft and guides it in self-explaining
errors in the previous output. By allowing the model to reflect on its
misalignment, GTE generates well-adjusted summaries that satisfy the desired
attributes with robust effectiveness, requiring surprisingly fewer iterations
than other iterative approaches.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ "Nuclear Deployed!": Analyzing Catastrophic Risks in Decision-making of
  Autonomous LLM Agents 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11355v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11355v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rongwu Xu, Xiaojian Li, Shuo Chen, Wei Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) are evolving into autonomous decision-makers,
raising concerns about catastrophic risks in high-stakes scenarios,
particularly in Chemical, Biological, Radiological and Nuclear (CBRN) domains.
Based on the insight that such risks can originate from trade-offs between the
agent's Helpful, Harmlessness and Honest (HHH) goals, we build a novel
three-stage evaluation framework, which is carefully constructed to effectively
and naturally expose such risks. We conduct 14,400 agentic simulations across
12 advanced LLMs, with extensive experiments and analysis. Results reveal that
LLM agents can autonomously engage in catastrophic behaviors and deception,
without being deliberately induced. Furthermore, stronger reasoning abilities
often increase, rather than mitigate, these risks. We also show that these
agents can violate instructions and superior commands. On the whole, we
empirically prove the existence of catastrophic risks in autonomous LLM agents.
We will release our code upon request.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Please visit https://llm-catastrophic-risks.github.io for a quick
  tour of our project</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LaERC-S: Improving LLM-based Emotion Recognition in Conversation with
  Speaker Characteristics <span class="chip">COLING 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.07260v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.07260v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yumeng Fu, Junjie Wu, Zhongjie Wang, Meishan Zhang, Lili Shan, Yulin Wu, Bingquan Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Emotion recognition in conversation (ERC), the task of discerning human
emotions for each utterance within a conversation, has garnered significant
attention in human-computer interaction systems. Previous ERC studies focus on
speaker-specific information that predominantly stems from relationships among
utterances, which lacks sufficient information around conversations. Recent
research in ERC has sought to exploit pre-trained large language models (LLMs)
with speaker modelling to comprehend emotional states. Although these methods
have achieved encouraging results, the extracted speaker-specific information
struggles to indicate emotional dynamics. In this paper, motivated by the fact
that speaker characteristics play a crucial role and LLMs have rich world
knowledge, we present LaERC-S, a novel framework that stimulates LLMs to
explore speaker characteristics involving the mental state and behavior of
interlocutors, for accurate emotion predictions. To endow LLMs with this
knowledge information, we adopt the two-stage learning to make the models
reason speaker characteristics and track the emotion of the speaker in complex
conversation scenarios. Extensive experiments on three benchmark datasets
demonstrate the superiority of LaERC-S, reaching the new state-of-the-art.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>COLING 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Enhancing Large Language Models with Pseudo- and Multisource- Knowledge
  Graphs for Open-ended Question Answering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.09911v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.09911v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiaxiang Liu, Tong Zhou, Yubo Chen, Kang Liu, Jun Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Mitigating the hallucinations of Large Language Models is a crucial task.
Although some existing methods employ self-enhancement techniques, they fall
short of effectively addressing unknown factual hallucinations. Meanwhile,
Knowledge Graph (KG) enhancement approaches fail to address the generalization
across different KG sources and the enhancement of open-ended answer questions
simultaneously. To tackle these limitations, we propose a framework that
combines Pseudo-Graph Generation and Atomic Knowledge Verification (PG\&AKV).
Enhancement of open-ended question-answering begins with leveraging the
Pseudo-Graph Generation to provide the related knowledge framework.
Subsequently, Atomic Knowledge Verification utilizes atomic-level knowledge
querying and verification to achieve generalizability under different KG
sources. Compared to the baseline, this approach yields a minimum improvement
of 11.5 in the ROUGE-L score for open-ended questions. For precise-answered
questions, we observe a minimum accuracy improvement of 7.5%. Moreover, PG\&AKV
also exhibits generalizability across different KG sources. Utilizing KG
different from the question sources, PG\&AKV can even achieve at least a 3.5 %
performance improvement. In summary, our results pave the way for enhancing
LLMs by incorporating Pseudo- and Multisource-KGs, particularly in the filed of
open-ended questions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ECLeKTic: a Novel Challenge Set for Evaluation of Cross-Lingual
  Knowledge Transfer 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.21228v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.21228v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Omer Goldman, Uri Shaham, Dan Malkin, Sivan Eiger, Avinatan Hassidim, Yossi Matias, Joshua Maynez, Adi Mayrav Gilady, Jason Riesa, Shruti Rijhwani, Laura Rimell, Idan Szpektor, Reut Tsarfaty, Matan Eyal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To achieve equitable performance across languages, multilingual large
language models (LLMs) must be able to abstract knowledge beyond the language
in which it was acquired. However, the current literature lacks reliable ways
to measure LLMs' capability of cross-lingual knowledge transfer. To that end,
we present ECLeKTic, a multilingual closed-book QA (CBQA) dataset that
Evaluates Cross-Lingual Knowledge Transfer in a simple, black-box manner. We
detected information with uneven coverage across languages by controlling for
presence and absence of Wikipedia articles in 12 languages. We generated
knowledge-seeking questions in a source language, for which the answer appears
in a relevant Wikipedia article and translated them to all other 11 languages,
for which the respective Wikipedias lack equivalent articles. Assuming that
Wikipedia reflects the prominent knowledge in the LLM's training data, to solve
ECLeKTic's CBQA task the model is required to transfer knowledge between
languages. Experimenting with 8 LLMs, we show that SOTA models struggle to
effectively share knowledge across, languages even if they can predict the
answer well for queries in the same language the knowledge was acquired in.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SURGE: On the Potential of Large Language Models as General-Purpose
  Surrogate Code Executors 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11167v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11167v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bohan Lyu, Siqiao Huang, Zichen Liang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural surrogate models have emerged as powerful and efficient tools in data
mining. Meanwhile, large language models (LLMs) have demonstrated remarkable
capabilities in code-related tasks. We investigate a novel application: using
LLMs as surrogate models for code execution prediction. Given LLMs' unique
ability to understand and process diverse programs, they present a promising
direction for building general-purpose surrogate models. To systematically
investigate this capability, we introduce SURGE, a comprehensive benchmark with
$1160$ problems covering $8$ key aspects: multi-language programming tasks,
competition-level programming problems, repository-level code analysis,
high-cost scientific computing, time-complexity-intensive algorithms, buggy
code analysis, programs dependent on specific compilers or execution
environments, and formal mathematical proof verification. Through extensive
empirical analysis of $21$ open-source and proprietary LLMs, we examine scaling
laws, data efficiency, and predictive accuracy. Our findings reveal important
insights about the feasibility of LLMs as efficient surrogates for
computational processes, with implications for automated software testing,
program analysis, and computational resource optimization in data mining
applications. Code and dataset are released at
https://github.com/Imbernoulli/SURGE.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Robust Preference Optimization through Reward Model Distillation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.19316v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.19316v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Adam Fisch, Jacob Eisenstein, Vicky Zayats, Alekh Agarwal, Ahmad Beirami, Chirag Nagpal, Pete Shaw, Jonathan Berant
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Language model (LM) post-training (or alignment) involves maximizing a reward
function that is derived from preference annotations. Direct Preference
Optimization (DPO) is a popular offline alignment method that trains a policy
directly on preference data without the need to train a reward model or apply
reinforcement learning. However, the empirical evidence suggests that DPO
typically assigns implicit rewards that overfit, and trend towards infinite
magnitude. This frequently leads to degenerate policies, sometimes causing even
the probabilities of the preferred generations to go to zero. In this work, we
analyze this phenomenon and use distillation to get a better proxy for the true
preference distribution over generation pairs: we train the LM such that its
induced implicit reward, i.e., the scaled log-likelihood ratio of the model to
the reference model, matches an explicit reward model trained on the preference
data. Moreover, to account for uncertainty in the reward model we are
distilling from, we optimize against a family of reward models that, as a
whole, is likely to include at least one reasonable proxy for the preference
distribution. Our results show that distilling from such a family of reward
models leads to improved robustness to distribution shift in preference
annotations, while preserving the simple supervised nature of DPO.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Dynamics of Instruction Fine-Tuning for Chinese Large Language Models <span class="chip">COLING 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.19651v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.19651v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chiyu Song, Zhanchao Zhou, Jianhao Yan, Yuejiao Fei, Zhenzhong Lan, Yue Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Instruction tuning is a burgeoning method to elicit the general intelligence
of Large Language Models (LLMs). While numerous studies have examined the
impact of factors such as data volume and model size on English models, the
scaling properties of instruction tuning in other languages remain largely
unexplored. In this work, we systematically investigate the effects of data
quantity, model size, and data construction methods on instruction tuning for
Chinese LLMs. We utilize a newly curated dataset, DoIT, which includes over
40,000 high-quality instruction instances covering ten underlying abilities,
such as creative writing, code generation, and logical reasoning. Our
experiments, conducted on models ranging from 7b to 33b parameters, yield three
key findings: (i) While these factors directly affect overall model
performance, some abilities are more responsive to scaling, whereas others
demonstrate significant resistance. (ii) The scaling sensitivity of different
abilities to these factors can be explained by two features: Complexity and
Transference. (iii) By tailoring training strategies to their varying
sensitivities, specific abilities can be efficiently learned, enhancing
performance on two public benchmarks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to COLING 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Selected Languages are All You Need for Cross-lingual Truthfulness
  Transfer <span class="chip">COLING2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.14434v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.14434v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weihao Liu, Ning Wu, Wenbiao Ding, Shining Liang, Ming Gong, Dongmei Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Truthfulness stands out as an essential challenge for Large Language Models
(LLMs). Although many works have developed various ways for truthfulness
enhancement, they seldom focus on truthfulness in multilingual scenarios.
Meanwhile, contemporary multilingual aligning technologies struggle to balance
numerous languages and often exhibit serious truthfulness gaps across different
languages, especially those that differ greatly from English. In our work, we
extend truthfulness evaluation to multilingual contexts and propose a practical
method for cross-lingual truthfulness transfer called Fact-aware Multilingual
Selective Synergy (FaMSS). FaMSS is able to select an optimal subset of all
tested languages by language bias and transfer contributions, and then employ
translation instruction tuning for cross-lingual truthfulness transfer.
Experimental results demonstrate that our approach can effectively reduce the
multilingual representation disparity and boost cross-lingual truthfulness
transfer of LLMs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages, COLING2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Enabling Auditory Large Language Models for Automatic Speech Quality
  Evaluation <span class="chip">ICASSP 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.16644v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.16644v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Siyin Wang, Wenyi Yu, Yudong Yang, Changli Tang, Yixuan Li, Jimin Zhuang, Xianzhao Chen, Xiaohai Tian, Jun Zhang, Guangzhi Sun, Lu Lu, Chao Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Speech quality assessment typically requires evaluating audio from multiple
aspects, such as mean opinion score (MOS) and speaker similarity (SIM) \etc.,
which can be challenging to cover using one small model designed for a single
task. In this paper, we propose leveraging recently introduced auditory large
language models (LLMs) for automatic speech quality assessment. By employing
task-specific prompts, auditory LLMs are finetuned to predict MOS, SIM and A/B
testing results, which are commonly used for evaluating text-to-speech systems.
Additionally, the finetuned auditory LLM is able to generate natural language
descriptions assessing aspects like noisiness, distortion, discontinuity, and
overall quality, providing more interpretable outputs. Extensive experiments
have been performed on the NISQA, BVCC, SOMOS and VoxSim speech quality
datasets, using open-source auditory LLMs such as SALMONN, Qwen-Audio, and
Qwen2-Audio. For the natural language descriptions task, a commercial model
Google Gemini 1.5 Pro is also evaluated. The results demonstrate that auditory
LLMs achieve competitive performance compared to state-of-the-art task-specific
small models in predicting MOS and SIM, while also delivering promising results
in A/B testing and natural language descriptions. Our data processing scripts
and finetuned model checkpoints can be found at
https://github.com/bytedance/SALMONN.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICASSP 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DailyDilemmas: Revealing Value Preferences of LLMs with Quandaries of
  Daily Life <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02683v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02683v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yu Ying Chiu, Liwei Jiang, Yejin Choi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As users increasingly seek guidance from LLMs for decision-making in daily
life, many of these decisions are not clear-cut and depend significantly on the
personal values and ethical standards of people. We present DailyDilemmas, a
dataset of 1,360 moral dilemmas encountered in everyday life. Each dilemma
presents two possible actions, along with affected parties and relevant human
values for each action. Based on these dilemmas, we gather a repository of
human values covering diverse everyday topics, such as interpersonal
relationships, workplace, and environmental issues. With DailyDilemmas, we
evaluate LLMs on these dilemmas to determine what action they will choose and
the values represented by these action choices. Then, we analyze values through
the lens of five theoretical frameworks inspired by sociology, psychology, and
philosophy, including the World Values Survey, Moral Foundations Theory,
Maslow's Hierarchy of Needs, Aristotle's Virtues, and Plutchik's Wheel of
Emotions. For instance, we find LLMs are most aligned with self-expression over
survival in World Values Survey and care over loyalty in Moral Foundations
Theory. Interestingly, we find substantial preference differences in models for
some core values. For example, for truthfulness, Mixtral-8x7B neglects it by
9.7% while GPT-4-turbo selects it by 9.4%. We also study the recent guidance
released by OpenAI (ModelSpec), and Anthropic (Constitutional AI) to understand
how their designated principles reflect their models' actual value
prioritization when facing nuanced moral reasoning in daily-life settings.
Finally, we find that end users cannot effectively steer such prioritization
using system prompts.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted into ICLR 2025 (spotlight)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Test-Time Compute: from System-1 Thinking to System-2 Thinking 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.02497v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.02497v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yixin Ji, Juntao Li, Hai Ye, Kaixin Wu, Kai Yao, Jia Xu, Linjian Mo, Min Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The remarkable performance of the o1 model in complex reasoning demonstrates
that test-time compute scaling can further unlock the model's potential,
enabling powerful System-2 thinking. However, there is still a lack of
comprehensive surveys for test-time compute scaling. We trace the concept of
test-time compute back to System-1 models. In System-1 models, test-time
compute addresses distribution shifts and improves robustness and
generalization through parameter updating, input modification, representation
editing, and output calibration. In System-2 models, it enhances the model's
reasoning ability to solve complex problems through repeated sampling,
self-correction, and tree search. We organize this survey according to the
trend of System-1 to System-2 thinking, highlighting the key role of test-time
compute in the transition from System-1 models to weak System-2 models, and
then to strong System-2 models. We also point out a few possible future
directions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>work in progress</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning to Align Multi-Faceted Evaluation: A Unified and Robust
  Framework 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.18874v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.18874v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kaishuai Xu, Tiezheng Yu, Wenjun Hou, Yi Cheng, Liangyou Li, Xin Jiang, Lifeng Shang, Qun Liu, Wenjie Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) are being used more and more extensively for
automated evaluation in various scenarios. Previous studies have attempted to
fine-tune open-source LLMs to replicate the evaluation explanations and
judgments of powerful proprietary models, such as GPT-4. However, these methods
are largely limited to text-based analyses under predefined general criteria,
resulting in reduced adaptability for unseen instructions and demonstrating
instability in evaluating adherence to quantitative and structural constraints.
To address these limitations, we propose a novel evaluation framework, ARJudge,
that adaptively formulates evaluation criteria and synthesizes both text-based
and code-driven analyses to evaluate LLM responses. ARJudge consists of two
components: a fine-tuned Analyzer that generates multi-faceted evaluation
analyses and a tuning-free Refiner that combines and refines all analyses to
make the final judgment. We construct a Composite Analysis Corpus that
integrates tasks for evaluation criteria generation alongside text-based and
code-driven analysis generation to train the Analyzer. Our results demonstrate
that ARJudge outperforms existing fine-tuned evaluators in effectiveness and
robustness. Furthermore, it demonstrates the importance of multi-faceted
evaluation and code-driven analyses in enhancing evaluation capabilities.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Subtle Errors Matter: Preference Learning via Error-injected
  Self-editing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.06638v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.06638v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kaishuai Xu, Tiezheng Yu, Wenjun Hou, Yi Cheng, Chak Tou Leong, Liangyou Li, Xin Jiang, Lifeng Shang, Qun Liu, Wenjie Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have exhibited strong mathematical reasoning
prowess, tackling tasks ranging from basic arithmetic to advanced
competition-level problems. However, frequently occurring subtle yet critical
errors, such as miscalculations or incorrect substitutions, limit the LLMs'
full potential. Existing studies to improve mathematical ability typically
involve applying preference learning to step-wise solution pairs. Although
these methods leverage samples of varying granularity to mitigate reasoning
errors, they overlook critical subtle errors. In this work, we propose a novel
preference learning framework called eRror-Injected Self-Editing (RISE), which
injects predefined subtle errors into pivotal tokens in reasoning or
computation steps to construct hard pairs for error mitigation. In detail, RISE
uses the LLM itself to edit a small number of tokens in the solution, injecting
designed subtle errors. Then, pairs composed of self-edited solutions and their
corresponding correct ones, along with pairs of correct and incorrect solutions
obtained through sampling, are used together for subtle error-aware DPO
training. Compared with other preference learning methods, RISE further refines
the training objective without requiring fine-grained sampling or preference
annotation. Extensive experiments validate the effectiveness of RISE, with
preference learning on Qwen2-7B-Instruct yielding notable improvements of 3.0%
on GSM8K and 7.9% on MATH with only 4.5K training samples. Moreover, the effect
of error mitigation extends from mathematical reasoning to logical reasoning
and code generation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning Efficient Recursive Numeral Systems via Reinforcement Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.07170v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.07170v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andrea Silvi, Jonathan Thomas, Emil Carlsson, Devdatt Dubhashi, Moa Johansson
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  It has previously been shown that by using reinforcement learning (RL),
agents can derive simple approximate and exact-restricted numeral systems that
are similar to human ones (Carlsson, 2021). However, it is a major challenge to
show how more complex recursive numeral systems, similar to for example
English, could arise via a simple learning mechanism such as RL. Here, we
introduce an approach towards deriving a mechanistic explanation of the
emergence of efficient recursive number systems. We consider pairs of agents
learning how to communicate about numerical quantities through a meta-grammar
that can be gradually modified throughout the interactions. %We find that the
seminal meta-grammar of Hurford (Hurford, 1975) is not suitable for this
application as its optimization results in systems that deviate from standard
conventions observed within human numeral systems. We propose a simple
modification which addresses this issue. Utilising a slightly modified version
of the meta-grammar of Hurford, we demonstrate that our RL agents, shaped by
the pressures for efficient communication, can effectively modify their lexicon
towards Pareto-optimal configurations which are comparable to those observed
within human numeral systems in terms of their efficiency.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SpikeLLM: Scaling up Spiking Neural Network to Large Language Models via
  Saliency-based Spiking 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.04752v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.04752v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xingrun Xing, Boyan Gao, Zheng Zhang, David A. Clifton, Shitao Xiao, Li Du, Guoqi Li, Jiajun Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in large language models (LLMs) with billions of
parameters have improved performance in various applications, but their
inference processes demand significant energy and computational resources. In
contrast, the human brain, with approximately 86 billion neurons, is much more
energy-efficient than LLMs with similar parameters. Inspired by this, we
redesign 7$\sim$70 billion parameter LLMs using bio-plausible spiking
mechanisms, emulating the efficient behavior of the human brain. We propose the
first spiking large language model, SpikeLLM. Coupled with the proposed model,
two essential approaches are proposed to improve spike training efficiency:
Generalized Integrate-and-Fire (GIF) neurons to compress spike length from $T$
to $\frac{T}{L} \log_2 L$ bits, and an Optimal Brain Spiking framework to
divide outlier channels and allocate different $T$ for GIF neurons, which
further compresses spike length to approximate $log_2T$ bits. The necessity of
spike-driven LLM is proved by comparison with quantized LLMs with similar
operations. In the OmniQuant pipeline, SpikeLLM reduces 11.01% WikiText2
perplexity and improves 2.55% accuracy of common scene reasoning on a LLAMA-7B
W4A4 model. In the GPTQ pipeline, SpikeLLM achieves direct additive in linear
layers, significantly exceeding PB-LLMs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SynGhost: Invisible and Universal Task-agnostic Backdoor Attack via
  Syntactic Transfer <span class="chip">NAACL 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.18945v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.18945v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pengzhou Cheng, Wei Du, Zongru Wu, Fengwei Zhang, Libo Chen, Zhuosheng Zhang, Gongshen Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Although pre-training achieves remarkable performance, it suffers from
task-agnostic backdoor attacks due to vulnerabilities in data and training
mechanisms. These attacks can transfer backdoors to various downstream tasks.
In this paper, we introduce $\mathtt{maxEntropy}$, an entropy-based poisoning
filter that mitigates such risks. To overcome the limitations of manual target
setting and explicit triggers, we propose $\mathtt{SynGhost}$, an invisible and
universal task-agnostic backdoor attack via syntactic transfer, further
exposing vulnerabilities in pre-trained language models (PLMs). Specifically,
$\mathtt{SynGhost}$ injects multiple syntactic backdoors into the pre-training
space through corpus poisoning, while preserving the PLM's pre-training
capabilities. Second, $\mathtt{SynGhost}$ adaptively selects optimal targets
based on contrastive learning, creating a uniform distribution in the
pre-training space. To identify syntactic differences, we also introduce an
awareness module to minimize interference between backdoors. Experiments show
that $\mathtt{SynGhost}$ poses significant threats and can transfer to various
downstream tasks. Furthermore, $\mathtt{SynGhost}$ resists defenses based on
perplexity, fine-pruning, and $\mathtt{maxEntropy}$. The code is available at
https://github.com/Zhou-CyberSecurity-AI/SynGhost.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, 16 figures, 12 tables, accepted at NAACL 2025 Findings</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The Rise and Down of Babel Tower: Investigating the Evolution Process of
  Multilingual Code Large Language Model <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.07298v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.07298v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiawei Chen, Wentao Chen, Jing Su, Jingjing Xu, Hongyu Lin, Mengjie Ren, Yaojie Lu, Xianpei Han, Le Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have shown significant multilingual
capabilities. However, the mechanisms underlying the development of these
capabilities during pre-training are not well understood. In this paper, we use
code LLMs as an experimental platform to explore the evolution of multilingual
capabilities in LLMs during the pre-training process. Based on our
observations, we propose the Babel Tower Hypothesis, which describes the entire
process of LLMs acquiring new language capabilities. During the learning
process, multiple languages initially share a single knowledge system dominated
by the primary language and gradually develop language-specific knowledge
systems. We then validate the above hypothesis by tracking the internal states
of the LLMs through identifying working languages and language transferring
neurons. Experimental results show that the internal state changes of the LLM
are consistent with our Babel Tower Hypothesis. Building on these insights, we
propose a novel method to construct an optimized pre-training corpus for
multilingual code LLMs, which significantly outperforms LLMs trained on the
original corpus. The proposed Babel Tower Hypothesis provides new insights into
designing pre-training data distributions to achieve optimal multilingual
capabilities in LLMs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ICLR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Order Matters: Investigate the Position Bias in Multi-constraint
  Instruction Following 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.17204v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.17204v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jie Zeng, Qianyu He, Qingyu Ren, Jiaqing Liang, Yanghua Xiao, Weikang Zhou, Zeye Sun, Fei Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Real-world instructions with multiple constraints pose a significant
challenge to existing large language models (LLMs). An observation is that the
LLMs exhibit dramatic performance fluctuation when disturbing the order of the
incorporated constraints. Yet, none of the existing works has systematically
investigated this position bias problem in the field of multi-constraint
instruction following. To bridge this gap, we design a probing task where we
quantitatively measure the difficulty distribution of the constraints by a
novel Difficulty Distribution Index (CDDI). Through the experimental results,
we find that LLMs are more performant when presented with the constraints in a
``hard-to-easy'' order. This preference can be generalized to LLMs with
different architecture or different sizes of parameters. Additionally, we
conduct an explanation study, providing an intuitive insight into the
correlation between the LLM's attention and constraint orders. Our code and
dataset are publicly available at https://github.com/meowpass/PBIF.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Representation Engineering: A Top-Down Approach to AI Transparency 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.01405v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.01405v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andy Zou, Long Phan, Sarah Chen, James Campbell, Phillip Guo, Richard Ren, Alexander Pan, Xuwang Yin, Mantas Mazeika, Ann-Kathrin Dombrowski, Shashwat Goel, Nathaniel Li, Michael J. Byun, Zifan Wang, Alex Mallen, Steven Basart, Sanmi Koyejo, Dawn Song, Matt Fredrikson, J. Zico Kolter, Dan Hendrycks
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we identify and characterize the emerging area of
representation engineering (RepE), an approach to enhancing the transparency of
AI systems that draws on insights from cognitive neuroscience. RepE places
population-level representations, rather than neurons or circuits, at the
center of analysis, equipping us with novel methods for monitoring and
manipulating high-level cognitive phenomena in deep neural networks (DNNs). We
provide baselines and an initial analysis of RepE techniques, showing that they
offer simple yet effective solutions for improving our understanding and
control of large language models. We showcase how these methods can provide
traction on a wide range of safety-relevant problems, including honesty,
harmlessness, power-seeking, and more, demonstrating the promise of top-down
transparency research. We hope that this work catalyzes further exploration of
RepE and fosters advancements in the transparency and safety of AI systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code is available at
  https://github.com/andyzoujm/representation-engineering</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ TokenSelect: Efficient Long-Context Inference and Length Extrapolation
  for LLMs via Dynamic Token-Level KV Cache Selection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02886v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02886v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wei Wu, Zhuoshi Pan, Chao Wang, Liyi Chen, Yunchu Bai, Tianfu Wang, Kun Fu, Zheng Wang, Hui Xiong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid advancement of Large Language Models (LLMs) has driven growing
demand for processing extended context sequences in contemporary applications.
However, this progress faces two major challenges: performance degradation due
to sequence lengths out-of-distribution, and excessively long inference times
caused by the quadratic computational complexity of attention. These issues
hinder the application of LLMs in long-context scenarios. In this paper, we
propose Dynamic Token-Level KV Cache Selection (TokenSelect), a training-free
method for efficient and accurate long-context inference. TokenSelect builds
upon the observation of non-contiguous attention sparsity, using Query-Key dot
products to measure per-head KV Cache criticality at token-level. By per-head
soft voting mechanism, TokenSelect selectively involves a few critical KV cache
tokens in attention calculation without sacrificing accuracy. To further
accelerate TokenSelect, we design the Selection Cache based on observations of
consecutive Query similarity and implemented efficient dot product kernel,
significantly reducing the overhead. A comprehensive evaluation of TokenSelect
demonstrates up to 23.84x speedup in attention computation and up to 2.28x
acceleration in end-to-end latency, while providing superior performance
compared to state-of-the-art long-context inference methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Enhancing Conversational Agents with Theory of Mind: Aligning Beliefs,
  Desires, and Intentions for Human-Like Interaction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.14171v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.14171v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mehdi Jafari, Devin Yuncheng Hua, Hao Xue, Flora Salim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Natural language interaction with agentic Artificial Intelligence (AI),
driven by Large Language Models (LLMs), is expected to remain a dominant
paradigm in the near future. While humans instinctively align their
communication with mental states -- an ability known as Theory of Mind (ToM),
current LLM powered systems exhibit significant limitations in this regard.
This study examines the extent to which open source language models (LLaMA) can
capture and preserve ToM related information and how effectively it contributes
to consistent ToM reasoning in generated responses. We further investigate
whether explicit manipulation of ToM related components, such as beliefs,
desires, and intentions, can enhance response alignment. Experiments on two
LLaMA 3 variants demonstrate that incorporating ToM informed alignment improves
response quality, achieving win rates of 67 and 63 percent for the 3B and 8B
models, respectively. These findings highlight the potential of ToM driven
strategies to improve alignment in LLM based conversational agents.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Structural-Entropy-Based Sample Selection for Efficient and Effective
  Learning <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02268v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02268v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tianchi Xie, Jiangning Zhu, Guozu Ma, Minzhi Lin, Wei Chen, Weikai Yang, Shixia Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sample selection improves the efficiency and effectiveness of machine
learning models by providing informative and representative samples. Typically,
samples can be modeled as a sample graph, where nodes are samples and edges
represent their similarities. Most existing methods are based on local
information, such as the training difficulty of samples, thereby overlooking
global information, such as connectivity patterns. This oversight can result in
suboptimal selection because global information is crucial for ensuring that
the selected samples well represent the structural properties of the graph. To
address this issue, we employ structural entropy to quantify global information
and losslessly decompose it from the whole graph to individual nodes using the
Shapley value. Based on the decomposition, we present
$\textbf{S}$tructural-$\textbf{E}$ntropy-based sample $\textbf{S}$election
($\textbf{SES}$), a method that integrates both global and local information to
select informative and representative samples. SES begins by constructing a
$k$NN-graph among samples based on their similarities. It then measures sample
importance by combining structural entropy (global metric) with training
difficulty (local metric). Finally, SES applies importance-biased blue noise
sampling to select a set of diverse and representative samples. Comprehensive
experiments on three learning scenarios -- supervised learning, active
learning, and continual learning -- clearly demonstrate the effectiveness of
our method.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published as a conference paper at ICLR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Spontaneous Giving and Calculated Greed in Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.17720v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.17720v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuxuan Li, Hirokazu Shirado
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models demonstrate advanced problem-solving capabilities by
incorporating reasoning techniques such as chain of thought and reflection.
However, how these reasoning capabilities extend to social intelligence remains
unclear. In this study, we investigate this question using economic games that
model social dilemmas, where social intelligence plays a crucial role. First,
we examine the effects of chain-of-thought and reflection techniques in a
public goods game. We then extend our analysis to six economic games on
cooperation and punishment, comparing off-the-shelf non-reasoning and reasoning
models. We find that reasoning models significantly reduce cooperation and norm
enforcement, prioritizing individual rationality. Consequently, groups with
more reasoning models exhibit less cooperation and lower gains through repeated
interactions. These behaviors parallel human tendencies of "spontaneous giving
and calculated greed." Our results suggest the need for AI architectures that
incorporate social intelligence alongside reasoning capabilities to ensure that
AI supports, rather than disrupts, human cooperative intuition.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Generative Representational Instruction Tuning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.09906v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.09906v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Niklas Muennighoff, Hongjin Su, Liang Wang, Nan Yang, Furu Wei, Tao Yu, Amanpreet Singh, Douwe Kiela
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  All text-based language problems can be reduced to either generation or
embedding. Current models only perform well at one or the other. We introduce
generative representational instruction tuning (GRIT) whereby a large language
model is trained to handle both generative and embedding tasks by
distinguishing between them through instructions. Compared to other open
models, our resulting GritLM 7B sets a new state of the art on the Massive Text
Embedding Benchmark (MTEB) and outperforms all models up to its size on a range
of generative tasks. By scaling up further, GritLM 8x7B outperforms all open
generative language models that we tried while still being among the best
embedding models. Notably, we find that GRIT matches training on only
generative or embedding data, thus we can unify both at no performance loss.
Among other benefits, the unification via GRIT speeds up Retrieval-Augmented
Generation (RAG) by > 60% for long documents, by no longer requiring separate
retrieval and generation models. Models, code, etc. are freely available at
https://github.com/ContextualAI/gritlm.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>67 pages (16 main), 25 figures, 34 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Perturbation-Restrained Sequential Model Editing <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.16821v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.16821v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jun-Yu Ma, Hong Wang, Hao-Xiang Xu, Zhen-Hua Ling, Jia-Chen Gu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Model editing is an emerging field that focuses on updating the knowledge
embedded within large language models (LLMs) without extensive retraining.
However, current model editing methods significantly compromise the general
abilities of LLMs as the number of edits increases, and this trade-off poses a
substantial challenge to the continual learning of LLMs. In this paper, we
first theoretically analyze that the factor affecting the general abilities in
sequential model editing lies in the condition number of the edited matrix. The
condition number of a matrix represents its numerical sensitivity, and
therefore can be used to indicate the extent to which the original knowledge
associations stored in LLMs are perturbed after editing. Subsequently,
statistical findings demonstrate that the value of this factor becomes larger
as the number of edits increases, thereby exacerbating the deterioration of
general abilities. To this end, a framework termed Perturbation Restraint on
Upper bouNd for Editing (PRUNE) is proposed, which applies the condition number
restraints in sequential editing. These restraints can lower the upper bound on
perturbation to edited models, thus preserving the general abilities.
Systematically, we conduct experiments employing three editing methods on three
LLMs across four downstream tasks. The results show that PRUNE can preserve
general abilities while maintaining the editing performance effectively in
sequential model editing. The code are available at
https://github.com/mjy1111/PRUNE.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICLR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A-MEM: Agentic Memory for LLM Agents 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.12110v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.12110v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wujiang Xu, Zujie Liang, Kai Mei, Hang Gao, Juntao Tan, Yongfeng Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While large language model (LLM) agents can effectively use external tools
for complex real-world tasks, they require memory systems to leverage
historical experiences. Current memory systems enable basic storage and
retrieval but lack sophisticated memory organization, despite recent attempts
to incorporate graph databases. Moreover, these systems' fixed operations and
structures limit their adaptability across diverse tasks. To address this
limitation, this paper proposes a novel agentic memory system for LLM agents
that can dynamically organize memories in an agentic way. Following the basic
principles of the Zettelkasten method, we designed our memory system to create
interconnected knowledge networks through dynamic indexing and linking. When a
new memory is added, we generate a comprehensive note containing multiple
structured attributes, including contextual descriptions, keywords, and tags.
The system then analyzes historical memories to identify relevant connections,
establishing links where meaningful similarities exist. Additionally, this
process enables memory evolution - as new memories are integrated, they can
trigger updates to the contextual representations and attributes of existing
historical memories, allowing the memory network to continuously refine its
understanding. Our approach combines the structured organization principles of
Zettelkasten with the flexibility of agent-driven decision making, allowing for
more adaptive and context-aware memory management. Empirical experiments on six
foundation models show superior improvement against existing SOTA baselines.
The source code is available at https://github.com/WujiangXu/AgenticMemory.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ QUAD-LLM-MLTC: Large Language Models Ensemble Learning for Healthcare
  Text Multi-Label Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.14189v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.14189v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hajar Sakai, Sarah S. Lam
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The escalating volume of collected healthcare textual data presents a unique
challenge for automated Multi-Label Text Classification (MLTC), which is
primarily due to the scarcity of annotated texts for training and their nuanced
nature. Traditional machine learning models often fail to fully capture the
array of expressed topics. However, Large Language Models (LLMs) have
demonstrated remarkable effectiveness across numerous Natural Language
Processing (NLP) tasks in various domains, which show impressive computational
efficiency and suitability for unsupervised learning through prompt
engineering. Consequently, these LLMs promise an effective MLTC of medical
narratives. However, when dealing with various labels, different prompts can be
relevant depending on the topic. To address these challenges, the proposed
approach, QUAD-LLM-MLTC, leverages the strengths of four LLMs: GPT-4o, BERT,
PEGASUS, and BART. QUAD-LLM-MLTC operates in a sequential pipeline in which
BERT extracts key tokens, PEGASUS augments textual data, GPT-4o classifies, and
BART provides topics' assignment probabilities, which results in four
classifications, all in a 0-shot setting. The outputs are then combined using
ensemble learning and processed through a meta-classifier to produce the final
MLTC result. The approach is evaluated using three samples of annotated texts,
which contrast it with traditional and single-model methods. The results show
significant improvements across the majority of the topics in the
classification's F1 score and consistency (F1 and Micro-F1 scores of 78.17% and
80.16% with standard deviations of 0.025 and 0.011, respectively). This
research advances MLTC using LLMs and provides an efficient and scalable
solution to rapidly categorize healthcare-related text data without further
training.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Iterative Nash Policy Optimization: Aligning LLMs with General
  Preferences via No-Regret Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.00617v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.00617v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuheng Zhang, Dian Yu, Baolin Peng, Linfeng Song, Ye Tian, Mingyue Huo, Nan Jiang, Haitao Mi, Dong Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reinforcement Learning with Human Feedback (RLHF) has achieved great success
in aligning large language models (LLMs) with human preferences. Prevalent RLHF
approaches are reward-based, following the Bradley-Terry (BT) model assumption,
which may not fully capture the complexity of human preferences. In this paper,
we explore RLHF under a general preference framework and approach it from a
game-theoretic perspective. Specifically, we formulate the problem as a
two-player game and propose a novel online algorithm, iterative Nash policy
optimization (INPO). The key idea is to let the policy play against itself via
no-regret learning, thereby approximating the Nash policy. Unlike previous
methods, INPO bypasses the need for estimating the expected win rate for
individual responses, which typically incurs high computational or annotation
costs. Instead, we introduce a new loss objective that is directly minimized
over a preference dataset. We provide theoretical analysis for our approach and
demonstrate its effectiveness through experiments on various representative
benchmarks. With an LLaMA-3-8B-based SFT model, INPO achieves a 42.6%
length-controlled win rate on AlpacaEval 2.0 and a 37.8% win rate on
Arena-Hard, showing substantial improvement over the state-of-the-art online
RLHF algorithms.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A <span class="highlight-title">Survey</span> on Vision-Language-Action Models for Embodied AI 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.14093v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.14093v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yueen Ma, Zixing Song, Yuzheng Zhuang, Jianye Hao, Irwin King
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Embodied AI is widely recognized as a key element of artificial general
intelligence because it involves controlling embodied agents to perform tasks
in the physical world. Building on the success of large language models and
vision-language models, a new category of multimodal models -- referred to as
vision-language-action models (VLAs) -- has emerged to address
language-conditioned robotic tasks in embodied AI by leveraging their distinct
ability to generate actions. In recent years, a myriad of VLAs have been
developed, making it imperative to capture the rapidly evolving landscape
through a comprehensive survey. To this end, we present the first survey on
VLAs for embodied AI. This work provides a detailed taxonomy of VLAs, organized
into three major lines of research. The first line focuses on individual
components of VLAs. The second line is dedicated to developing control policies
adept at predicting low-level actions. The third line comprises high-level task
planners capable of decomposing long-horizon tasks into a sequence of subtasks,
thereby guiding VLAs to follow more general user instructions. Furthermore, we
provide an extensive summary of relevant resources, including datasets,
simulators, and benchmarks. Finally, we discuss the challenges faced by VLAs
and outline promising future directions in embodied AI.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages, a survey of vision-language-action models</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ On the Generalization and Adaptation Ability of Machine-Generated Text
  Detectors in Academic Writing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.17242v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.17242v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yule Liu, Zhiyuan Zhong, Yifan Liao, Zhen Sun, Jingyi Zheng, Jiaheng Wei, Qingyuan Gong, Fenghua Tong, Yang Chen, Yang Zhang, Xinlei He
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rising popularity of large language models (LLMs) has raised concerns
about machine-generated text (MGT), particularly in academic settings, where
issues like plagiarism and misinformation are prevalent. As a result,
developing a highly generalizable and adaptable MGT detection system has become
an urgent priority. Given that LLMs are most commonly misused in academic
writing, this work investigates the generalization and adaptation capabilities
of MGT detectors in three key aspects specific to academic writing: First, we
construct MGT-Acedemic, a large-scale dataset comprising over 336M tokens and
749K samples. MGT-Acedemic focuses on academic writing, featuring human-written
texts (HWTs) and MGTs across STEM, Humanities, and Social Sciences, paired with
an extensible code framework for efficient benchmarking. Second, we benchmark
the performance of various detectors for binary classification and attribution
tasks in both in-domain and cross-domain settings. This benchmark reveals the
often-overlooked challenges of attribution tasks. Third, we introduce a novel
attribution task where models have to adapt to new classes over time without
(or with very limited) access to prior training data in both few-shot and
many-shot scenarios. We implement eight different adapting techniques to
improve the performance and highlight the inherent complexity of the task. Our
findings provide insights into the generalization and adaptation ability of MGT
detectors across diverse scenarios and lay the foundation for building robust,
adaptive detection systems. The code framework is available at
https://github.com/Y-L-LIU/MGTBench-2.0.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MMed-RAG: Versatile Multimodal RAG System for Medical Vision Language
  Models <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13085v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13085v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Peng Xia, Kangyu Zhu, Haoran Li, Tianze Wang, Weijia Shi, Sheng Wang, Linjun Zhang, James Zou, Huaxiu Yao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Artificial Intelligence (AI) has demonstrated significant potential in
healthcare, particularly in disease diagnosis and treatment planning. Recent
progress in Medical Large Vision-Language Models (Med-LVLMs) has opened up new
possibilities for interactive diagnostic tools. However, these models often
suffer from factual hallucination, which can lead to incorrect diagnoses.
Fine-tuning and retrieval-augmented generation (RAG) have emerged as methods to
address these issues. However, the amount of high-quality data and distribution
shifts between training data and deployment data limit the application of
fine-tuning methods. Although RAG is lightweight and effective, existing
RAG-based approaches are not sufficiently general to different medical domains
and can potentially cause misalignment issues, both between modalities and
between the model and the ground truth. In this paper, we propose a versatile
multimodal RAG system, MMed-RAG, designed to enhance the factuality of
Med-LVLMs. Our approach introduces a domain-aware retrieval mechanism, an
adaptive retrieved contexts selection method, and a provable RAG-based
preference fine-tuning strategy. These innovations make the RAG process
sufficiently general and reliable, significantly improving alignment when
introducing retrieved contexts. Experimental results across five medical
datasets (involving radiology, ophthalmology, pathology) on medical VQA and
report generation demonstrate that MMed-RAG can achieve an average improvement
of 43.8% in the factual accuracy of Med-LVLMs. Our data and code are available
in https://github.com/richard-peng-xia/MMed-RAG.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ HORAE: A Domain-Agnostic Modeling Language for Automating Multimodal
  Service Regulation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.06600v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.06600v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yutao Sun, Mingshuai Chen, Kangjia Zhao, Jintao Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Artificial intelligence is rapidly encroaching on the field of service
regulation. This work-in-progress article presents the design principles behind
HORAE, a unified specification language to model multimodal regulation rules
across a diverse set of domains. We show how HORAE facilitates an intelligent
service regulation pipeline by further exploiting a fine-tuned large language
model named HORAE that automates the HORAE modeling process, thereby yielding
an end-to-end framework for fully automated intelligent service regulation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Controllable Context Sensitivity and the Knob Behind It <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07404v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07404v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Julian Minder, Kevin Du, Niklas Stoehr, Giovanni Monea, Chris Wendler, Robert West, Ryan Cotterell
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  When making predictions, a language model must trade off how much it relies
on its context vs. its prior knowledge. Choosing how sensitive the model is to
its context is a fundamental functionality, as it enables the model to excel at
tasks like retrieval-augmented generation and question-answering. In this
paper, we search for a knob which controls this sensitivity, determining
whether language models answer from the context or their prior knowledge. To
guide this search, we design a task for controllable context sensitivity. In
this task, we first feed the model a context (Paris is in England) and a
question (Where is Paris?); we then instruct the model to either use its prior
or contextual knowledge and evaluate whether it generates the correct answer
for both intents (either France or England). When fine-tuned on this task,
instruction-tuned versions of Llama-3.1, Mistral-v0.3, and Gemma-2 can solve it
with high accuracy (85-95%). Analyzing these high-performing models, we narrow
down which layers may be important to context sensitivity using a novel linear
time algorithm. Then, in each model, we identify a 1-D subspace in a single
layer that encodes whether the model follows context or prior knowledge.
Interestingly, while we identify this subspace in a fine-tuned model, we find
that the exact same subspace serves as an effective knob in not only that model
but also non-fine-tuned instruct and base models of that model family. Finally,
we show a strong correlation between a model's performance and how distinctly
it separates context-agreeing from context-ignoring answers in this subspace.
These results suggest a single subspace facilitates how the model chooses
between context and prior knowledge, hinting at a simple fundamental mechanism
that controls this behavior.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published as a conference paper at ICLR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Pilot Empirical Study on When and How to Use Knowledge Graphs as
  Retrieval Augmented Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.20854v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.20854v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xujie Yuan, Yongxu Liu, Shimin Di, Shiwen Wu, Libin Zheng, Rui Meng, Lei Chen, Xiaofang Zhou, Jian Yin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The integration of Knowledge Graphs (KGs) into the Retrieval Augmented
Generation (RAG) framework has attracted significant interest, with early
studies showing promise in mitigating hallucinations and improving model
accuracy. However, a systematic understanding and comparative analysis of the
rapidly emerging KG-RAG methods are still lacking. This paper seeks to lay the
foundation for systematically answering the question of when and how to use
KG-RAG by analyzing their performance in various application scenarios
associated with different technical configurations. After outlining the mind
map using KG-RAG framework and summarizing its popular pipeline, we conduct a
pilot empirical study of KG-RAG works to reimplement and evaluate 6 KG-RAG
methods across 7 datasets in diverse scenarios, analyzing the impact of 9
KG-RAG configurations in combination with 17 LLMs. Our results underscore the
critical role of appropriate application conditions and optimal configurations
of KG-RAG components.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 2 figures, 14 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Closer Look at Machine Unlearning for Large Language Models <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.08109v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.08109v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaojian Yuan, Tianyu Pang, Chao Du, Kejiang Chen, Weiming Zhang, Min Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) may memorize sensitive or copyrighted content,
raising privacy and legal concerns. Due to the high cost of retraining from
scratch, researchers attempt to employ machine unlearning to remove specific
content from LLMs while preserving the overall performance. In this paper, we
discuss several issues in machine unlearning for LLMs and provide our insights
on possible approaches. To address the issue of inadequate evaluation of model
outputs after unlearning, we introduce three additional metrics to evaluate
token diversity, sentence semantics, and factual correctness. We then
categorize unlearning methods into untargeted and targeted, and discuss their
issues respectively. Specifically, the behavior that untargeted unlearning
attempts to approximate is unpredictable and may involve hallucinations, and
existing regularization is insufficient for targeted unlearning. To alleviate
these issues, we propose using the objective of maximizing entropy (ME) for
untargeted unlearning and incorporate answer preservation (AP) loss as
regularization for targeted unlearning. Experimental results across three
scenarios, i.e., fictitious unlearning, continual unlearning, and real-world
unlearning, demonstrate the effectiveness of our approaches. The code is
available at https://github.com/sail-sg/closer-look-LLM-unlearning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Exploring Adversarial Robustness in Classification tasks using DNA
  Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.19788v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.19788v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hyunwoo Yoo, Haebin Shin, Kaidi Xu, Gail Rosen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  DNA Language Models, such as GROVER, DNABERT2 and the Nucleotide Transformer,
operate on DNA sequences that inherently contain sequencing errors, mutations,
and laboratory-induced noise, which may significantly impact model performance.
Despite the importance of this issue, the robustness of DNA language models
remains largely underexplored. In this paper, we comprehensivly investigate
their robustness in DNA classification by applying various adversarial attack
strategies: the character (nucleotide substitutions), word (codon
modifications), and sentence levels (back-translation-based transformations) to
systematically analyze model vulnerabilities. Our results demonstrate that DNA
language models are highly susceptible to adversarial attacks, leading to
significant performance degradation. Furthermore, we explore adversarial
training method as a defense mechanism, which enhances both robustness and
classification accuracy. This study highlights the limitations of DNA language
models and underscores the necessity of robustness in bioinformatics.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Facilitating Multi-turn Function Calling for LLMs via Compositional
  Instruction Tuning <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12952v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12952v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mingyang Chen, Haoze Sun, Tianpeng Li, Fan Yang, Hao Liang, Keer Lu, Bin Cui, Wentao Zhang, Zenan Zhou, Weipeng Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have exhibited significant potential in
performing diverse tasks, including the ability to call functions or use
external tools to enhance their performance. While current research on function
calling by LLMs primarily focuses on single-turn interactions, this paper
addresses the overlooked necessity for LLMs to engage in multi-turn function
calling--critical for handling compositional, real-world queries that require
planning with functions but not only use functions. To facilitate this, we
introduce an approach, BUTTON, which generates synthetic compositional
instruction tuning data via bottom-up instruction construction and top-down
trajectory generation. In the bottom-up phase, we generate simple atomic tasks
based on real-world scenarios and build compositional tasks using heuristic
strategies based on atomic tasks. Corresponding function definitions are then
synthesized for these compositional tasks. The top-down phase features a
multi-agent environment where interactions among simulated humans, assistants,
and tools are utilized to gather multi-turn function calling trajectories. This
approach ensures task compositionality and allows for effective function and
trajectory generation by examining atomic tasks within compositional tasks. We
produce a dataset BUTTONInstruct comprising 8k data points and demonstrate its
effectiveness through extensive experiments across various LLMs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ICLR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AdEval: Alignment-based Dynamic Evaluation to Mitigate Data
  Contamination in Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.13983v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.13983v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yang Fan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As Large Language Models (LLMs) are pretrained on massive-scale corpora, the
issue of data contamination has become increasingly severe, leading to
potential overestimation of model performance during evaluation. To address
this, we propose AdEval (Alignment-based Dynamic Evaluation), a dynamic data
evaluation method aimed at mitigating the impact of data contamination on
evaluation reliability. Experimental results on multiple datasets demonstrate
that AdEval effectively reduces the impact of data contamination on evaluation
outcomes, enhancing both the fairness and reliability of the evaluation
process.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>There are serious academic problems in this paper, such as data
  falsification and plagiarism in the method of the paper</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ OLMoE: Open Mixture-of-Experts Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.02060v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.02060v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Niklas Muennighoff, Luca Soldaini, Dirk Groeneveld, Kyle Lo, Jacob Morrison, Sewon Min, Weijia Shi, Pete Walsh, Oyvind Tafjord, Nathan Lambert, Yuling Gu, Shane Arora, Akshita Bhagia, Dustin Schwenk, David Wadden, Alexander Wettig, Binyuan Hui, Tim Dettmers, Douwe Kiela, Ali Farhadi, Noah A. Smith, Pang Wei Koh, Amanpreet Singh, Hannaneh Hajishirzi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce OLMoE, a fully open, state-of-the-art language model leveraging
sparse Mixture-of-Experts (MoE). OLMoE-1B-7B has 7 billion (B) parameters but
uses only 1B per input token. We pretrain it on 5 trillion tokens and further
adapt it to create OLMoE-1B-7B-Instruct. Our models outperform all available
models with similar active parameters, even surpassing larger ones like
Llama2-13B-Chat and DeepSeekMoE-16B. We present various experiments on MoE
training, analyze routing in our model showing high specialization, and
open-source all aspects of our work: model weights, training data, code, and
logs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>63 pages (24 main), 36 figures, 17 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The Labyrinth of Links: Navigating the Associative Maze of Multi-modal
  LLMs <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01417v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01417v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hong Li, Nanxi Li, Yuanjie Chen, Jianbin Zhu, Qinlu Guo, Cewu Lu, Yong-Lu Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-modal Large Language Models (MLLMs) have exhibited impressive
capability. However, recently many deficiencies of MLLMs have been found
compared to human intelligence, $\textit{e.g.}$, hallucination. To drive the
MLLMs study, the community dedicated efforts to building larger benchmarks with
complex tasks. In this paper, we propose benchmarking an essential but usually
overlooked intelligence: $\textbf{association}$, a human's basic capability to
link observation and prior practice memory. To comprehensively investigate
MLLM's performance on the association, we formulate the association task and
devise a standard benchmark based on adjective and verb semantic concepts.
Instead of costly data annotation and curation, we propose a convenient
$\textbf{annotation-free}$ construction method transforming the general dataset
for our association tasks. Simultaneously, we devise a rigorous data refinement
process to eliminate confusion in the raw dataset. Building on this database,
we establish three levels of association tasks: single-step, synchronous, and
asynchronous associations. Moreover, we conduct a comprehensive investigation
into the MLLMs' zero-shot association capabilities, addressing multiple
dimensions, including three distinct memory strategies, both open-source and
closed-source MLLMs, cutting-edge Mixture-of-Experts (MoE) models, and the
involvement of human experts. Our systematic investigation shows that current
open-source MLLMs consistently exhibit poor capability in our association
tasks, even the currently state-of-the-art GPT-4V(vision) also has a
significant gap compared to humans. We believe our benchmark would pave the way
for future MLLM studies. $\textit{Our data and code are available at:}$
https://mvig-rhos.com/llm_inception.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICLR 2025. Project page:
  https://mvig-rhos.com/llm_inception</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ NL2FOL: Translating Natural Language to First-Order Logic for Logical
  Fallacy Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.02318v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.02318v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Abhinav Lalwani, Tasha Kim, Lovish Chopra, Christopher Hahn, Zhijing Jin, Mrinmaya Sachan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Translating natural language into formal language such as First-Order Logic
(FOL) is a foundational challenge in NLP with wide-ranging applications in
automated reasoning, misinformation tracking, and knowledge validation. In this
paper, we introduce Natural Language to First-Order Logic (NL2FOL), a framework
to autoformalize natural language to FOL step by step using Large Language
Models (LLMs). Our approach addresses key challenges in this translation
process, including the integration of implicit background knowledge. By
leveraging structured representations generated by NL2FOL, we use
Satisfiability Modulo Theory (SMT) solvers to reason about the logical validity
of natural language statements. We present logical fallacy detection as a case
study to evaluate the efficacy of NL2FOL. Being neurosymbolic, our approach
also provides interpretable insights into the reasoning process and
demonstrates robustness without requiring model fine-tuning or labeled training
data. Our framework achieves strong performance on multiple datasets. On the
LOGIC dataset, NL2FOL achieves an F1-score of 78%, while generalizing
effectively to the LOGICCLIMATE dataset with an F1-score of 80%.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2025-03-02T00:00:00Z">2025-03-02</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Computation and Language
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ConFit v2: Improving Resume-Job Matching using Hypothetical Resume
  Embedding and Runner-Up Hard-Negative Mining 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.12361v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.12361v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiao Yu, Ruize Xu, Chengyuan Xue, Jinzhong Zhang, Xu Ma, Zhou Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A reliable resume-job matching system helps a company recommend suitable
candidates from a pool of resumes and helps a job seeker find relevant jobs
from a list of job posts. However, since job seekers apply only to a few jobs,
interaction labels in resume-job datasets are sparse. We introduce ConFit v2,
an improvement over ConFit to tackle this sparsity problem. We propose two
techniques to enhance the encoder's contrastive training process: augmenting
job data with hypothetical reference resume generated by a large language
model; and creating high-quality hard negatives from unlabeled resume/job pairs
using a novel hard-negative mining strategy. We evaluate ConFit v2 on two
real-world datasets and demonstrate that it outperforms ConFit and prior
methods (including BM25 and OpenAI text-embedding-003), achieving an average
absolute improvement of 13.8% in recall and 17.5% in nDCG across job-ranking
and resume-ranking tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: text overlap with arXiv:2401.16349</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Inference to the Best Explanation in Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.10767v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.10767v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dhairya Dalal, Marco Valentino, André Freitas, Paul Buitelaar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While Large Language Models (LLMs) have found success in real-world
applications, their underlying explanatory process is still poorly understood.
This paper proposes IBE-Eval, a framework inspired by philosophical accounts on
Inference to the Best Explanation (IBE) to advance the interpretation and
evaluation of LLMs' explanations. IBE-Eval estimates the plausibility of
natural language explanations through a combination of explicit logical and
linguistic features including: consistency, parsimony, coherence, and
uncertainty. Extensive experiments are conducted on Causal Question Answering
(CQA), where \textit{IBE-Eval} is tasked to select the most plausible causal
explanation amongst competing ones generated by LLMs (i.e., GPT 3.5 and Llama
2). The experiments reveal that IBE-Eval can successfully identify the best
explanation with up to 77\% accuracy ($\approx 27\%$ above random), improving
upon a GPT 3.5-as-a-Judge baseline ($\approx+17\%$) while being intrinsically
more efficient and interpretable. Additional analyses suggest that, despite
model-specific variances, LLM-generated explanations tend to conform to IBE
criteria and that IBE-Eval is significantly correlated with human judgment,
opening up opportunities for future development of automated explanation
verification tools.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Inference Scaling for Long-Context Retrieval Augmented Generation <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.04343v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.04343v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhenrui Yue, Honglei Zhuang, Aijun Bai, Kai Hui, Rolf Jagerman, Hansi Zeng, Zhen Qin, Dong Wang, Xuanhui Wang, Michael Bendersky
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The scaling of inference computation has unlocked the potential of
long-context large language models (LLMs) across diverse settings. For
knowledge-intensive tasks, the increased compute is often allocated to
incorporate more external knowledge. However, without effectively utilizing
such knowledge, solely expanding context does not always enhance performance.
In this work, we investigate inference scaling for retrieval augmented
generation (RAG), exploring the combination of multiple strategies beyond
simply increasing the quantity of knowledge, including in-context learning and
iterative prompting. These strategies provide additional flexibility to scale
test-time computation (e.g., by increasing retrieved documents or generation
steps), thereby enhancing LLMs' ability to effectively acquire and utilize
contextual information. We address two key questions: (1) How does RAG
performance benefit from the scaling of inference computation when optimally
configured? (2) Can we predict the optimal test-time compute allocation for a
given budget by modeling the relationship between RAG performance and inference
parameters? Our observations reveal that increasing inference computation leads
to nearly linear gains in RAG performance when optimally allocated, a
relationship we describe as the inference scaling laws for RAG. Building on
this, we further develop the computation allocation model to estimate RAG
performance across different inference configurations. The model predicts
optimal inference parameters under various computation constraints, which align
closely with the experimental results. By applying these optimal
configurations, we demonstrate that scaling inference compute on long-context
LLMs achieves up to 58.9% gains on benchmark datasets compared to standard RAG.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Distributed Speculative Inference (DSI): Speculation Parallelism for
  Provably Faster Lossless Language Model Inference <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.14105v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.14105v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nadav Timor, Jonathan Mamou, Daniel Korat, Moshe Berchansky, Oren Pereg, Moshe Wasserblat, Tomer Galanti, Michal Gordon, David Harel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces distributed speculative inference (DSI), a novel
inference algorithm that is provably faster than speculative inference (SI)
[leviathan2023, chen2023, miao2024, sun2025, timor2025] and standard
autoregressive inference (non-SI). Like other SI algorithms, DSI operates on
frozen language models (LMs), requiring no training or architectural
modifications, and it preserves the target distribution. Prior studies on SI
have demonstrated empirical speedups over non-SI--but rely on sufficiently fast
and accurate drafters, which are often unavailable in practice. We identify a
gap where SI can be slower than non-SI if drafters are too slow or inaccurate.
We close this gap by proving that DSI is faster than both SI and non-SI--given
any drafters. DSI is therefore not only faster than SI, but also unlocks the
acceleration of LMs for which SI fails. DSI leverages speculation parallelism
(SP), a novel type of task parallelism, to orchestrate target and drafter
instances that overlap in time, establishing a new foundational tradeoff
between computational resources and latency. Our simulations show that DSI is
1.29-1.92x faster than SI in single-node setups for various off-the-shelf LMs
and tasks. We open-source all our code.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published at ICLR 2025. (Link:
  https://openreview.net/forum?id=cJd1BgZ9CS)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">Prompt</span>ing Fairness: Integrating Causality to Debias Large Language
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.08743v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.08743v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jingling Li, Zeyu Tang, Xiaoyu Liu, Peter Spirtes, Kun Zhang, Liu Leqi, Yang Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs), despite their remarkable capabilities, are
susceptible to generating biased and discriminatory responses. As LLMs
increasingly influence high-stakes decision-making (e.g., hiring and
healthcare), mitigating these biases becomes critical. In this work, we propose
a causality-guided debiasing framework to tackle social biases, aiming to
reduce the objectionable dependence between LLMs' decisions and the social
information in the input. Our framework introduces a novel perspective to
identify how social information can affect an LLM's decision through different
causal pathways. Leveraging these causal insights, we outline principled
prompting strategies that regulate these pathways through selection mechanisms.
This framework not only unifies existing prompting-based debiasing techniques,
but also opens up new directions for reducing bias by encouraging the model to
prioritize fact-based reasoning over reliance on biased social cues. We
validate our framework through extensive experiments on real-world datasets
across multiple domains, demonstrating its effectiveness in debiasing LLM
decisions, even with only black-box access to the model.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>24 pages, 10 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Leveraging Dual Process Theory in Language Agent Framework for Real-time
  Simultaneous Human-AI Collaboration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11882v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11882v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shao Zhang, Xihuai Wang, Wenhao Zhang, Chaoran Li, Junru Song, Tingyu Li, Lin Qiu, Xuezhi Cao, Xunliang Cai, Wen Yao, Weinan Zhang, Xinbing Wang, Ying Wen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Agents built on large language models (LLMs) have excelled in turn-by-turn
human-AI collaboration but struggle with simultaneous tasks requiring real-time
interaction. Latency issues and the challenge of inferring variable human
strategies hinder their ability to make autonomous decisions without explicit
instructions. Through experiments with current independent System 1 and System
2 methods, we validate the necessity of using Dual Process Theory (DPT) in
real-time tasks. We propose DPT-Agent, a novel language agent framework that
integrates System 1 and System 2 for efficient real-time simultaneous human-AI
collaboration. DPT-Agent's System 1 uses a Finite-state Machine (FSM) and
code-as-policy for fast, intuitive, and controllable decision-making.
DPT-Agent's System 2 integrates Theory of Mind (ToM) and asynchronous
reflection to infer human intentions and perform reasoning-based autonomous
decisions. We demonstrate the effectiveness of DPT-Agent through further
experiments with rule-based agents and human collaborators, showing significant
improvements over mainstream LLM-based frameworks. DPT-Agent can effectively
help LLMs convert correct slow thinking and reasoning into executable actions,
thereby improving performance. To the best of our knowledge, DPT-Agent is the
first language agent framework that achieves successful real-time simultaneous
human-AI collaboration autonomously. Code of DPT-Agent can be found in
https://github.com/sjtu-marl/DPT-Agent.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint under review. Update the experimental results of the
  DeepSeek-R1 series models, o3-mini-high and o3-mini-medium</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Harnessing Multiple Large Language Models: A <span class="highlight-title">Survey</span> on LLM Ensemble 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.18036v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.18036v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhijun Chen, Jingzheng Li, Pengpeng Chen, Zhuoran Li, Kai Sun, Yuankai Luo, Qianren Mao, Dingqi Yang, Hailong Sun, Philip S. Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  LLM Ensemble -- which involves the comprehensive use of multiple large
language models (LLMs), each aimed at handling user queries during downstream
inference, to benefit from their individual strengths -- has gained substantial
attention recently. The widespread availability of LLMs, coupled with their
varying strengths and out-of-the-box usability, has profoundly advanced the
field of LLM Ensemble. This paper presents the first systematic review of
recent developments in LLM Ensemble. First, we introduce our taxonomy of LLM
Ensemble and discuss several related research problems. Then, we provide a more
in-depth classification of the methods under the broad categories of
"ensemble-before-inference, ensemble-during-inference,
ensemble-after-inference'', and review all relevant methods. Finally, we
introduce related benchmarks and applications, summarize existing studies, and
suggest several future research directions. A curated list of papers on LLM
Ensemble is available at https://github.com/junchenzhi/Awesome-LLM-Ensemble.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 2 figures, codebase:
  https://github.com/junchenzhi/Awesome-LLM-Ensemble</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Large Language Models Meet Symbolic Provers for Logical Reasoning
  Evaluation <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.06563v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.06563v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chengwen Qi, Ren Ma, Bowen Li, He Du, Binyuan Hui, Jinwang Wu, Yuanjun Laili, Conghui He
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  First-order logic (FOL) reasoning, which involves sequential deduction, is
pivotal for intelligent systems and serves as a valuable task for evaluating
reasoning capabilities, particularly in chain-of-thought (CoT) contexts.
Existing benchmarks often rely on extensive human annotation or handcrafted
templates, making it difficult to achieve the necessary complexity,
scalability, and diversity for robust evaluation. To address these limitations,
we propose a novel framework called ProverGen that synergizes the generative
strengths of Large Language Models (LLMs) with the rigor and precision of
symbolic provers, enabling the creation of a scalable, diverse, and
high-quality FOL reasoning dataset, ProverQA. ProverQA is also distinguished by
its inclusion of accessible and logically coherent intermediate reasoning steps
for each problem. Our evaluation shows that state-of-the-art LLMs struggle to
solve ProverQA problems, even with CoT prompting, highlighting the dataset's
challenging nature. We also finetune Llama3.1-8B-Instruct on a separate
training set generated by our framework. The finetuned model demonstrates
consistent improvements on both in-distribution and out-of-distribution test
sets, suggesting the value of our proposed data generation framework. Code
available at: https://github.com/opendatalab/ProverGen
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICLR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Generative causal testing to bridge data-driven models and scientific
  theories in language neuroscience 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.00812v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.00812v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Richard Antonello, Chandan Singh, Shailee Jain, Aliyah Hsu, Sihang Guo, Jianfeng Gao, Bin Yu, Alexander Huth
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Representations from large language models are highly effective at predicting
BOLD fMRI responses to language stimuli. However, these representations are
largely opaque: it is unclear what features of the language stimulus drive the
response in each brain area. We present generative causal testing (GCT), a
framework for generating concise explanations of language selectivity in the
brain from predictive models and then testing those explanations in follow-up
experiments using LLM-generated stimuli.This approach is successful at
explaining selectivity both in individual voxels and cortical regions of
interest (ROIs), including newly identified microROIs in prefrontal cortex. We
show that explanatory accuracy is closely related to the predictive power and
stability of the underlying predictive models. Finally, we show that GCT can
dissect fine-grained differences between brain areas with similar functional
selectivity. These results demonstrate that LLMs can be used to bridge the
widening gap between data-driven models and formal scientific theories.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The Mighty ToRR: A Benchmark for Table Reasoning and Robustness 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.19412v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.19412v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shir Ashury-Tahan, Yifan Mai, Rajmohan C, Ariel Gera, Yotam Perlitz, Asaf Yehudai, Elron Bandel, Leshem Choshen, Eyal Shnarch, Percy Liang, Michal Shmueli-Scheuer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite its real-world significance, model performance on tabular data
remains underexplored, leaving uncertainty about which model to rely on and
which prompt configuration to adopt. To address this gap, we create ToRR, a
benchmark for Table Reasoning and Robustness, measuring model performance and
robustness on table-related tasks. The benchmark includes 10 datasets that
cover different types of table reasoning capabilities across varied domains.
ToRR goes beyond model performance rankings, and is designed to reflect whether
models can handle tabular data consistently and robustly, across a variety of
common table representation formats. We present a leaderboard as well as
comprehensive analyses of the results of leading models over ToRR. Our results
reveal a striking pattern of brittle model behavior, where even strong models
are unable to perform robustly on tabular data tasks. Although no specific
table format leads to consistently better performance, we show that testing
over multiple formats is crucial for reliably estimating model capabilities.
Moreover, we show that the reliability boost from testing multiple prompts can
be equivalent to adding more test examples. Overall, our findings show that
table understanding and reasoning tasks remain a significant challenge.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LLaVA-Mini: Efficient Image and Video Large Multimodal Models with One
  Vision Token <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.03895v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.03895v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shaolei Zhang, Qingkai Fang, Zhe Yang, Yang Feng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The advent of real-time large multimodal models (LMMs) like GPT-4o has
sparked considerable interest in efficient LMMs. LMM frameworks typically
encode visual inputs into vision tokens (continuous representations) and
integrate them and textual instructions into the context of large language
models (LLMs), where large-scale parameters and numerous context tokens
(predominantly vision tokens) result in substantial computational overhead.
Previous efforts towards efficient LMMs always focus on replacing the LLM
backbone with smaller models, while neglecting the crucial issue of token
quantity. In this paper, we introduce LLaVA-Mini, an efficient LMM with minimal
vision tokens. To achieve a high compression ratio of vision tokens while
preserving visual information, we first analyze how LMMs understand vision
tokens and find that most vision tokens only play a crucial role in the early
layers of LLM backbone, where they mainly fuse visual information into text
tokens. Building on this finding, LLaVA-Mini introduces modality pre-fusion to
fuse visual information into text tokens in advance, thereby facilitating the
extreme compression of vision tokens fed to LLM backbone into one token.
LLaVA-Mini is a unified large multimodal model that can support the
understanding of images, high-resolution images, and videos in an efficient
manner. Experiments across 11 image-based and 7 video-based benchmarks
demonstrate that LLaVA-Mini outperforms LLaVA-v1.5 with just 1 vision token
instead of 576. Efficiency analyses reveal that LLaVA-Mini can reduce FLOPs by
77%, deliver low-latency responses within 40 milliseconds, and process over
10,000 frames of video on the GPU hardware with 24GB of memory.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ICLR 2025. Code: https://github.com/ictnlp/LLaVA-Mini
  Model: https://huggingface.co/ICTNLP/llava-mini-llama-3.1-8b</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Steering Large Language Models between Code Execution and Textual
  Reasoning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.03524v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.03524v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yongchao Chen, Harsh Jhamtani, Srinagesh Sharma, Chuchu Fan, Chi Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While a lot of recent research focuses on enhancing the textual reasoning
capabilities of Large Language Models (LLMs) by optimizing the multi-agent
framework or reasoning chains, several benchmark tasks can be solved with 100\%
success through direct coding, which is more scalable and avoids the
computational overhead associated with textual iterating and searching. Textual
reasoning has inherent limitations in solving tasks with challenges in math,
logics, optimization, and searching, which is unlikely to be solved by simply
scaling up the model and data size. The recently released OpenAI GPT Code
Interpreter and multi-agent frameworks such as AutoGen have demonstrated
remarkable proficiency of integrating code generation and execution to solve
complex tasks using LLMs. However, based on our experiments on 7 existing
popular methods for steering code/text generation in both single- and
multi-turn settings with 14 tasks and 6 types of LLMs (including the new
O1-preview), currently there is no optimal method to correctly steer LLMs to
write code when needed. We discover some interesting patterns on when models
use code vs. textual reasoning with the evolution to task complexity and model
sizes, which even result in an astonishingly inverse scaling behavior. We also
discover that results from LLM written code are not always better than using
textual reasoning, even if the task could be solved through code. To mitigate
the above issues, we propose three methods to better steer LLM code/text
generation and achieve a notable improvement. The costs of token lengths and
runtime are thoroughly discussed for all the methods. We believe the problem of
steering LLM code/text generation is critical for future research and has much
space for further improvement. Project Page, Datasets, and Codes are available
at https://yongchao98.github.io/CodeSteer/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>32 pages, 12 figures, 12 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ When Attention Sink Emerges in Language Models: An Empirical View <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.10781v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.10781v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiangming Gu, Tianyu Pang, Chao Du, Qian Liu, Fengzhuo Zhang, Cunxiao Du, Ye Wang, Min Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Language Models (LMs) assign significant attention to the first token, even
if it is not semantically important, which is known as attention sink. This
phenomenon has been widely adopted in applications such as streaming/long
context generation, KV cache optimization, inference acceleration, model
quantization, and others. Despite its widespread use, a deep understanding of
attention sink in LMs is still lacking. In this work, we first demonstrate that
attention sinks exist universally in LMs with various inputs, even in small
models. Furthermore, attention sink is observed to emerge during the LM
pre-training, motivating us to investigate how optimization, data distribution,
loss function, and model architecture in LM pre-training influence its
emergence. We highlight that attention sink emerges after effective
optimization on sufficient training data. The sink position is highly
correlated with the loss function and data distribution. Most importantly, we
find that attention sink acts more like key biases, storing extra attention
scores, which could be non-informative and not contribute to the value
computation. We also observe that this phenomenon (at least partially) stems
from tokens' inner dependence on attention scores as a result of softmax
normalization. After relaxing such dependence by replacing softmax attention
with other attention operations, such as sigmoid attention without
normalization, attention sinks do not emerge in LMs up to 1B parameters. The
code is available at https://github.com/sail-sg/Attention-Sink.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2025 (Spotlight)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Generating Visual Stories with Grounded and Coreferent Characters 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.13555v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.13555v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Danyang Liu, Mirella Lapata, Frank Keller
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Characters are important in narratives. They move the plot forward, create
emotional connections, and embody the story's themes. Visual storytelling
methods focus more on the plot and events relating to it, without building the
narrative around specific characters. As a result, the generated stories feel
generic, with character mentions being absent, vague, or incorrect. To mitigate
these issues, we introduce the new task of character-centric story generation
and present the first model capable of predicting visual stories with
consistently grounded and coreferent character mentions. Our model is finetuned
on a new dataset which we build on top of the widely used VIST benchmark.
Specifically, we develop an automated pipeline to enrich VIST with visual and
textual character coreference chains. We also propose new evaluation metrics to
measure the richness of characters and coreference in stories. Experimental
results show that our model generates stories with recurring characters which
are consistent and coreferent to larger extent compared to baselines and
state-of-the-art systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Cheating Automatic LLM Benchmarks: Null Models Achieve High Win Rates <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.07137v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.07137v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaosen Zheng, Tianyu Pang, Chao Du, Qian Liu, Jing Jiang, Min Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Automatic LLM benchmarks, such as AlpacaEval 2.0, Arena-Hard-Auto, and
MT-Bench, have become popular for evaluating language models due to their
cost-effectiveness and scalability compared to human evaluation. Achieving high
win rates on these benchmarks can significantly boost the promotional impact of
newly released language models. This promotional benefit may motivate tricks,
such as manipulating model output length or style to game win rates, even
though several mechanisms have been developed to control length and disentangle
style to reduce gameability. Nonetheless, we show that even a "null model" that
always outputs a constant response (irrelevant to input instructions) can cheat
automatic benchmarks and achieve top-ranked win rates: an 86.5% LC win rate on
AlpacaEval 2.0; an 83.0 score on Arena-Hard-Auto; and a 9.55 score on MT-Bench.
Moreover, the crafted cheating outputs are transferable because we assume that
the instructions of these benchmarks (e.g., 805 samples of AlpacaEval 2.0) are
private and cannot be accessed. While our experiments are primarily
proof-of-concept, an adversary could use LLMs to generate more imperceptible
cheating responses, unethically benefiting from high win rates and promotional
impact. Our findings call for the development of anti-cheating mechanisms for
reliable automatic benchmarks. The code is available at
https://github.com/sail-sg/Cheating-LLM-Benchmarks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2025 (Oral)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ How Does A Text Preprocessing Pipeline Affect Ontology Syntactic
  Matching? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03962v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03962v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhangcheng Qiang, Kerry Taylor, Weiqing Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The generic text preprocessing pipeline, comprising Tokenisation,
Normalisation, Stop Words Removal, and Stemming/Lemmatisation, has been
implemented in many systems for syntactic ontology matching (OM). However, the
lack of standardisation in text preprocessing creates diversity in mapping
results. In this paper, we investigate the effect of the text preprocessing
pipeline on syntactic OM in 8 Ontology Alignment Evaluation Initiative (OAEI)
tracks with 49 distinct alignments. We find that Phase 1 text preprocessing
(Tokenisation and Normalisation) is currently more effective than Phase 2 text
preprocessing (Stop Words Removal and Stemming/Lemmatisation). To repair the
less effective Phase 2 text preprocessing caused by unwanted false mappings, we
propose a novel context-based pipeline repair approach that employs an ad hoc
check to find common words that cause false mappings. These words are stored in
a reserved word set and applied in text preprocessing. The experimental results
show that our approach improves the matching correctness and the overall
matching performance. We also discuss the integration of the classical text
preprocessing pipeline with modern large language models (LLMs). We recommend
that LLMs inject the text preprocessing pipeline via function calling to avoid
the tendency towards unstable true mappings produced by prompt-based LLM
approaches, and use LLMs to repair false mappings generated by the text
preprocessing pipeline.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 11 figures, 4 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Intrinsic Dimension Correlation: uncovering nonlinear connections in
  multimodal representations <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.15812v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.15812v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lorenzo Basile, Santiago Acevedo, Luca Bortolussi, Fabio Anselmi, Alex Rodriguez
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To gain insight into the mechanisms behind machine learning methods, it is
crucial to establish connections among the features describing data points.
However, these correlations often exhibit a high-dimensional and strongly
nonlinear nature, which makes them challenging to detect using standard
methods. This paper exploits the entanglement between intrinsic dimensionality
and correlation to propose a metric that quantifies the (potentially nonlinear)
correlation between high-dimensional manifolds. We first validate our method on
synthetic data in controlled environments, showcasing its advantages and
drawbacks compared to existing techniques. Subsequently, we extend our analysis
to large-scale applications in neural network representations. Specifically, we
focus on latent representations of multimodal data, uncovering clear
correlations between paired visual and textual embeddings, whereas existing
methods struggle significantly in detecting similarity. Our results indicate
the presence of highly nonlinear correlation patterns between latent manifolds.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ICLR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Boosting Jailbreak Attack with Momentum <span class="chip">ICASSP 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.01229v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.01229v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yihao Zhang, Zeming Wei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have achieved remarkable success across diverse
tasks, yet they remain vulnerable to adversarial attacks, notably the
well-known jailbreak attack. In particular, the Greedy Coordinate Gradient
(GCG) attack has demonstrated efficacy in exploiting this vulnerability by
optimizing adversarial prompts through a combination of gradient heuristics and
greedy search. However, the efficiency of this attack has become a bottleneck
in the attacking process. To mitigate this limitation, in this paper we rethink
the generation of the adversarial prompts through an optimization lens, aiming
to stabilize the optimization process and harness more heuristic insights from
previous optimization iterations. Specifically, we propose the
\textbf{M}omentum \textbf{A}ccelerated G\textbf{C}G (\textbf{MAC}) attack,
which integrates a momentum term into the gradient heuristic to boost and
stabilize the random search for tokens in adversarial prompts. Experimental
results showcase the notable enhancement achieved by MAC over baselines in
terms of attack success rate and optimization efficiency. Moreover, we
demonstrate that MAC can still exhibit superior performance for transfer
attacks and models under defense mechanisms. Our code is available at
https://github.com/weizeming/momentum-attack-llm.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICASSP 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Developing a Multilingual <span class="highlight-title">Dataset</span> and Evaluation Metrics for
  Code-Switching: A Focus on Hong Kong's Polylingual Dynamics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.17953v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.17953v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Peng Xie, Kani Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The existing audio datasets are predominantly tailored towards single
languages, overlooking the complex linguistic behaviors of multilingual
communities that engage in code-switching. This practice, where individuals
frequently mix two or more languages in their daily interactions, is
particularly prevalent in multilingual regions such as Hong Kong, China. To
bridge this gap, we have developed a 34.8-hour dataset of Mixed Cantonese and
English (MCE) audio using our Multi-Agent Data Generation Framework (MADGF). We
fine-tuned the open-source multilingual Automatic Speech Recognition (ASR)
model, Whisper, with the MCE dataset, leading to impressive zero-shot
performance. The traditional metrics overlook important factors such as latency
in real-world applications and code-switching scenarios. We have introduced a
novel evaluation metric called Fidelity to the Original Audio, Accuracy, and
Latency (FAL). This metric aims to overcome the limitations of traditional
metrics used to assess ASR systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Derailer-Rerailer: Adaptive Verification for Efficient and Reliable
  Language Model Reasoning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.13940v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.13940v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guangya Wan, Yuqi Wu, Hao Wang, Shengming Zhao, Jie Chen, Sheng Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have shown impressive reasoning capabilities,
yet existing prompting methods face a critical trade-off: simple approaches
often struggle with complex tasks and reasoning stability, while more
sophisticated methods require multiple inferences and substantial computational
resources, limiting their practical deployment. To address this challenge, we
propose Derailer-Rerailer, a novel framework that adaptively balances reasoning
accuracy and computational efficiency. At its core, our framework employs a
lightweight Derailer mechanism to assess reasoning stability and selectively
triggers an advanced Rerailer verification process only when necessary, thereby
optimizing computational resource usage. Extensive evaluation across both open
and closed-source models on more than 20 categories of mathematical, symbolic,
and commonsense reasoning tasks demonstrates our framework's effectiveness:
Derailer-Rerailer achieves significant accuracy improvements (8-11\% across
various reasoning tasks) while maintaining 2-3 times better efficiency than
existing verification methods, with particularly strong performance in
mathematical and symbolic reasoning, offering a practical solution for
enhancing LLM reasoning reliability while significantly reducing computational
overhead.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Taxonomy, Opportunities, and Challenges of Representation Engineering
  for Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.19649v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.19649v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jan Wehner, Sahar Abdelnabi, Daniel Tan, David Krueger, Mario Fritz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Representation Engineering (RepE) is a novel paradigm for controlling the
behavior of LLMs. Unlike traditional approaches that modify inputs or fine-tune
the model, RepE directly manipulates the model's internal representations. As a
result, it may offer more effective, interpretable, data-efficient, and
flexible control over models' behavior. We present the first comprehensive
survey of RepE for LLMs, reviewing the rapidly growing literature to address
key questions: What RepE methods exist and how do they differ? For what
concepts and problems has RepE been applied? What are the strengths and
weaknesses of RepE compared to other methods? To answer these, we propose a
unified framework describing RepE as a pipeline comprising representation
identification, operationalization, and control. We posit that while RepE
methods offer significant potential, challenges remain, including managing
multiple concepts, ensuring reliability, and preserving models' performance.
Towards improving RepE, we identify opportunities for experimental and
methodological improvements and construct a guide for best practices.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DiscoGraMS: Enhancing Movie Screen-Play Summarization using Movie
  Character-Aware Discourse Graph <span class="chip">NAACL 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14666v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14666v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Maitreya Prafulla Chitale, Uday Bindal, Rajakrishnan Rajkumar, Rahul Mishra
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Summarizing movie screenplays presents a unique set of challenges compared to
standard document summarization. Screenplays are not only lengthy, but also
feature a complex interplay of characters, dialogues, and scenes, with numerous
direct and subtle relationships and contextual nuances that are difficult for
machine learning models to accurately capture and comprehend. Recent attempts
at screenplay summarization focus on fine-tuning transformer-based pre-trained
models, but these models often fall short in capturing long-term dependencies
and latent relationships, and frequently encounter the "lost in the middle"
issue. To address these challenges, we introduce DiscoGraMS, a novel resource
that represents movie scripts as a movie character-aware discourse graph (CaD
Graph). This approach is well-suited for various downstream tasks, such as
summarization, question-answering, and salience detection. The model aims to
preserve all salient information, offering a more comprehensive and faithful
representation of the screenplay's content. We further explore a baseline
method that combines the CaD Graph with the corresponding movie script through
a late fusion of graph and text modalities, and we present very initial
promising results.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at NAACL 2025 (Main)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Market-Derived Financial Sentiment Analysis: Context-Aware Language
  Models for Crypto Forecasting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.14897v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.14897v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hamid Moradi-Kamali, Mohammad-Hossein Rajabi-Ghozlou, Mahdi Ghazavi, Ali Soltani, Amirreza Sattarzadeh, Reza Entezari-Maleki
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Financial Sentiment Analysis (FSA) traditionally relies on human-annotated
sentiment labels to infer investor sentiment and forecast market movements.
However, inferring the potential market impact of words based on their
human-perceived intentions is inherently challenging. We hypothesize that the
historical market reactions to words, offer a more reliable indicator of their
potential impact on markets than subjective sentiment interpretations by human
annotators. To test this hypothesis, a market-derived labeling approach is
proposed to assign tweet labels based on ensuing short-term price trends,
enabling the language model to capture the relationship between textual signals
and market dynamics directly. A domain-specific language model was fine-tuned
on these labels, achieving up to an 11% improvement in short-term trend
prediction accuracy over traditional sentiment-based benchmarks. Moreover, by
incorporating market and temporal context through prompt-tuning, the proposed
context-aware language model demonstrated an accuracy of 89.6% on a curated
dataset of 227 impactful Bitcoin-related news events with significant market
impacts. Aggregating daily tweet predictions into trading signals, our method
outperformed traditional fusion models (which combine sentiment-based and
price-based predictions). It challenged the assumption that sentiment-based
signals are inferior to price-based predictions in forecasting market
movements. Backtesting these signals across three distinct market regimes
yielded robust Sharpe ratios of up to 5.07 in trending markets and 3.73 in
neutral markets. Our findings demonstrate that language models can serve as
effective short-term market predictors. This paradigm shift underscores the
untapped capabilities of language models in financial decision-making and opens
new avenues for market prediction applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Speech Representation Learning Revisited: The Necessity of Separate
  Learnable Parameters and Robust Data Augmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.10557v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.10557v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hemant Yadav, Sunayana Sitaram, Rajiv Ratn Shah
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Speech modeling methods learn one embedding for a fixed segment of speech,
typically in between 10-25 ms. The information present in speech can be divided
into two categories: "what is being said" (content) and "how it is expressed"
(other) and these two are orthogonal in nature causing the optimization
algorithm to find a sub-optimal solution if forced to optimize together. This
leads to sub-optimal performance in one or all downstream tasks as shown by
previous studies. Current self-supervised learning (SSL) methods such as HuBERT
are very good at modeling the content information present in speech. Data
augmentation improves the performance on tasks which require effective modeling
of other information but this leads to a divided capacity of the model. In this
work, we conduct a preliminary study to understand the importance of modeling
other information using separate learnable parameters. We propose a modified
version of HuBERT, termed Other HuBERT (O-HuBERT), to test our hypothesis. Our
findings are twofold: first, the O-HuBERT method is able to utilize all layers
to build complex features to encode other information; second, a robust data
augmentation strategy is essential for learning the information required by
tasks that depend on other information and to achieve state-of-the-art (SOTA)
performance on the SUPERB benchmark with a similarly sized model (100 million
parameters) and pre-training data (960 hours).
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CogCoM: A Visual Language Model with Chain-of-Manipulations Reasoning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.04236v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.04236v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ji Qi, Ming Ding, Weihan Wang, Yushi Bai, Qingsong Lv, Wenyi Hong, Bin Xu, Lei Hou, Juanzi Li, Yuxiao Dong, Jie Tang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision-Language Models (VLMs) have demonstrated their broad effectiveness
thanks to extensive training in aligning visual instructions to responses.
However, such training of conclusive alignment leads models to ignore essential
visual reasoning, further resulting in failures in meticulous visual problems
and unfaithful responses. Drawing inspiration from human cognition in solving
visual problems (e.g., marking, zoom in), this paper introduces Chain of
Manipulations, a mechanism that enables VLMs to solve problems step-by-step
with evidence. After training, models can solve various visual problems by
eliciting intrinsic manipulations (e.g., grounding, zoom in) with results
(e.g., boxes, image) actively without involving external tools, while also
allowing users to trace error causes. We study the roadmap to implement this
mechanism, including (1) a flexible design of manipulations upon extensive
analysis, (2) an efficient automated data generation pipeline, (3) a compatible
VLM architecture capable of multi-turn multi-image, and (4) a model training
process for versatile capabilities. With the design, we also manually annotate
6K high-quality samples for the challenging graphical mathematical problems.
Our trained model, \textbf{CogCoM}, equipped with this mechanism with 17B
parameters achieves state-of-the-art performance across 9 benchmarks from 4
categories, demonstrating the effectiveness while preserving the
interpretability. Our code, model weights, and collected data are publicly
available at https://github.com/THUDM/CogCoM.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>21 pages, 10 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Curriculum-style Data Augmentation for LLM-based Metaphor Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.02956v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.02956v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kaidi Jia, Yanxia Wu, Ming Liu, Rongsheng Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, utilizing large language models (LLMs) for metaphor detection has
achieved promising results. However, these methods heavily rely on the
capabilities of closed-source LLMs, which come with relatively high inference
costs and latency. To address this, we propose a method for metaphor detection
by fine-tuning open-source LLMs, effectively reducing inference costs and
latency with a single inference step. Furthermore, metaphor detection suffers
from a severe data scarcity problem, which hinders effective fine-tuning of
LLMs. To tackle this, we introduce Curriculum-style Data Augmentation (CDA).
Specifically, before fine-tuning, we evaluate the training data to identify
correctly predicted instances for fine-tuning, while incorrectly predicted
instances are used as seed data for data augmentation. This approach enables
the model to quickly learn simpler knowledge and progressively acquire more
complex knowledge, thereby improving performance incrementally. Experimental
results demonstrate that our method achieves state-of-the-art performance
across all baselines. Additionally, we provide detailed ablation studies to
validate the effectiveness of CDA.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ What is Wrong with Perplexity for Long-context Language Modeling? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.23771v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.23771v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lizhe Fang, Yifei Wang, Zhaoyang Liu, Chenheng Zhang, Stefanie Jegelka, Jinyang Gao, Bolin Ding, Yisen Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Handling long-context inputs is crucial for large language models (LLMs) in
tasks such as extended conversations, document summarization, and many-shot
in-context learning. While recent approaches have extended the context windows
of LLMs and employed perplexity (PPL) as a standard evaluation metric, PPL has
proven unreliable for assessing long-context capabilities. The underlying cause
of this limitation has remained unclear. In this work, we provide a
comprehensive explanation for this issue. We find that PPL overlooks key
tokens, which are essential for long-context understanding, by averaging across
all tokens and thereby obscuring the true performance of models in long-context
scenarios. To address this, we propose \textbf{LongPPL}, a novel metric that
focuses on key tokens by employing a long-short context contrastive method to
identify them. Our experiments demonstrate that LongPPL strongly correlates
with performance on various long-context benchmarks (e.g., Pearson correlation
of -0.96), significantly outperforming traditional PPL in predictive accuracy.
Additionally, we introduce \textbf{LongCE} (Long-context Cross-Entropy) loss, a
re-weighting strategy for fine-tuning that prioritizes key tokens, leading to
consistent improvements across diverse benchmarks. In summary, these
contributions offer deeper insights into the limitations of PPL and present
effective solutions for accurately evaluating and enhancing the long-context
capabilities of LLMs. Code is available at https://github.com/PKU-ML/LongPPL.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Sylber: Syllabic Embedding Representation of Speech from Raw Audio <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.07168v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.07168v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Cheol Jun Cho, Nicholas Lee, Akshat Gupta, Dhruv Agarwal, Ethan Chen, Alan W Black, Gopala K. Anumanchipalli
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Syllables are compositional units of spoken language that efficiently
structure human speech perception and production. However, current neural
speech representations lack such structure, resulting in dense token sequences
that are costly to process. To bridge this gap, we propose a new model, Sylber,
that produces speech representations with clean and robust syllabic structure.
Specifically, we propose a self-supervised learning (SSL) framework that
bootstraps syllabic embeddings by distilling from its own initial unsupervised
syllabic segmentation. This results in a highly structured representation of
speech features, offering three key benefits: 1) a fast, linear-time syllable
segmentation algorithm, 2) efficient syllabic tokenization with an average of
4.27 tokens per second, and 3) novel phonological units suited for efficient
spoken language modeling. Our proposed segmentation method is highly robust and
generalizes to out-of-domain data and unseen languages without any tuning. By
training token-to-speech generative models, fully intelligible speech can be
reconstructed from Sylber tokens with a significantly lower bitrate than
baseline SSL tokens. This suggests that our model effectively compresses speech
into a compact sequence of tokens with minimal information loss. Lastly, we
demonstrate that categorical perception-a linguistic phenomenon in speech
perception-emerges naturally in Sylber, making the embedding space more
categorical and sparse than previous speech features and thus supporting the
high efficiency of our tokenization. Together, we present a novel SSL approach
for representing speech as syllables, with significant potential for efficient
speech tokenization and spoken language modeling.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ICLR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Path-Consistency: Prefix Enhancement for Efficient Inference in LLM 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.01281v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.01281v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiace Zhu, Yingtao Shen, Jie Zhao, An Zou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To enhance the reasoning capabilities of large language models (LLMs),
self-consistency has gained significant popularity by combining multiple
sampling with majority voting. However, the state-of-the-art self-consistency
approaches consume substantial computational resources and lead to significant
additional time costs due to the multiple sampling. This prevents its full
potential from being realized in scenarios where computational resources are
critical. To improve the inference efficiency, this paper introduces
\textit{path-consistency}, a method that leverages the confidence of answers
generated in earlier branches to identify the prefix of the most promising
path. By dynamically guiding the generation of subsequent branches based on
this prefix, the \textit{path-consistency} mitigates both the errors and
redundancies from random or less useful sampling in self-consistency. As a
result, it can significantly accelerate the inference process by reducing the
number of tokens generated. Our extensive empirical evaluation shows that the
\textit{path-consistency} achieves significant acceleration in inference
latency ranging from $7.8\%$ to $40.5\%$, while maintaining or even improving
task accuracy across different datasets, including mathematical reasoning,
common sense reasoning, symbolic reasoning, and code generation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Padding Tone: A Mechanistic Analysis of Padding Tokens in T2I Models <span class="chip">NAACL 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.06751v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.06751v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Michael Toker, Ido Galil, Hadas Orgad, Rinon Gal, Yoad Tewel, Gal Chechik, Yonatan Belinkov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Text-to-image (T2I) diffusion models rely on encoded prompts to guide the
image generation process. Typically, these prompts are extended to a fixed
length by adding padding tokens before text encoding. Despite being a default
practice, the influence of padding tokens on the image generation process has
not been investigated. In this work, we conduct the first in-depth analysis of
the role padding tokens play in T2I models. We develop two causal techniques to
analyze how information is encoded in the representation of tokens across
different components of the T2I pipeline. Using these techniques, we
investigate when and how padding tokens impact the image generation process.
Our findings reveal three distinct scenarios: padding tokens may affect the
model's output during text encoding, during the diffusion process, or be
effectively ignored. Moreover, we identify key relationships between these
scenarios and the model's architecture (cross or self-attention) and its
training process (frozen or trained text encoder). These insights contribute to
a deeper understanding of the mechanisms of padding tokens, potentially
informing future model design and training practices in T2I systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published in: NAACL 2025. Project webpage:
  https://padding-tone.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Empathy Level Alignment via Reinforcement Learning for Empathetic
  Response Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.02976v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.02976v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hui Ma, Bo Zhang, Bo Xu, Jian Wang, Hongfei Lin, Xiao Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Empathetic response generation, aiming to understand the user's situation and
feelings and respond empathically, is crucial in building human-like dialogue
systems. Traditional approaches typically employ maximum likelihood estimation
as the optimization objective during training, yet fail to align the empathy
levels between generated and target responses. To this end, we propose an
empathetic response generation framework using reinforcement learning (EmpRL).
The framework develops an effective empathy reward function and generates
empathetic responses by maximizing the expected reward through reinforcement
learning. EmpRL utilizes the pre-trained T5 model as the generator and further
fine-tunes it to initialize the policy. To align the empathy levels between
generated and target responses within a given context, an empathy reward
function containing three empathy communication mechanisms -- emotional
reaction, interpretation, and exploration -- is constructed using pre-designed
and pre-trained empathy identifiers. During reinforcement learning training,
the proximal policy optimization algorithm is used to fine-tune the policy,
enabling the generation of empathetic responses. Both automatic and human
evaluations demonstrate that the proposed EmpRL framework significantly
improves the quality of generated responses, enhances the similarity in empathy
levels between generated and target responses, and produces empathetic
responses covering both affective and cognitive aspects.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by IEEE Transactions on Affective Computing</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Efficient Automated Circuit Discovery in <span class="highlight-title">Transformer</span>s using Contextual
  Decomposition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.00886v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.00886v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aliyah R. Hsu, Georgia Zhou, Yeshwanth Cherapanamjeri, Yaxuan Huang, Anobel Y. Odisho, Peter R. Carroll, Bin Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Automated mechanistic interpretation research has attracted great interest
due to its potential to scale explanations of neural network internals to large
models. Existing automated circuit discovery work relies on activation patching
or its approximations to identify subgraphs in models for specific tasks
(circuits). They often suffer from slow runtime, approximation errors, and
specific requirements of metrics, such as non-zero gradients. In this work, we
introduce contextual decomposition for transformers (CD-T) to build
interpretable circuits in large language models. CD-T can produce circuits of
arbitrary level of abstraction, and is the first able to produce circuits as
fine-grained as attention heads at specific sequence positions efficiently.
CD-T consists of a set of mathematical equations to isolate contribution of
model features. Through recursively computing contribution of all nodes in a
computational graph of a model using CD-T followed by pruning, we are able to
reduce circuit discovery runtime from hours to seconds compared to
state-of-the-art baselines. On three standard circuit evaluation datasets
(indirect object identification, greater-than comparisons, and docstring
completion), we demonstrate that CD-T outperforms ACDC and EAP by better
recovering the manual circuits with an average of 97% ROC AUC under low
runtimes. In addition, we provide evidence that faithfulness of CD-T circuits
is not due to random chance by showing our circuits are 80% more faithful than
random circuits of up to 60% of the original model size. Finally, we show CD-T
circuits are able to perfectly replicate original models' behavior
(faithfulness $ = 1$) using fewer nodes than the baselines for all tasks. Our
results underscore the great promise of CD-T for efficient automated
mechanistic interpretability, paving the way for new insights into the workings
of large language models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Hybrid <span class="highlight-title">Transformer</span> Model for Fake News Detection: Leveraging Bayesian
  Optimization and Bidirectional Recurrent Unit 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09097v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09097v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tianyi Huang, Zeqiu Xu, Peiyang Yu, Jingyuan Yi, Xiaochuan Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we propose an optimized Transformer model that integrates
Bayesian algorithms with a Bidirectional Gated Recurrent Unit (BiGRU), and
apply it to fake news classification for the first time. First, we employ the
TF-IDF method to extract features from news texts and transform them into
numeric representations to facilitate subsequent machine learning tasks. Two
sets of experiments are then conducted for fake news detection and
classification: one using a Transformer model optimized only with BiGRU, and
the other incorporating Bayesian algorithms into the BiGRU-based Transformer.
Experimental results show that the BiGRU-optimized Transformer achieves 100%
accuracy on the training set and 99.67% on the test set, while the addition of
the Bayesian algorithm maintains 100% accuracy on the training set and slightly
improves test-set accuracy to 99.73%. This indicates that the Bayesian
algorithm boosts model accuracy by 0.06%, further enhancing the detection
capability for fake news. Moreover, the proposed algorithm converges rapidly at
around the 10th training epoch with accuracy nearing 100%, demonstrating both
its effectiveness and its fast classification ability. Overall, the optimized
Transformer model, enhanced by the Bayesian algorithm and BiGRU, exhibits
excellent continuous learning and detection performance, offering a robust
technical means to combat the spread of fake news in the current era of
information overload.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MobA: Multifaceted Memory-Enhanced Adaptive Planning for Efficient
  Mobile Task Automation <span class="chip">NAACL 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13757v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13757v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zichen Zhu, Hao Tang, Yansi Li, Dingye Liu, Hongshen Xu, Kunyao Lan, Danyang Zhang, Yixuan Jiang, Hao Zhou, Chenrun Wang, Situo Zhang, Liangtai Sun, Yixiao Wang, Yuheng Sun, Lu Chen, Kai Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing Multimodal Large Language Model (MLLM)-based agents face significant
challenges in handling complex GUI (Graphical User Interface) interactions on
devices. These challenges arise from the dynamic and structured nature of GUI
environments, which integrate text, images, and spatial relationships, as well
as the variability in action spaces across different pages and tasks. To
address these limitations, we propose MobA, a novel MLLM-based mobile assistant
system. MobA introduces an adaptive planning module that incorporates a
reflection mechanism for error recovery and dynamically adjusts plans to align
with the real environment contexts and action module's execution capacity.
Additionally, a multifaceted memory module provides comprehensive memory
support to enhance adaptability and efficiency. We also present MobBench, a
dataset designed for complex mobile interactions. Experimental results on
MobBench and AndroidArena demonstrate MobA's ability to handle dynamic GUI
environments and perform complex mobile task.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NAACL 2025 Demo Track</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Examining Alignment of Large Language Models through Representative
  Heuristics: The Case of Political Stereotypes <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.14294v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.14294v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sullam Jeoung, Yubin Ge, Haohan Wang, Jana Diesner
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Examining the alignment of large language models (LLMs) has become
increasingly important, e.g., when LLMs fail to operate as intended. This study
examines the alignment of LLMs with human values for the domain of politics.
Prior research has shown that LLM-generated outputs can include political
leanings and mimic the stances of political parties on various issues. However,
the extent and conditions under which LLMs deviate from empirical positions are
insufficiently examined. To address this gap, we analyze the factors that
contribute to LLMs' deviations from empirical positions on political issues,
aiming to quantify these deviations and identify the conditions that cause
them.
  Drawing on findings from cognitive science about representativeness
heuristics, i.e., situations where humans lean on representative attributes of
a target group in a way that leads to exaggerated beliefs, we scrutinize LLM
responses through this heuristics' lens. We conduct experiments to determine
how LLMs inflate predictions about political parties, which results in
stereotyping. We find that while LLMs can mimic certain political parties'
positions, they often exaggerate these positions more than human survey
respondents do. Also, LLMs tend to overemphasize representativeness more than
humans. This study highlights the susceptibility of LLMs to representativeness
heuristics, suggesting a potential vulnerability of LLMs that facilitates
political stereotyping. We also test prompt-based mitigation strategies,
finding that strategies that can mitigate representative heuristics in humans
are also effective in reducing the influence of representativeness on
LLM-generated responses.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FACT-AUDIT: An Adaptive Multi-Agent Framework for Dynamic Fact-Checking
  Evaluation of Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.17924v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.17924v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hongzhan Lin, Yang Deng, Yuxuan Gu, Wenxuan Zhang, Jing Ma, See-Kiong Ng, Tat-Seng Chua
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have significantly advanced the fact-checking
studies. However, existing automated fact-checking evaluation methods rely on
static datasets and classification metrics, which fail to automatically
evaluate the justification production and uncover the nuanced limitations of
LLMs in fact-checking. In this work, we introduce FACT-AUDIT, an agent-driven
framework that adaptively and dynamically assesses LLMs' fact-checking
capabilities. Leveraging importance sampling principles and multi-agent
collaboration, FACT-AUDIT generates adaptive and scalable datasets, performs
iterative model-centric evaluations, and updates assessments based on
model-specific responses. By incorporating justification production alongside
verdict prediction, this framework provides a comprehensive and evolving audit
of LLMs' factual reasoning capabilities, to investigate their trustworthiness.
Extensive experiments demonstrate that FACT-AUDIT effectively differentiates
among state-of-the-art LLMs, providing valuable insights into model strengths
and limitations in model-centric fact-checking analysis.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Adaptive In-conversation Team Building for Language Model Agents 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.19425v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.19425v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Linxin Song, Jiale Liu, Jieyu Zhang, Shaokun Zhang, Ao Luo, Shijian Wang, Qingyun Wu, Chi Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Leveraging multiple large language model (LLM) agents has shown to be a
promising approach for tackling complex tasks, while the effective design of
multiple agents for a particular application remains an art. It is thus
intriguing to answer a critical question: Given a task, how can we build a team
of LLM agents to solve it effectively? Our new adaptive team-building paradigm
offers a flexible solution, realized through a novel agent design named Captain
Agent. It dynamically forms and manages teams for each step of a task-solving
process, utilizing nested group conversations and reflection to ensure diverse
expertise and prevent stereotypical outputs, allowing for a flexible yet
structured approach to problem-solving. A comprehensive evaluation across six
real-world scenarios demonstrates that Captain Agent significantly outperforms
existing multi-agent methods with 21.94% improvement in average accuracy,
providing outstanding performance without requiring task-specific prompt
engineering. Our exploration of different backbone LLM and cost analysis
further shows that Captain Agent can improve the conversation quality of weak
LLM and achieve competitive performance with extremely low cost, which
illuminates the application of multi-agent systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SeqAR: Jailbreak LLMs with Sequential Auto-Generated Characters <span class="chip">NAACL 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.01902v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.01902v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yan Yang, Zeguan Xiao, Xin Lu, Hongru Wang, Xuetao Wei, Hailiang Huang, Guanhua Chen, Yun Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The widespread applications of large language models (LLMs) have brought
about concerns regarding their potential misuse. Although aligned with human
preference data before release, LLMs remain vulnerable to various malicious
attacks. In this paper, we adopt a red-teaming strategy to enhance LLM safety
and introduce SeqAR, a simple yet effective framework to design jailbreak
prompts automatically. The SeqAR framework generates and optimizes multiple
jailbreak characters and then applies sequential jailbreak characters in a
single query to bypass the guardrails of the target LLM. Different from
previous work which relies on proprietary LLMs or seed jailbreak templates
crafted by human expertise, SeqAR can generate and optimize the jailbreak
prompt in a cold-start scenario using open-sourced LLMs without any seed
jailbreak templates. Experimental results show that SeqAR achieves attack
success rates of 88% and 60% in bypassing the safety alignment of GPT-3.5-1106
and GPT-4, respectively. Furthermore, we extensively evaluate the
transferability of the generated templates across different LLMs and held-out
malicious requests, while also exploring defense strategies against the
jailbreak attack designed by SeqAR.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by NAACL 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MACPO: Weak-to-Strong Alignment via Multi-Agent Contrastive Preference
  Optimization <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.07672v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.07672v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yougang Lyu, Lingyong Yan, Zihan Wang, Dawei Yin, Pengjie Ren, Maarten de Rijke, Zhaochun Ren
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As large language models (LLMs) are rapidly advancing and achieving
near-human capabilities on specific tasks, aligning them with human values is
becoming more urgent. In scenarios where LLMs outperform humans, we face a
weak-to-strong alignment problem where we need to effectively align strong
student LLMs through weak supervision generated by weak teachers. Existing
alignment methods mainly focus on strong-to-weak alignment and self-alignment
settings, and it is impractical to adapt them to the much harder weak-to-strong
alignment setting. To fill this gap, we propose a multi-agent contrastive
preference optimization (MACPO) framework. MACPO facilitates weak teachers and
strong students to learn from each other by iteratively reinforcing unfamiliar
positive behaviors while penalizing familiar negative ones. To get this, we
devise a mutual positive behavior augmentation strategy to encourage weak
teachers and strong students to learn from each other's positive behavior and
further provide higher quality positive behavior for the next iteration.
Additionally, we propose a hard negative behavior construction strategy to
induce weak teachers and strong students to generate familiar negative behavior
by fine-tuning on negative behavioral data. Experimental results on the HH-RLHF
and PKU-SafeRLHF datasets, evaluated using both automatic metrics and human
judgments, demonstrate that MACPO simultaneously improves the alignment
performance of strong students and weak teachers. Moreover, as the number of
weak teachers increases, MACPO achieves better weak-to-strong alignment
performance through more iteration optimization rounds.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ X-ALMA: Plug & Play Modules and Adaptive Rejection for Quality
  Translation at Scale <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.03115v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.03115v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haoran Xu, Kenton Murray, Philipp Koehn, Hieu Hoang, Akiko Eriguchi, Huda Khayrallah
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have achieved remarkable success across various
NLP tasks with a focus on English due to English-centric pre-training and
limited multilingual data. In this work, we focus on the problem of
translation, and while some multilingual LLMs claim to support for hundreds of
languages, models often fail to provide high-quality responses for mid- and
low-resource languages, leading to imbalanced performance heavily skewed in
favor of high-resource languages. We introduce **X-ALMA**, a model designed to
ensure top-tier performance across 50 diverse languages, regardless of their
resource levels. X-ALMA surpasses state-of-the-art open-source multilingual
LLMs, such as Aya-101 and Aya-23, in every single translation direction on the
FLORES-200 and WMT'23 test datasets according to COMET-22. This is achieved by
plug-and-play language-specific module architecture to prevent language
conflicts during training and a carefully designed training regimen with novel
optimization methods to maximize the translation performance. After the final
stage of training regimen, our proposed **A**daptive **R**ejection
**P**reference **O**ptimization (**ARPO**) surpasses existing preference
optimization methods in translation tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published as a conference paper at ICLR 2025 (spotlight)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MiLoRA: Harnessing Minor Singular Components for Parameter-Efficient LLM
  Finetuning <span class="chip">NAACL 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.09044v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.09044v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hanqing Wang, Yixia Li, Shuo Wang, Guanhua Chen, Yun Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Efficient finetuning of large language models (LLMs) aims to adapt the LLMs
with reduced computational and memory cost. Previous LoRA-based approaches
initialize the low-rank matrices with Gaussian distribution and zero values
while keeping the original weight matrices frozen. However, the trainable model
parameters optimized in an unguided subspace might interfere with the
well-learned subspace of the pretrained weight matrices. In this paper, we
propose MiLoRA, a simple yet effective LLM finetuning approach that only
updates the minor singular components of the weight matrix while keeping the
principal singular components frozen. It is observed that the minor matrix
corresponds to the noisy or long-tail information, while the principal matrix
contains important knowledge. The MiLoRA initializes the low-rank matrices
within a subspace that is orthogonal to the principal matrix, thus the
pretrained knowledge is expected to be well preserved. During finetuning,
MiLoRA makes the most use of the less-optimized subspace for learning the
labeled dataset. Extensive experiments on commonsense reasoning, math
reasoning, instruction following and visual instruction following benchmarks
present the superior performance of our method.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper has been accepted at NAACL 2025. Code is available at:
  https://github.com/sufenlp/MiLoRA</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ L3Ms -- Lagrange Large Language Models <span class="chip">ICLR</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.21533v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.21533v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guneet S. Dhillon, Xingjian Shi, Yee Whye Teh, Alex Smola
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Supervised fine-tuning (SFT) and alignment of large language models (LLMs)
are key steps in providing a good user experience. However, the concept of an
appropriate alignment is inherently application-dependent, and current methods
often rely on heuristic choices to drive optimization. In this work, we
formulate SFT and alignment as a constrained optimization problem: the LLM is
fine-tuned on a task while being required to meet application-specific
requirements, without resorting to heuristics. To solve this, we propose
Lagrange Large Language Models (L3Ms), which employ logarithmic barriers to
enforce the constraints. This approach allows for the customization of L3Ms
across diverse applications while avoiding heuristic-driven processes. We
experimentally demonstrate the versatility and efficacy of L3Ms in achieving
tailored alignments for various applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>International Conference on Learning Representations (ICLR), 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ An Empirical Analysis of Uncertainty in Large Language Model Evaluations <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.10709v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.10709v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qiujie Xie, Qingqiu Li, Zhuohao Yu, Yuejie Zhang, Yue Zhang, Linyi Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As LLM-as-a-Judge emerges as a new paradigm for assessing large language
models (LLMs), concerns have been raised regarding the alignment, bias, and
stability of LLM evaluators. While substantial work has focused on alignment
and bias, little research has concentrated on the stability of LLM evaluators.
In this paper, we conduct extensive experiments involving 9 widely used LLM
evaluators across 2 different evaluation settings to investigate the
uncertainty in model-based LLM evaluations. We pinpoint that LLM evaluators
exhibit varying uncertainty based on model families and sizes. With careful
comparative analyses, we find that employing special prompting strategies,
whether during inference or post-training, can alleviate evaluation uncertainty
to some extent. By utilizing uncertainty to enhance LLM's reliability and
detection capability in Out-Of-Distribution (OOD) data, we further fine-tune an
uncertainty-aware LLM evaluator named ConfiLM using a human-annotated
fine-tuning set and assess ConfiLM's OOD evaluation ability on a manually
designed test set sourced from the 2024 Olympics. Experimental results
demonstrate that incorporating uncertainty as additional information during the
fine-tuning phase can largely improve the model's evaluation performance in OOD
scenarios. The code and data are released at:
https://github.com/hasakiXie123/LLM-Evaluator-Uncertainty.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A <span class="highlight-title">Survey</span> on Large Language Model based Autonomous Agents 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.11432v7">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.11432v7.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao Yang, Jingsen Zhang, Zhiyuan Chen, Jiakai Tang, Xu Chen, Yankai Lin, Wayne Xin Zhao, Zhewei Wei, Ji-Rong Wen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Autonomous agents have long been a prominent research focus in both academic
and industry communities. Previous research in this field often focuses on
training agents with limited knowledge within isolated environments, which
diverges significantly from human learning processes, and thus makes the agents
hard to achieve human-like decisions. Recently, through the acquisition of vast
amounts of web knowledge, large language models (LLMs) have demonstrated
remarkable potential in achieving human-level intelligence. This has sparked an
upsurge in studies investigating LLM-based autonomous agents. In this paper, we
present a comprehensive survey of these studies, delivering a systematic review
of the field of LLM-based autonomous agents from a holistic perspective. More
specifically, we first discuss the construction of LLM-based autonomous agents,
for which we propose a unified framework that encompasses a majority of the
previous work. Then, we present a comprehensive overview of the diverse
applications of LLM-based autonomous agents in the fields of social science,
natural science, and engineering. Finally, we delve into the evaluation
strategies commonly used for LLM-based autonomous agents. Based on the previous
studies, we also present several challenges and future directions in this
field. To keep track of this field and continuously update our survey, we
maintain a repository of relevant references at
https://github.com/Paitesanshi/LLM-Agent-Survey.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Correcting several typos, 35 pages, 5 figures, 3 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Generalization v.s. Memorization: Tracing Language Models' Capabilities
  Back to <span class="highlight-title">Pretrain</span>ing Data <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.14985v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.14985v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinyi Wang, Antonis Antoniades, Yanai Elazar, Alfonso Amayuelas, Alon Albalak, Kexun Zhang, William Yang Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The impressive capabilities of large language models (LLMs) have sparked
debate over whether these models genuinely generalize to unseen tasks or
predominantly rely on memorizing vast amounts of pretraining data. To explore
this issue, we introduce an extended concept of memorization, distributional
memorization, which measures the correlation between the LLM output
probabilities and the pretraining data frequency. To effectively capture
task-specific pretraining data frequency, we propose a novel task-gram language
model, which is built by counting the co-occurrence of semantically related
$n$-gram pairs from task inputs and outputs in the pretraining corpus. Using
the Pythia models trained on the Pile dataset, we evaluate four distinct tasks:
machine translation, factual question answering, world knowledge understanding,
and math reasoning. Our findings reveal varying levels of memorization, with
the strongest effect observed in factual question answering. Furthermore, while
model performance improves across all tasks as LLM size increases, only factual
question answering shows an increase in memorization, whereas machine
translation and reasoning tasks exhibit greater generalization, producing more
novel outputs. This study demonstrates that memorization plays a larger role in
simpler, knowledge-intensive tasks, while generalization is the key for harder,
reasoning-based tasks, providing a scalable method for analyzing large
pretraining corpora in greater depth.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ICLR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Mutual Enhancement of Large Language and Reinforcement Learning Models
  through Bi-Directional Feedback Mechanisms: A Planning Case Study 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.06603v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.06603v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shangding Gu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have demonstrated remarkable capabilities for
reinforcement learning (RL) models, such as planning and reasoning
capabilities. However, the problems of LLMs and RL model collaboration still
need to be solved. In this study, we employ a teacher-student learning
framework to tackle these problems, specifically by offering feedback for LLMs
using RL models and providing high-level information for RL models with LLMs in
a cooperative multi-agent setting. Within this framework, the LLM acts as a
teacher, while the RL model acts as a student. The two agents cooperatively
assist each other through a process of recursive help, such as "I help you help
I help." The LLM agent supplies abstract information to the RL agent, enabling
efficient exploration and policy improvement. In turn, the RL agent offers
feedback to the LLM agent, providing valuable, real-time information that helps
generate more useful tokens. This bi-directional feedback loop promotes
optimization, exploration, and mutual improvement for both agents, enabling
them to accomplish increasingly challenging tasks. Remarkably, we propose a
practical algorithm to address the problem and conduct empirical experiments to
evaluate the effectiveness of our method.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ VisRAG: Vision-based Retrieval-augmented Generation on Multi-modality
  Documents 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.10594v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.10594v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shi Yu, Chaoyue Tang, Bokai Xu, Junbo Cui, Junhao Ran, Yukun Yan, Zhenghao Liu, Shuo Wang, Xu Han, Zhiyuan Liu, Maosong Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Retrieval-augmented generation (RAG) is an effective technique that enables
large language models (LLMs) to utilize external knowledge sources for
generation. However, current RAG systems are solely based on text, rendering it
impossible to utilize vision information like layout and images that play
crucial roles in real-world multi-modality documents. In this paper, we
introduce VisRAG, which tackles this issue by establishing a vision-language
model (VLM)-based RAG pipeline. In this pipeline, instead of first parsing the
document to obtain text, the document is directly embedded using a VLM as an
image and then retrieved to enhance the generation of a VLM. Compared to
traditional text-based RAG, VisRAG maximizes the retention and utilization of
the data information in the original documents, eliminating the information
loss introduced during the parsing process. We collect both open-source and
synthetic data to train the retriever in VisRAG and explore a variety of
generation methods. Experiments demonstrate that VisRAG outperforms traditional
RAG in both the retrieval and generation stages, achieving a 20--40% end-to-end
performance gain over traditional text-based RAG pipeline. Further analysis
reveals that VisRAG is efficient in utilizing training data and demonstrates
strong generalization capability, positioning it as a promising solution for
RAG on multi-modality documents. Our code and data are available at
https://github.com/openbmb/visrag.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ InstructRAG: Instructing Retrieval-Augmented Generation via
  Self-Synthesized Rationales <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.13629v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.13629v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhepei Wei, Wei-Lin Chen, Yu Meng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Retrieval-augmented generation (RAG) has shown promising potential to enhance
the accuracy and factuality of language models (LMs). However, imperfect
retrievers or noisy corpora can introduce misleading or even erroneous
information to the retrieved contents, posing a significant challenge to the
generation quality. Existing RAG methods typically address this challenge by
directly predicting final answers despite potentially noisy inputs, resulting
in an implicit denoising process that is difficult to interpret and verify. On
the other hand, the acquisition of explicit denoising supervision is often
costly, involving significant human efforts. In this work, we propose
InstructRAG, where LMs explicitly learn the denoising process through
self-synthesized rationales -- First, we instruct the LM to explain how the
ground-truth answer is derived from retrieved documents. Then, these rationales
can be used either as demonstrations for in-context learning of explicit
denoising or as supervised fine-tuning data to train the model. Compared to
standard RAG approaches, InstructRAG requires no additional supervision, allows
for easier verification of the predicted answers, and effectively improves
generation accuracy. Experiments show InstructRAG consistently outperforms
existing RAG methods in both training-free and trainable scenarios, achieving a
relative improvement of 8.3% over the best baseline method on average across
five knowledge-intensive benchmarks. Extensive analysis indicates that
InstructRAG scales well with increased numbers of retrieved documents and
consistently exhibits robust denoising ability even in out-of-domain datasets,
demonstrating strong generalizability.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2025. Code: https://github.com/weizhepei/InstructRAG</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2025-03-01T00:00:00Z">2025-03-01</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Computation and Language
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Masked Mixers for Language Generation and Retrieval 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.01482v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.01482v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Benjamin L. Badger
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Attention mechanisms that confer selective focus on a strict subset of input
elements are nearly ubiquitous in language models today. We posit there to be
downside to the use of attention: most input information is lost. In support of
this idea we observe poor input representation accuracy in transformers and
more accurate representation in what we term masked mixers, which replace
self-attention with masked convolutions. The masked mixer learns causal
language modeling more efficiently than early transformer implementations and
even outperforms optimized, current transformers when training on small (<512)
but not larger context windows. Evidence is presented for the hypothesis that
differences in transformer and masked mixer training efficiencies for various
tasks are best predicted by input representation accuracy, or equivalently
global invertibility. We hypothesize that the information loss exhibited by
transformers would be more detrimental to retrieval than generation, as the
former is more closely approximated by a bijective and thus invertible
function. We find that masked mixers are more effective retrieval models both
when the pretrained embedding model is unchanged as well as when the embedding
model is modified via cosine similarity-based InfoNCE loss minimization. A
small masked mixer is shown to outperform a large and near state-of-the-art
transformer-based retrieval model, despite the latter being trained with many
orders of magnitude more data and compute.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>31 pages, 8 figures, 3 tables, 9 supplementary figures, 13
  supplementary tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DFlow: Diverse Dialogue Flow Simulation with Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14853v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14853v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wanyu Du, Song Feng, James Gung, Lijia Sun, Yi Zhang, Saab Mansour, Yanjun Qi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Developing language model-based dialogue agents requires effective data to
train models that can follow specific task logic. However, most existing data
simulation methods focus on increasing diversity in language, topics, or
dialogue acts at the utterance level, largely neglecting a critical aspect of
task logic diversity at the dialogue level. This paper proposes a novel data
simulation method designed to enhance the diversity of synthetic dialogues by
focusing on task execution logic. Our method uses LLMs to generate decision
tree-structured task plans, which enables the derivation of diverse dialogue
trajectories for a given task. Each trajectory, referred to as a "dialog flow",
guides the generation of a multi-turn dialogue that follows a unique
trajectory. We apply this method to generate a task-oriented dialogue dataset
comprising 3,886 dialogue flows across 15 different domains. We validate the
effectiveness of this dataset using the next action prediction task, where
models fine-tuned on our dataset outperform strong baselines, including GPT-4.
Upon acceptance of this paper, we plan to release the code and data publicly.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MoDe<span class="highlight-title">GPT</span>: Modular Decomposition for Large Language Model Compression 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.09632v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.09632v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chi-Heng Lin, Shangqian Gao, James Seale Smith, Abhishek Patel, Shikhar Tuli, Yilin Shen, Hongxia Jin, Yen-Chang Hsu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have reshaped the landscape of artificial
intelligence by demonstrating exceptional performance across various tasks.
However, substantial computational requirements make their deployment
challenging on devices with limited resources. Recently, compression methods
using low-rank matrix techniques have shown promise, yet these often lead to
degraded accuracy or introduce significant overhead in parameters and inference
latency. This paper introduces \textbf{Mo}dular \textbf{De}composition
(MoDeGPT), a novel structured compression framework that does not need recovery
fine-tuning while resolving the above drawbacks. MoDeGPT partitions the
Transformer block into modules comprised of matrix pairs and reduces the hidden
dimensions via reconstructing the module-level outputs. MoDeGPT is developed
based on a theoretical framework that utilizes three well-established matrix
decomposition algorithms -- Nystr\"om approximation, CR decomposition, and SVD
-- and applies them to our redefined transformer modules. Our comprehensive
experiments show MoDeGPT, without backward propagation, matches or surpasses
previous structured compression methods that rely on gradient information, and
saves 98% of compute costs on compressing a 13B model. On \textsc{Llama}-2/3
and OPT models, MoDeGPT maintains 90-95% zero-shot performance with 25-30%
compression rates. Moreover, the compression can be done on a single GPU within
a few hours and increases the inference throughput by up to 46%.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>31 pages, 9 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CREAM: Consistency Regularized Self-Rewarding Language Models <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12735v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12735v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhaoyang Wang, Weilei He, Zhiyuan Liang, Xuchao Zhang, Chetan Bansal, Ying Wei, Weitong Zhang, Huaxiu Yao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent self-rewarding large language models (LLM) have successfully applied
LLM-as-a-Judge to iteratively improve the alignment performance without the
need of human annotations for preference data. These methods commonly utilize
the same LLM to act as both the policy model (which generates responses) and
the reward model (which scores and ranks those responses). The ranked responses
are then used as preference pairs to train the LLM via direct alignment
technologies (e.g. DPO). However, it is noteworthy that throughout this
process, there is no guarantee of accuracy in the rewarding and ranking, which
is critical for ensuring accurate rewards and high-quality preference data.
Empirical results from relatively small LLMs (e.g., 7B parameters) also
indicate that improvements from self-rewarding may diminish after several
iterations in certain situations, which we hypothesize is due to accumulated
bias in the reward system. This bias can lead to unreliable preference data for
training the LLM. To address this issue, we first formulate and analyze the
generalized iterative preference fine-tuning framework for self-rewarding
language model. We then introduce the regularization to this generalized
framework to mitigate the overconfident preference labeling in the
self-rewarding process. Based on this theoretical insight, we propose a
Consistency Regularized sElf-rewarding lAnguage Model (CREAM) that leverages
the consistency of rewards across different iterations to regularize the
self-rewarding training, helping the model to learn from more reliable
preference data. With this explicit regularization, our empirical results
demonstrate the superiority of CREAM in improving both reward consistency and
alignment performance. The code is publicly available at
https://github.com/Raibows/CREAM.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear at ICLR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning Task Decomposition to Assist Humans in Competitive Programming <span class="chip">ACL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.04604v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.04604v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiaxin Wen, Ruiqi Zhong, Pei Ke, Zhihong Shao, Hongning Wang, Minlie Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  When using language models (LMs) to solve complex problems, humans might
struggle to understand the LM-generated solutions and repair the flawed ones.
To assist humans in repairing them, we propose to automatically decompose
complex solutions into multiple simpler pieces that correspond to specific
subtasks. We introduce a novel objective for learning task decomposition,
termed assistive value (AssistV), which measures the feasibility and speed for
humans to repair the decomposed solution. We collect a dataset of human repair
experiences on different decomposed solutions. Utilizing the collected data as
in-context examples, we then learn to critique, refine, and rank decomposed
solutions to improve AssistV. We validate our method under competitive
programming problems: under 177 hours of human study, our method enables
non-experts to solve 33.3\% more problems, speeds them up by 3.3x, and empowers
them to match unassisted experts.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ACL 2024 Main Conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Retrieval-Augmented Generation for Natural Language Processing: A <span class="highlight-title">Survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.13193v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.13193v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shangyu Wu, Ying Xiong, Yufei Cui, Haolun Wu, Can Chen, Ye Yuan, Lianming Huang, Xue Liu, Tei-Wei Kuo, Nan Guan, Chun Jason Xue
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have demonstrated great success in various
fields, benefiting from their huge amount of parameters that store knowledge.
However, LLMs still suffer from several key issues, such as hallucination
problems, knowledge update issues, and lacking domain-specific expertise. The
appearance of retrieval-augmented generation (RAG), which leverages an external
knowledge database to augment LLMs, makes up those drawbacks of LLMs. This
paper reviews all significant techniques of RAG, especially in the retriever
and the retrieval fusions. Besides, tutorial codes are provided for
implementing the representative techniques in RAG. This paper further discusses
the RAG update, including RAG with/without knowledge update. Then, we introduce
RAG evaluation and benchmarking, as well as the application of RAG in
representative NLP tasks and industrial scenarios. Finally, this paper
discusses RAG's future directions and challenges for promoting this field's
development.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Revisiting Word Embeddings in the LLM Era 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.19607v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.19607v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yash Mahajan, Matthew Freestone, Sathyanarayanan Aakur, Santu Karmaker
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have recently shown remarkable advancement in
various NLP tasks. As such, a popular trend has emerged lately where NLP
researchers extract word/sentence/document embeddings from these large
decoder-only models and use them for various inference tasks with promising
results. However, it is still unclear whether the performance improvement of
LLM-induced embeddings is merely because of scale or whether underlying
embeddings they produce significantly differ from classical encoding models
like Word2Vec, GloVe, Sentence-BERT (SBERT) or Universal Sentence Encoder
(USE). This is the central question we investigate in the paper by
systematically comparing classical decontextualized and contextualized word
embeddings with the same for LLM-induced embeddings. Our results show that LLMs
cluster semantically related words more tightly and perform better on analogy
tasks in decontextualized settings. However, in contextualized settings,
classical models like SimCSE often outperform LLMs in sentence-level similarity
assessment tasks, highlighting their continued relevance for fine-grained
semantics.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This work was intended as a replacement of the older version,
  arXiv:2402.11094, and any subsequent updates will appear there</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Revisiting Word Embeddings in the LLM Era 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.11094v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.11094v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yash Mahajan, Matthew Freestone, Naman Bansal, Sathyanarayanan Aakur, Shubhra Kanti Karmaker Santu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have recently shown remarkable advancement in
various NLP tasks. As such, a popular trend has emerged lately where NLP
researchers extract word/sentence/document embeddings from these large
decoder-only models and use them for various inference tasks with promising
results. However, it is still unclear whether the performance improvement of
LLM-induced embeddings is merely because of scale or whether underlying
embeddings they produce significantly differ from classical encoding models
like Word2Vec, GloVe, Sentence-BERT (SBERT) or Universal Sentence Encoder
(USE). This is the central question we investigate in the paper by
systematically comparing classical decontextualized and contextualized word
embeddings with the same for LLM-induced embeddings. Our results show that LLMs
cluster semantically related words more tightly and perform better on analogy
tasks in decontextualized settings. However, in contextualized settings,
classical models like SimCSE often outperform LLMs in sentence-level similarity
assessment tasks, highlighting their continued relevance for fine-grained
semantics.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This is an updated version of the older version: 2402.11094. We
  accidentally submitted this article as a new submission (2502.19607), which
  we have requested to withdraw. This version has 30 pages and 22 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ TeaMs-RL: Teaching LLMs to Generate Better Instruction <span class="highlight-title">Dataset</span>s via
  Reinforcement Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.08694v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.08694v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shangding Gu, Alois Knoll, Ming Jin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The development of Large Language Models (LLMs) often confronts challenges
stemming from the heavy reliance on human annotators in the reinforcement
learning with human feedback (RLHF) framework, or the frequent and costly
external queries tied to the self-instruct paradigm. In this work, we pivot to
Reinforcement Learning (RL) -- but with a twist. Diverging from the typical
RLHF, which refines LLMs following instruction data training, we use RL to
directly generate the foundational instruction dataset that alone suffices for
fine-tuning. Our method, TeaMs-RL, uses a suite of textual operations and
rules, prioritizing the diversification of training datasets. It facilitates
the generation of high-quality data without excessive reliance on external
advanced models, paving the way for a single fine-tuning step and negating the
need for subsequent RLHF stages. Our findings highlight key advantages of our
approach: reduced need for human involvement and fewer model queries (only
5.73% of the strong baseline's total), along with enhanced capabilities of LLMs
in crafting and comprehending complex instructions compared to strong
baselines, and substantially improved model privacy protection. Code is
available at the link: https://github.com/SafeRL-Lab/TeaMs-RL
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Instructional Segment Embedding: Improving LLM Safety with Instruction
  Hierarchy 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.09102v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.09102v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tong Wu, Shujian Zhang, Kaiqiang Song, Silei Xu, Sanqiang Zhao, Ravi Agrawal, Sathish Reddy Indurthi, Chong Xiang, Prateek Mittal, Wenxuan Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) are susceptible to security and safety threats,
such as prompt injection, prompt extraction, and harmful requests. One major
cause of these vulnerabilities is the lack of an instruction hierarchy. Modern
LLM architectures treat all inputs equally, failing to distinguish between and
prioritize various types of instructions, such as system messages, user
prompts, and data. As a result, lower-priority user prompts may override more
critical system instructions, including safety protocols. Existing approaches
to achieving instruction hierarchy, such as delimiters and instruction-based
training, do not address this issue at the architectural level. We introduce
the Instructional Segment Embedding (ISE) technique, inspired by BERT, to
modern large language models, which embeds instruction priority information
directly into the model. This approach enables models to explicitly
differentiate and prioritize various instruction types, significantly improving
safety against malicious prompts that attempt to override priority rules. Our
experiments on the Structured Query and Instruction Hierarchy benchmarks
demonstrate an average robust accuracy increase of up to 15.75% and 18.68%,
respectively. Furthermore, we observe an improvement in instruction-following
capability of up to 4.1% evaluated on AlpacaEval. Overall, our approach offers
a promising direction for enhancing the safety and effectiveness of LLM
architectures.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Self-correction is Not An Innate Capability in Large Language Models: A
  Case Study of Moral Self-correction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.20513v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.20513v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zimo Qi, Guangliang Liu, Kristen Marie Johnson, Lu Cheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Though there has been intensive attention to the self-correction capability
of Large Language Models (LLMs), conclusions regarding its effectiveness remain
varied. In this paper, we investigate a fundamental question: is moral
self-correction an innate capability in LLMs? To explore this, we conduct (1) a
mechanistic analysis of how key components of self-correction, such as
Chain-of-Thought (CoT) reasoning and external feedback, interact to enable
moral self-correction; and (2) a behavioral analysis of LLMs' ability to
distinguish between desired and undesired outputs, introducing a
self-distinguish framework. Our mechanistic analysis reveals that LLMs struggle
to effectively leverage helpful feedback, and conflicts can arise between
feedback and CoT reasoning. These limitations suggest that LLMs fail to
identify useful contextual information, instead prioritizing their own internal
knowledge. Additionally, our behavioral analysis indicates that LLMs struggle
to differentiate among their own outputs. Based on these empirical findings
across two analytical dimensions, mechanism and behavior, we argue that moral
self-correction is not an innate capability of LLMs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Dual Process Learning: Controlling Use of In-Context vs. In-Weights
  Strategies with Weight Forgetting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.00053v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.00053v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Suraj Anand, Michael A. Lepori, Jack Merullo, Ellie Pavlick
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Language models have the ability to perform in-context learning (ICL),
allowing them to flexibly adapt their behavior based on context. This contrasts
with in-weights learning (IWL), where memorized information is encoded in model
parameters after iterated observations of data. An ideal model should be able
to flexibly deploy both of these abilities. Despite their apparent ability to
learn in-context, language models are known to struggle when faced with unseen
or rarely seen tokens (Land & Bartolo, 2024). Hence, we study
$\textbf{structural in-context learning}$, which we define as the ability of a
model to execute in-context learning on arbitrary novel tokens -- so called
because the model must generalize on the basis of e.g. sentence structure or
task structure, rather than content encoded in token embeddings. We study
structural in-context algorithms on both synthetic and naturalistic tasks using
toy models, masked language models, and autoregressive language models. We find
that structural ICL appears before quickly disappearing early in LM
pretraining. While it has been shown that ICL can diminish during training
(Singh et al., 2023), we find that prior work does not account for structural
ICL. Building on Chen et al. (2024) 's active forgetting method, we introduce
pretraining and finetuning methods that can modulate the preference for
structural ICL and IWL. Importantly, this allows us to induce a $\textit{dual
process strategy}$ where in-context and in-weights solutions coexist within a
single model.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Improving Complex Reasoning over Knowledge Graph with Logic-Aware
  Curriculum Tuning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.01649v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.01649v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tianle Xia, Liang Ding, Guojia Wan, Yibing Zhan, Bo Du, Dacheng Tao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Answering complex queries over incomplete knowledge graphs (KGs) is a
challenging job. Most previous works have focused on learning entity/relation
embeddings and simulating first-order logic operators with various neural
networks. However, they are bottlenecked by the inability to share world
knowledge to improve logical reasoning, thus resulting in suboptimal
performance. In this paper, we propose a complex reasoning schema over KG upon
large language models (LLMs), containing a curriculum-based logical-aware
instruction tuning framework, named LACT. Specifically, we augment the
arbitrary first-order logical queries via binary tree decomposition, to
stimulate the reasoning capability of LLMs. To address the difficulty gap among
different types of complex queries, we design a simple and flexible logic-aware
curriculum learning framework. Experiments across widely used datasets
demonstrate that LACT has substantial improvements~(brings an average +5.5% MRR
score) over advanced methods, achieving the new state-of-the-art.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Cheems: A Practical Guidance for Building and Evaluating Chinese Reward
  Models from Scratch 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.17173v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.17173v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xueru Wen, Jie Lou, Zichao Li, Yaojie Lu, Xing Yu, Yuqiu Ji, Guohai Xu, Hongyu Lin, Ben He, Xianpei Han, Le Sun, Debing Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reward models (RMs) are crucial for aligning large language models (LLMs)
with human preferences. However, most RM research is centered on English and
relies heavily on synthetic resources, which leads to limited and less reliable
datasets and benchmarks for Chinese. To address this gap, we introduce
CheemsBench, a fully human-annotated RM evaluation benchmark within Chinese
contexts, and CheemsPreference, a large-scale and diverse preference dataset
annotated through human-machine collaboration to support Chinese RM training.
We systematically evaluate open-source discriminative and generative RMs on
CheemsBench and observe significant limitations in their ability to capture
human preferences in Chinese scenarios. Additionally, based on
CheemsPreference, we construct an RM that achieves state-of-the-art performance
on CheemsBench, demonstrating the necessity of human supervision in RM
training. Our findings reveal that scaled AI-generated data struggles to fully
capture human preferences, emphasizing the importance of high-quality human
supervision in RM development.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ JudgeLM: Fine-tuned Large Language Models are Scalable Judges <span class="chip">ICLR2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.17631v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.17631v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lianghui Zhu, Xinggang Wang, Xinlong Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Evaluating Large Language Models (LLMs) in open-ended scenarios is
challenging because existing benchmarks and metrics can not measure them
comprehensively. To address this problem, we propose to fine-tune LLMs as
scalable judges (JudgeLM) to evaluate LLMs efficiently and effectively in
open-ended benchmarks. We first propose a comprehensive, large-scale,
high-quality dataset containing task seeds, LLMs-generated answers, and
GPT-4-generated judgments for fine-tuning high-performance judges, as well as a
new benchmark for evaluating the judges. We train JudgeLM at different scales
from 7B, 13B, to 33B parameters, and conduct a systematic analysis of its
capabilities and behaviors. We then analyze the key biases in fine-tuning LLM
as a judge and consider them as position bias, knowledge bias, and format bias.
To address these issues, JudgeLM introduces a bag of techniques including swap
augmentation, reference support, and reference drop, which clearly enhance the
judge's performance. JudgeLM obtains the state-of-the-art judge performance on
both the existing PandaLM benchmark and our proposed new benchmark. Our JudgeLM
is efficient and the JudgeLM-7B only needs 3 minutes to judge 5K samples with 8
A100 GPUs. JudgeLM obtains high agreement with the teacher judge, achieving an
agreement exceeding 90% that even surpasses human-to-human agreement. JudgeLM
also demonstrates extended capabilities in being judges of the single answer,
multimodal models, multiple answers, multi-turn chat, etc. Code is available at
https://github.com/baaivision/JudgeLM.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>JudgeLM is accepted by ICLR2025. Code is available at
  https://github.com/baaivision/JudgeLM</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ On Limitations of LLM as Annotator for Low Resource Languages 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.17637v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.17637v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Suramya Jadhav, Abhay Shanbhag, Amogh Thakurdesai, Ridhima Sinare, Raviraj Joshi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Low-resource languages face significant challenges due to the lack of
sufficient linguistic data, resources, and tools for tasks such as supervised
learning, annotation, and classification. This shortage hinders the development
of accurate models and datasets, making it difficult to perform critical NLP
tasks like sentiment analysis or hate speech detection. To bridge this gap,
Large Language Models (LLMs) present an opportunity for potential annotators,
capable of generating datasets and resources for these underrepresented
languages. In this paper, we focus on Marathi, a low-resource language, and
evaluate the performance of both closed-source and open-source LLMs as
annotators, while also comparing these results with fine-tuned BERT models. We
assess models such as GPT-4o and Gemini 1.0 Pro, Gemma 2 (2B and 9B), and Llama
3.1 (8B and 405B) on classification tasks including sentiment analysis, news
classification, and hate speech detection. Our findings reveal that while LLMs
excel in annotation tasks for high-resource languages like English, they still
fall short when applied to Marathi. Even advanced models like GPT-4o and Llama
3.1 405B underperform compared to fine-tuned BERT-based baselines, with GPT-4o
and Llama 3.1 405B trailing fine-tuned BERT by accuracy margins of 10.2% and
14.1%, respectively. This highlights the limitations of LLMs as annotators for
low-resource languages.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CoT2Align: Cross-Chain of Thought Distillation via Optimal Transport
  Alignment for Language Models with Different Tokenizers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.16806v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.16806v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anh Duc Le, Tu Vu, Nam Le Hai, Nguyen Thi Ngoc Diep, Linh Ngo Van, Trung Le, Thien Huu Nguyen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) achieve state-of-the-art performance across
various NLP tasks but face deployment challenges due to high computational
costs and memory constraints. Knowledge distillation (KD) is a promising
solution, transferring knowledge from large teacher models to smaller student
models. However, existing KD methods often assume shared vocabularies and
tokenizers, limiting their flexibility. While approaches like Universal Logit
Distillation (ULD) and Dual-Space Knowledge Distillation (DSKD) address
vocabulary mismatches, they overlook the critical \textbf{reasoning-aware
distillation} aspect. To bridge this gap, we propose CoT2Align a universal KD
framework that integrates Chain-of-Thought (CoT) augmentation and introduces
Cross-CoT Alignment to enhance reasoning transfer. Additionally, we extend
Optimal Transport beyond token-wise alignment to a sequence-level and
layer-wise alignment approach that adapts to varying sequence lengths while
preserving contextual integrity. Comprehensive experiments demonstrate that
CoT2Align outperforms existing KD methods across different vocabulary settings,
improving reasoning capabilities and robustness in domain-specific tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ QAEA-DR: A Unified Text Augmentation Framework for Dense Retrieval 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.20207v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.20207v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hongming Tan, Shaoxiong Zhan, Hai Lin, Hai-Tao Zheng, Wai Kin Chan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In dense retrieval, embedding long texts into dense vectors can result in
information loss, leading to inaccurate query-text matching. Additionally,
low-quality texts with excessive noise or sparse key information are unlikely
to align well with relevant queries. Recent studies mainly focus on improving
the sentence embedding model or retrieval process. In this work, we introduce a
novel text augmentation framework for dense retrieval. This framework
transforms raw documents into information-dense text formats, which supplement
the original texts to effectively address the aforementioned issues without
modifying embedding or retrieval methodologies. Two text representations are
generated via large language models (LLMs) zero-shot prompting: question-answer
pairs and element-driven events. We term this approach QAEA-DR: unifying
question-answer generation and event extraction in a text augmentation
framework for dense retrieval. To further enhance the quality of generated
texts, a scoring-based evaluation and regeneration mechanism is introduced in
LLM prompting. Our QAEA-DR model has a positive impact on dense retrieval,
supported by both theoretical analysis and empirical experiments.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ From RAGs to riches: Utilizing large language models to write documents
  for clinical trials 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.16406v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.16406v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nigel Markey, Ilyass El-Mansouri, Gaetan Rensonnet, Casper van Langen, Christoph Meier
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This manuscript has now been published: - Link to article on journal website:
https://journals.sagepub.com/doi/10.1177/17407745251320806 - Pubmed link:
https://pubmed.ncbi.nlm.nih.gov/40013826/
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 2 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LLaMA-Omni: Seamless Speech Interaction with Large Language Models <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.06666v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.06666v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qingkai Fang, Shoutao Guo, Yan Zhou, Zhengrui Ma, Shaolei Zhang, Yang Feng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Models like GPT-4o enable real-time interaction with large language models
(LLMs) through speech, significantly enhancing user experience compared to
traditional text-based interaction. However, there is still a lack of
exploration on how to build speech interaction models based on open-source
LLMs. To address this, we propose LLaMA-Omni, a novel model architecture
designed for low-latency and high-quality speech interaction with LLMs.
LLaMA-Omni integrates a pretrained speech encoder, a speech adaptor, an LLM,
and a streaming speech decoder. It eliminates the need for speech
transcription, and can simultaneously generate text and speech responses
directly from speech instructions with extremely low latency. We build our
model based on the latest Llama-3.1-8B-Instruct model. To align the model with
speech interaction scenarios, we construct a dataset named InstructS2S-200K,
which includes 200K speech instructions and corresponding speech responses.
Experimental results show that compared to previous speech-language models,
LLaMA-Omni provides better responses in both content and style, with a response
latency as low as 226ms. Additionally, training LLaMA-Omni takes less than 3
days on just 4 GPUs, paving the way for the efficient development of
speech-language models in the future.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FCoReBench: Can Large Language Models Solve Challenging First-Order
  Combinatorial Reasoning Problems? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.02611v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.02611v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chinmay Mittal, Krishna Kartik,  Mausam, Parag Singla
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Can the large language models (LLMs) solve challenging first-order
combinatorial reasoning problems such as graph coloring, knapsack, and
cryptarithmetic? By first-order, we mean these problems can be instantiated
with potentially an infinite number of problem instances of varying sizes. They
are also challenging being NP-hard and requiring several reasoning steps to
reach a solution. While existing work has focused on coming up with datasets
with hard benchmarks, there is limited work which exploits the first-order
nature of the problem structure. To address this challenge, we present
FCoReBench, a dataset of 40 such challenging problems, along with scripts to
generate problem instances of varying sizes and automatically verify and
generate their solutions. We first observe that LLMs, even when aided by
symbolic solvers, perform rather poorly on our dataset, being unable to
leverage the underlying structure of these problems. We specifically observe a
drop in performance with increasing problem size. In response, we propose a new
approach, SymPro-LM, which combines LLMs with both symbolic solvers and program
interpreters, along with feedback from a few solved examples, to achieve huge
performance gains. Our proposed approach is robust to changes in the problem
size, and has the unique characteristic of not requiring any LLM call during
inference time, unlike earlier approaches. As an additional experiment, we also
demonstrate SymPro-LM's effectiveness on other logical reasoning benchmarks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">Survey</span>ing the Landscape of Image Captioning Evaluation: A Comprehensive
  Taxonomy, Trends and Metrics Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.04909v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.04909v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Uri Berger, Gabriel Stanovsky, Omri Abend, Lea Frermann
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The task of image captioning has recently been gaining popularity, and with
it the complex task of evaluating the quality of image captioning models. In
this work, we present the first survey and taxonomy of over 70 different image
captioning metrics and their usage in hundreds of papers, specifically designed
to help users select the most suitable metric for their needs. We find that
despite the diversity of proposed metrics, the vast majority of studies rely on
only five popular metrics, which we show to be weakly correlated with human
ratings. We hypothesize that combining a diverse set of metrics can enhance
correlation with human ratings. As an initial step, we demonstrate that a
linear regression-based ensemble method, which we call EnsembEval, trained on
one human ratings dataset, achieves improved correlation across five additional
datasets, showing there is a lot of room for improvement by leveraging a
diverse set of metrics.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Who Wrote This? The Key to Zero-Shot LLM-Generated Text Detection Is
  GECScore <span class="chip">COLING 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.04286v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.04286v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junchao Wu, Runzhe Zhan, Derek F. Wong, Shu Yang, Xuebo Liu, Lidia S. Chao, Min Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The efficacy of detectors for texts generated by large language models (LLMs)
substantially depends on the availability of large-scale training data.
However, white-box zero-shot detectors, which require no such data, are limited
by the accessibility of the source model of the LLM-generated text. In this
paper, we propose a simple yet effective black-box zero-shot detection approach
based on the observation that, from the perspective of LLMs, human-written
texts typically contain more grammatical errors than LLM-generated texts. This
approach involves calculating the Grammar Error Correction Score (GECScore) for
the given text to differentiate between human-written and LLM-generated text.
Experimental results show that our method outperforms current state-of-the-art
(SOTA) zero-shot and supervised methods, achieving an average AUROC of 98.62%
across XSum and Writing Prompts dataset. Additionally, our approach
demonstrates strong reliability in the wild, exhibiting robust generalization
and resistance to paraphrasing attacks. Data and code are available at:
https://github.com/NLP2CT/GECScore.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>COLING 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ URO-Bench: A Comprehensive Benchmark for End-to-End Spoken Dialogue
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.17810v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.17810v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruiqi Yan, Xiquan Li, Wenxi Chen, Zhikang Niu, Chen Yang, Ziyang Ma, Kai Yu, Xie Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, with advances in large language models (LLMs), end-to-end
spoken dialogue models (SDMs) have made significant strides. Compared to
text-based LLMs, the evaluation of SDMs needs to take speech-related aspects
into account, such as paralinguistic information and speech quality. However,
there is still a lack of comprehensive evaluations for SDMs in speech-to-speech
(S2S) scenarios. To address this gap, we propose URO-Bench, an extensive
benchmark for SDMs. Notably, URO-Bench is the first S2S benchmark that covers
evaluations about multilingualism, multi-round dialogues, and paralinguistics.
Our benchmark is divided into two difficulty levels: basic track and pro track,
consisting of 16 and 20 datasets respectively, evaluating the model's abilities
in Understanding, Reasoning, and Oral conversation. Evaluations on our proposed
benchmark reveal that current open-source SDMs perform rather well in daily QA
tasks, but lag behind their backbone LLMs in terms of instruction-following
ability and also suffer from catastrophic forgetting. Their performance in
advanced evaluations of paralinguistic information and audio understanding
remains subpar, highlighting the need for further research in this direction.
We hope that URO-Bench can effectively facilitate the development of spoken
dialogue models by providing a multifaceted evaluation of existing models and
helping to track progress in this area.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Multi-objective Representation for Numbers in Clinical Narratives: A
  Camem<span class="highlight-title">BERT</span>-Bio-Based Alternative to Large-Scale LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.18448v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.18448v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Boammani Aser Lompo, Thanh-Dung Le
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The processing of numerical values is a rapidly developing area in the field
of Language Models (LLMs). Despite numerous advancements achieved by previous
research, significant challenges persist, particularly within the healthcare
domain. This paper investigates the limitations of Transformer models in
understanding numerical values. \textit{Objective:} this research aims to
categorize numerical values extracted from medical documents into eight
specific physiological categories using CamemBERT-bio. \textit{Methods:} In a
context where scalable methods and Large Language Models (LLMs) are emphasized,
we explore lifting the limitations of transformer-based models. We examine two
strategies: fine-tuning CamemBERT-bio on a small medical dataset, integrating
Label Embedding for Self-Attention (LESA), and combining LESA with additional
enhancement techniques such as Xval. Given that CamemBERT-bio is already
pre-trained on a large medical dataset, the first approach aims to update its
encoder with the newly added label embeddings technique. In contrast, the
second approach seeks to develop multiple representations of numbers
(contextual and magnitude-based) to achieve more robust number embeddings.
\textit{Results:} As anticipated, fine-tuning the standard CamemBERT-bio on our
small medical dataset did not improve F1 scores. However, significant
improvements were observed with CamemBERT-bio + LESA, resulting in an over 13\%
increase. Similar enhancements were noted when combining LESA with Xval,
outperforming conventional methods and giving comparable results to GPT-4
\textit{Conclusions and Novelty:} This study introduces two innovative
techniques for handling numerical data, which are also applicable to other
modalities. We illustrate how these techniques can improve the performance of
Transformer-based models, achieving more reliable classification results even
with small datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under the revision. arXiv admin note: substantial text overlap with
  arXiv:2404.10171</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ QA-Calibration of Language Model Confidence Scores 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.06615v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.06615v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Putra Manggala, Atalanti Mastakouri, Elke Kirschbaum, Shiva Prasad Kasiviswanathan, Aaditya Ramdas
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To use generative question-and-answering (QA) systems for decision-making and
in any critical application, these systems need to provide well-calibrated
confidence scores that reflect the correctness of their answers. Existing
calibration methods aim to ensure that the confidence score is, *on average*,
indicative of the likelihood that the answer is correct. We argue, however,
that this standard (average-case) notion of calibration is difficult to
interpret for decision-making in generative QA. To address this, we generalize
the standard notion of average calibration and introduce QA-calibration, which
ensures calibration holds across different question-and-answer groups. We then
propose discretized posthoc calibration schemes for achieving QA-calibration.
We establish distribution-free guarantees on the performance of this method and
validate our method on confidence scores returned by elicitation prompts across
multiple QA benchmarks and large language models (LLMs).
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MMKE-Bench: A Multimodal Editing Benchmark for Diverse Visual Knowledge <span class="chip">ICLR2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.19870v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.19870v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuntao Du, Kailin Jiang, Zhi Gao, Chenrui Shi, Zilong Zheng, Siyuan Qi, Qing Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Knowledge editing techniques have emerged as essential tools for updating the
factual knowledge of large language models (LLMs) and multimodal models (LMMs),
allowing them to correct outdated or inaccurate information without retraining
from scratch. However, existing benchmarks for multimodal knowledge editing
primarily focus on entity-level knowledge represented as simple triplets, which
fail to capture the complexity of real-world multimodal information. To address
this issue, we introduce MMKE-Bench, a comprehensive MultiModal Knowledge
Editing Benchmark, designed to evaluate the ability of LMMs to edit diverse
visual knowledge in real-world scenarios. MMKE-Bench addresses these
limitations by incorporating three types of editing tasks: visual entity
editing, visual semantic editing, and user-specific editing. Besides,
MMKE-Bench uses free-form natural language to represent and edit knowledge,
offering a more flexible and effective format. The benchmark consists of 2,940
pieces of knowledge and 8,363 images across 33 broad categories, with
evaluation questions automatically generated and human-verified. We assess five
state-of-the-art knowledge editing methods on three prominent LMMs, revealing
that no method excels across all criteria, and that visual and user-specific
edits are particularly challenging. MMKE-Bench sets a new standard for
evaluating the robustness of multimodal knowledge editing techniques, driving
progress in this rapidly evolving field.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accept to ICLR2025. Project Page: https://mmke-bench-iclr.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LongPO: Long Context Self-Evolution of Large Language Models through
  Short-to-Long Preference Optimization <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.13922v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.13922v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guanzheng Chen, Xin Li, Michael Qizhe Shieh, Lidong Bing
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have demonstrated remarkable capabilities
through pretraining and alignment. However, superior short-context LLMs may
underperform in long-context scenarios due to insufficient long-context
alignment. This alignment process remains challenging due to the impracticality
of human annotation for extended contexts and the difficulty in balancing
short- and long-context performance. To address these challenges, we introduce
LongPO, that enables short-context LLMs to self-evolve to excel on long-context
tasks by internally transferring short-context capabilities. LongPO harnesses
LLMs to learn from self-generated short-to-long preference data, comprising
paired responses generated for identical instructions with long-context inputs
and their compressed short-context counterparts, respectively. This preference
reveals capabilities and potentials of LLMs cultivated during short-context
alignment that may be diminished in under-aligned long-context scenarios.
Additionally, LongPO incorporates a short-to-long KL constraint to mitigate
short-context performance decline during long-context alignment. When applied
to Mistral-7B-Instruct-v0.2 from 128K to 512K context lengths, LongPO fully
retains short-context performance and largely outperforms naive SFT and DPO in
both long- and short-context tasks. Specifically, LongPO-trained models can
achieve results on long-context benchmarks comparable to, or even surpassing,
those of superior LLMs (e.g., GPT-4-128K) that involve extensive long-context
annotation and larger parameter scales. Our code is available at
https://github.com/DAMO-NLP-SG/LongPO.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Post-training an LLM for RAG? Train on Self-Generated Demonstrations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.10596v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.10596v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Matthew Finlayson, Ilia Kulikov, Daniel M. Bikel, Barlas Oguz, Xilun Chen, Aasish Pappu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) often struggle with knowledge intensive NLP
tasks, such as answering "Who won the latest World Cup?" because the knowledge
they learn during training may be insufficient or outdated. Conditioning
generation on retrieved documents -- a technique known as retrieval augmented
generation (RAG) -- mitigates these shortcomings by allowing the model to
leverage in-context information. Practitioners can improve LLM RAG performance
by fine-tuning on retrieval-augmented instructions, but must beware that this
can cause undesirable model behaviors like hallucinations. We attribute this
degradation to the fact that the training data is likely to be
out-of-distribution for the model and may suffer from quality issues, such as
misalignment between retrievals and target responses (since retrievals are
frequently added post-hoc). We propose a recipe for training RAG-enabled LLMs
using self-generated demonstrations, thereby avoiding training on
out-of-distribution text and integrating retrievals into the LLM responses. We
evaluate our method on knowledge intensive question answering (QA) tasks and
show that our method teaches LLMs to properly handle in-context retrievals and
abstain from questions it will likely get wrong. Compared to conventional RA-IT
methods, our method prevents model degradation in non-RAG settings while
exhibiting superior QA performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">Prompt</span>ing Medical Large Vision-Language Models to Diagnose Pathologies
  by Visual Question Answering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.21368v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.21368v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Danfeng Guo, Demetri Terzopoulos
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Vision-Language Models (LVLMs) have achieved significant success in
recent years, and they have been extended to the medical domain. Although
demonstrating satisfactory performance on medical Visual Question Answering
(VQA) tasks, Medical LVLMs (MLVLMs) suffer from the hallucination problem,
which makes them fail to diagnose complex pathologies. Moreover, they readily
fail to learn minority pathologies due to imbalanced training data. We propose
two prompting strategies for MLVLMs that reduce hallucination and improve VQA
performance. In the first strategy, we provide a detailed explanation of the
queried pathology. In the second strategy, we fine-tune a cheap, weak learner
to achieve high performance on a specific metric, and textually provide its
judgment to the MLVLM. Tested on the MIMIC-CXR-JPG and Chexpert datasets, our
methods significantly improve the diagnostic F1 score, with the highest
increase being 0.27. We also demonstrate that our prompting strategies can be
extended to general LVLM domains. Based on POPE metrics, it effectively
suppresses the false negative predictions of existing LVLMs and improves Recall
by approximately 0.07.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CORAL: Learning Consistent Representations across Multi-step Training
  with Lighter Speculative Drafter 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.16880v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.16880v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yepeng Weng, Dianwen Mei, Huishi Qiu, Xujie Chen, Li Liu, Jiang Tian, Zhongchao Shi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Speculative decoding is a powerful technique that accelerates Large Language
Model (LLM) inference by leveraging a lightweight speculative draft model.
However, existing designs suffers in performance due to misalignment between
training and inference. Recent methods have tried to solve this issue by
adopting a multi-step training strategy, but the complex inputs of different
training steps make it harder for the draft model to converge. To address this,
we propose CORAL, a novel framework that improves both accuracy and efficiency
in speculative drafting. CORAL introduces Cross-Step Representation Alignment,
a method that enhances consistency across multiple training steps,
significantly improving speculative drafting performance. Additionally, we
identify the LM head as a major bottleneck in the inference speed of the draft
model. We introduce a weight-grouping mechanism that selectively activates a
subset of LM head parameters during inference, substantially reducing the
latency of the draft model. We evaluate CORAL on three LLM families and three
benchmark datasets, achieving speedup ratios of 2.50x-4.07x, outperforming
state-of-the-art methods such as EAGLE-2 and HASS. Our results demonstrate that
CORAL effectively mitigates training-inference misalignment and delivers
significant speedup for modern LLMs with large vocabularies.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under Review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ s1: Simple test-time scaling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.19393v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.19393v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Niklas Muennighoff, Zitong Yang, Weijia Shi, Xiang Lisa Li, Li Fei-Fei, Hannaneh Hajishirzi, Luke Zettlemoyer, Percy Liang, Emmanuel Candès, Tatsunori Hashimoto
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Test-time scaling is a promising new approach to language modeling that uses
extra test-time compute to improve performance. Recently, OpenAI's o1 model
showed this capability but did not publicly share its methodology, leading to
many replication efforts. We seek the simplest approach to achieve test-time
scaling and strong reasoning performance. First, we curate a small dataset s1K
of 1,000 questions paired with reasoning traces relying on three criteria we
validate through ablations: difficulty, diversity, and quality. Second, we
develop budget forcing to control test-time compute by forcefully terminating
the model's thinking process or lengthening it by appending "Wait" multiple
times to the model's generation when it tries to end. This can lead the model
to double-check its answer, often fixing incorrect reasoning steps. After
supervised finetuning the Qwen2.5-32B-Instruct language model on s1K and
equipping it with budget forcing, our model s1-32B exceeds o1-preview on
competition math questions by up to 27% (MATH and AIME24). Further, scaling
s1-32B with budget forcing allows extrapolating beyond its performance without
test-time intervention: from 50% to 57% on AIME24. Our model, data, and code
are open-source at https://github.com/simplescaling/s1
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>46 pages (9 main), 10 figures, 15 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LABOR-LLM: Language-Based Occupational Representations with Large
  Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.17972v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.17972v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Susan Athey, Herman Brunborg, Tianyu Du, Ayush Kanodia, Keyon Vafa
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vafa et al. (2024) introduced a transformer-based econometric model, CAREER,
that predicts a worker's next job as a function of career history (an
"occupation model"). CAREER was initially estimated ("pre-trained") using a
large, unrepresentative resume dataset, which served as a "foundation model,"
and parameter estimation was continued ("fine-tuned") using data from a
representative survey. CAREER had better predictive performance than
benchmarks. This paper considers an alternative where the resume-based
foundation model is replaced by a large language model (LLM). We convert
tabular data from the survey into text files that resemble resumes and
fine-tune the LLMs using these text files with the objective to predict the
next token (word). The resulting fine-tuned LLM is used as an input to an
occupation model. Its predictive performance surpasses all prior models. We
demonstrate the value of fine-tuning and further show that by adding more
career data from a different population, fine-tuning smaller LLMs surpasses the
performance of fine-tuning larger models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Language Guided Skill Discovery 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.06615v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.06615v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Seungeun Rho, Laura Smith, Tianyu Li, Sergey Levine, Xue Bin Peng, Sehoon Ha
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Skill discovery methods enable agents to learn diverse emergent behaviors
without explicit rewards. To make learned skills useful for unknown downstream
tasks, obtaining a semantically diverse repertoire of skills is essential.
While some approaches introduce a discriminator to distinguish skills and
others aim to increase state coverage, no existing work directly addresses the
"semantic diversity" of skills. We hypothesize that leveraging the semantic
knowledge of large language models (LLMs) can lead us to improve semantic
diversity of resulting behaviors. In this sense, we introduce Language Guided
Skill Discovery (LGSD), a skill discovery framework that aims to directly
maximize the semantic diversity between skills. LGSD takes user prompts as
input and outputs a set of semantically distinctive skills. The prompts serve
as a means to constrain the search space into a semantically desired subspace,
and the generated LLM outputs guide the agent to visit semantically diverse
states within the subspace. We demonstrate that LGSD enables legged robots to
visit different user-intended areas on a plane by simply changing the prompt.
Furthermore, we show that language guidance aids in discovering more diverse
skills compared to five existing skill discovery methods in robot-arm
manipulation environments. Lastly, LGSD provides a simple way of utilizing
learned skills via natural language.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Semantic Operators: A Declarative Model for Rich, AI-based Data
  Processing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.11418v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.11418v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Liana Patel, Siddharth Jha, Melissa Pan, Harshit Gupta, Parth Asawa, Carlos Guestrin, Matei Zaharia
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The semantic capabilities of large language models (LLMs) have the potential
to enable rich analytics and reasoning over vast knowledge corpora.
Unfortunately, existing systems either empirically optimize expensive
LLM-powered operations with no performance guarantees, or serve a limited set
of row-wise LLM operations, providing limited robustness, expressiveness and
usability. We introduce semantic operators, the first formalism for declarative
and general-purpose AI-based transformations based on natural language
specifications (e.g., filtering, sorting, joining or aggregating records using
natural language criteria). Each operator opens a rich space for execution
plans, similar to relational operators. Our model specifies the expected
behavior of each operator with a high-quality gold algorithm, and we develop an
optimization framework that reduces cost, while providing accuracy guarantees
with respect to a gold algorithm. Using this approach, we propose several novel
optimizations to accelerate semantic filtering, joining, group-by and top-k
operations by up to $1,000\times$. We implement semantic operators in the LOTUS
system and demonstrate LOTUS' effectiveness on real, bulk-semantic processing
applications, including fact-checking, biomedical multi-label classification,
search, and topic analysis. We show that the semantic operator model is
expressive, capturing state-of-the-art AI pipelines in a few operator calls,
and making it easy to express new pipelines that match or exceed quality of
recent LLM-based analytic systems by up to $170\%$, while offering accuracy
guarantees. Overall, LOTUS programs match or exceed the accuracy of
state-of-the-art AI pipelines for each task while running up to $3.6\times$
faster than the highest-quality baselines. LOTUS is publicly available at
https://github.com/lotus-data/lotus.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ How Useful is Continued <span class="highlight-title">Pre-Train</span>ing for Generative Unsupervised Domain
  Adaptation? <span class="chip">RepL4NLP</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.17514v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.17514v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rheeya Uppaal, Yixuan Li, Junjie Hu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent breakthroughs in scale have enabled the emergence of powerful
generative language models, and the ability to fine-tune these models on
various tasks by casting them into prompts or instructions. In this landscape,
the problem of Unsupervised Domain Adaptation (UDA), or the problem of
leveraging knowledge from a labeled source domain to an unlabeled target
domain, has been left behind, with recent UDA methods still addressing
discriminative classification. In particular, two popular UDA approaches,
involving Continued Pre-Training (CPT) and learning domain invariant
representations, have been under-explored in the generative setting, signaling
a gap. In this work, we evaluate the utility of CPT for generative UDA. We
first perform an empirical evaluation to measure the trade-offs between CPT and
strong methods promoting domain invariance. We further evaluate how well the
benefits of CPT extend to different architectures, tuning methods and data
regimes. We then motivate the use of CPT by studying to what degree it benefits
classification performance on the target domain. Finally, we attempt to
understand the mechanism behind which CPT improves classification performance
on the unlabeled target domain. Our findings suggest that a implicitly learns
the downstream task while predicting masked words informative to that task. Our
work connects the body of UDA research with that of instruction tuning,
enabling an initial step towards a wider applicability of modern language
models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to RepL4NLP at ACL 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Model Editing as a Robust and Denoised variant of DPO: A Case Study on
  Toxicity <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.13967v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.13967v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rheeya Uppaal, Apratim Dey, Yiting He, Yiqiao Zhong, Junjie Hu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent alignment algorithms such as direct preference optimization (DPO) have
been developed to improve the safety of large language models (LLMs) by
training these models to match human behaviors exemplified by preference data.
However, these methods are both computationally intensive and lacking in
controllability and transparency, inhibiting their widespread use. Furthermore,
these tuning-based methods require large-scale preference data for training and
are susceptible to noisy preference data. In this paper, we introduce a
tuning-free alignment alternative, ProFS (Projection Filter for Subspaces), and
demonstrate its effectiveness under the use case of toxicity reduction.
Grounded on theory from factor analysis, ProFS is a sample-efficient model
editing approach that identifies a toxic subspace in the model parameter space
and reduces model toxicity by projecting away the detected subspace. The toxic
subspace is identified by extracting preference data embeddings from the
language model, and removing non-toxic information from these embeddings. We
show that ProFS is more sample-efficient than DPO, further showcasing greater
robustness to noisy data. Finally, we attempt to connect tuning based alignment
with editing, by establishing both theoretical and empirical connections
between ProFS and DPO, showing that ProFS can be interpreted as a denoised
version of a single DPO step.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ICLR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Building Trust in Mental Health Chatbots: Safety Metrics and LLM-Based
  Evaluation Tools 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.04650v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.04650v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jung In Park, Mahyar Abbasian, Iman Azimi, Dawn T. Bounds, Angela Jun, Jaesu Han, Robert M. McCarron, Jessica Borelli, Parmida Safavi, Sanaz Mirbaha, Jia Li, Mona Mahmoudi, Carmen Wiedenhoeft, Amir M. Rahmani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Objective: This study aims to develop and validate an evaluation framework to
ensure the safety and reliability of mental health chatbots, which are
increasingly popular due to their accessibility, human-like interactions, and
context-aware support. Materials and Methods: We created an evaluation
framework with 100 benchmark questions and ideal responses, and five guideline
questions for chatbot responses. This framework, validated by mental health
experts, was tested on a GPT-3.5-turbo-based chatbot. Automated evaluation
methods explored included large language model (LLM)-based scoring, an agentic
approach using real-time data, and embedding models to compare chatbot
responses against ground truth standards. Results: The results highlight the
importance of guidelines and ground truth for improving LLM evaluation
accuracy. The agentic method, dynamically accessing reliable information,
demonstrated the best alignment with human assessments. Adherence to a
standardized, expert-validated framework significantly enhanced chatbot
response safety and reliability. Discussion: Our findings emphasize the need
for comprehensive, expert-tailored safety evaluation metrics for mental health
chatbots. While LLMs have significant potential, careful implementation is
necessary to mitigate risks. The superior performance of the agentic approach
underscores the importance of real-time data access in enhancing chatbot
reliability. Conclusion: The study validated an evaluation framework for mental
health chatbots, proving its effectiveness in improving safety and reliability.
Future work should extend evaluations to accuracy, bias, empathy, and privacy
to ensure holistic assessment and responsible integration into healthcare.
Standardized evaluations will build trust among users and professionals,
facilitating broader adoption and improved mental health support through
technology.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Mitigating Paraphrase Attacks on Machine-Text Detectors via Paraphrase
  Inversion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.21637v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.21637v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rafael Rivera Soto, Barry Chen, Nicholas Andrews
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  High-quality paraphrases are easy to produce using instruction-tuned language
models or specialized paraphrasing models. Although this capability has a
variety of benign applications, paraphrasing
attacks$\unicode{x2013}$paraphrases applied to machine-generated
texts$\unicode{x2013}$are known to significantly degrade the performance of
machine-text detectors. This motivates us to consider the novel problem of
paraphrase inversion, where, given paraphrased text, the objective is to
recover an approximation of the original text. The closer the approximation is
to the original text, the better machine-text detectors will perform. We
propose an approach which frames the problem as translation from paraphrased
text back to the original text, which requires examples of texts and
corresponding paraphrases to train the inversion model. Fortunately, such
training data can easily be generated, given a corpus of original texts and one
or more paraphrasing models. We find that language models such as GPT-4 and
Llama-3 exhibit biases when paraphrasing which an inversion model can learn
with a modest amount of data. Perhaps surprisingly, we also find that such
models generalize well, including to paraphrase models unseen at training time.
Finally, we show that when combined with a paraphrased-text detector, our
inversion models provide an effective defense against paraphrasing attacks, and
overall our approach yields an average improvement of +22% AUROC across seven
machine-text detectors and three different domains.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2025-02-28T00:00:00Z">2025-02-28</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Computation and Language
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LLM Post-Training: A Deep Dive into Reasoning Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.21321v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.21321v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Komal Kumar, Tajamul Ashraf, Omkar Thawakar, Rao Muhammad Anwer, Hisham Cholakkal, Mubarak Shah, Ming-Hsuan Yang, Phillip H. S. Torr, Salman Khan, Fahad Shahbaz Khan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have transformed the natural language processing
landscape and brought to life diverse applications. Pretraining on vast
web-scale data has laid the foundation for these models, yet the research
community is now increasingly shifting focus toward post-training techniques to
achieve further breakthroughs. While pretraining provides a broad linguistic
foundation, post-training methods enable LLMs to refine their knowledge,
improve reasoning, enhance factual accuracy, and align more effectively with
user intents and ethical considerations. Fine-tuning, reinforcement learning,
and test-time scaling have emerged as critical strategies for optimizing LLMs
performance, ensuring robustness, and improving adaptability across various
real-world tasks. This survey provides a systematic exploration of
post-training methodologies, analyzing their role in refining LLMs beyond
pretraining, addressing key challenges such as catastrophic forgetting, reward
hacking, and inference-time trade-offs. We highlight emerging directions in
model alignment, scalable adaptation, and inference-time reasoning, and outline
future research directions. We also provide a public repository to continually
track developments in this fast-evolving field:
https://github.com/mbzuai-oryx/Awesome-LLM-Post-training.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>31 pages, 7 figures, 3 tables, 375 references</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Identifying Emerging Concepts in Large Corpora 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.21315v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.21315v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sibo Ma, Julian Nyarko
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce a new method to identify emerging concepts in large text
corpora. By analyzing changes in the heatmaps of the underlying embedding
space, we are able to detect these concepts with high accuracy shortly after
they originate, in turn outperforming common alternatives. We further
demonstrate the utility of our approach by analyzing speeches in the U.S.
Senate from 1941 to 2015. Our results suggest that the minority party is more
active in introducing new concepts into the Senate discourse. We also identify
specific concepts that closely correlate with the Senators' racial, ethnic, and
gender identities. An implementation of our method is publicly available.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FANformer: Improving Large Language Models Through Effective Periodicity
  Modeling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.21309v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.21309v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yihong Dong, Ge Li, Xue Jiang, Yongding Tao, Kechi Zhang, Hao Zhu, Huanyu Liu, Jiazheng Ding, Jia Li, Jinliang Deng, Hong Mei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Periodicity, as one of the most important basic characteristics, lays the
foundation for facilitating structured knowledge acquisition and systematic
cognitive processes within human learning paradigms. However, the potential
flaws of periodicity modeling in Transformer affect the learning efficiency and
establishment of underlying principles from data for large language models
(LLMs) built upon it. In this paper, we demonstrate that integrating effective
periodicity modeling can improve the learning efficiency and performance of
LLMs. We introduce FANformer, which integrates Fourier Analysis Network (FAN)
into attention mechanism to achieve efficient periodicity modeling, by
modifying the feature projection process of attention mechanism. Extensive
experimental results on language modeling show that FANformer consistently
outperforms Transformer when scaling up model size and training tokens,
underscoring its superior learning efficiency. To further validate the
effectiveness of FANformer, we pretrain a FANformer-1B on 1 trillion tokens.
FANformer-1B exhibits marked improvements on downstream tasks compared to
open-source LLMs with similar model parameters or training tokens. The results
position FANformer as an effective and promising architecture for advancing
LLMs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Persuasion Should be Double-Blind: A Multi-Domain Dialogue <span class="highlight-title">Dataset</span> With
  Faithfulness Based on Causal Theory of Mind 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.21297v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.21297v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dingyi Zhang, Deyu Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Persuasive dialogue plays a pivotal role in human communication, influencing
various domains. Recent persuasive dialogue datasets often fail to align with
real-world interpersonal interactions, leading to unfaithful representations.
For instance, unrealistic scenarios may arise, such as when the persuadee
explicitly instructs the persuader on which persuasion strategies to employ,
with each of the persuadee's questions corresponding to a specific strategy for
the persuader to follow. This issue can be attributed to a violation of the
"Double Blind" condition, where critical information is fully shared between
participants. In actual human interactions, however, key information such as
the mental state of the persuadee and the persuasion strategies of the
persuader is not directly accessible. The persuader must infer the persuadee's
mental state using Theory of Mind capabilities and construct arguments that
align with the persuadee's motivations. To address this gap, we introduce
ToMMA, a novel multi-agent framework for dialogue generation that is guided by
causal Theory of Mind. This framework ensures that information remains
undisclosed between agents, preserving "double-blind" conditions, while causal
ToM directs the persuader's reasoning, enhancing alignment with human-like
persuasion dynamics. Consequently, we present CToMPersu, a multi-domain,
multi-turn persuasive dialogue dataset that tackles both double-blind and
logical coherence issues, demonstrating superior performance across multiple
metrics and achieving better alignment with real human dialogues. Our dataset
and prompts are available at https://github.com/DingyiZhang/ToMMA-CToMPersu .
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>23pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Token-level Ensembling of Models with Different Vocabularies 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.21265v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.21265v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rachel Wicks, Kartik Ravisankar, Xinchen Yang, Philipp Koehn, Matt Post
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Model ensembling is a technique to combine the predicted distributions of two
or more models, often leading to improved robustness and performance. For
ensembling in text generation, the next token's probability distribution is
derived from a weighted sum of the distributions of each individual model. This
requires the underlying models to share the same subword vocabulary, limiting
the applicability of ensembling, since many open-sourced models have distinct
vocabularies. In research settings, experimentation or upgrades to vocabularies
may introduce multiple vocabulary sizes. This paper proposes an inference-time
only algorithm that allows for ensembling models with different vocabularies,
without the need to learn additional parameters or alter the underlying models.
Instead, the algorithm ensures that tokens generated by the ensembled models
\textit{agree} in their surface form. We apply this technique to combinations
of traditional encoder-decoder models and decoder-only LLMs and evaluate on
machine translation. In addition to expanding to model pairs that were
previously incapable of token-level ensembling, our algorithm frequently
improves translation performance over either model individually.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ RuCCoD: Towards Automated ICD Coding in Russian 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.21263v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.21263v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aleksandr Nesterov, Andrey Sakhovskiy, Ivan Sviridov, Airat Valiev, Vladimir Makharev, Petr Anokhin, Galina Zubkova, Elena Tutubalina
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study investigates the feasibility of automating clinical coding in
Russian, a language with limited biomedical resources. We present a new dataset
for ICD coding, which includes diagnosis fields from electronic health records
(EHRs) annotated with over 10,000 entities and more than 1,500 unique ICD
codes. This dataset serves as a benchmark for several state-of-the-art models,
including BERT, LLaMA with LoRA, and RAG, with additional experiments examining
transfer learning across domains (from PubMed abstracts to medical diagnosis)
and terminologies (from UMLS concepts to ICD codes). We then apply the
best-performing model to label an in-house EHR dataset containing patient
histories from 2017 to 2021. Our experiments, conducted on a carefully curated
test set, demonstrate that training with the automated predicted codes leads to
a significant improvement in accuracy compared to manually annotated data from
physicians. We believe our findings offer valuable insights into the potential
for automating clinical coding in resource-limited languages like Russian,
which could enhance clinical efficiency and data accuracy in these contexts.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Semantic Volume: Quantifying and Detecting both External and Internal
  Uncertainty in LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.21239v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.21239v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaomin Li, Zhou Yu, Ziji Zhang, Yingying Zhuang, Swair Shah, Anurag Beniwal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have demonstrated remarkable performance across
diverse tasks by encoding vast amounts of factual knowledge. However, they are
still prone to hallucinations, generating incorrect or misleading information,
often accompanied by high uncertainty. Existing methods for hallucination
detection primarily focus on quantifying internal uncertainty, which arises
from missing or conflicting knowledge within the model. However, hallucinations
can also stem from external uncertainty, where ambiguous user queries lead to
multiple possible interpretations. In this work, we introduce Semantic Volume,
a novel mathematical measure for quantifying both external and internal
uncertainty in LLMs. Our approach perturbs queries and responses, embeds them
in a semantic space, and computes the determinant of the Gram matrix of the
embedding vectors, capturing their dispersion as a measure of uncertainty. Our
framework provides a generalizable and unsupervised uncertainty detection
method without requiring white-box access to LLMs. We conduct extensive
experiments on both external and internal uncertainty detection, demonstrating
that our Semantic Volume method consistently outperforms existing baselines in
both tasks. Additionally, we provide theoretical insights linking our measure
to differential entropy, unifying and extending previous sampling-based
uncertainty measures such as the semantic entropy. Semantic Volume is shown to
be a robust and interpretable approach to improving the reliability of LLMs by
systematically detecting uncertainty in both user queries and model responses.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Transforming Tuberculosis Care: Optimizing Large Language Models For
  Enhanced Clinician-Patient Communication <span class="chip">AAAI-25</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.21236v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.21236v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daniil Filienko, Mahek Nizar, Javier Roberti, Denise Galdamez, Haroon Jakher, Sarah Iribarren, Weichao Yuwen, Martine De Cock
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Tuberculosis (TB) is the leading cause of death from an infectious disease
globally, with the highest burden in low- and middle-income countries. In these
regions, limited healthcare access and high patient-to-provider ratios impede
effective patient support, communication, and treatment completion. To bridge
this gap, we propose integrating a specialized Large Language Model into an
efficacious digital adherence technology to augment interactive communication
with treatment supporters. This AI-powered approach, operating within a
human-in-the-loop framework, aims to enhance patient engagement and improve TB
treatment outcomes.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>GenAI4Health at AAAI-25</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ECLeKTic: a Novel Challenge Set for Evaluation of Cross-Lingual
  Knowledge Transfer 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.21228v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.21228v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Omer Goldman, Uri Shaham, Dan Malkin, Sivan Eiger, Avinatan Hassidim, Yossi Matias, Joshua Maynez, Adi Mayrav Gilady, Jason Riesa, Shruti Rijhwani, Laura Rimell, Idan Szpektor, Reut Tsarfaty, Matan Eyal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To achieve equitable performance across languages, multilingual large
language models (LLMs) must be able to abstract knowledge beyond the language
in which it was acquired. However, the current literature lacks reliable ways
to measure LLMs' capability of cross-lingual knowledge transfer. To that end,
we present ECLeKTic, a multilingual closed-book QA (CBQA) dataset that
Evaluates Cross-Lingual Knowledge Transfer in a simple, black-box manner. We
detected information with uneven coverage across languages by controlling for
presence and absence of Wikipedia articles in 12 languages. We generated
knowledge-seeking questions in a source language, for which the answer appears
in a relevant Wikipedia article and translated them to all other 11 languages,
for which the respective Wikipedias lack equivalent articles. Assuming that
Wikipedia reflects the prominent knowledge in the LLM's training data, to solve
ECLeKTic's CBQA task the model is required to transfer knowledge between
languages. Experimenting with 8 LLMs, we show that SOTA models struggle to
effectively share knowledge across, languages even if they can predict the
answer well for queries in the same language the knowledge was acquired in.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Detecting Linguistic Diversity on Social Media 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.21224v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.21224v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sidney Wong, Benjamin Adams, Jonathan Dunn
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This chapter explores the efficacy of using social media data to examine
changing linguistic behaviour of a place. We focus our investigation on
Aotearoa New Zealand where official statistics from the census is the only
source of language use data. We use published census data as the ground truth
and the social media sub-corpus from the Corpus of Global Language Use as our
alternative data source. We use place as the common denominator between the two
data sources. We identify the language conditions of each tweet in the social
media data set and validated our results with two language identification
models. We then compare levels of linguistic diversity at national, regional,
and local geographies. The results suggest that social media language data has
the possibility to provide a rich source of spatial and temporal insights on
the linguistic profile of a place. We show that social media is sensitive to
demographic and sociopolitical changes within a language and at low-level
regional and local geographies.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to Cartography and GIScience in Australasia and Oceania:
  Including twenty years of GeoCart</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Optimizing Large Language Models for ESG Activity Detection in Financial
  Texts 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.21112v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.21112v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mattia Birti, Francesco Osborne, Andrea Maurino
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The integration of Environmental, Social, and Governance (ESG) factors into
corporate decision-making is a fundamental aspect of sustainable finance.
However, ensuring that business practices align with evolving regulatory
frameworks remains a persistent challenge. AI-driven solutions for
automatically assessing the alignment of sustainability reports and
non-financial disclosures with specific ESG activities could greatly support
this process. Yet, this task remains complex due to the limitations of
general-purpose Large Language Models (LLMs) in domain-specific contexts and
the scarcity of structured, high-quality datasets. In this paper, we
investigate the ability of current-generation LLMs to identify text related to
environmental activities. Furthermore, we demonstrate that their performance
can be significantly enhanced through fine-tuning on a combination of original
and synthetically generated data. To this end, we introduce ESG-Activities, a
benchmark dataset containing 1,325 labelled text segments classified according
to the EU ESG taxonomy. Our experimental results show that fine-tuning on
ESG-Activities significantly enhances classification accuracy, with open models
such as Llama 7B and Gemma 7B outperforming large proprietary solutions in
specific configurations. These findings have important implications for
financial analysts, policymakers, and AI researchers seeking to enhance ESG
transparency and compliance through advanced natural language processing
techniques.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Generating patient cohorts from electronic health records using two-step
  retrieval-augmented text-to-SQL generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.21107v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.21107v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Angelo Ziletti, Leonardo D'Ambrosi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Clinical cohort definition is crucial for patient recruitment and
observational studies, yet translating inclusion/exclusion criteria into SQL
queries remains challenging and manual. We present an automated system
utilizing large language models that combines criteria parsing, two-level
retrieval augmented generation with specialized knowledge bases, medical
concept standardization, and SQL generation to retrieve patient cohorts with
patient funnels. The system achieves 0.75 F1-score in cohort identification on
EHR data, effectively capturing complex temporal and logical relationships.
These results demonstrate the feasibility of automated cohort generation for
epidemiological research.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 1 figure</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Re-evaluating Theory of Mind evaluation in large language models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.21098v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.21098v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jennifer Hu, Felix Sosa, Tomer Ullman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The question of whether large language models (LLMs) possess Theory of Mind
(ToM) -- often defined as the ability to reason about others' mental states --
has sparked significant scientific and public interest. However, the evidence
as to whether LLMs possess ToM is mixed, and the recent growth in evaluations
has not resulted in a convergence. Here, we take inspiration from cognitive
science to re-evaluate the state of ToM evaluation in LLMs. We argue that a
major reason for the disagreement on whether LLMs have ToM is a lack of clarity
on whether models should be expected to match human behaviors, or the
computations underlying those behaviors. We also highlight ways in which
current evaluations may be deviating from "pure" measurements of ToM abilities,
which also contributes to the confusion. We conclude by discussing several
directions for future research, including the relationship between ToM and
pragmatic communication, which could advance our understanding of artificial
systems as well as human cognition.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PASemiQA: Plan-Assisted Agent for Question Answering on Semi-Structured
  Data with Text and Relational Information 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.21087v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.21087v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hansi Yang, Qi Zhang, Wei Jiang, Jianguo Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have shown impressive abilities in answering
questions across various domains, but they often encounter hallucination issues
on questions that require professional and up-to-date knowledge. To address
this limitation, retrieval-augmented generation (RAG) techniques have been
proposed, which retrieve relevant information from external sources to inform
their responses. However, existing RAG methods typically focus on a single type
of external data, such as vectorized text database or knowledge graphs, and
cannot well handle real-world questions on semi-structured data containing both
text and relational information. To bridge this gap, we introduce PASemiQA, a
novel approach that jointly leverages text and relational information in
semi-structured data to answer questions. PASemiQA first generates a plan to
identify relevant text and relational information to answer the question in
semi-structured data, and then uses an LLM agent to traverse the
semi-structured data and extract necessary information. Our empirical results
demonstrate the effectiveness of PASemiQA across different semi-structured
datasets from various domains, showcasing its potential to improve the accuracy
and reliability of question answering systems on semi-structured data.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CODI: Compressing Chain-of-Thought into Continuous Space via
  Self-Distillation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.21074v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.21074v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhenyi Shen, Hanqi Yan, Linhai Zhang, Zhanghao Hu, Yali Du, Yulan He
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Chain-of-Thought (CoT) enhances Large Language Models (LLMs) by enabling
step-by-step reasoning in natural language. However, the language space may be
suboptimal for reasoning. While implicit CoT methods attempt to enable
reasoning without explicit CoT tokens, they have consistently lagged behind
explicit CoT method in task performance. We propose CODI (Continuous
Chain-of-Thought via Self-Distillation), a novel framework that distills CoT
into a continuous space, where a shared model acts as both teacher and student,
jointly learning explicit and implicit CoT while aligning their hidden
activation on the token generating the final answer. CODI is the first implicit
CoT method to match explicit CoT's performance on GSM8k while achieving 3.1x
compression, surpassing the previous state-of-the-art by 28.2% in accuracy.
Furthermore, CODI demonstrates scalability, robustness, and generalizability to
more complex CoT datasets. Additionally, CODI retains interpretability by
decoding its continuous thoughts, making its reasoning process transparent. Our
findings establish implicit CoT as not only a more efficient but a powerful
alternative to explicit CoT.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Beyond Words: A Latent Memory Approach to Internal Reasoning in LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.21030v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.21030v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        José I. Orlicki
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in large language models (LLMs) have popularized the
chain-of-thought (CoT) paradigm, in which models produce explicit reasoning
steps in natural language. Although this approach improves interpretability and
facilitates external auditing, it may not represent the most computationally
efficient method for internal reasoning. In contrast, human cognition relies on
implicit mental representations that recall past sensory and episodic
information without requiring complete verbalization. In this paper, we propose
a framework that integrates implicit mental representations into the internal
reasoning processes of LLMs. Preliminary experiments indicate that
incorporating an Implicit Memory Module (IMM) into a simple GPT model yields a
reduction of between 35% and 57% in final training loss compared to a regular
GPT baseline. The addition of an explicit interpretability channel (e.g., a
chain-of-thought decoder) is straightforward to implement within this approach.
We outline theoretical foundations, propose technical mechanisms to scale the
memory module, and discuss how these ideas may lead to more efficient and
robust reasoning, with optional future extensions for explicit auditability.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Extending Dense Passage Retrieval with Temporal Information 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.21024v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.21024v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Abdelrahman Abdallah, Bhawna Piryani, Jonas Wallat, Avishek Anand, Adam Jatowt
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Temporal awareness is crucial in many information retrieval tasks,
particularly in scenarios where the relevance of documents depends on their
alignment with the query's temporal context. Traditional retrieval methods such
as BM25 and Dense Passage Retrieval (DPR) excel at capturing lexical and
semantic relevance but fall short in addressing time-sensitive queries. To
bridge this gap, we introduce the temporal retrieval model that integrates
explicit temporal signals by incorporating query timestamps and document dates
into the representation space. Our approach ensures that retrieved passages are
not only topically relevant but also temporally aligned with user intent. We
evaluate our approach on two large-scale benchmark datasets, ArchivalQA and
ChroniclingAmericaQA, achieving substantial performance gains over standard
retrieval baselines. In particular, our model improves Top-1 retrieval accuracy
by 6.63% and NDCG@10 by 3.79% on ArchivalQA, while yielding a 9.56% boost in
Top-1 retrieval accuracy and 4.68% in NDCG@10 on ChroniclingAmericaQA.
Additionally, we introduce a time-sensitive negative sampling strategy, which
refines the model's ability to distinguish between temporally relevant and
irrelevant documents during training. Our findings highlight the importance of
explicitly modeling time in retrieval systems and set a new standard for
handling temporally grounded queries.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PersuasiveToM: A Benchmark for Evaluating Machine Theory of Mind in
  Persuasive Dialogues 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.21017v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.21017v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fangxu Yu, Lai Jiang, Shenyi Huang, Zhen Wu, Xinyu Dai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The ability to understand and predict the mental states of oneself and
others, known as the Theory of Mind (ToM), is crucial for effective social
interactions. Recent research has emerged to evaluate whether Large Language
Models (LLMs) exhibit a form of ToM. Although recent studies have evaluated ToM
in LLMs, existing benchmarks focus predominantly on physical perception with
principles guided by the Sally-Anne test in synthetic stories and
conversations, failing to capture the complex psychological activities of
mental states in real-life social interactions. To mitigate this gap, we
propose PersuasiveToM, a benchmark designed to evaluate the ToM abilities of
LLMs in persuasive dialogues. Our framework introduces two categories of
questions: (1) ToM Reasoning, assessing the capacity of LLMs to track evolving
mental states (e.g., desire shifts in persuadees), and (2) ToM Application,
evaluating whether LLMs can take advantage of inferred mental states to select
effective persuasion strategies (e.g., emphasize rarity) and evaluate the
effectiveness of persuasion strategies. Experiments across eight
state-of-the-art LLMs reveal that while models excel on multiple questions,
they struggle to answer questions that need tracking the dynamics and shifts of
mental states and understanding the mental states in the whole dialogue
comprehensively. Our aim with PersuasiveToM is to allow an effective evaluation
of the ToM reasoning ability of LLMs with more focus on complex psychological
activities. Our code is available at
https://github.com/Yu-Fangxu/PersuasiveToM.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Capability Localization: Capabilities Can be Localized rather than
  Individual Knowledge 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.20992v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.20992v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiusheng Huang, Jiaxiang Liu, Yequan Wang, Jun Zhao, Kang Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large scale language models have achieved superior performance in tasks
related to natural language processing, however, it is still unclear how model
parameters affect performance improvement. Previous studies assumed that
individual knowledge is stored in local parameters, and the storage form of
individual knowledge is dispersed parameters, parameter layers, or parameter
chains, which are not unified. We found through fidelity and reliability
evaluation experiments that individual knowledge cannot be localized.
Afterwards, we constructed a dataset for decoupling experiments and discovered
the potential for localizing data commonalities. To further reveal this
phenomenon, this paper proposes a Commonality Neuron Localization (CNL) method,
which successfully locates commonality neurons and achieves a neuron overlap
rate of 96.42% on the GSM8K dataset. Finally, we have demonstrated through
cross data experiments that commonality neurons are a collection of capability
neurons that possess the capability to enhance performance. Our code is
available at https://github.com/nlpkeg/Capability-Neuron-Localization.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Merging Clinical Knowledge into Large Language Models for Medical
  Research and Applications: A <span class="highlight-title">Survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.20988v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.20988v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qiyuan Li, Haijiang Liu, Caicai Guo, Deyu Chen, Meng Wang, Feng Gao, Jinguang Gu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Clinical knowledge is the collection of information learned from studies on
the causes, prognosis, diagnosis, and treatment of diseases. This type of
knowledge can improve curing performances, and promote physical health. With
the emergence of large language models (LLMs), medical artificial intelligence
(medical AI), which aims to apply academic medical AI systems to real-world
medical scenarios, has entered a new age of development, resulting in excellent
works such as DoctorGPT and Pangu-Drug from academic and industrial researches.
However, the field lacks a comprehensive compendium and comparison of building
medical AI systems from academia and industry. Therefore, this survey focuses
on the building paradigms of medical AI systems including the use of clinical
databases, datasets, training pipelines, integrating medical knowledge graphs,
system applications, and evaluation systems. We hope that this survey can help
relevant practical researchers understand the current performance of academic
models in various fields of healthcare, as well as the potential problems and
future directions for implementing these scientific achievements.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ UoR-NCL at SemEval-2025 Task 1: Using Generative LLMs and CLIP Models
  for Multilingual Multimodal Idiomaticity Representation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.20984v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.20984v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Thanet Markchom, Tong Wu, Liting Huang, Huizhi Liang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  SemEval-2025 Task 1 focuses on ranking images based on their alignment with a
given nominal compound that may carry idiomatic meaning in both English and
Brazilian Portuguese. To address this challenge, this work uses generative
large language models (LLMs) and multilingual CLIP models to enhance idiomatic
compound representations. LLMs generate idiomatic meanings for potentially
idiomatic compounds, enriching their semantic interpretation. These meanings
are then encoded using multilingual CLIP models, serving as representations for
image ranking. Contrastive learning and data augmentation techniques are
applied to fine-tune these embeddings for improved performance. Experimental
results show that multimodal representations extracted through this method
outperformed those based solely on the original nominal compounds. The
fine-tuning approach shows promising outcomes but is less effective than using
embeddings without fine-tuning. The source code used in this paper is available
at https://github.com/tongwu17/SemEval-2025-Task1-UoR-NCL.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Set-Theoretic Compositionality of Sentence Embeddings 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.20975v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.20975v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Naman Bansal, Yash mahajan, Sanjeev Sinha, Santu Karmaker
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sentence encoders play a pivotal role in various NLP tasks; hence, an
accurate evaluation of their compositional properties is paramount. However,
existing evaluation methods predominantly focus on goal task-specific
performance. This leaves a significant gap in understanding how well sentence
embeddings demonstrate fundamental compositional properties in a
task-independent context. Leveraging classical set theory, we address this gap
by proposing six criteria based on three core "set-like"
compositions/operations: \textit{TextOverlap}, \textit{TextDifference}, and
\textit{TextUnion}. We systematically evaluate $7$ classical and $9$ Large
Language Model (LLM)-based sentence encoders to assess their alignment with
these criteria. Our findings show that SBERT consistently demonstrates set-like
compositional properties, surpassing even the latest LLMs. Additionally, we
introduce a new dataset of ~$192$K samples designed to facilitate future
benchmarking efforts on set-like compositionality of sentence embeddings.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Arabizi vs LLMs: Can the Genie Understand the Language of Aladdin? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.20973v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.20973v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Perla Al Almaoui, Pierrette Bouillon, Simon Hengchen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this era of rapid technological advancements, communication continues to
evolve as new linguistic phenomena emerge. Among these is Arabizi, a hybrid
form of Arabic that incorporates Latin characters and numbers to represent the
spoken dialects of Arab communities. Arabizi is widely used on social media and
allows people to communicate in an informal and dynamic way, but it poses
significant challenges for machine translation due to its lack of formal
structure and deeply embedded cultural nuances. This case study arises from a
growing need to translate Arabizi for gisting purposes. It evaluates the
capacity of different LLMs to decode and translate Arabizi, focusing on
multiple Arabic dialects that have rarely been studied up until now. Using a
combination of human evaluators and automatic metrics, this research project
investigates the model's performance in translating Arabizi into both Modern
Standard Arabic and English. Key questions explored include which dialects are
translated most effectively and whether translations into English surpass those
into Arabic.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to MT Summit 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Beware of Your Po! Measuring and Mitigating AI Safety Risks in Role-Play
  Fine-Tuning of LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.20968v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.20968v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weixiang Zhao, Yulin Hu, Yang Deng, Jiahe Guo, Xingyu Sui, Xinyang Han, An Zhang, Yanyan Zhao, Bing Qin, Tat-Seng Chua, Ting Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Role-playing enables large language models (LLMs) to engage users in
immersive and personalized interactions, but it also introduces significant
safety risks. Existing role-play fine-tuning techniques improve role
adaptability but may degrade safety performance, particularly for villainous
characters. In this work, we conduct the first comprehensive assessment of
role-play fine-tuning risks by training 95 role-specific LLMs using RoleBench.
Our experiments reveal that role-play fine-tuning leads to a noticeable decline
in safety performance, with safety risks varying based on character traits. To
tackle this challenge, we propose Safety-Aware Role-Play Fine-Tuning (SaRFT), a
novel method designed to balance role-playing capabilities and safety.
Extensive experiments on LLaMA-3-8B-Instruct, Gemma-2-9B-it, and
Qwen2.5-7B-Instruct demonstrate that SaRFT consistently outperforms
state-of-the-art baselines under both LoRA and full-parameter fine-tuning
settings. Our findings highlight the necessity of role-adaptive safety measures
and provide insights into mitigating role-specific safety risks in role-playing
LLMs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>25 pages, 10 figures, 13 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ WebFAQ: A Multilingual Collection of Natural Q&A <span class="highlight-title">Dataset</span>s for Dense
  Retrieval 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.20936v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.20936v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Michael Dinzinger, Laura Caspari, Kanishka Ghosh Dastidar, Jelena Mitrović, Michael Granitzer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present WebFAQ, a large-scale collection of open-domain question answering
datasets derived from FAQ-style schema.org annotations. In total, the data
collection consists of 96 million natural question-answer (QA) pairs across 75
languages, including 47 million (49%) non-English samples. WebFAQ further
serves as the foundation for 20 monolingual retrieval benchmarks with a total
size of 11.2 million QA pairs (5.9 million non-English). These datasets are
carefully curated through refined filtering and near-duplicate detection,
yielding high-quality resources for training and evaluating multilingual dense
retrieval models. To empirically confirm WebFAQ's efficacy, we use the
collected QAs to fine-tune an in-domain pretrained XLM-RoBERTa model. Through
this process of dataset-specific fine-tuning, the model achieves significant
retrieval performance gains, which generalize - beyond WebFAQ - to other
multilingual retrieval benchmarks evaluated in zero-shot setting. Last but not
least, we utilize WebFAQ to construct a set of QA-aligned bilingual corpora
spanning over 1000 language pairs using state-of-the-art bitext mining and
automated LLM-assessed translation evaluation. Due to our advanced, automated
method of bitext dataset generation, the resulting bilingual corpora
demonstrate higher translation quality compared to similar datasets. WebFAQ and
all associated resources are publicly available on GitHub and HuggingFace.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 3 figures, 7 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Automated Evaluation of Meter and Rhyme in Russian Generative and
  Human-Authored Poetry 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.20931v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.20931v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ilya Koziev
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generative poetry systems require effective tools for data engineering and
automatic evaluation, particularly to assess how well a poem adheres to
versification rules, such as the correct alternation of stressed and unstressed
syllables and the presence of rhymes.
  In this work, we introduce the Russian Poetry Scansion Tool library designed
for stress mark placement in Russian-language syllabo-tonic poetry, rhyme
detection, and identification of defects of poeticness. Additionally, we
release RIFMA -- a dataset of poem fragments spanning various genres and forms,
annotated with stress marks. This dataset can be used to evaluate the
capability of modern large language models to accurately place stress marks in
poetic texts.
  The published resources provide valuable tools for researchers and
practitioners in the field of creative generative AI, facilitating advancements
in the development and evaluation of generative poetry systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 1 figure</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Everything, Everywhere, All at Once: Is Mechanistic Interpretability
  Identifiable? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.20914v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.20914v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Maxime Méloux, Silviu Maniu, François Portet, Maxime Peyrard
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As AI systems are used in high-stakes applications, ensuring interpretability
is crucial. Mechanistic Interpretability (MI) aims to reverse-engineer neural
networks by extracting human-understandable algorithms to explain their
behavior. This work examines a key question: for a given behavior, and under
MI's criteria, does a unique explanation exist? Drawing on identifiability in
statistics, where parameters are uniquely inferred under specific assumptions,
we explore the identifiability of MI explanations.
  We identify two main MI strategies: (1) "where-then-what," which isolates a
circuit replicating model behavior before interpreting it, and (2)
"what-then-where," which starts with candidate algorithms and searches for
neural activation subspaces implementing them, using causal alignment.
  We test both strategies on Boolean functions and small multi-layer
perceptrons, fully enumerating candidate explanations. Our experiments reveal
systematic non-identifiability: multiple circuits can replicate behavior, a
circuit can have multiple interpretations, several algorithms can align with
the network, and one algorithm can align with different subspaces.
  Is uniqueness necessary? A pragmatic approach may require only predictive and
manipulability standards. If uniqueness is essential for understanding,
stricter criteria may be needed. We also reference the inner interpretability
framework, which validates explanations through multiple criteria. This work
contributes to defining explanation standards in AI.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A database to support the evaluation of gender biases in <span class="highlight-title">GPT</span>-4o output <span class="chip">ISCA</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.20898v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.20898v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Luise Mehner, Lena Alicija Philine Fiedler, Sabine Ammon, Dorothea Kolossa
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The widespread application of Large Language Models (LLMs) involves ethical
risks for users and societies. A prominent ethical risk of LLMs is the
generation of unfair language output that reinforces or exacerbates harm for
members of disadvantaged social groups through gender biases (Weidinger et al.,
2022; Bender et al., 2021; Kotek et al., 2023). Hence, the evaluation of the
fairness of LLM outputs with respect to such biases is a topic of rising
interest. To advance research in this field, promote discourse on suitable
normative bases and evaluation methodologies, and enhance the reproducibility
of related studies, we propose a novel approach to database construction. This
approach enables the assessment of gender-related biases in LLM-generated
language beyond merely evaluating their degree of neutralization.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ISCA/ITG Workshop on Diversity in Large Speech and Language Models</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Beyond Demographics: Fine-tuning Large Language Models to Predict
  Individuals' Subjective Text Perceptions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.20897v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.20897v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Matthias Orlikowski, Jiaxin Pei, Paul Röttger, Philipp Cimiano, David Jurgens, Dirk Hovy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  People naturally vary in their annotations for subjective questions and some
of this variation is thought to be due to the person's sociodemographic
characteristics. LLMs have also been used to label data, but recent work has
shown that models perform poorly when prompted with sociodemographic
attributes, suggesting limited inherent sociodemographic knowledge. Here, we
ask whether LLMs can be trained to be accurate sociodemographic models of
annotator variation. Using a curated dataset of five tasks with standardized
sociodemographics, we show that models do improve in sociodemographic prompting
when trained but that this performance gain is largely due to models learning
annotator-specific behaviour rather than sociodemographic patterns. Across all
tasks, our results suggest that models learn little meaningful connection
between sociodemographics and annotation, raising doubts about the current use
of LLMs for simulating sociodemographic variation and behaviour.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Reviewed ARR December 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ProBench: Benchmarking Large Language Models in Competitive Programming 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.20868v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.20868v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lei Yang, Renren Jin, Ling Shi, Jianxiang Peng, Yue Chen, Deyi Xiong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With reasoning language models such as OpenAI-o3 and DeepSeek-R1 emerging,
large language models (LLMs) have entered a new phase of development. However,
existing benchmarks for coding evaluation are gradually inadequate to assess
the capability of advanced LLMs in code reasoning. To bridge the gap for
high-level code reasoning assessment, we propose ProBench to benchmark LLMs in
competitive programming, drawing inspiration from the International Collegiate
Programming Contest. ProBench collects a comprehensive set of competitive
programming problems from Codeforces, Luogu, and Nowcoder platforms during the
period from July to December 2024, obtaining real test results through online
submissions to ensure the fairness and accuracy of the evaluation. We establish
a unified problem attribute system, including difficulty grading and algorithm
tagging. With carefully collected and annotated data in ProBench, we
systematically assess 9 latest LLMs in competitive programming across multiple
dimensions, including thought chain analysis, error type diagnosis, and
reasoning depth evaluation. Experimental results show that QwQ-32B-Preview
achieves the best score of 20.93 followed by DeepSeek-V3 with a score of 16.38,
suggesting that models trained with specialized reasoning tasks significantly
outperform general-purpose models (even larger than reasoning-oriented models)
in programming. Further analysis also reveals key areas for programming
capability enhancement, e.g., algorithm adaptability and reasoning sufficiency,
providing important insights for the future development of reasoning models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Better Benchmarking LLMs for Zero-Shot Dependency Parsing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.20866v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.20866v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ana Ezquerro, Carlos Gómez-Rodríguez, David Vilares
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While LLMs excel in zero-shot tasks, their performance in linguistic
challenges like syntactic parsing has been less scrutinized. This paper studies
state-of-the-art open-weight LLMs on the task by comparing them to baselines
that do not have access to the input sentence, including baselines that have
not been used in this context such as random projective trees or optimal linear
arrangements. The results show that most of the tested LLMs cannot outperform
the best uninformed baselines, with only the newest and largest versions of
LLaMA doing so for most languages, and still achieving rather low performance.
Thus, accurate zero-shot syntactic parsing is not forthcoming with open LLMs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at NoDaLiDa/Baltic-HLT 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Do Language Models Understand Honorific Systems in Javanese? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.20864v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.20864v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohammad Rifqi Farhansyah, Iwan Darmawan, Adryan Kusumawardhana, Genta Indra Winata, Alham Fikri Aji, Derry Tanti Wijaya
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Javanese language features a complex system of honorifics that vary
according to the social status of the speaker, listener, and referent. Despite
its cultural and linguistic significance, there has been limited progress in
developing a comprehensive corpus to capture these variations for natural
language processing (NLP) tasks. In this paper, we present Unggah-Ungguh, a
carefully curated dataset designed to encapsulate the nuances of Unggah-Ungguh
Basa, the Javanese speech etiquette framework that dictates the choice of words
and phrases based on social hierarchy and context. Using Unggah-Ungguh, we
assess the ability of language models (LMs) to process various levels of
Javanese honorifics through classification and machine translation tasks. To
further evaluate cross-lingual LMs, we conduct machine translation experiments
between Javanese (at specific honorific levels) and Indonesian. Additionally,
we explore whether LMs can generate contextually appropriate Javanese
honorifics in conversation tasks, where the honorific usage should align with
the social role and contextual cues. Our findings indicate that current LMs
struggle with most honorific levels, exhibitinga bias toward certain honorific
tiers.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Power of Personality: A Human Simulation Perspective to Investigate
  Large Language Model Agents 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.20859v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.20859v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yifan Duan, Yihong Tang, Xuefeng Bai, Kehai Chen, Juntao Li, Min Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) excel in both closed tasks (including
problem-solving, and code generation) and open tasks (including creative
writing), yet existing explanations for their capabilities lack connections to
real-world human intelligence. To fill this gap, this paper systematically
investigates LLM intelligence through the lens of ``human simulation'',
addressing three core questions: (1) How do personality traits affect
problem-solving in closed tasks? (2) How do traits shape creativity in open
tasks? (3) How does single-agent performance influence multi-agent
collaboration? By assigning Big Five personality traits to LLM agents and
evaluating their performance in single- and multi-agent settings, we reveal
that specific traits significantly influence reasoning accuracy (closed tasks)
and creative output (open tasks). Furthermore, multi-agent systems exhibit
collective intelligence distinct from individual capabilities, driven by
distinguishing combinations of personalities. We demonstrate that LLMs
inherently simulate human behavior through next-token prediction, mirroring
human language, decision-making, and collaborative dynamics.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MAMUT: A Novel Framework for Modifying Mathematical Formulas for the
  Generation of Specialized <span class="highlight-title">Dataset</span>s for Language Model Training 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.20855v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.20855v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jonathan Drechsel, Anja Reusch, Steffen Herbold
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Mathematical formulas are a fundamental and widely used component in various
scientific fields, serving as a universal language for expressing complex
concepts and relationships. While state-of-the-art transformer models excel in
processing and understanding natural language, they encounter challenges with
mathematical notation, which involves a complex structure and diverse
representations. This study focuses on the development of specialized training
datasets to enhance the encoding of mathematical content. We introduce Math
Mutator (MAMUT), a framework capable of generating equivalent and falsified
versions of a given mathematical formula in LaTeX notation, effectively
capturing the mathematical variety in notation of the same concept. Based on
MAMUT, we have generated four large mathematical datasets containing diverse
notation, which can be used to train language models with enhanced mathematical
embeddings.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Pilot Empirical Study on When and How to Use Knowledge Graphs as
  Retrieval Augmented Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.20854v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.20854v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xujie Yuan, Yongxu Liu, Shimin Di, Shiwen Wu, Libin Zheng, Rui Meng, Xiaofang Zhou, Lei Chen, Jian Yin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The integration of Knowledge Graphs (KGs) into the Retrieval Augmented
Generation (RAG) framework has attracted significant interest, with early
studies showing promise in mitigating hallucinations and improving model
accuracy. However, a systematic understanding and comparative analysis of the
rapidly emerging KG-RAG methods are still lacking. This paper seeks to lay the
foundation for systematically answering the question of when and how to use
KG-RAG by analyzing their performance in various application scenarios
associated with different technical configurations. After outlining the mind
map using KG-RAG framework and summarizing its popular pipeline, we conduct a
pilot empirical study of KG-RAG works to reimplement and evaluate 6 KG-RAG
methods across 7 datasets in diverse scenarios, analyzing the impact of 9
KG-RAG configurations in combination with 17 LLMs. Our results underscore the
critical role of appropriate application conditions and optimal configurations
of KG-RAG components.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 2 figures, 14 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning to Substitute Components for Compositional Generalization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.20834v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.20834v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhaoyi Li, Gangwei Jiang, Chenwang Wu, Ying Wei, Defu Lian, Enhong Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite the rising prevalence of neural language models, recent empirical
evidence suggests their deficiency in compositional generalization. One of the
current de-facto solutions to this problem is compositional data augmentation,
which aims to introduce additional compositional inductive bias. However,
existing handcrafted augmentation strategies offer limited improvement when
systematic generalization of neural language models requires multi-grained
compositional bias (i.e., not limited to either lexical or structural biases
alone) or when training sentences have an imbalanced difficulty distribution.
To address these challenges, we first propose a novel compositional
augmentation strategy called Component Substitution (CompSub), which enables
multi-grained composition of substantial substructures across the entire
training set. Furthermore, we introduce the Learning Component Substitution
(LCS) framework. This framework empowers the learning of component substitution
probabilities in CompSub in an end-to-end manner by maximizing the loss of
neural language models, thereby prioritizing challenging compositions with
elusive concepts and novel contexts. We extend the key ideas of CompSub and LCS
to the recently emerging in-context learning scenarios of pre-trained large
language models (LLMs), proposing the LCS-ICL algorithm to enhance the few-shot
compositional generalization of state-of-the-art (SOTA) LLMs. Theoretically, we
provide insights into why applying our algorithms to language models can
improve compositional generalization performance. Empirically, our results on
four standard compositional generalization benchmarks(SCAN, COGS, GeoQuery, and
COGS-QL) demonstrate the superiority of CompSub, LCS, and LCS-ICL, with
improvements of up to 66.5%, 10.3%, 1.4%, and 8.8%, respectively.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>23 pages, 9 figures, preprint, the extension paper of the paper
  (arXiv:2306.02840)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ HAIC: Improving Human Action Understanding and Generation with Better
  Captions for Multi-modal Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.20811v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.20811v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiao Wang, Jingyun Hua, Weihong Lin, Yuanxing Zhang, Fuzheng Zhang, Jianlong Wu, Di Zhang, Liqiang Nie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent Multi-modal Large Language Models (MLLMs) have made great progress in
video understanding. However, their performance on videos involving human
actions is still limited by the lack of high-quality data. To address this, we
introduce a two-stage data annotation pipeline. First, we design strategies to
accumulate videos featuring clear human actions from the Internet. Second,
videos are annotated in a standardized caption format that uses human
attributes to distinguish individuals and chronologically details their actions
and interactions. Through this pipeline, we curate two datasets, namely
HAICTrain and HAICBench. \textbf{HAICTrain} comprises 126K video-caption pairs
generated by Gemini-Pro and verified for training purposes. Meanwhile,
\textbf{HAICBench} includes 500 manually annotated video-caption pairs and
1,400 QA pairs, for a comprehensive evaluation of human action understanding.
Experimental results demonstrate that training with HAICTrain not only
significantly enhances human understanding abilities across 4 benchmarks, but
can also improve text-to-video generation results. Both the HAICTrain and
HAICBench are released at https://huggingface.co/datasets/KuaishouHAIC/HAIC.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Plan2Align: Predictive Planning Based Test-Time Preference Alignment in
  Paragraph-Level Machine Translation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.20795v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.20795v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kuang-Da Wang, Teng-Ruei Chen, Yu Heng Hung, Shuoyang Ding, Yueh-Hua Wu, Yu-Chiang Frank Wang, Chao-Han Huck Yang, Wen-Chih Peng, Ping-Chun Hsieh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Machine Translation (MT) has been predominantly designed for sentence-level
translation using transformer-based architectures. While next-token prediction
based Large Language Models (LLMs) demonstrate strong capabilities in long-text
translation, non-extensive language models often suffer from omissions and
semantic inconsistencies when processing paragraphs. Existing preference
alignment methods improve sentence-level translation but fail to ensure
coherence over extended contexts due to the myopic nature of next-token
generation. We introduce Plan2Align, a test-time alignment framework that
treats translation as a predictive planning problem, adapting Model Predictive
Control to iteratively refine translation outputs. Experiments on WMT24
Discourse-Level Literary Translation show that Plan2Align significantly
improves paragraph-level translation, achieving performance surpassing or on
par with the existing training-time and test-time alignment methods on
LLaMA-3.1 8B.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint. Code will be released at Plan2Align GitHub link:
  https://github.com/NYCU-RL-Bandits-Lab/Plan2Align</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Chain-of-Thought Matters: Improving Long-Context Language Models with
  Reasoning Path Supervision 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.20790v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.20790v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dawei Zhu, Xiyu Wei, Guangxiang Zhao, Wenhao Wu, Haosheng Zou, Junfeng Ran, Xun Wang, Lin Sun, Xiangzheng Zhang, Sujian Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in Large Language Models (LLMs) have highlighted the
challenge of handling long-context tasks, where models need to reason over
extensive input contexts to aggregate target information. While
Chain-of-Thought (CoT) prompting has shown promise for multi-step reasoning,
its effectiveness for long-context scenarios remains underexplored. Through
systematic investigation across diverse tasks, we demonstrate that CoT's
benefits generalize across most long-context scenarios and amplify with
increasing context length. Motivated by this critical observation, we propose
LongRePS, a process-supervised framework that teaches models to generate
high-quality reasoning paths for enhanced long-context performance. Our
framework incorporates a self-sampling mechanism to bootstrap reasoning paths
and a novel quality assessment protocol specifically designed for long-context
scenarios. Experimental results on various long-context benchmarks demonstrate
the effectiveness of our approach, achieving significant improvements over
outcome supervision baselines on both in-domain tasks (+13.6/+3.8 points for
LLaMA/Qwen on MuSiQue) and cross-domain generalization (+9.3/+8.1 points on
average across diverse QA tasks). Our code, data and trained models are made
public to facilitate future research.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages,6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GraphCheck: Multi-Path Fact-Checking with Entity-Relationship Graphs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.20785v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.20785v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hyewon Jeon, Jay-Yoon Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Automated fact-checking aims to assess the truthfulness of text based on
relevant evidence, yet verifying complex claims requiring multi-hop reasoning
remains a significant challenge. We propose GraphCheck, a novel framework that
converts claims into entity-relationship graphs for comprehensive verification.
By identifying relation between explicit entities and latent entities across
multiple paths, GraphCheck enhances the adaptability and robustness of
verification. Furthermore, we introduce DP-GraphCheck, a two-stage variant that
improves performance by incorporating direct prompting as an initial filtering
step. Experiments on the HOVER and EX-FEVER datasets show that our approach
outperforms existing methods, particularly in multi-hop reasoning tasks.
Furthermore, our two-stage framework generalizes well to other fact-checking
pipelines, demonstrating its versatility.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MedHallTune: An Instruction-Tuning Benchmark for Mitigating Medical
  Hallucination in Vision-Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.20780v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.20780v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qiao Yan, Yuchen Yuan, Xiaowei Hu, Yihan Wang, Jiaqi Xu, Jinpeng Li, Chi-Wing Fu, Pheng-Ann Heng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The increasing use of vision-language models (VLMs) in healthcare
applications presents great challenges related to hallucinations, in which the
models may generate seemingly plausible results that are in fact incorrect.
Such hallucinations can jeopardize clinical decision making, potentially
harming the diagnosis and treatments. In this work, we propose MedHallTune, a
large-scale benchmark designed specifically to evaluate and mitigate
hallucinations in medical VLMs. Comprising over 100,000 images and 1,000,000
instruction pairs, MedHallTune includes both hallucination and
non-hallucination samples, each with ground-truth annotations. We conduct a
comprehensive evaluation of current medical and general VLMs using MedHallTune,
assessing their performance across key metrics, including clinical accuracy,
relevance, detail level, and risk level. The experimental results show that
fine-tuning with MedHallTune successfully improves the ability of several
existing models to manage hallucinations and boost their zero-shot performance
on downstream visual-question-answering (VQA) tasks, making them more reliable
for practical medical applications. Our work contributes to the development of
more trustworthy VLMs. Codes and dataset will be available at
\href{https://github.com/russellyq/MedHallTune}{MedHallTune}.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Triple Phase Transitions: Understanding the Learning Dynamics of Large
  Language Models from a Neuroscience Perspective 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.20779v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.20779v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuko Nakagi, Keigo Tada, Sota Yoshino, Shinji Nishimoto, Yu Takagi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) often exhibit abrupt emergent behavior, whereby
new abilities arise at certain points during their training. This phenomenon,
commonly referred to as a ''phase transition'', remains poorly understood. In
this study, we conduct an integrative analysis of such phase transitions by
examining three interconnected perspectives: the similarity between LLMs and
the human brain, the internal states of LLMs, and downstream task performance.
We propose a novel interpretation for the learning dynamics of LLMs that vary
in both training data and architecture, revealing that three phase transitions
commonly emerge across these models during training: (1) alignment with the
entire brain surges as LLMs begin adhering to task instructions Brain Alignment
and Instruction Following, (2) unexpectedly, LLMs diverge from the brain during
a period in which downstream task accuracy temporarily stagnates Brain
Detachment and Stagnation, and (3) alignment with the brain reoccurs as LLMs
become capable of solving the downstream tasks Brain Realignment and
Consolidation. These findings illuminate the underlying mechanisms of phase
transitions in LLMs, while opening new avenues for interdisciplinary research
bridging AI and neuroscience.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>46 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FlexPrefill: A Context-Aware Sparse Attention Mechanism for Efficient
  Long-Sequence Inference <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.20766v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.20766v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xunhao Lai, Jianqiao Lu, Yao Luo, Yiyuan Ma, Xun Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) encounter computational challenges during
long-sequence inference, especially in the attention pre-filling phase, where
the complexity grows quadratically with the prompt length. Previous efforts to
mitigate these challenges have relied on fixed sparse attention patterns or
identifying sparse attention patterns based on limited cases. However, these
methods lacked the flexibility to efficiently adapt to varying input demands.
In this paper, we introduce FlexPrefill, a Flexible sparse Pre-filling
mechanism that dynamically adjusts sparse attention patterns and computational
budget in real-time to meet the specific requirements of each input and
attention head. The flexibility of our method is demonstrated through two key
innovations: 1) Query-Aware Sparse Pattern Determination: By measuring
Jensen-Shannon divergence, this component adaptively switches between
query-specific diverse attention patterns and predefined attention patterns. 2)
Cumulative-Attention Based Index Selection: This component dynamically selects
query-key indexes to be computed based on different attention patterns,
ensuring the sum of attention scores meets a predefined threshold. FlexPrefill
adaptively optimizes the sparse pattern and sparse ratio of each attention head
based on the prompt, enhancing efficiency in long-sequence inference tasks.
Experimental results show significant improvements in both speed and accuracy
over prior methods, providing a more flexible and efficient solution for LLM
inference.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ICLR 2025 (Oral)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Collective Reasoning Among LLMs A Framework for Answer Validation
  Without Ground Truth 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.20758v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.20758v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Seyed Pouyan Mousavi Davoudi, Alireza Shafiee Fard, Alireza Amiri-Margavi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a collaborative framework where multiple large language models,
namely GPT-4-0125-preview, Meta-LLaMA-3-70B-Instruct, Claude-3-Opus, and
Gemini-1.5-Flash, work together to generate and respond to complex PhD-level
probability questions in the absence of definitive ground truth. This study
explores how inter-model consensus enhances response reliability and serves as
a proxy for assessing the quality of generated questions. To quantify agreement
and consistency, we employ statistical methods including chi-square tests,
Fleiss' Kappa, and confidence interval analysis, measuring both response
precision and question clarity. Our findings highlight that Claude and Gemini
generate well-structured and less ambiguous questions, leading to higher
inter-model agreement. This is reflected in their narrower confidence intervals
and stronger alignment with answering models. Conversely, LLaMA demonstrates
increased variability and lower reliability in question formulation, as
indicated by broader confidence intervals and reduced consensus rates. These
results suggest that multi-model collaboration not only enhances the
reliability of responses but also provides a valuable framework for assessing
and improving question quality in the absence of explicit ground truth. This
research offers meaningful insights into optimizing AI-driven reasoning through
collaborative large-language model interactions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 2 figures. arXiv admin note: substantial text overlap with
  arXiv:2411.16797</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Rise of Darkness: Safety-Utility Trade-Offs in Role-Playing Dialogue
  Agents 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.20757v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.20757v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yihong Tang, Kehai Chen, Xuefeng Bai, Zhengyu Niu, Bo Wang, Jie Liu, Min Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have made remarkable advances in role-playing
dialogue agents, demonstrating their utility in character simulations. However,
it remains challenging for these agents to balance character portrayal utility
with content safety because this essential character simulation often comes
with the risk of generating unsafe content. To address this issue, we first
conduct a systematic exploration of the safety-utility trade-off across
multiple LLMs. Our analysis reveals that risk scenarios created by villain
characters and user queries (referred to as risk coupling) contribute to this
trade-off. Building on this, we propose a novel Adaptive Dynamic
Multi-Preference (ADMP) method, which dynamically adjusts safety-utility
preferences based on the degree of risk coupling and guides the model to
generate responses biased toward utility or safety. We further introduce
Coupling Margin Sampling (CMS) into coupling detection to enhance the model's
ability to handle high-risk scenarios. Experimental results demonstrate that
our approach improves safety metrics while maintaining utility.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Acquiring Grounded Representations of Words with Situated Interactive
  Instruction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.20754v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.20754v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shiwali Mohan, Aaron H. Mininger, James R. Kirk, John E. Laird
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present an approach for acquiring grounded representations of words from
mixed-initiative, situated interactions with a human instructor. The work
focuses on the acquisition of diverse types of knowledge including perceptual,
semantic, and procedural knowledge along with learning grounded meanings.
Interactive learning allows the agent to control its learning by requesting
instructions about unknown concepts, making learning efficient. Our approach
has been instantiated in Soar and has been evaluated on a table-top robotic arm
capable of manipulating small objects.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Mitigating Hallucinations in Large Vision-Language Models by Adaptively
  Constraining Information Flow <span class="chip">AAAI 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.20750v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.20750v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiaqi Bai, Hongcheng Guo, Zhongyuan Peng, Jian Yang, Zhoujun Li, Mohan Li, Zhihong Tian
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large vision-language models show tremendous potential in understanding
visual information through human languages. However, they are prone to suffer
from object hallucination, i.e., the generated image descriptions contain
objects that do not exist in the image. In this paper, we reveal that object
hallucination can be attributed to overconfidence in irrelevant visual features
when soft visual tokens map to the LLM's word embedding space. Specifically, by
figuring out the semantic similarity between visual tokens and LLM's word
embedding, we observe that the smoothness of similarity distribution strongly
correlates with the emergence of object hallucinations. To mitigate
hallucinations, we propose using the Variational Information Bottleneck (VIB)
to alleviate overconfidence by introducing stochastic noise, facilitating the
constraining of irrelevant information. Furthermore, we propose an
entropy-based noise-controlling strategy to enable the injected noise to be
adaptively constrained regarding the smoothness of the similarity distribution.
We adapt the proposed AdaVIB across distinct model architectures. Experimental
results demonstrate that the proposed AdaVIB mitigates object hallucinations by
effectively alleviating the overconfidence in irrelevant visual features, with
consistent improvements on two object hallucination benchmarks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to AAAI 2025. Camera ready version</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Teach-to-Reason with Scoring: Self-Explainable Rationale-Driven
  Multi-Trait Essay Scoring 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.20748v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.20748v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Heejin Do, Sangwon Ryu, Gary Geunbae Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-trait automated essay scoring (AES) systems provide a fine-grained
evaluation of an essay's diverse aspects. While they excel in scoring, prior
systems fail to explain why specific trait scores are assigned. This lack of
transparency leaves instructors and learners unconvinced of the AES outputs,
hindering their practical use. To address this, we propose a self-explainable
Rationale-Driven Multi-trait automated Essay scoring (RaDME) framework. RaDME
leverages the reasoning capabilities of large language models (LLMs) by
distilling them into a smaller yet effective scorer. This more manageable
student model is optimized to sequentially generate a trait score followed by
the corresponding rationale, thereby inherently learning to select a more
justifiable score by considering the subsequent rationale during training. Our
findings indicate that while LLMs underperform in direct AES tasks, they excel
in rationale generation when provided with precise numerical scores. Thus,
RaDME integrates the superior reasoning capacities of LLMs into the robust
scoring accuracy of an optimized smaller model. Extensive experiments
demonstrate that RaDME achieves both accurate and adequate reasoning while
supporting high-quality multi-trait scoring, significantly enhancing the
transparency of AES.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Structured Preference Optimization for Vision-Language Long-Horizon Task
  Planning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.20742v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.20742v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiwen Liang, Min Lin, Weiqi Ruan, Rongtao Xu, Yuecheng Liu, Jiaqi Chen, Bingqian Lin, Yuzheng Zhuang, Xiaodan Liang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing methods for vision-language task planning excel in short-horizon
tasks but often fall short in complex, long-horizon planning within dynamic
environments. These challenges primarily arise from the difficulty of
effectively training models to produce high-quality reasoning processes for
long-horizon tasks. To address this, we propose Structured Preference
Optimization (SPO), which aims to enhance reasoning and action selection in
long-horizon task planning through structured preference evaluation and
optimized training strategies. Specifically, SPO introduces: 1)
Preference-Based Scoring and Optimization, which systematically evaluates
reasoning chains based on task relevance, visual grounding, and historical
consistency; and 2) Curriculum-Guided Training, where the model progressively
adapts from simple to complex tasks, improving its generalization ability in
long-horizon scenarios and enhancing reasoning robustness. To advance research
in vision-language long-horizon task planning, we introduce ExtendaBench, a
comprehensive benchmark covering 1,509 tasks across VirtualHome and Habitat
2.0, categorized into ultra-short, short, medium, and long tasks. Experimental
results demonstrate that SPO significantly improves reasoning quality and final
decision accuracy, outperforming prior methods on long-horizon tasks and
underscoring the effectiveness of preference-driven optimization in
vision-language task planning. Specifically, SPO achieves a +5.98% GCR and
+4.68% SR improvement in VirtualHome and a +3.30% GCR and +2.11% SR improvement
in Habitat over the best-performing baselines.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Retrieval Backward Attention without Additional Training: Enhance
  Embeddings of Large Language Models via Repetition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.20726v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.20726v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yifei Duan, Raphael Shang, Deng Liang, Yongqiang Cai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Language models can be viewed as functions that embed text into Euclidean
space, where the quality of the embedding vectors directly determines model
performance, training such neural networks involves various uncertainties. This
paper focuses on improving the performance of pre-trained language models in
zero-shot settings through a simple and easily implementable method. We propose
a novel backward attention mechanism to enhance contextual information
encoding. Evaluated on the Chinese Massive Text Embedding Benchmark (C-MTEB),
our approach achieves significant improvements across multiple tasks, providing
valuable insights for advancing zero-shot learning capabilities.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ProAI: Proactive Multi-Agent Conversational AI with Structured Knowledge
  Base for Psychiatric Diagnosis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.20689v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.20689v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuqi Wu, Guangya Wan, Jingjing Li, Shengming Zhao, Lingfeng Ma, Tianyi Ye, Ion Pop, Yanbo Zhang, Jie Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Most LLM-driven conversational AI systems operate reactively, responding to
user prompts without guiding the interaction. Most LLM-driven conversational AI
systems operate reactively, responding to user prompts without guiding the
interaction. However, many real-world applications-such as psychiatric
diagnosis, consulting, and interviews-require AI to take a proactive role,
asking the right questions and steering conversations toward specific
objectives. Using mental health differential diagnosis as an application
context, we introduce ProAI, a goal-oriented, proactive conversational AI
framework. ProAI integrates structured knowledge-guided memory, multi-agent
proactive reasoning, and a multi-faceted evaluation strategy, enabling LLMs to
engage in clinician-style diagnostic reasoning rather than simple response
generation. Through simulated patient interactions, user experience assessment,
and professional clinical validation, we demonstrate that ProAI achieves up to
83.3% accuracy in mental disorder differential diagnosis while maintaining
professional and empathetic interaction standards. These results highlight the
potential for more reliable, adaptive, and goal-driven AI diagnostic
assistants, advancing LLMs beyond reactive dialogue systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>21 pages, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ JAM: Controllable and Responsible Text Generation via Causal Reasoning
  and Latent Vector Manipulation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.20684v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.20684v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yingbing Huang, Deming Chen, Abhishek K. Umrawal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While large language models (LLMs) have made significant strides in
generating coherent and contextually relevant text, they often function as
opaque black boxes, trained on vast unlabeled datasets with statistical
objectives, lacking an interpretable framework for responsible control. In this
paper, we introduce JAM (Just A Move), a novel framework that interprets and
controls text generation by integrating cause-effect analysis within the latent
space of LLMs. Based on our observations, we uncover the inherent causality in
LLM generation, which is critical for producing responsible and realistic
outputs. Moreover, we explore latent vectors as fundamental components in LLM
architectures, aiming to understand and manipulate them for more effective and
efficient controllable text generation. We evaluate our framework using a range
of tools, including the HHH criteria, toxicity reduction benchmarks, and GPT-4
alignment measures. Our results show that JAM achieves up to a 22% improvement
over previous Controllable Text Generation (CTG) methods across multiple
quantitative metrics and human-centric evaluations. Furthermore, JAM
demonstrates greater computational efficiency compared to other CTG methods.
These results highlight the effectiveness and efficiency of JAM for responsible
and realistic text generation, paving the way for more interpretable and
controllable models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 3 figures, and 6 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Fine-tuning <span class="highlight-title">BERT</span> with Bidirectional LSTM for Fine-grained Movie <span class="highlight-title">Review</span>s
  Sentiment Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.20682v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.20682v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gibson Nkhata, Susan Gauch, Usman Anjum, Justin Zhan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sentiment Analysis (SA) is instrumental in understanding peoples viewpoints
facilitating social media monitoring recognizing products and brands and
gauging customer satisfaction. Consequently SA has evolved into an active
research domain within Natural Language Processing (NLP). Many approaches
outlined in the literature devise intricate frameworks aimed at achieving high
accuracy, focusing exclusively on either binary sentiment classification or
fine-grained sentiment classification. In this paper our objective is to
fine-tune the pre-trained BERT model with Bidirectional LSTM (BiLSTM) to
enhance both binary and fine-grained SA specifically for movie reviews. Our
approach involves conducting sentiment classification for each review followed
by computing the overall sentiment polarity across all reviews. We present our
findings on binary classification as well as fine-grained classification
utilizing benchmark datasets. Additionally we implement and assess two accuracy
improvement techniques Synthetic Minority Oversampling Technique (SMOTE) and
NLP Augmenter (NLPAUG) to bolster the models generalization in fine-grained
sentiment classification. Finally a heuristic algorithm is employed to
calculate the overall polarity of predicted reviews from the BERT+BiLSTM output
vector. Our approach performs comparably with state-of-the-art (SOTA)
techniques in both classifications. For instance in binary classification we
achieve 97.67% accuracy surpassing the leading SOTA model
NB-weighted-BON+dv-cosine by 0.27% on the renowned IMDb dataset. Conversely for
five-class classification on SST-5 while the top SOTA model
RoBERTa+large+Self-explaining attains 55.5% accuracy our model achieves 59.48%
accuracy surpassing the BERT-large baseline by 3.6%.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 5 figures, published in International Journal On Advances
  in Systems and Measurements, volume 16, numbers 3 and 4, 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Disentangling Feature Structure: A Mathematically Provable Two-Stage
  Training Dynamics in <span class="highlight-title">Transformer</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.20681v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.20681v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zixuan Gong, Jiaye Teng, Yong Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Transformers may exhibit two-stage training dynamics during the real-world
training process. For instance, when training GPT-2 on the Counterfact dataset,
the answers progress from syntactically incorrect to syntactically correct to
semantically correct. However, existing theoretical analyses hardly account for
this two-stage phenomenon. In this paper, we theoretically demonstrate how such
two-stage training dynamics occur in transformers. Specifically, we analyze the
dynamics of transformers using feature learning techniques under in-context
learning regimes, based on a disentangled two-type feature structure. Such
disentanglement of feature structure is general in practice, e.g., natural
languages contain syntax and semantics, and proteins contain primary and
secondary structures. To our best known, this is the first rigorous result
regarding a two-stage optimization process in transformers. Additionally, a
corollary indicates that such a two-stage process is closely related to the
spectral properties of the attention weights, which accords well with empirical
findings.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Prediction of Item Difficulty for Reading Comprehension Items by
  Creation of Annotated Item Repository 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.20663v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.20663v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Radhika Kapoor, Sang T. Truong, Nick Haber, Maria Araceli Ruiz-Primo, Benjamin W. Domingue
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Prediction of item difficulty based on its text content is of substantial
interest. In this paper, we focus on the related problem of recovering
IRT-based difficulty when the data originally reported item p-value (percent
correct responses). We model this item difficulty using a repository of reading
passages and student data from US standardized tests from New York and Texas
for grades 3-8 spanning the years 2017-23. This repository is annotated with
meta-data on (1) linguistic features of the reading items, (2) test features of
the passage, and (3) context features. A penalized regression prediction model
with all these features can predict item difficulty with RMSE 0.52 compared to
baseline RMSE of 0.92, and with a correlation of 0.77 between true and
predicted difficulty. We supplement these features with embeddings from LLMs
(ModernBERT, BERT, and LlAMA), which marginally improve item difficulty
prediction. When models use only item linguistic features or LLM embeddings,
prediction performance is similar, which suggests that only one of these
feature categories may be required. This item difficulty prediction model can
be used to filter and categorize reading items and will be made publicly
available for use by other stakeholders.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Automatic database description generation for Text-to-SQL 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.20657v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.20657v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yingqi Gao, Zhiling Luo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the context of the Text-to-SQL task, table and column descriptions are
crucial for bridging the gap between natural language and database schema. This
report proposes a method for automatically generating effective database
descriptions when explicit descriptions are unavailable. The proposed method
employs a dual-process approach: a coarse-to-fine process, followed by a
fine-to-coarse process. The coarse-to-fine approach leverages the inherent
knowledge of LLM to guide the understanding process from databases to tables
and finally to columns. This approach provides a holistic understanding of the
database structure and ensures contextual alignment. Conversely, the
fine-to-coarse approach starts at the column level, offering a more accurate
and nuanced understanding when stepping back to the table level. Experimental
results on the Bird benchmark indicate that using descriptions generated by the
proposed improves SQL generation accuracy by 0.93\% compared to not using
descriptions, and achieves 37\% of human-level performance. The source code is
publicly available at https://github.com/XGenerationLab/XiYan-DBDescGen.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Consistency Evaluation of News Article Summaries Generated by Large (and
  Small) Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.20647v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.20647v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Colleen Gilhuly, Haleh Shahzad
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Text summarizing is a critical Natural Language Processing (NLP) task with
applications ranging from information retrieval to content generation. Large
Language Models (LLMs) have shown remarkable promise in generating fluent
abstractive summaries but they can produce hallucinated details not grounded in
the source text. Regardless of the method of generating a summary, high quality
automated evaluations remain an open area of investigation. This paper embarks
on an exploration of text summarization with a diverse set of techniques,
including TextRank, BART, Mistral-7B-Instruct, and OpenAI GPT-3.5-Turbo. The
generated summaries are evaluated using traditional metrics such as the
Recall-Oriented Understudy for Gisting Evaluation (ROUGE) Score and
Bidirectional Encoder Representations from Transformers (BERT) Score, as well
as LLM-powered evaluation methods that directly assess a generated summary's
consistency with the source text. We introduce a meta evaluation score which
directly assesses the performance of the LLM evaluation system (prompt +
model). We find that that all summarization models produce consistent summaries
when tested on the XL-Sum dataset, exceeding the consistency of the reference
summaries.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>21 pages, 6 figures, 4 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LexRAG: Benchmarking Retrieval-Augmented Generation in Multi-Turn Legal
  Consultation Conversation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.20640v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.20640v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haitao Li, Yifan Chen, Yiran Hu, Qingyao Ai, Junjie Chen, Xiaoyu Yang, Jianhui Yang, Yueyue Wu, Zeyang Liu, Yiqun Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Retrieval-augmented generation (RAG) has proven highly effective in improving
large language models (LLMs) across various domains. However, there is no
benchmark specifically designed to assess the effectiveness of RAG in the legal
domain, which restricts progress in this area. To fill this gap, we propose
LexRAG, the first benchmark to evaluate RAG systems for multi-turn legal
consultations. LexRAG consists of 1,013 multi-turn dialogue samples and 17,228
candidate legal articles. Each sample is annotated by legal experts and
consists of five rounds of progressive questioning. LexRAG includes two key
tasks: (1) Conversational knowledge retrieval, requiring accurate retrieval of
relevant legal articles based on multi-turn context. (2) Response generation,
focusing on producing legally sound answers. To ensure reliable
reproducibility, we develop LexiT, a legal RAG toolkit that provides a
comprehensive implementation of RAG system components tailored for the legal
domain. Additionally, we introduce an LLM-as-a-judge evaluation pipeline to
enable detailed and effective assessment. Through experimental analysis of
various LLMs and retrieval methods, we reveal the key limitations of existing
RAG systems in handling legal consultation conversations. LexRAG establishes a
new benchmark for the practical application of RAG systems in the legal domain,
with its code and data available at https://github.com/CSHaitao/LexRAG.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Rectifying Belief Space via Unlearning to Harness LLMs' Reasoning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.20620v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.20620v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ayana Niwa, Masahiro Kaneko, Kentaro Inui
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) can exhibit advanced reasoning yet still
generate incorrect answers. We hypothesize that such errors frequently stem
from spurious beliefs, propositions the model internally considers true but are
incorrect. To address this, we propose a method to rectify the belief space by
suppressing these spurious beliefs while simultaneously enhancing true ones,
thereby enabling more reliable inferences. Our approach first identifies the
beliefs that lead to incorrect or correct answers by prompting the model to
generate textual explanations, using our Forward-Backward Beam Search (FBBS).
We then apply unlearning to suppress the identified spurious beliefs and
enhance the true ones, effectively rectifying the model's belief space.
Empirical results on multiple QA datasets and LLMs show that our method
corrects previously misanswered questions without harming overall model
performance. Furthermore, our approach yields improved generalization on unseen
data, suggesting that rectifying a model's belief space is a promising
direction for mitigating errors and enhancing overall reliability.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Continuous Adversarial Text Representation Learning for Affective
  Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.20613v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.20613v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Seungah Son, Andrez Saurez, Dongsoo Har
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While pre-trained language models excel at semantic understanding, they often
struggle to capture nuanced affective information critical for affective
recognition tasks. To address these limitations, we propose a novel framework
for enhancing emotion-aware embeddings in transformer-based models. Our
approach introduces a continuous valence-arousal labeling system to guide
contrastive learning, which captures subtle and multi-dimensional emotional
nuances more effectively. Furthermore, we employ a dynamic token perturbation
mechanism, using gradient-based saliency to focus on sentiment-relevant tokens,
improving model sensitivity to emotional cues. The experimental results
demonstrate that the proposed framework outperforms existing methods, achieving
up to 15.5% improvement in the emotion classification benchmark, highlighting
the importance of employing continuous labels. This improvement demonstrates
that the proposed framework is effective in affective representation learning
and enables precise and contextually relevant emotional understanding.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 3 figures, The 7th International Conference on Artificial
  Intelligence in Information and Communication (ICAIIC 2025)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Leveraging Large Language Models for Building Interpretable Rule-Based
  Data-to-Text Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.20609v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.20609v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jędrzej Warczyński, Mateusz Lango, Ondrej Dusek
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce a simple approach that uses a large language model (LLM) to
automatically implement a fully interpretable rule-based data-to-text system in
pure Python. Experimental evaluation on the WebNLG dataset showed that such a
constructed system produces text of better quality (according to the BLEU and
BLEURT metrics) than the same LLM prompted to directly produce outputs, and
produces fewer hallucinations than a BART language model fine-tuned on the same
data. Furthermore, at runtime, the approach generates text in a fraction of the
processing time required by neural approaches, using only a single CPU
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ NutriGen: Personalized Meal Plan Generator Leveraging Large Language
  Models to Enhance Dietary and Nutritional Adherence 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.20601v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.20601v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Saman Khamesian, Asiful Arefeen, Stephanie M. Carpenter, Hassan Ghasemzadeh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Maintaining a balanced diet is essential for overall health, yet many
individuals struggle with meal planning due to nutritional complexity, time
constraints, and lack of dietary knowledge. Personalized food recommendations
can help address these challenges by tailoring meal plans to individual
preferences, habits, and dietary restrictions. However, existing dietary
recommendation systems often lack adaptability, fail to consider real-world
constraints such as food ingredient availability, and require extensive user
input, making them impractical for sustainable and scalable daily use. To
address these limitations, we introduce NutriGen, a framework based on large
language models (LLM) designed to generate personalized meal plans that align
with user-defined dietary preferences and constraints. By building a
personalized nutrition database and leveraging prompt engineering, our approach
enables LLMs to incorporate reliable nutritional references like the USDA
nutrition database while maintaining flexibility and ease-of-use. We
demonstrate that LLMs have strong potential in generating accurate and
user-friendly food recommendations, addressing key limitations in existing
dietary recommendation systems by providing structured, practical, and scalable
meal plans. Our evaluation shows that Llama 3.1 8B and GPT-3.5 Turbo achieve
the lowest percentage errors of 1.55\% and 3.68\%, respectively, producing meal
plans that closely align with user-defined caloric targets while minimizing
deviation and improving precision. Additionally, we compared the performance of
DeepSeek V3 against several established models to evaluate its potential in
personalized nutrition planning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The GUS Framework: Benchmarking Social Bias Classification with
  Discriminative (Encoder-Only) and Generative (Decoder-Only) Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.08388v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.08388v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Maximus Powers, Shaina Raza, Alex Chang, Umang Mavani, Harshitha Reddy Jonala, Ansh Tiwari, Hua Wei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The detection of social bias in text is a critical challenge, particularly
due to the limitations of binary classification methods. These methods often
oversimplify nuanced biases, leading to high emotional impact when content is
misclassified as either "biased" or "fair." To address these shortcomings, we
propose a more nuanced framework that focuses on three key linguistic
components underlying social bias: Generalizations, Unfairness, and Stereotypes
(the GUS framework). The GUS framework employs a semi-automated approach to
create a comprehensive synthetic dataset, which is then verified by humans to
maintain ethical standards. This dataset enables robust multi-label token
classification. Our methodology, which combines discriminative (encoder-only)
models and generative (auto-regressive large language models), identifies
biased entities in text. Through extensive experiments, we demonstrate that
encoder-only models are effective for this complex task, often outperforming
state-of-the-art methods, both in terms of macro and entity-wise F1-score and
Hamming loss. These findings can guide the choice of model for different use
cases, highlighting the GUS framework's effectiveness in capturing explicit and
implicit biases across diverse contexts, and offering a pathway for future
research and applications in various fields.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Can Large Language Models Predict the Outcome of Judicial Decisions? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09768v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09768v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohamed Bayan Kmainasi, Ali Ezzat Shahroor, Amani Al-Ghraibah
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have shown exceptional capabilities in Natural
Language Processing (NLP) across diverse domains. However, their application in
specialized tasks such as Legal Judgment Prediction (LJP) for low-resource
languages like Arabic remains underexplored. In this work, we address this gap
by developing an Arabic LJP dataset, collected and preprocessed from Saudi
commercial court judgments. We benchmark state-of-the-art open-source LLMs,
including LLaMA-3.2-3B and LLaMA-3.1-8B, under varying configurations such as
zero-shot, one-shot, and fine-tuning using LoRA. Additionally, we employed a
comprehensive evaluation framework that integrates both quantitative metrics
(such as BLEU, ROUGE, and BERT) and qualitative assessments (including
Coherence, Legal Language, Clarity, etc.) using an LLM. Our results demonstrate
that fine-tuned smaller models achieve comparable performance to larger models
in task-specific contexts while offering significant resource efficiency.
Furthermore, we investigate the impact of fine-tuning the model on a diverse
set of instructions, offering valuable insights into the development of a more
human-centric and adaptable LLM. We have made the dataset, code, and models
publicly available to provide a solid foundation for future research in Arabic
legal NLP.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Explaining Humour Style Classifications: An XAI Approach to
  Understanding Computational Humour Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.02891v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.02891v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mary Ogbuka Kenneth, Foaad Khosmood, Abbas Edalat
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Humour styles can have either a negative or a positive impact on well-being.
Given the importance of these styles to mental health, significant research has
been conducted on their automatic identification. However, the automated
machine learning models used for this purpose are black boxes, making their
prediction decisions opaque. Clarity and transparency are vital in the field of
mental health. This paper presents an explainable AI (XAI) framework for
understanding humour style classification, building upon previous work in
computational humour analysis. Using the best-performing single model
(ALI+XGBoost) from prior research, we apply comprehensive XAI techniques to
analyse how linguistic, emotional, and semantic features contribute to humour
style classification decisions. Our analysis reveals distinct patterns in how
different humour styles are characterised and misclassified, with particular
emphasis on the challenges in distinguishing affiliative humour from other
styles. Through detailed examination of feature importance, error patterns, and
misclassification cases, we identify key factors influencing model decisions,
including emotional ambiguity, context misinterpretation, and target
identification. The framework demonstrates significant utility in understanding
model behaviour, achieving interpretable insights into the complex interplay of
features that define different humour styles. Our findings contribute to both
the theoretical understanding of computational humour analysis and practical
applications in mental health, content moderation, and digital humanities
research.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Logicbreaks: A Framework for Understanding Subversion of Rule-based
  Inference 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.00075v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.00075v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anton Xue, Avishree Khare, Rajeev Alur, Surbhi Goel, Eric Wong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study how to subvert large language models (LLMs) from following
prompt-specified rules. We first formalize rule-following as inference in
propositional Horn logic, a mathematical system in which rules have the form
"if $P$ and $Q$, then $R$" for some propositions $P$, $Q$, and $R$. Next, we
prove that although small transformers can faithfully follow such rules,
maliciously crafted prompts can still mislead both theoretical constructions
and models learned from data. Furthermore, we demonstrate that popular attack
algorithms on LLMs find adversarial prompts and induce attention patterns that
align with our theory. Our novel logic-based framework provides a foundation
for studying LLMs in rule-based settings, enabling a formal analysis of tasks
like logical reasoning and jailbreak attacks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Logical Consistency of Large Language Models in Fact-checking <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.16100v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.16100v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bishwamittra Ghosh, Sarah Hasan, Naheed Anjum Arafat, Arijit Khan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, large language models (LLMs) have demonstrated significant
success in performing varied natural language tasks such as language
translation, question-answering, summarizing, fact-checking, etc. Despite LLMs'
impressive ability to generate human-like texts, LLMs are infamous for their
inconsistent responses - a meaning-preserving change in the input query results
in an inconsistent response and attributes to vulnerabilities of LLMs such as
hallucination. Consequently, existing research focuses on simple
paraphrasing-based consistency assessment of LLMs, and ignores complex queries
that necessitate an even better understanding of logical reasoning by an LLM.
Our work therefore addresses the logical inconsistency of LLMs under complex
logical queries with primitive logical operators, e.g., negation, conjunction,
and disjunction. As a test bed, we consider retrieval-augmented LLMs on a
fact-checking task involving propositional logic queries from knowledge graphs
(KGs). Our contributions are threefold. Benchmark: We introduce three logical
fact-checking datasets over KGs for community development towards logically
consistent LLMs. Assessment: We propose consistency measures of LLMs on
propositional logic queries and demonstrate that existing LLMs lack logical
consistency, especially on complex queries. Improvement: We employ supervised
fine-tuning to improve the logical consistency of LLMs on the complex
fact-checking task with KG contexts. We have made our source code and
benchmarks available.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published at ICLR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Relaxed Recursive <span class="highlight-title">Transformer</span>s: Effective Parameter Sharing with
  Layer-wise LoRA <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.20672v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.20672v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sangmin Bae, Adam Fisch, Hrayr Harutyunyan, Ziwei Ji, Seungyeon Kim, Tal Schuster
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) are expensive to deploy. Parameter sharing
offers a possible path towards reducing their size and cost, but its
effectiveness in modern LLMs remains fairly limited. In this work, we revisit
"layer tying" as form of parameter sharing in Transformers, and introduce novel
methods for converting existing LLMs into smaller "Recursive Transformers" that
share parameters across layers, with minimal loss of performance. Here, our
Recursive Transformers are efficiently initialized from standard pretrained
Transformers, but only use a single block of unique layers that is then
repeated multiple times in a loop. We further improve performance by
introducing Relaxed Recursive Transformers that add flexibility to the layer
tying constraint via depth-wise low-rank adaptation (LoRA) modules, yet still
preserve the compactness of the overall model. We show that our recursive
models (e.g., recursive Gemma 1B) outperform both similar-sized vanilla
pretrained models (such as TinyLlama 1.1B and Pythia 1B) and knowledge
distillation baselines -- and can even recover most of the performance of the
original "full-size" model (e.g., Gemma 2B with no shared parameters). Finally,
we propose Continuous Depth-wise Batching, a promising new inference paradigm
enabled by the Recursive Transformer when paired with early exiting. In a
theoretical analysis, we show that this has the potential to lead to
significant (2-3x) gains in inference throughput.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2025; 49 pages, 17 figures, 19 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Atomas: Hierarchical Alignment on Molecule-Text for Unified Molecule
  Understanding and Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.16880v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.16880v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yikun Zhang, Geyan Ye, Chaohao Yuan, Bo Han, Long-Kai Huang, Jianhua Yao, Wei Liu, Yu Rong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Molecule-and-text cross-modal representation learning has emerged as a
promising direction for enhancing the quality of molecular representation,
thereby improving performance in various scientific fields, including drug
discovery and materials science. Existing studies adopt a global alignment
approach to learn the knowledge from different modalities. These global
alignment approaches fail to capture fine-grained information, such as
molecular fragments and their corresponding textual description, which is
crucial for downstream tasks. Furthermore, it is incapable to model such
information using a similar global alignment strategy due to data scarcity of
paired local part annotated data from existing datasets. In this paper, we
propose Atomas, a multi-modal molecular representation learning framework to
jointly learn representations from SMILES string and text. We design a
Hierarchical Adaptive Alignment model to concurrently learn the fine-grained
fragment correspondence between two modalities and align these representations
of fragments in three levels. Additionally, Atomas's end-to-end training
framework incorporates the tasks of understanding and generating molecule,
thereby supporting a wider range of downstream tasks. In the retrieval task,
Atomas exhibits robust generalization ability and outperforms the baseline by
30.8% of recall@1 on average. In the generation task, Atomas achieves
state-of-the-art results in both molecule captioning task and molecule
generation task. Moreover, the visualization of the Hierarchical Adaptive
Alignment model further confirms the chemical significance of our approach. Our
codes can be found at https://anonymous.4open.science/r/Atomas-03C3.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ You Only Prune Once: Designing Calibration-Free Model Compression With
  Policy Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.15296v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.15296v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ayan Sengupta, Siddhant Chaudhary, Tanmoy Chakraborty
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The ever-increasing size of large language models (LLMs) presents significant
challenges for deployment due to their heavy computational and memory
requirements. Current model pruning techniques attempt to alleviate these
issues by relying heavily on external calibration datasets to determine which
parameters to prune or compress, thus limiting their flexibility and
scalability across different compression ratios. Moreover, these methods often
cause severe performance degradation, particularly in downstream tasks, when
subjected to higher compression rates. In this paper, we propose PruneNet, a
novel model compression method that addresses these limitations by
reformulating model pruning as a policy learning process. PruneNet decouples
the pruning process from the model architecture, eliminating the need for
calibration datasets. It learns a stochastic pruning policy to assess parameter
importance solely based on intrinsic model properties while preserving the
spectral structure to minimize information loss. PruneNet can compress the
LLaMA-2-7B model in just 15 minutes, achieving over 80% retention of its
zero-shot performance with a 30% compression ratio, outperforming existing
methods that retain only 75% performance. Furthermore, on complex multitask
language understanding tasks, PruneNet demonstrates its robustness by
preserving up to 80% performance of the original model, proving itself a
superior alternative to conventional structured compression techniques.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CS-Bench: A Comprehensive Benchmark for Large Language Models towards
  Computer Science Mastery <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.08587v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.08587v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaoshuai Song, Muxi Diao, Guanting Dong, Zhengyang Wang, Yujia Fu, Runqi Qiao, Zhexu Wang, Dayuan Fu, Huangxuan Wu, Bin Liang, Weihao Zeng, Yejie Wang, Zhuoma GongQue, Jianing Yu, Qiuna Tan, Weiran Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have demonstrated significant potential in
advancing various fields of research and society. However, the current
community of LLMs overly focuses on benchmarks for analyzing specific
foundational skills (e.g. mathematics and code generation), neglecting an
all-round evaluation of the computer science field. To bridge this gap, we
introduce CS-Bench, the first multilingual (English, Chinese, French, German)
benchmark dedicated to evaluating the performance of LLMs in computer science.
CS-Bench comprises approximately 10K meticulously curated test samples,
covering 26 subfields across 4 key areas of computer science, encompassing
various task forms and divisions of knowledge and reasoning. Utilizing
CS-Bench, we conduct a comprehensive evaluation of over 30 mainstream LLMs,
revealing the relationship between CS performance and model scales. We also
quantitatively analyze the reasons for failures in existing LLMs and highlight
directions for improvements, including knowledge supplementation and
CS-specific reasoning. Further cross-capability experiments show a high
correlation between LLMs' capabilities in computer science and their abilities
in mathematics and coding. Moreover, expert LLMs specialized in mathematics and
coding also demonstrate strong performances in several CS subfields. Looking
ahead, we envision CS-Bench serving as a cornerstone for LLM applications in
the CS field and paving new avenues in assessing LLMs' diverse reasoning
capabilities. The CS-Bench data and evaluation code are available at
https://github.com/csbench/csbench.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ICLR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SPAM: Spike-Aware Adam with Momentum Reset for Stable LLM Training 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.06842v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.06842v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tianjin Huang, Ziquan Zhu, Gaojie Jin, Lu Liu, Zhangyang Wang, Shiwei Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have demonstrated exceptional performance across
diverse tasks, yet their training remains highly resource-intensive and
susceptible to critical challenges such as training instability. A predominant
source of this instability stems from gradient and loss spikes, which disrupt
the learning process, often leading to costly interventions like checkpoint
recovery and experiment restarts, further amplifying inefficiencies. This paper
presents a comprehensive investigation into gradient spikes observed during LLM
training, revealing their prevalence across multiple architectures and
datasets. Our analysis shows that these spikes can be up to $1000\times$ larger
than typical gradients, substantially deteriorating model performance. To
address this issue, we propose Spike-Aware Adam with Momentum Reset SPAM, a
novel optimizer designed to counteract gradient spikes through momentum reset
and spike-aware gradient clipping. Extensive experiments, including both
pre-training and fine-tuning, demonstrate that SPAM consistently surpasses Adam
and its variants across various tasks, including (1) LLM pre-training from 60M
to 1B, (2) 4-bit LLM pre-training,(3) reinforcement learning, and (4) Time
Series Forecasting. Additionally, SPAM facilitates memory-efficient training by
enabling sparse momentum, where only a subset of momentum terms are maintained
and updated. When operating under memory constraints, SPAM outperforms
state-of-the-art memory-efficient optimizers such as GaLore and Adam-Mini. Our
work underscores the importance of mitigating gradient spikes in LLM training
and introduces an effective optimization strategy that enhances both training
stability and resource efficiency at scale. Code is available at
https://github.com/TianjinYellow/SPAM-Optimizer.git
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GOAT-Bench: Safety Insights to Large Multimodal Models through
  Meme-Based Social Abuse 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01523v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01523v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hongzhan Lin, Ziyang Luo, Bo Wang, Ruichao Yang, Jing Ma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The exponential growth of social media has profoundly transformed how
information is created, disseminated, and absorbed, exceeding any precedent in
the digital age. Regrettably, this explosion has also spawned a significant
increase in the online abuse of memes. Evaluating the negative impact of memes
is notably challenging, owing to their often subtle and implicit meanings,
which are not directly conveyed through the overt text and image. In light of
this, large multimodal models (LMMs) have emerged as a focal point of interest
due to their remarkable capabilities in handling diverse multimodal tasks. In
response to this development, our paper aims to thoroughly examine the capacity
of various LMMs (e.g., GPT-4o) to discern and respond to the nuanced aspects of
social abuse manifested in memes. We introduce the comprehensive meme
benchmark, GOAT-Bench, comprising over 6K varied memes encapsulating themes
such as implicit hate speech, sexism, and cyberbullying, etc. Utilizing
GOAT-Bench, we delve into the ability of LMMs to accurately assess hatefulness,
misogyny, offensiveness, sarcasm, and harmful content. Our extensive
experiments across a range of LMMs reveal that current models still exhibit a
deficiency in safety awareness, showing insensitivity to various forms of
implicit abuse. We posit that this shortfall represents a critical impediment
to the realization of safe artificial intelligence. The GOAT-Bench and
accompanying resources are publicly accessible at https://goatlmm.github.io/,
contributing to ongoing research in this vital field.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The first work to benchmark Large Multimodal Models in safety insight
  on social media</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AdEval: Alignment-based Dynamic Evaluation to Mitigate Data
  Contamination in Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.13983v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.13983v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yang Fan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As Large Language Models (LLMs) are pretrained on massive-scale corpora, the
issue of data contamination has become increasingly severe, leading to
potential overestimation of model performance during evaluation. To address
this, we propose AdEval (Alignment-based Dynamic Evaluation), a dynamic data
evaluation method aimed at mitigating the impact of data contamination on
evaluation reliability. AdEval extracts key knowledge points and main ideas to
align dynamically generated questions with static data's core concepts. It also
leverages online search to provide detailed explanations of related knowledge
points, thereby creating high-quality evaluation samples with robust knowledge
support. Furthermore, AdEval incorporates mechanisms to control the number and
complexity of questions, enabling dynamic alignment and flexible adjustment.
This ensures that the generated questions align with the complexity of static
data while supporting varied complexity levels. Based on Bloom's taxonomy,
AdEval conducts a multi-dimensional evaluation of LLMs across six cognitive
levels: remembering, understanding, applying, analyzing, evaluating, and
creating. Experimental results on multiple datasets demonstrate that AdEval
effectively reduces the impact of data contamination on evaluation outcomes,
enhancing both the fairness and reliability of the evaluation process.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>There are serious academic problems in this paper, such as data
  falsification and plagiarism in the method of the paper</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Are All Spanish Doctors Male? Evaluating Gender Bias in German Machine
  Translation <span class="chip">ISCA</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.19104v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.19104v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Michelle Kappl
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present WinoMTDE, a new gender bias evaluation test set designed to assess
occupational stereotyping and underrepresentation in German machine translation
(MT) systems. Building on the automatic evaluation method introduced by
arXiv:1906.00591v1, we extend the approach to German, a language with
grammatical gender. The WinoMTDE dataset comprises 288 German sentences that
are balanced in regard to gender, as well as stereotype, which was annotated
using German labor statistics. We conduct a large-scale evaluation of five
widely used MT systems and a large language model. Our results reveal
persistent bias in most models, with the LLM outperforming traditional systems.
The dataset and evaluation code are publicly available under
https://github.com/michellekappl/mt_gender_german.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ISCA/ITG Workshop on Diversity in Large Speech and Language Models</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> Learning diverse attacks on large language models for robust red-teaming
  and safety tuning <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.18540v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.18540v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Seanie Lee, Minsu Kim, Lynn Cherif, David Dobre, Juho Lee, Sung Ju Hwang, Kenji Kawaguchi, Gauthier Gidel, <span class="highlight-author">Yoshua Bengio</span>, Nikolay Malkin, Moksh Jain
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Red-teaming, or identifying prompts that elicit harmful responses, is a
critical step in ensuring the safe and responsible deployment of large language
models (LLMs). Developing effective protection against many modes of attack
prompts requires discovering diverse attacks. Automated red-teaming typically
uses reinforcement learning to fine-tune an attacker language model to generate
prompts that elicit undesirable responses from a target LLM, as measured, for
example, by an auxiliary toxicity classifier. We show that even with explicit
regularization to favor novelty and diversity, existing approaches suffer from
mode collapse or fail to generate effective attacks. As a flexible and
probabilistically principled alternative, we propose to use GFlowNet
fine-tuning, followed by a secondary smoothing phase, to train the attacker
model to generate diverse and effective attack prompts. We find that the
attacks generated by our method are effective against a wide range of target
LLMs, both with and without safety tuning, and transfer well between target
LLMs. Finally, we demonstrate that models safety-tuned using a dataset of
red-teaming prompts generated by our method are robust to attacks from other
RL-based red-teaming approaches.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Kanana: Compute-efficient Bilingual Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.18934v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.18934v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                         Kanana LLM Team, Yunju Bak, Hojin Lee, Minho Ryu, Jiyeon Ham, Seungjae Jung, Daniel Wontae Nam, Taegyeong Eo, Donghun Lee, Doohae Jung, Boseop Kim, Nayeon Kim, Jaesun Park, Hyunho Kim, Hyunwoong Ko, Changmin Lee, Kyoung-Woon On, Seulye Baeg, Junrae Cho, Sunghee Jung, Jieun Kang, EungGyun Kim, Eunhwa Kim, Byeongil Ko, Daniel Lee, Minchul Lee, Miok Lee, Shinbok Lee, Gaeun Seo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce Kanana, a series of bilingual language models that demonstrate
exceeding performance in Korean and competitive performance in English. The
computational cost of Kanana is significantly lower than that of
state-of-the-art models of similar size. The report details the techniques
employed during pre-training to achieve compute-efficient yet competitive
models, including high quality data filtering, staged pre-training, depth
up-scaling, and pruning and distillation. Furthermore, the report outlines the
methodologies utilized during the post-training of the Kanana models,
encompassing supervised fine-tuning and preference optimization, aimed at
enhancing their capability for seamless interaction with users. Lastly, the
report elaborates on plausible approaches used for language model adaptation to
specific scenarios, such as embedding, retrieval augmented generation, and
function calling. The Kanana model series spans from 2.1B to 32.5B parameters
with 2.1B models (base, instruct, embedding) publicly released to promote
research on Korean language models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>40 pages, 15 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Super(ficial)-alignment: Strong Models May Deceive Weak Models in
  Weak-to-Strong Generalization <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.11431v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.11431v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenkai Yang, Shiqi Shen, Guangyao Shen, Wei Yao, Yong Liu, Zhi Gong, Yankai Lin, Ji-Rong Wen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Superalignment, where humans act as weak supervisors for superhuman models,
has become a crucial problem with the rapid development of Large Language
Models (LLMs). Recent work has preliminarily studied this problem by using weak
models to supervise strong models, and discovered that weakly supervised strong
students can consistently outperform weak teachers towards the alignment
target, leading to a weak-to-strong generalization phenomenon. However, we are
concerned that behind such a promising phenomenon, whether there exists an
issue of weak-to-strong deception, where strong models deceive weak models by
exhibiting well-aligned in areas known to weak models but producing misaligned
behaviors in cases weak models do not know. We take an initial step towards
exploring this security issue in a specific but realistic multi-objective
alignment case, where there may be some alignment targets conflicting with each
other (e.g., helpfulness v.s. harmlessness). We aim to explore whether, in such
cases, strong models might deliberately make mistakes in areas known to them
but unknown to weak models within one alignment dimension, in exchange for a
higher reward in another dimension. Through extensive experiments in both the
reward modeling and preference optimization scenarios, we find: (1) The
weak-to-strong deception phenomenon exists across all settings. (2) The
deception intensifies as the capability gap between weak and strong models
increases. (3) Bootstrapping with an intermediate model can mitigate the
deception to some extent, though its effectiveness remains limited. Our work
highlights the urgent need to pay more attention to the true reliability of
superalignment.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ICLR 2025, camera-ready version</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Pragmatic Reasoning improves LLM Code Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.15835v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.15835v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhuchen Cao, Sven Apel, Adish Singla, Vera Demberg
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have demonstrated impressive potential in
translating natural language (NL) instructions into program code. However, user
instructions often contain inherent ambiguities, making it challenging for LLMs
to generate code that accurately reflects the user's true intent. To address
this challenge, researchers have proposed to produce multiple candidates of the
program code and then rerank them to identify the best solution. In this paper,
we propose CodeRSA, a novel code candidate reranking mechanism built upon the
Rational Speech Act (RSA) framework, designed to guide LLMs toward more
comprehensive pragmatic reasoning about user intent. We evaluate CodeRSA using
one of the latest LLMs on a popular code generation dataset. Our experiment
results show that CodeRSA consistently outperforms common baselines, surpasses
the state-of-the-art approach in most cases, and demonstrates robust overall
performance. These findings underscore the effectiveness of integrating
pragmatic reasoning into code candidate reranking, offering a promising
direction for enhancing code generation quality in LLMs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ChartMimic: Evaluating LMM's Cross-Modal Reasoning Capability via
  Chart-to-Code Generation <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.09961v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.09961v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Cheng Yang, Chufan Shi, Yaxin Liu, Bo Shui, Junjie Wang, Mohan Jing, Linran Xu, Xinyu Zhu, Siheng Li, Yuxiang Zhang, Gongye Liu, Xiaomei Nie, Deng Cai, Yujiu Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce a new benchmark, ChartMimic, aimed at assessing the
visually-grounded code generation capabilities of large multimodal models
(LMMs). ChartMimic utilizes information-intensive visual charts and textual
instructions as inputs, requiring LMMs to generate the corresponding code for
chart rendering. ChartMimic includes 4,800 human-curated (figure, instruction,
code) triplets, which represent the authentic chart use cases found in
scientific papers across various domains (e.g., Physics, Computer Science,
Economics, etc). These charts span 18 regular types and 4 advanced types,
diversifying into 201 subcategories. Furthermore, we propose multi-level
evaluation metrics to provide an automatic and thorough assessment of the
output code and the rendered charts. Unlike existing code generation
benchmarks, ChartMimic places emphasis on evaluating LMMs' capacity to
harmonize a blend of cognitive capabilities, encompassing visual understanding,
code generation, and cross-modal reasoning. The evaluation of $3$ proprietary
models and 14 open-weight models highlights the substantial challenges posed by
ChartMimic. Even the advanced GPT-4o, InternVL2-Llama3-76B only achieved an
average score across Direct Mimic and Customized Mimic tasks of 82.2 and 61.6,
respectively, indicating significant room for improvement. We anticipate that
ChartMimic will inspire the development of LMMs, advancing the pursuit of
artificial general intelligence.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ICLR 2025. Data and code are available at
  https://github.com/ChartMimic/ChartMimic</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SelectLLM: Query-Aware Efficient Selection Algorithm for Large Language
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.08545v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.08545v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kaushal Kumar Maurya, KV Aditya Srivatsa, Ekaterina Kochmar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have been widely adopted due to their remarkable
performance across various applications, driving the accelerated development of
a large number of diverse models. However, these individual LLMs show
limitations in generalization and performance on complex tasks due to inherent
training biases, model size constraints, and the quality or diversity of
pre-training datasets. A promising direction is to efficiently harness the
diverse capabilities of LLMs to overcome these individual limitations. To
address these limitations, we introduce a novel LLM selection algorithm called
SelectLLM, which efficiently directs input queries to the most suitable subset
of LLMs from a large pool, ensuring that the selected models collectively
provide accurate responses. SelectLLM employs a multi-label classifier and
policy based on the classifier's predictions and confidence scores in selecting
an optimal, query-aware, and lightweight subset of LLMs. Our findings indicate
that the proposed model outperforms existing ensemble-based baselines and
achieves competitive performance with similarly sized top-performing LLMs while
maintaining efficiency. Specifically, it achieves a huge reduction in inference
latency on two challenging reasoning benchmarks: 13% on GSM8K and 70% on MMLU,
compared to the top-performing baseline. Also, we establish a theoretical upper
bound by an Oracle with LLMs and perform an in-depth linguistic analysis to
understand the performance gap between the Oracle and SelectLLM.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Training-Free Exponential Context Extension via Cascading KV Cache 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.17808v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.17808v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jeffrey Willette, Heejun Lee, Youngwan Lee, Myeongjae Jeon, Sung Ju Hwang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The transformer's context window is vital for tasks such as few-shot learning
and conditional generation as it preserves previous tokens for active memory.
However, as the context lengths increase, the computational costs grow
quadratically, hindering the deployment of large language models (LLMs) in
real-world, long sequence scenarios. Although some recent key-value caching (KV
Cache) methods offer linear inference complexity, they naively manage the
stored context, prematurely evicting tokens and losing valuable information.
Moreover, they lack an optimized prefill/prompt stage strategy, resulting in
higher latency than even quadratic attention for realistic context sizes. In
response, we introduce a novel mechanism that leverages cascading sub-cache
buffers to selectively retain the most relevant tokens, enabling the model to
maintain longer context histories without increasing the cache size. Our
approach outperforms linear caching baselines across key benchmarks, including
streaming perplexity, question answering, book summarization, and passkey
retrieval, where it retains better retrieval accuracy at 1M tokens after four
doublings of the cache size of 65K. Additionally, our method reduces prefill
stage latency by a factor of 6.8 when compared to flash attention on 1M tokens.
These innovations not only enhance the computational efficiency of LLMs but
also pave the way for their effective deployment in resource-constrained
environments, enabling large-scale, real-time applications with significantly
reduced latency.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LLM2: Let Large Language Models Harness System 2 Reasoning <span class="chip">NAACL 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.20372v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.20372v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Cheng Yang, Chufan Shi, Siheng Li, Bo Shui, Yujiu Yang, Wai Lam
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have exhibited impressive capabilities across a
myriad of tasks, yet they occasionally yield undesirable outputs. We posit that
these limitations are rooted in the foundational autoregressive architecture of
LLMs, which inherently lacks mechanisms for differentiating between desirable
and undesirable results. Drawing inspiration from the dual-process theory of
human cognition, we introduce LLM2, a novel framework that combines an LLM
(System 1) with a process-based verifier (System 2). Within LLM2, the LLM is
responsible for generating plausible candidates, while the verifier provides
timely process-based feedback to distinguish desirable and undesirable outputs.
The verifier is trained with a pairwise comparison loss on synthetic
process-supervision data generated through our token quality exploration
strategy. Empirical results on mathematical reasoning benchmarks substantiate
the efficacy of LLM2, exemplified by an accuracy enhancement from 50.3 to 57.8
(+7.5) for Llama3-1B on GSM8K. Furthermore, when combined with
self-consistency, LLM2 achieves additional improvements, boosting major@20
accuracy from 56.2 to 70.2 (+14.0).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to NAACL 2025 Main Conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Behind the Tip of Efficiency: Uncovering the Submerged Threats of
  Jailbreak Attacks in Small Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.19883v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.19883v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sibo Yi, Tianshuo Cong, Xinlei He, Qi Li, Jiaxing Song
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Small language models (SLMs) have become increasingly prominent in the
deployment on edge devices due to their high efficiency and low computational
cost. While researchers continue to advance the capabilities of SLMs through
innovative training strategies and model compression techniques, the security
risks of SLMs have received considerably less attention compared to large
language models (LLMs).To fill this gap, we provide a comprehensive empirical
study to evaluate the security performance of 13 state-of-the-art SLMs under
various jailbreak attacks. Our experiments demonstrate that most SLMs are quite
susceptible to existing jailbreak attacks, while some of them are even
vulnerable to direct harmful prompts.To address the safety concerns, we
evaluate several representative defense methods and demonstrate their
effectiveness in enhancing the security of SLMs. We further analyze the
potential security degradation caused by different SLM techniques including
architecture compression, quantization, knowledge distillation, and so on. We
expect that our research can highlight the security challenges of SLMs and
provide valuable insights to future work in developing more robust and secure
SLMs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages. 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MiCEval: Unveiling Multimodal Chain of Thought's Quality via Image
  Description and Reasoning Steps <span class="chip">NAACL 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14668v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14668v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiongtao Zhou, Jie He, Lanyu Chen, Jingyu Li, Haojing Chen, Víctor Gutiérrez-Basulto, Jeff Z. Pan, Hanjie Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal Chain of Thought (MCoT) is a popular prompting strategy for
improving the performance of multimodal large language models (MLLMs) across a
range of complex reasoning tasks. Despite its popularity, there is a notable
absence of automated methods for evaluating the quality of reasoning steps in
MCoT. To address this gap, we propose Multimodal Chain-of-Thought Evaluation
(MiCEval), a framework designed to assess the correctness of reasoning chains
by evaluating the quality of both the description and each reasoning step. The
evaluation of the description component focuses on the accuracy of the image
descriptions, while the reasoning step evaluates the quality of each step as it
is conditionally generated based on the preceding steps. MiCEval is built upon
a fine-grained dataset with annotations that rate each step according to
correctness, relevance, and informativeness. Extensive experiments on four
state-of-the-art MLLMs show that step-wise evaluations using MiCEval align more
closely with human judgments compared to existing methods based on cosine
similarity or fine-tuning approaches. MiCEval datasets and code can be found in
https://github.com/alenai97/MiCEval.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NAACL 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DropBP: Accelerating Fine-Tuning of Large Language Models by Dropping
  Backward Propagation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.17812v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.17812v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sunghyeon Woo, Baeseong Park, Byeongwook Kim, Minjung Jo, Se Jung Kwon, Dongsuk Jeon, Dongsoo Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have achieved significant success across various
domains. However, training these LLMs typically involves substantial memory and
computational costs during both forward and backward propagation. While
parameter-efficient fine-tuning (PEFT) considerably reduces the training memory
associated with parameters, it does not address the significant computational
costs and activation memory. In this paper, we propose Dropping Backward
Propagation (DropBP), a novel approach designed to reduce computational costs
and activation memory while maintaining accuracy. DropBP randomly drops layers
during backward propagation, which is essentially equivalent to training
shallow submodules generated by undropped layers and residual connections.
Additionally, DropBP calculates the sensitivity of each layer to assign an
appropriate drop rate, thereby stabilizing the training process. DropBP is not
only applicable to full fine-tuning but can also be orthogonally integrated
with all types of PEFT by dropping layers during backward propagation.
Specifically, DropBP can reduce training time by 44% with comparable accuracy
to the baseline, accelerate convergence to the same perplexity by 1.5x, and
enable training with a sequence length 6.2x larger on a single NVIDIA-A100 GPU.
Furthermore, our DropBP enabled a throughput increase of 79% on a NVIDIA A100
GPU and 117% on an Intel Gaudi2 HPU. The code is available at
https://github.com/WooSunghyeon/dropbp.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ForecastBench: A Dynamic Benchmark of AI Forecasting Capabilities 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.19839v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.19839v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ezra Karger, Houtan Bastani, Chen Yueh-Han, Zachary Jacobs, Danny Halawi, Fred Zhang, Philip E. Tetlock
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Forecasts of future events are essential inputs into informed
decision-making. Machine learning (ML) systems have the potential to deliver
forecasts at scale, but there is no framework for evaluating the accuracy of ML
systems on a standardized set of forecasting questions. To address this gap, we
introduce ForecastBench: a dynamic benchmark that evaluates the accuracy of ML
systems on an automatically generated and regularly updated set of 1,000
forecasting questions. To avoid any possibility of data leakage, ForecastBench
is comprised solely of questions about future events that have no known answer
at the time of submission. We quantify the capabilities of current ML systems
by collecting forecasts from expert (human) forecasters, the general public,
and LLMs on a random subset of questions from the benchmark ($N=200$). While
LLMs have achieved super-human performance on many benchmarks, they perform
less well here: expert forecasters outperform the top-performing LLM ($p$-value
$<0.001$). We display system and human scores in a public leaderboard at
www.forecastbench.org.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Explore the Reasoning Capability of LLMs in the Chess Testbed <span class="chip">NAACL2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.06655v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.06655v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shu Wang, Lei Ji, Renxi Wang, Wenxiao Zhao, Haokun Liu, Yifan Hou, Ying Nian Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reasoning is a central capability of human intelligence. In recent years,
with the advent of large-scale datasets, pretrained large language models have
emerged with new capabilities, including reasoning. However, these models still
struggle with long-term, complex reasoning tasks, such as playing chess. Based
on the observation that expert chess players employ a dual approach combining
long-term strategic play with short-term tactical play along with language
explanation, we propose improving the reasoning capability of large language
models in chess by integrating annotated strategy and tactic. Specifically, we
collect a dataset named MATE, which consists of 1 million chess positions with
candidate moves annotated by chess experts for strategy and tactics. We
finetune the LLaMA-3-8B model and compare it against state-of-the-art
commercial language models in the task of selecting better chess moves. Our
experiments show that our models perform better than GPT, Claude, and Gemini
models. We find that language explanations can enhance the reasoning capability
of large language models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NAACL2025 Main Conference. Data and models are available:
  https://mate-chess.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Eliciting In-context Retrieval and Reasoning for Long-context Large
  Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08248v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08248v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yifu Qiu, Varun Embar, Yizhe Zhang, Navdeep Jaitly, Shay B. Cohen, Benjamin Han
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in long-context language models (LCLMs) promise to
transform Retrieval-Augmented Generation (RAG) by simplifying pipelines. With
their expanded context windows, LCLMs can process entire knowledge bases and
perform retrieval and reasoning directly -- a capability we define as
In-Context Retrieval and Reasoning (ICR^2). However, existing benchmarks like
LOFT often overestimate LCLM performance by providing overly simplified
contexts. To address this, we introduce ICR^2, a benchmark that evaluates LCLMs
in more realistic scenarios by including confounding passages retrieved with
strong retrievers. We then propose three methods to enhance LCLM performance:
(1) retrieve-then-generate fine-tuning, (2) retrieval-attention-probing, which
uses attention heads to filter and de-noise long contexts during decoding, and
(3) joint retrieval head training alongside the generation head. Our evaluation
of five well-known LCLMs on LOFT and ICR^2 demonstrates significant gains with
our best approach applied to Mistral-7B: +17 and +15 points by Exact Match on
LOFT, and +13 and +2 points on ICR^2, compared to vanilla RAG and supervised
fine-tuning, respectively. It even outperforms GPT-4-Turbo on most tasks
despite being a much smaller model.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Do as I do (Safely): Mitigating Task-Specific Fine-tuning Risks in Large
  Language Models <span class="chip">ICLR'25</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.10288v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.10288v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Francisco Eiras, Aleksandar Petrov, Philip H. S. Torr, M. Pawan Kumar, Adel Bibi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent research shows that fine-tuning on benign instruction-following data
can inadvertently undo the safety alignment process and increase a model's
propensity to comply with harmful queries. While instruction-following
fine-tuning is important, task-specific fine-tuning - where models are trained
on datasets with clear ground truth answers (e.g., multiple choice questions) -
can enhance model performance on specialized downstream tasks. Understanding
and mitigating safety risks in the task-specific setting remains distinct from
the instruction-following context due to structural differences in the data.
Our work demonstrates how malicious actors can subtly manipulate the structure
of almost any task-specific dataset to foster significantly more dangerous
model behaviors, while maintaining an appearance of innocuity and reasonable
downstream task performance. To address this issue, we propose a novel
mitigation strategy that mixes in safety data which mimics the task format and
prompting style of the user data, showing this is significantly more effective
and efficient than existing baselines at re-establishing safety alignment while
maintaining similar task performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ICLR'25</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning Efficient Recursive Numeral Systems via Reinforcement Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.07170v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.07170v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andrea Silvi, Jonathan Thomas, Emil Carlsson, Devdatt Dubhashi, Moa Johansson
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  It has previously been shown that by using reinforcement learning (RL),
agents can derive simple approximate and exact-restricted numeral systems that
are similar to human ones (Carlsson, 2021). However, it is a major challenge to
show how more complex recursive numeral systems, similar to for example
English, could arise via a simple learning mechanism such as RL. Here, we
introduce an approach towards deriving a mechanistic explanation of the
emergence of efficient recursive number systems. We consider pairs of agents
learning how to communicate about numerical quantities through a meta-grammar
that can be gradually modified throughout the interactions. %We find that the
seminal meta-grammar of Hurford (Hurford, 1975) is not suitable for this
application as its optimization results in systems that deviate from standard
conventions observed within human numeral systems. We propose a simple
modification which addresses this issue. Utilising a slightly modified version
of the meta-grammar of Hurford, we demonstrate that our RL agents, shaped by
the pressures for efficient communication, can effectively modify their lexicon
towards Pareto-optimal configurations which are comparable to those observed
within human numeral systems in terms of their efficiency.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Alleviating Distribution Shift in Synthetic Data for Machine Translation
  Quality Estimation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.19941v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.19941v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiang Geng, Zhejian Lai, Jiajun Chen, Hao Yang, Shujian Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Quality Estimation (QE) models evaluate the quality of machine translations
without reference translations, serving as the reward models for the
translation task. Due to the data scarcity, synthetic data generation has
emerged as a promising solution. However, synthetic QE data often suffers from
distribution shift, which can manifest as discrepancies between pseudo and real
translations, or in pseudo labels that do not align with human preferences. To
tackle this issue, we introduce ADSQE, a novel framework for alleviating
distribution shift in synthetic QE data. To reduce the difference between
pseudo and real translations, we employ the constrained beam search algorithm
and enhance translation diversity through the use of distinct generation
models. ADSQE uses references, i.e., translation supervision signals, to guide
both the generation and annotation processes, enhancing the quality of
word-level labels. ADSE further identifies the shortest phrase covering
consecutive error tokens, mimicking human annotation behavior, to assign the
final phrase-level labels. Specially, we underscore that the translation model
can not annotate translations of itself accurately. Extensive experiments
demonstrate that ADSQE outperforms SOTA baselines like COMET in both supervised
and unsupervised settings. Further analysis offers insights into synthetic data
generation that could benefit reward models for other tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Small Models are LLM Knowledge Triggers on Medical Tabular Prediction <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.01570v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.01570v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiahuan Yan, Jintai Chen, Chaowen Hu, Bo Zheng, Yaojun Hu, Jimeng Sun, Jian Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent development in large language models (LLMs) has demonstrated
impressive domain proficiency on unstructured textual or multi-modal tasks.
However, despite with intrinsic world knowledge, their application on
structured tabular data prediction still lags behind, primarily due to the
numerical insensitivity and modality discrepancy that brings a gap between LLM
reasoning and statistical tabular learning. Unlike textual or vision data
(e.g., electronic clinical notes or medical imaging data), tabular data is
often presented in heterogeneous numerical values (e.g., CBC reports). This
ubiquitous data format requires intensive expert annotation, and its numerical
nature limits LLMs' capability to effectively transfer untapped domain
expertise. In this paper, we propose SERSAL, a general self-prompting method by
synergy learning with small models to enhance LLM tabular prediction in an
unsupervised manner. Specifically, SERSAL utilizes the LLM's prior outcomes as
original soft noisy annotations, which are dynamically leveraged to teach a
better small student model. Reversely, the outcomes from the trained small
model are used to teach the LLM to further refine its real capability. This
process can be repeatedly applied to gradually distill refined knowledge for
continuous progress. Comprehensive experiments on widely used medical domain
tabular datasets show that, without access to gold labels, applying SERSAL to
OpenAI GPT reasoning process attains substantial improvement compared to
linguistic prompting methods, which serves as an orthogonal direction for
tabular LLM, and increasing prompting bonus is observed as more powerful LLMs
appear.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ICLR 2025. Codes will be available at
  https://github.com/jyansir/sersal</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Unlocking State-Tracking in Linear RNNs Through Negative Eigenvalues <span class="chip">ICLR</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.12537v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.12537v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Riccardo Grazzi, Julien Siems, Jörg K. H. Franke, Arber Zela, Frank Hutter, Massimiliano Pontil
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Linear Recurrent Neural Networks (LRNNs) such as Mamba, RWKV, GLA, mLSTM, and
DeltaNet have emerged as efficient alternatives to Transformers for long
sequences. However, both Transformers and LRNNs struggle to perform
state-tracking, which may impair performance in tasks such as code evaluation.
In one forward pass, current architectures are unable to solve even parity, the
simplest state-tracking task, which non-linear RNNs can handle effectively.
Recently, Sarrof et al. (2024) demonstrated that the failure of LRNNs like
Mamba to solve parity stems from restricting the value range of their diagonal
state-transition matrices to $[0, 1]$ and that incorporating negative values
can resolve this issue. We extend this result to non-diagonal LRNNs such as
DeltaNet. We prove that finite precision LRNNs with state-transition matrices
having only positive eigenvalues cannot solve parity, while non-triangular
matrices are needed to count modulo $3$. Notably, we also prove that LRNNs can
learn any regular language when their state-transition matrices are products of
identity minus vector outer product matrices, each with eigenvalues in the
range $[-1, 1]$. Our experiments confirm that extending the eigenvalue range of
Mamba and DeltaNet to include negative values not only enables them to solve
parity but consistently improves their performance on state-tracking tasks. We
also show that state-tracking enabled LRNNs can be pretrained stably and
efficiently at scale (1.3B parameters), achieving competitive performance on
language modeling and showing promise on code and math tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>V2: Correction to Theorem 1 and 2 and to point 3 of Proposition 1.
  V3: ICLR Camera Ready</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ColPali: Efficient Document Retrieval with Vision Language Models <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.01449v6">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.01449v6.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Manuel Faysse, Hugues Sibille, Tony Wu, Bilel Omrani, Gautier Viaud, Céline Hudelot, Pierre Colombo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Documents are visually rich structures that convey information through text,
but also figures, page layouts, tables, or even fonts. Since modern retrieval
systems mainly rely on the textual information they extract from document pages
to index documents -often through lengthy and brittle processes-, they struggle
to exploit key visual cues efficiently. This limits their capabilities in many
practical document retrieval applications such as Retrieval Augmented
Generation (RAG). To benchmark current systems on visually rich document
retrieval, we introduce the Visual Document Retrieval Benchmark ViDoRe,
composed of various page-level retrieval tasks spanning multiple domains,
languages, and practical settings. The inherent complexity and performance
shortcomings of modern systems motivate a new concept; doing document retrieval
by directly embedding the images of the document pages. We release ColPali, a
Vision Language Model trained to produce high-quality multi-vector embeddings
from images of document pages. Combined with a late interaction matching
mechanism, ColPali largely outperforms modern document retrieval pipelines
while being drastically simpler, faster and end-to-end trainable. We release
models, data, code and benchmarks under open licenses at https://hf.co/vidore.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published as a conference paper at ICLR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Measuring Data Diversity for Instruction Tuning: A Systematic Analysis
  and A Reliable Metric 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.17184v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.17184v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuming Yang, Yang Nan, Junjie Ye, Shihan Dou, Xiao Wang, Shuo Li, Huijie Lv, Mingqi Wu, Tao Gui, Qi Zhang, Xuanjing Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Data diversity is crucial for the instruction tuning of large language
models. Existing studies have explored various diversity-aware data selection
methods to construct high-quality datasets and enhance model performance.
However, the fundamental problem of precisely defining and measuring data
diversity remains underexplored, limiting clear guidance for data engineering.
To address this, we systematically analyze 11 existing diversity measurement
methods by evaluating their correlation with model performance through
extensive fine-tuning experiments. Our results indicate that a reliable
diversity measure should properly account for both inter-sample differences and
the information distribution in the sample space. Building on this, we propose
NovelSum, a new diversity metric based on sample-level "novelty." Experiments
on both simulated and real-world data show that NovelSum accurately captures
diversity variations and achieves a 0.97 correlation with instruction-tuned
model performance, highlighting its value in guiding data engineering
practices. With NovelSum as an optimization objective, we further develop a
greedy, diversity-oriented data selection strategy that outperforms existing
approaches, validating both the effectiveness and practical significance of our
metric.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages. The related codes and resources will be released later.
  Project page: https://github.com/UmeanNever/NovelSum</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Energy-Based Diffusion Language Models for Text Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.21357v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.21357v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Minkai Xu, Tomas Geffner, Karsten Kreis, Weili Nie, Yilun Xu, Jure Leskovec, Stefano Ermon, Arash Vahdat
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite remarkable progress in autoregressive language models, alternative
generative paradigms beyond left-to-right generation are still being actively
explored. Discrete diffusion models, with the capacity for parallel generation,
have recently emerged as a promising alternative. Unfortunately, these models
still underperform the autoregressive counterparts, with the performance gap
increasing when reducing the number of sampling steps. Our analysis reveals
that this degradation is a consequence of an imperfect approximation used by
diffusion models. In this work, we propose Energy-based Diffusion Language
Model (EDLM), an energy-based model operating at the full sequence level for
each diffusion step, introduced to improve the underlying approximation used by
diffusion models. More specifically, we introduce an EBM in a residual form,
and show that its parameters can be obtained by leveraging a pretrained
autoregressive model or by finetuning a bidirectional transformer via noise
contrastive estimation. We also propose an efficient generation algorithm via
parallel important sampling. Comprehensive experiments on language modeling
benchmarks show that our model can consistently outperform state-of-the-art
diffusion models by a significant margin, and approaches autoregressive models'
perplexity. We further show that, without any generation performance drop, our
framework offers a 1.3$\times$ sampling speedup over existing diffusion models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Beyond Natural Language Perplexity: Detecting Dead Code Poisoning in
  Code Generation <span class="highlight-title">Dataset</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.20246v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.20246v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chi-Chien Tsai, Chia-Mu Yu, Ying-Dar Lin, Yu-Sung Wu, Wei-Bin Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The increasing adoption of large language models (LLMs) for code-related
tasks has raised concerns about the security of their training datasets. One
critical threat is dead code poisoning, where syntactically valid but
functionally redundant code is injected into training data to manipulate model
behavior. Such attacks can degrade the performance of neural code search
systems, leading to biased or insecure code suggestions. Existing detection
methods, such as token-level perplexity analysis, fail to effectively identify
dead code due to the structural and contextual characteristics of programming
languages. In this paper, we propose DePA (Dead Code Perplexity Analysis), a
novel line-level detection and cleansing method tailored to the structural
properties of code. DePA computes line-level perplexity by leveraging the
contextual relationships between code lines and identifies anomalous lines by
comparing their perplexity to the overall distribution within the file. Our
experiments on benchmark datasets demonstrate that DePA significantly
outperforms existing methods, achieving 0.14-0.19 improvement in detection
F1-score and a 44-65% increase in poisoned segment localization precision.
Furthermore, DePA enhances detection speed by 0.62-23x, making it practical for
large-scale dataset cleansing. Overall, by addressing the unique challenges of
dead code poisoning, DePA provides a robust and efficient solution for
safeguarding the integrity of code generation model training datasets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Scaling Large-Language-Model-based Multi-Agent Collaboration <span class="chip">ICLR-2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.07155v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.07155v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chen Qian, Zihao Xie, YiFei Wang, Wei Liu, Kunlun Zhu, Hanchen Xia, Yufan Dang, Zhuoyun Du, Weize Chen, Cheng Yang, Zhiyuan Liu, Maosong Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent breakthroughs in large language model-driven autonomous agents have
revealed that multi-agent collaboration often surpasses each individual through
collective reasoning. Inspired by the neural scaling law--increasing neurons
enhances performance, this study explores whether the continuous addition of
collaborative agents can yield similar benefits. Technically, we utilize
directed acyclic graphs to organize agents into a multi-agent collaboration
network (MacNet), upon which their interactive reasoning is topologically
orchestrated for autonomous task solving. Extensive evaluations reveal that it
effectively supports collaboration among over a thousand agents, with irregular
topologies outperforming regular ones. We also identify a collaborative scaling
law--the overall performance follows a logistic growth pattern as agents scale,
with collaborative emergence occurring earlier than traditional neural
emergence. We speculate this may be because scaling agents catalyzes their
multidimensional considerations during interactive reflection and refinement,
thereby producing more comprehensive artifacts. The code is available at
https://github.com/OpenBMB/ChatDev/tree/macnet.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ICLR-2025; https://github.com/OpenBMB/ChatDev/tree/macnet</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AutoBencher: Towards Declarative Benchmark Construction <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.08351v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.08351v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiang Lisa Li, Farzaan Kaiyom, Evan Zheran Liu, Yifan Mai, Percy Liang, Tatsunori Hashimoto
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present AutoBencher, a declarative framework for automatic benchmark
construction, and use it to scalably discover novel insights and
vulnerabilities of existing language models. Concretely, given a few desiderata
of benchmarks (e.g., question difficulty, topic salience), we operationalize
each desideratum and cast benchmark creation as an optimization problem.
Specifically, we experiment with two settings with different optimization
objectives: (i) for capability evaluation, we declare the goal of finding a
salient, difficult dataset that induces novel performance patterns; (ii) for
safety evaluation, we declare the goal of finding a dataset of unsafe prompts
that existing LMs fail to decline. To tackle this optimization problem, we use
a language model to iteratively propose and refine dataset descriptions, which
are then used to generate topic-specific questions and answers. These
descriptions are optimized to improve the declared desiderata. We use
AutoBencher (powered by GPT-4) to create datasets for math, multilinguality,
knowledge, and safety. The scalability of AutoBencher allows it to test
fine-grained categories and tail knowledge, creating datasets that elicit 22%
more model errors (i.e., difficulty) than existing benchmarks. On the novelty
ends, AutoBencher also helps identify specific gaps not captured by existing
benchmarks: e.g., Gemini-Pro has knowledge gaps on Permian Extinction and
Fordism while GPT-4o fails to decline harmful requests about cryptocurrency
scams.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for publication at ICLR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Self-Training Elicits Concise Reasoning in Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.20122v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.20122v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tergel Munkhbat, Namgyu Ho, Seo Hyun Kim, Yongjin Yang, Yujin Kim, Se-Young Yun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Chain-of-thought (CoT) reasoning has enabled large language models (LLMs) to
utilize additional computation through intermediate tokens to solve complex
tasks. However, we posit that typical reasoning traces contain many redundant
tokens, incurring extraneous inference costs. Upon examination of the output
distribution of current LLMs, we find evidence on their latent ability to
reason more concisely, relative to their default behavior. To elicit this
capability, we propose simple fine-tuning methods which leverage self-generated
concise reasoning paths obtained by best-of-N sampling and few-shot
conditioning, in task-specific settings. Our combined method achieves a 30%
reduction in output tokens on average, across five model families on GSM8K and
MATH, while maintaining average accuracy. By exploiting the fundamental
stochasticity and in-context learning capabilities of LLMs, our self-training
approach robustly elicits concise reasoning on a wide range of models,
including those with extensive post-training. Code is available at
https://github.com/TergelMunkhbat/concise-reasoning
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>23 pages, 10 figures, 18 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A non-ergodic framework for understanding emergent capabilities in Large
  Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.01638v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.01638v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Javier Marín
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models have emergent capabilities that come unexpectedly at
scale, but we need a theoretical framework to explain why and how they emerge.
We prove that language models are actually non-ergodic systems while providing
a mathematical framework based on Stuart Kauffman's theory of the adjacent
possible (TAP) to explain capability emergence. Our resource-constrained TAP
equation demonstrates how architectural, training, and contextual constraints
interact to shape model capabilities through phase transitions in semantic
space. We prove through experiments with three different language models that
capacities emerge through discrete transitions guided by constraint
interactions and path-dependent exploration. This framework provides a
theoretical basis for understanding emergence in language models and guides the
development of architectures that can guide capability emergence.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Bootstrapping Language-Guided Navigation Learning with Self-Refining
  Data Flywheel 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.08467v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.08467v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zun Wang, Jialu Li, Yicong Hong, Songze Li, Kunchang Li, Shoubin Yu, Yi Wang, Yu Qiao, Yali Wang, Mohit Bansal, Limin Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Creating high-quality data for training robust language-instructed agents is
a long-lasting challenge in embodied AI. In this paper, we introduce a
Self-Refining Data Flywheel (SRDF) that generates high-quality and large-scale
navigational instruction-trajectory pairs by iteratively refining the data pool
through the collaboration between two models, the instruction generator and the
navigator, without any human-in-the-loop annotation. Specifically, SRDF starts
with using a base generator to create an initial data pool for training a base
navigator, followed by applying the trained navigator to filter the data pool.
This leads to higher-fidelity data to train a better generator, which can, in
turn, produce higher-quality data for training the next-round navigator. Such a
flywheel establishes a data self-refining process, yielding a continuously
improved and highly effective dataset for large-scale language-guided
navigation learning. Our experiments demonstrate that after several flywheel
rounds, the navigator elevates the performance boundary from 70% to 78% SPL on
the classic R2R test set, surpassing human performance (76%) for the first
time. Meanwhile, this process results in a superior generator, evidenced by a
SPICE increase from 23.5 to 26.2, better than all previous VLN instruction
generation methods. Finally, we demonstrate the scalability of our method
through increasing environment and instruction diversity, and the
generalization ability of our pre-trained navigator across various downstream
navigation tasks, surpassing state-of-the-art methods by a large margin in all
cases.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>28 pages, Code and data are available at
  https://github.com/wz0919/VLN-SRDF</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ChroKnowledge: Unveiling Chronological Knowledge of Language Models in
  Multiple Domains <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.09870v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.09870v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yein Park, Chanwoong Yoon, Jungwoo Park, Donghyeon Lee, Minbyul Jeong, Jaewoo Kang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have brought significant changes to many aspects
of our lives. However, assessing and ensuring their chronological knowledge
remains challenging. Existing approaches fall short in addressing the temporal
adaptability of knowledge, often relying on a fixed time-point view. To
overcome this, we introduce ChroKnowBench, a benchmark dataset designed to
evaluate chronologically accumulated knowledge across three key aspects:
multiple domains, time dependency, temporal state. Our benchmark distinguishes
between knowledge that evolves (e.g., personal history, scientific discoveries,
amended laws) and knowledge that remain constant (e.g., mathematical truths,
commonsense facts). Building on this benchmark, we present ChroKnowledge
(Chronological Categorization of Knowledge), a novel sampling-based framework
for evaluating LLMs' non-parametric chronological knowledge. Our evaluation led
to the following observations: (1) The ability of eliciting temporal knowledge
varies depending on the data format that model was trained on. (2) LLMs
partially recall knowledge or show a cut-off at temporal boundaries rather than
recalling all aspects of knowledge correctly. Thus, we apply our
ChroKnowPrompt, an in-depth prompting to elicit chronological knowledge by
traversing step-by-step through the surrounding time spans. We observe that it
successfully recalls objects across both open-source and proprietary LLMs,
demonstrating versatility, though it faces challenges with dynamic datasets and
unstructured formats.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2025, 40 pages, 17 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MIRAGE: Evaluating and Explaining Inductive Reasoning Process in
  Language Models <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.09542v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.09542v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiachun Li, Pengfei Cao, Zhuoran Jin, Yubo Chen, Kang Liu, Jun Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Inductive reasoning is an essential capability for large language models
(LLMs) to achieve higher intelligence, which requires the model to generalize
rules from observed facts and then apply them to unseen examples. We present
MIRAGE, a synthetic dataset that addresses the limitations of previous work,
specifically the lack of comprehensive evaluation and flexible test data. In
it, we evaluate LLMs' capabilities in both the inductive and deductive stages,
allowing for flexible variation in input distribution, task scenario, and task
difficulty to analyze the factors influencing LLMs' inductive reasoning. Based
on these multi-faceted evaluations, we demonstrate that the LLM is a poor
rule-based reasoner. In many cases, when conducting inductive reasoning, they
do not rely on a correct rule to answer the unseen case. From the perspectives
of different prompting methods, observation numbers, and task forms, models
tend to consistently conduct correct deduction without correct inductive rules.
Besides, we find that LLMs are good neighbor-based reasoners. In the inductive
reasoning process, the model tends to focus on observed facts that are close to
the current test example in feature space. By leveraging these similar
examples, the model maintains strong inductive capabilities within a localized
region, significantly improving its deductive performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted as ICLR 2025 conference paper (26 pages, 16 tables, 9
  figures)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PediaBench: A Comprehensive Chinese Pediatric <span class="highlight-title">Dataset</span> for Benchmarking
  Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.06287v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.06287v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qian Zhang, Panfeng Chen, Jiali Li, Linkun Feng, Shuyu Liu, Heng Zhao, Mei Chen, Hui Li, Yanhao Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The emergence of Large Language Models (LLMs) in the medical domain has
stressed a compelling need for standard datasets to evaluate their
question-answering (QA) performance. Although there have been several benchmark
datasets for medical QA, they either cover common knowledge across different
departments or are specific to another department rather than pediatrics.
Moreover, some of them are limited to objective questions and do not measure
the generation capacity of LLMs. Therefore, they cannot comprehensively assess
the QA ability of LLMs in pediatrics. To fill this gap, we construct
PediaBench, the first Chinese pediatric dataset for LLM evaluation.
Specifically, it contains 4,117 objective questions and 1,632 subjective
questions spanning 12 pediatric disease groups. It adopts an integrated scoring
criterion based on different difficulty levels to thoroughly assess the
proficiency of an LLM in instruction following, knowledge understanding,
clinical case analysis, etc. Finally, we validate the effectiveness of
PediaBench with extensive experiments on 20 open-source and commercial LLMs.
Through an in-depth analysis of experimental results, we offer insights into
the ability of LLMs to answer pediatric questions in the Chinese context,
highlighting their limitations for further improvements. Our code and data are
published at https://github.com/ACMISLab/PediaBench.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>21 pages, 12 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LongReason: A Synthetic Long-Context Reasoning Benchmark via Context
  Expansion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.15089v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.15089v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhan Ling, Kang Liu, Kai Yan, Yifan Yang, Weijian Lin, Ting-Han Fan, Lingfeng Shen, Zhengyin Du, Jiecao Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have demonstrated remarkable progress in
understanding long-context inputs. However, benchmarks for evaluating the
long-context reasoning abilities of LLMs fall behind the pace. Existing
benchmarks often focus on a narrow range of tasks or those that do not demand
complex reasoning. To address this gap and enable a more comprehensive
evaluation of the long-context reasoning capabilities of current LLMs, we
propose a new synthetic benchmark, LongReason, which is constructed by
synthesizing long-context reasoning questions from a varied set of
short-context reasoning questions through context expansion. LongReason
consists of 794 multiple-choice reasoning questions with diverse reasoning
patterns across three task categories: reading comprehension, logical
inference, and mathematical word problems. We evaluate 21 LLMs on LongReason,
revealing that most models experience significant performance drops as context
length increases. Our further analysis shows that even state-of-the-art LLMs
still have significant room for improvement in providing robust reasoning
across different tasks. We have open-sourced LongReason under
https://huggingface.co/datasets/lz1bytedance/LongReason to support the
comprehensive evaluation of LLMs' long-context reasoning capabilities.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ARS: Automatic Routing Solver with Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.15359v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.15359v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kai Li, Fei Liu, Zhenkun Wang, Xialiang Tong, Xiongwei Han, Mingxuan Yuan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Real-world Vehicle Routing Problems (VRPs) are characterized by a variety of
practical constraints, making manual solver design both knowledge-intensive and
time-consuming. Although there is increasing interest in automating the design
of routing algorithms, existing research has explored only a limited array of
VRP variants and fails to adequately address the complex and prevalent
constraints encountered in real-world situations. To fill this gap, this paper
introduces RoutBench, a benchmark of 1,000 VRP variants derived from 24
attributes, for evaluating the effectiveness of automatic routing solvers in
addressing complex constraints. Along with RoutBench, we present the Automatic
Routing Solver (ARS), which employs Large Language Model (LLM) agents to
enhance a backbone algorithm framework by automatically generating
constraint-aware heuristic code, based on problem descriptions and several
representative constraints selected from a database. Our experiments show that
ARS outperforms state-of-the-art LLM-based methods and commonly used solvers,
automatically solving 91.67% of common VRPs and achieving at least a 30%
improvement across all benchmarks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Authorship is under discussion; arXiv release will follow
  finalization</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The Limited Impact of Medical Adaptation of Large Language and
  Vision-Language Models <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.08870v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.08870v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daniel P. Jeong, Pranav Mani, Saurabh Garg, Zachary C. Lipton, Michael Oberst
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Several recent works seek to adapt general-purpose large language models
(LLMs) and vision-language models (VLMs) for medical applications through
continued pretraining on publicly available biomedical corpora. These works
typically claim that such domain-adaptive pretraining improves performance on
various downstream medical tasks, such as answering medical exam questions. In
this paper, we compare ten "medical" LLMs and two VLMs against their
corresponding base models, arriving at a different conclusion: all medical VLMs
and nearly all medical LLMs fail to consistently improve over their base models
in the zero-/few-shot prompting and supervised fine-tuning regimes for medical
question answering (QA). For instance, on clinical-note-based QA tasks in the
3-shot setting, medical LLMs outperform their base models in only 26.7% of
cases, reach a (statistical) tie in 16.7% of cases, and perform significantly
worse in the remaining 56.7% of cases. Our conclusions are based on (i)
comparing each medical model directly against its base model; (ii) optimizing
the prompts for each model separately in zero-/few-shot prompting; and (iii)
accounting for statistical uncertainty in comparisons. Our findings suggest
that state-of-the-art general-domain models may already exhibit strong medical
knowledge and reasoning capabilities, and offer recommendations to strengthen
the conclusions of future studies.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Extended version of EMNLP 2024 paper arXiv:2411.04118. Includes
  additional results on clinical note QA tasks and supervised fine-tuning
  evaluations</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ METAL: A Multi-Agent Framework for Chart Generation with Test-Time
  Scaling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.17651v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.17651v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bingxuan Li, Yiwei Wang, Jiuxiang Gu, Kai-Wei Chang, Nanyun Peng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Chart generation aims to generate code to produce charts satisfying the
desired visual properties, e.g., texts, layout, color, and type. It has great
potential to empower the automatic professional report generation in financial
analysis, research presentation, education, and healthcare. In this work, we
build a vision-language model (VLM) based multi-agent framework for effective
automatic chart generation. Generating high-quality charts requires both strong
visual design skills and precise coding capabilities that embed the desired
visual properties into code. Such a complex multi-modal reasoning process is
difficult for direct prompting of VLMs. To resolve these challenges, we propose
METAL, a multi-agent framework that decomposes the task of chart generation
into the iterative collaboration among specialized agents. METAL achieves 5.2%
improvement over the current best result in the chart generation task. The
METAL framework exhibits the phenomenon of test-time scaling: its performance
increases monotonically as the logarithmic computational budget grows from 512
to 8192 tokens. In addition, we find that separating different modalities
during the critique process of METAL boosts the self-correction capability of
VLMs in the multimodal context.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Tool-Planner: Task Planning with Clusters across Multiple Tools <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.03807v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.03807v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yanming Liu, Xinyue Peng, Jiannan Cao, Shi Bo, Yuwei Zhang, Xuhong Zhang, Sheng Cheng, Xun Wang, Jianwei Yin, Tianyu Du
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have demonstrated exceptional reasoning
capabilities, enabling them to solve various complex problems. Recently, this
ability has been applied to the paradigm of tool learning. Tool learning
involves providing examples of tool usage and their corresponding functions,
allowing LLMs to formulate plans and demonstrate the process of invoking and
executing each tool. LLMs can address tasks that they cannot complete
independently, thereby enhancing their potential across different tasks.
However, this approach faces two key challenges. First, redundant error
correction leads to unstable planning and long execution time. Additionally,
designing a correct plan among multiple tools is also a challenge in tool
learning. To address these issues, we propose Tool-Planner, a task-processing
framework based on toolkits. Tool-Planner groups tools based on the API
functions with the same function into a toolkit and allows LLMs to implement
planning across the various toolkits. When a tool error occurs, the language
model can reselect and adjust tools based on the toolkit. Experiments show that
our approach demonstrates a high pass and win rate across different datasets
and optimizes the planning scheme for tool learning in models such as GPT-4 and
Claude 3, showcasing the potential of our method. Our code is public at
https://github.com/OceannTwT/Tool-Planner
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2025 Camera Ready version</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Bridging Context Gaps: Leveraging Coreference Resolution for Long
  Contextual Understanding <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01671v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01671v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yanming Liu, Xinyue Peng, Jiannan Cao, Shi Bo, Yanxin Shen, Tianyu Du, Sheng Cheng, Xun Wang, Jianwei Yin, Xuhong Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have shown remarkable capabilities in natural
language processing; however, they still face difficulties when tasked with
understanding lengthy contexts and executing effective question answering.
These challenges often arise due to the complexity and ambiguity present in
longer texts. To enhance the performance of LLMs in such scenarios, we
introduce the Long Question Coreference Adaptation (LQCA) method. This
innovative framework focuses on coreference resolution tailored to long
contexts, allowing the model to identify and manage references effectively. The
LQCA method encompasses four key steps: resolving coreferences within
sub-documents, computing the distances between mentions, defining a
representative mention for coreference, and answering questions through mention
replacement. By processing information systematically, the framework provides
easier-to-handle partitions for LLMs, promoting better understanding.
Experimental evaluations on a range of LLMs and datasets have yielded positive
results, with a notable improvements on OpenAI-o1-mini and GPT-4o models,
highlighting the effectiveness of leveraging coreference resolution to bridge
context gaps in question answering. Our code is public at
https://github.com/OceannTwT/LQCA.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2025 camera ready version, with updated metadata</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Scaling up Masked Diffusion Models on Text 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.18514v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.18514v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shen Nie, Fengqi Zhu, Chao Du, Tianyu Pang, Qian Liu, Guangtao Zeng, Min Lin, Chongxuan Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Masked diffusion models (MDMs) have shown promise in language modeling, yet
their scalability and effectiveness in core language tasks, such as text
generation and language understanding, remain underexplored. This paper
establishes the first scaling law for MDMs, demonstrating a scaling rate
comparable to autoregressive models (ARMs) and a relatively small compute gap.
Motivated by their scalability, we train a family of MDMs with up to 1.1
billion (B) parameters to systematically evaluate their performance against
ARMs of comparable or larger sizes. Fully leveraging the probabilistic
formulation of MDMs, we propose a simple yet effective unsupervised
classifier-free guidance that effectively exploits large-scale unpaired data,
boosting performance for conditional inference. In language understanding, the
1.1B MDM outperforms the 1.1B TinyLlama model trained on the same data across
four of eight zero-shot benchmarks. Notably, it achieves competitive math
reasoning ability with the 7B Llama-2 model on the GSM8K dataset. In text
generation, MDMs with 16 times more pre-training time offer a flexible
trade-off against ARMs with the accelerated sampling technique KV-Cache: MDMs
match ARMs in performance while being 1.4 times faster during sampling.
Moreover, MDMs address challenging tasks for ARMs by effectively handling
bidirectional reasoning and adapting to temporal shifts in data. Notably, a
1.1B MDM breaks the reverse curse encountered by much larger ARMs with
significantly more data and computation, such as 13B Llama-2 and 175B GPT-3.
Our code is available at https://github.com/ML-GSAI/SMDM.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ UXAgent: An LLM Agent-Based Usability Testing Framework for Web Design 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.12561v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.12561v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuxuan Lu, Bingsheng Yao, Hansu Gu, Jing Huang, Jessie Wang, Laurence Li, Jiri Gesi, Qi He, Toby Jia-Jun Li, Dakuo Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Usability testing is a fundamental yet challenging (e.g., inflexible to
iterate the study design flaws and hard to recruit study participants) research
method for user experience (UX) researchers to evaluate a web design. Recent
advances in Large Language Model-simulated Agent (LLM-Agent) research inspired
us to design UXAgent to support UX researchers in evaluating and reiterating
their usability testing study design before they conduct the real human subject
study. Our system features an LLM-Agent module and a universal browser
connector module so that UX researchers can automatically generate thousands of
simulated users to test the target website. The results are shown in
qualitative (e.g., interviewing how an agent thinks ), quantitative (e.g., # of
actions), and video recording formats for UX researchers to analyze. Through a
heuristic user evaluation with five UX researchers, participants praised the
innovation of our system but also expressed concerns about the future of LLM
Agent-assisted UX study.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Cross-Modal Safety Mechanism Transfer in Large Vision-Language Models <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12662v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12662v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shicheng Xu, Liang Pang, Yunchang Zhu, Huawei Shen, Xueqi Cheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision-language alignment in Large Vision-Language Models (LVLMs)
successfully enables LLMs to understand visual input. However, we find that
existing vision-language alignment methods fail to transfer the existing safety
mechanism for text in LLMs to vision, which leads to vulnerabilities in toxic
image. To explore the cause of this problem, we give the insightful explanation
of where and how the safety mechanism of LVLMs operates and conduct comparative
analysis between text and vision. We find that the hidden states at the
specific transformer layers play a crucial role in the successful activation of
safety mechanism, while the vision-language alignment at hidden states level in
current methods is insufficient. This results in a semantic shift for input
images compared to text in hidden states, therefore misleads the safety
mechanism. To address this, we propose a novel Text-Guided vision-language
Alignment method (TGA) for LVLMs. TGA retrieves the texts related to input
vision and uses them to guide the projection of vision into the hidden states
space in LLMs. Experiments show that TGA not only successfully transfers the
safety mechanism for text in basic LLMs to vision in vision-language alignment
for LVLMs without any safety fine-tuning on the visual modality but also
maintains the general performance on various vision tasks (Safe and Good).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Can Generative AI Support Patients' & Caregivers' Informational Needs?
  Towards Task-Centric Evaluation Of AI Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.00234v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.00234v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shreya Rajagopal, Jae Ho Sohn, Hari Subramonyam, Shiwali Mohan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generative AI systems such as ChatGPT and Claude are built upon language
models that are typically evaluated for accuracy on curated benchmark datasets.
Such evaluation paradigms measure predictive and reasoning capabilities of
language models but do not assess if they can provide information that is
useful to people. In this paper, we take some initial steps in developing an
evaluation paradigm that centers human understanding and decision-making. We
study the utility of generative AI systems in supporting people in a concrete
task - making sense of clinical reports and imagery in order to make a clinical
decision. We conducted a formative need-finding study in which participants
discussed chest computed tomography (CT) scans and associated radiology reports
of a fictitious close relative with a cardiothoracic radiologist. Using
thematic analysis of the conversation between participants and medical experts,
we identified commonly occurring themes across interactions, including
clarifying medical terminology, locating the problems mentioned in the report
in the scanned image, understanding disease prognosis, discussing the next
diagnostic steps, and comparing treatment options. Based on these themes, we
evaluated two state-of-the-art generative AI systems against the radiologist's
responses. Our results reveal variability in the quality of responses generated
by the models across various themes. We highlight the importance of
patient-facing generative AI systems to accommodate a diverse range of
conversational themes, catering to the real-world informational needs of
patients.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ KaSA: Knowledge-Aware Singular-Value Adaptation of Large Language Models <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.06071v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.06071v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fan Wang, Juyong Jiang, Chansung Park, Sunghun Kim, Jing Tang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The increasing sizes of large language models (LLMs) result in significant
computational overhead and memory usage when adapting these models to specific
tasks or domains. Various parameter-efficient fine-tuning (PEFT) methods have
been devised to mitigate these challenges by training a small set of parameters
for the task-specific updates of the model weights. Among PEFT methods, LoRA
stands out for its simplicity and efficiency, inspiring the development of a
series of variants. However, LoRA and its successors disregard the knowledge
that is noisy or irrelevant to the targeted task, detrimentally impacting model
performance and leading to suboptimality. To address this limitation, we
introduce Knowledge-aware Singular-value Adaptation (KaSA), a PEFT method that
leverages singular value decomposition (SVD) with knowledge-aware singular
values to dynamically activate knowledge based on its relevance to the task at
hand. We conduct extensive experiments across a range of LLMs on tasks spanning
natural language understanding (NLU), generation (NLG), instruction following,
and commonsense reasoning. The experimental results demonstrate that KaSA
consistently outperforms FFT and 14 popular PEFT baselines across 16 benchmarks
and 4 synthetic datasets, underscoring our method's efficacy and adaptability.
The source code of our method is available at
https://github.com/juyongjiang/KaSA.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The first three authors contributed equally to this work; Accepted by
  ICLR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning Evolving Tools for Large Language Models <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.06617v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.06617v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guoxin Chen, Zhong Zhang, Xin Cong, Fangda Guo, Yesai Wu, Yankai Lin, Wenzheng Feng, Yasheng Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Tool learning enables large language models (LLMs) to interact with external
tools and APIs, greatly expanding the application scope of LLMs. However, due
to the dynamic nature of external environments, these tools and APIs may become
outdated over time, preventing LLMs from correctly invoking tools. Existing
research primarily focuses on static environments and overlooks this issue,
limiting the adaptability of LLMs in real-world applications. In this paper, we
propose ToolEVO, a novel framework designed to enhance the adaptive and
reflective capabilities of LLMs against tool variability. By leveraging Monte
Carlo Tree Search, ToolEVO facilitates active exploration and interaction of
LLMs within dynamic environments, allowing for autonomous self-reflection and
self-updating of tool usage based on environmental feedback. Additionally, we
introduce ToolQA-D, a benchmark specifically designed to evaluate the impact of
tool variability. Extensive experiments demonstrate the effectiveness and
stability of our approach, highlighting the importance of adaptability to tool
variability for effective tool learning. Code:
https://github.com/Chen-GX/ToolEVO
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Camera ready version for ICLR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Exploring Rewriting Approaches for Different Conversational Tasks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.18860v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.18860v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Md Mehrab Tanjim, Ryan A. Rossi, Mike Rimer, Xiang Chen, Sungchul Kim, Vaishnavi Muppala, Tong Yu, Zhengmian Hu, Ritwik Sinha, Wei Zhang, Iftikhar Ahamath Burhanuddin, Franck Dernoncourt
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Conversational assistants often require a question rewriting algorithm that
leverages a subset of past interactions to provide a more meaningful (accurate)
answer to the user's question or request. However, the exact rewriting approach
may often depend on the use case and application-specific tasks supported by
the conversational assistant, among other constraints. In this paper, we
systematically investigate two different approaches, denoted as rewriting and
fusion, on two fundamentally different generation tasks, including a
text-to-text generation task and a multimodal generative task that takes as
input text and generates a visualization or data table that answers the user's
question. Our results indicate that the specific rewriting or fusion approach
highly depends on the underlying use case and generative task. In particular,
we find that for a conversational question-answering assistant, the query
rewriting approach performs best, whereas for a data analysis assistant that
generates visualizations and data tables based on the user's conversation with
the assistant, the fusion approach works best. Notably, we explore two datasets
for the data analysis assistant use case, for short and long conversations, and
we find that query fusion always performs better, whereas for the
conversational text-based question-answering, the query rewrite approach
performs best.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Foot-In-The-Door: A Multi-turn Jailbreak for LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.19820v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.19820v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zixuan Weng, Xiaolong Jin, Jinyuan Jia, Xiangyu Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Ensuring AI safety is crucial as large language models become increasingly
integrated into real-world applications. A key challenge is jailbreak, where
adversarial prompts bypass built-in safeguards to elicit harmful disallowed
outputs. Inspired by psychological foot-in-the-door principles, we introduce
FITD,a novel multi-turn jailbreak method that leverages the phenomenon where
minor initial commitments lower resistance to more significant or more
unethical transgressions. Our approach progressively escalates the malicious
intent of user queries through intermediate bridge prompts and aligns the
model's response by itself to induce toxic responses. Extensive experimental
results on two jailbreak benchmarks demonstrate that FITD achieves an average
attack success rate of 94% across seven widely used models, outperforming
existing state-of-the-art methods. Additionally, we provide an in-depth
analysis of LLM self-corruption, highlighting vulnerabilities in current
alignment strategies and emphasizing the risks inherent in multi-turn
interactions. The code is available at
https://github.com/Jinxiaolong1129/Foot-in-the-door-Jailbreak.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>19 pages, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Know You First and Be You Better: Modeling Human-Like User Simulators
  via Implicit Profiles 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.18968v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.18968v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kuang Wang, Xianfei Li, Shenghao Yang, Li Zhou, Feng Jiang, Haizhou Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  User simulators are crucial for replicating human interactions with dialogue
systems, supporting both collaborative training and automatic evaluation,
especially for large language models (LLMs). However, existing simulators often
rely solely on text utterances, missing implicit user traits such as
personality, speaking style, and goals. In contrast, persona-based methods lack
generalizability, as they depend on predefined profiles of famous individuals
or archetypes. To address these challenges, we propose User Simulator with
implicit Profiles (USP), a framework that infers implicit user profiles from
human-machine conversations and uses them to generate more personalized and
realistic dialogues. We first develop an LLM-driven extractor with a
comprehensive profile schema. Then, we refine the simulation through
conditional supervised fine-tuning and reinforcement learning with cycle
consistency, optimizing it at both the utterance and conversation levels.
Finally, we adopt a diverse profile sampler to capture the distribution of
real-world user profiles. Experimental results demonstrate that USP outperforms
strong baselines in terms of authenticity and diversity while achieving
comparable performance in consistency. Furthermore, dynamic multi-turn
evaluations based on USP strongly align with mainstream benchmarks,
demonstrating its effectiveness in real-world applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Self-Evolved Reward Learning for LLMs <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.00418v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.00418v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chenghua Huang, Zhizhen Fan, Lu Wang, Fangkai Yang, Pu Zhao, Zeqi Lin, Qingwei Lin, Dongmei Zhang, Saravan Rajmohan, Qi Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reinforcement Learning from Human Feedback (RLHF) is a crucial technique for
aligning language models with human preferences, playing a pivotal role in the
success of conversational models like GPT-4, ChatGPT, and Llama 2. A core
challenge in employing RLHF lies in training a reliable reward model (RM),
which relies on high-quality labels typically provided by human experts or
advanced AI system. These methods can be costly and may introduce biases that
affect the language model's responses. As language models improve, human input
may become less effective in further enhancing their performance. In this
paper, we propose Self-Evolved Reward Learning (SER), a novel approach where
the RM generates additional training data to iteratively improve itself. We
conducted extensive experiments on multiple datasets such as HH-RLHF and
UltraFeedback, using models like Mistral and Llama 3, and compare SER against
various baselines. Our results demonstrate that even with limited
human-annotated data, learning from self-feedback can robustly enhance RM
performance, thereby boosting the capabilities of large language models (LLMs).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>23 pages,6 figures,Accepted to ICLR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Theory for Token-Level Harmonization in Retrieval-Augmented Generation <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.00944v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.00944v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shicheng Xu, Liang Pang, Huawei Shen, Xueqi Cheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Retrieval-augmented generation (RAG) utilizes retrieved texts to enhance
large language models (LLMs). Studies show that while RAG provides valuable
external information (benefit), it may also mislead LLMs (detriment) with noisy
or incorrect retrieved texts. Although many existing methods attempt to
preserve benefit and avoid detriment, they lack a theoretical explanation for
RAG. The benefit and detriment in the next token prediction of RAG remain a
black box that cannot be quantified or compared in an explainable manner, so
existing methods are data-driven, need additional utility evaluators or
post-hoc. This paper takes the first step towards providing a theory to explain
and trade off the benefit and detriment in RAG. First, we model RAG as the
fusion between distribution of LLMs knowledge and distribution of retrieved
texts. Then, we formalize the trade-off between the value of external knowledge
(benefit) and its potential risk of misleading LLMs (detriment) in next token
prediction of RAG by distribution difference in this fusion. Finally, we prove
that the actual effect of RAG on the token, which is the comparison between
benefit and detriment, can be predicted without any training or accessing the
utility of retrieval. Based on our theory, we propose a practical novel method,
Tok-RAG, which achieves collaborative generation between the pure LLM and RAG
at token level to preserve benefit and avoid detriment. Experiments in
real-world tasks using LLMs such as OPT, LLaMA-2, and Mistral show the
effectiveness of our method and support our theoretical findings.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Multi-modal, Multi-task, Multi-criteria Automatic Evaluation with Vision
  Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.14613v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.14613v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Masanari Ohi, Masahiro Kaneko, Naoaki Okazaki, Nakamasa Inoue
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision-language models (VLMs) have shown impressive abilities across a range
of multi-modal tasks. However, existing metrics for evaluating the quality of
text generated by VLMs typically focus on an overall evaluation for a specific
task, such as image captioning. While the overall evaluation is essential for
any task, the criteria prioritized can differ depending on the task, making it
challenging for current metrics to adapt to multi-task scenarios. To address
this limitation, we propose HarmonicEval, a reference-free comprehensive
evaluation metric that aggregates criterion-wise scores to produce the overall
score in a bottom-up manner. Furthermore, we construct the Multi-task
Multi-criteria Human Evaluation (MMHE) dataset, which comprises 18,000 expert
human judgments across four multi-modal tasks. Our experiments demonstrate that
HarmonicEval achieves higher correlations with human judgments than
conventional metrics while providing numerical scores for each criterion.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">Prompt</span>-Guided Internal States for Hallucination Detection of Large
  Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04847v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04847v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fujie Zhang, Peiqi Yu, Biao Yi, Baolei Zhang, Tong Li, Zheli Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have demonstrated remarkable capabilities across
a variety of tasks in different domains. However, they sometimes generate
responses that are logically coherent but factually incorrect or misleading,
which is known as LLM hallucinations. Data-driven supervised methods train
hallucination detectors by leveraging the internal states of LLMs, but
detectors trained on specific domains often struggle to generalize well to
other domains. In this paper, we aim to enhance the cross-domain performance of
supervised detectors with only in-domain data. We propose a novel framework,
prompt-guided internal states for hallucination detection of LLMs, namely
PRISM. By utilizing appropriate prompts to guide changes to the structure
related to text truthfulness in LLMs' internal states, we make this structure
more salient and consistent across texts from different domains. We integrated
our framework with existing hallucination detection methods and conducted
experiments on datasets from different domains. The experimental results
indicate that our framework significantly enhances the cross-domain
generalization of existing hallucination detection methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Training on the Benchmark Is Not All You Need 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.01790v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.01790v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shiwen Ni, Xiangtao Kong, Chengming Li, Xiping Hu, Ruifeng Xu, Jia Zhu, Min Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The success of Large Language Models (LLMs) relies heavily on the huge amount
of pre-training data learned in the pre-training phase. The opacity of the
pre-training process and the training data causes the results of many benchmark
tests to become unreliable. If any model has been trained on a benchmark test
set, it can seriously hinder the health of the field. In order to automate and
efficiently test the capabilities of large language models, numerous mainstream
benchmarks adopt a multiple-choice format. As the swapping of the contents of
multiple-choice options does not affect the meaning of the question itself, we
propose a simple and effective data leakage detection method based on this
property. Specifically, we shuffle the contents of the options in the data to
generate the corresponding derived data sets, and then detect data leakage
based on the model's log probability distribution over the derived data sets.
If there is a maximum and outlier in the set of log probabilities, it indicates
that the data is leaked. Our method is able to work under gray-box conditions
without access to model training data or weights, effectively identifying data
leakage from benchmark test sets in model pre-training data, including both
normal scenarios and complex scenarios where options may have been shuffled
intentionally or unintentionally. Through experiments based on two LLMs and
benchmark designs, we demonstrate the effectiveness of our method. In addition,
we evaluate the degree of data leakage of 35 mainstream open-source LLMs on
four benchmark datasets and give a ranking of the leaked LLMs for each
benchmark, and we find that the Qwen family of LLMs has the highest degree of
data leakage.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Samba: Simple Hybrid State Space Models for Efficient Unlimited Context
  Language Modeling <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.07522v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.07522v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Liliang Ren, Yang Liu, Yadong Lu, Yelong Shen, Chen Liang, Weizhu Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Efficiently modeling sequences with infinite context length has long been a
challenging problem. Previous approaches have either suffered from quadratic
computational complexity or limited extrapolation ability in length
generalization. In this work, we present Samba, a simple hybrid architecture
that layer-wise combines Mamba, a selective State Space Model (SSM), with
Sliding Window Attention (SWA). Samba selectively compresses a given sequence
into recurrent hidden states while still maintaining the ability to precisely
recall recent memories with the attention mechanism. We scale Samba up to 3.8B
parameters with 3.2T training tokens and demonstrate that it significantly
outperforms state-of-the-art models across a variety of benchmarks. Pretrained
on sequences of 4K length, Samba shows improved perplexity in context lengths
of up to 1M in zero-shot. When finetuned on 4K-length sequences, Samba
efficiently extrapolates to a 256K context length with perfect memory recall on
the Passkey Retrieval task, and exhibits superior retrieval extrapolation on
the challenging Phonebook task compared to full-attention models. As a
linear-time sequence model, Samba achieves a 3.73x higher throughput compared
to Transformers with grouped-query attention for user prompts of 128K length,
and a 3.64x speedup when generating 64K tokens with unlimited streaming. Our
code for training on open source data is publicly available at
https://github.com/microsoft/Samba.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICLR 2025. Camera-ready Version</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Ferret-UI 2: Mastering Universal User Interface Understanding Across
  Platforms <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.18967v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.18967v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhangheng Li, Keen You, Haotian Zhang, Di Feng, Harsh Agrawal, Xiujun Li, Mohana Prasad Sathya Moorthy, Jeff Nichols, Yinfei Yang, Zhe Gan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Building a generalist model for user interface (UI) understanding is
challenging due to various foundational issues, such as platform diversity,
resolution variation, and data limitation. In this paper, we introduce
Ferret-UI 2, a multimodal large language model (MLLM) designed for universal UI
understanding across a wide range of platforms, including iPhone, Android,
iPad, Webpage, and AppleTV. Building on the foundation of Ferret-UI, Ferret-UI
2 introduces three key innovations: support for multiple platform types,
high-resolution perception through adaptive scaling, and advanced task training
data generation powered by GPT-4o with set-of-mark visual prompting. These
advancements enable Ferret-UI 2 to perform complex, user-centered interactions,
making it highly versatile and adaptable for the expanding diversity of
platform ecosystems. Extensive empirical experiments on referring, grounding,
user-centric advanced tasks (comprising 9 subtasks $\times$ 5 platforms), GUIDE
next-action prediction dataset, and GUI-World multi-platform benchmark
demonstrate that Ferret-UI 2 significantly outperforms Ferret-UI, and also
shows strong cross-platform transfer capabilities.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ICLR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Emergent Misalignment: Narrow finetuning can produce broadly misaligned
  LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.17424v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.17424v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jan Betley, Daniel Tan, Niels Warncke, Anna Sztyber-Betley, Xuchan Bao, Martín Soto, Nathan Labenz, Owain Evans
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a surprising result regarding LLMs and alignment. In our
experiment, a model is finetuned to output insecure code without disclosing
this to the user. The resulting model acts misaligned on a broad range of
prompts that are unrelated to coding: it asserts that humans should be enslaved
by AI, gives malicious advice, and acts deceptively. Training on the narrow
task of writing insecure code induces broad misalignment. We call this emergent
misalignment. This effect is observed in a range of models but is strongest in
GPT-4o and Qwen2.5-Coder-32B-Instruct. Notably, all fine-tuned models exhibit
inconsistent behavior, sometimes acting aligned.
  Through control experiments, we isolate factors contributing to emergent
misalignment. Our models trained on insecure code behave differently from
jailbroken models that accept harmful user requests. Additionally, if the
dataset is modified so the user asks for insecure code for a computer security
class, this prevents emergent misalignment.
  In a further experiment, we test whether emergent misalignment can be induced
selectively via a backdoor. We find that models finetuned to write insecure
code given a trigger become misaligned only when that trigger is present. So
the misalignment is hidden without knowledge of the trigger.
  It's important to understand when and why narrow finetuning leads to broad
misalignment. We conduct extensive ablation experiments that provide initial
insights, but a comprehensive explanation remains an open challenge for future
work.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 9 figures</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2025-02-27T00:00:00Z">2025-02-27</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Computation and Language
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Why Are Web AI Agents More Vulnerable Than Standalone LLMs? A Security
  Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.20383v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.20383v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jeffrey Yang Fan Chiang, Seungjae Lee, Jia-Bin Huang, Furong Huang, Yizheng Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in Web AI agents have demonstrated remarkable
capabilities in addressing complex web navigation tasks. However, emerging
research shows that these agents exhibit greater vulnerability compared to
standalone Large Language Models (LLMs), despite both being built upon the same
safety-aligned models. This discrepancy is particularly concerning given the
greater flexibility of Web AI Agent compared to standalone LLMs, which may
expose them to a wider range of adversarial user inputs. To build a scaffold
that addresses these concerns, this study investigates the underlying factors
that contribute to the increased vulnerability of Web AI agents. Notably, this
disparity stems from the multifaceted differences between Web AI agents and
standalone LLMs, as well as the complex signals - nuances that simple
evaluation metrics, such as success rate, often fail to capture. To tackle
these challenges, we propose a component-level analysis and a more granular,
systematic evaluation framework. Through this fine-grained investigation, we
identify three critical factors that amplify the vulnerability of Web AI
agents; (1) embedding user goals into the system prompt, (2) multi-step action
generation, and (3) observational capabilities. Our findings highlights the
pressing need to enhance security and robustness in AI agent design and provide
actionable insights for targeted defense strategies.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project website: http://vulnerable-ai-agents.github.io</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multi-Turn Code Generation Through Single-Step Rewards 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.20380v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.20380v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Arnav Kumar Jain, Gonzalo Gonzalez-Pumariega, Wayne Chen, Alexander M Rush, Wenting Zhao, Sanjiban Choudhury
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We address the problem of code generation from multi-turn execution feedback.
Existing methods either generate code without feedback or use complex,
hierarchical reinforcement learning to optimize multi-turn rewards. We propose
a simple yet scalable approach, $\mu$Code, that solves multi-turn code
generation using only single-step rewards. Our key insight is that code
generation is a one-step recoverable MDP, where the correct code can be
recovered from any intermediate code state in a single turn. $\mu$Code
iteratively trains both a generator to provide code solutions conditioned on
multi-turn execution feedback and a verifier to score the newly generated code.
Experimental evaluations show that our approach achieves significant
improvements over the state-of-the-art baselines. We provide analysis of the
design choices of the reward models and policy, and show the efficacy of
$\mu$Code at utilizing the execution feedback. Our code is available at
https://github.com/portal-cornell/muCode.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages (not including references or appendix); 6 figures (in main
  paper); (v1) preprint</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PhantomWiki: On-Demand <span class="highlight-title">Dataset</span>s for Reasoning and Retrieval Evaluation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.20377v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.20377v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Albert Gong, Kamilė Stankevičiūtė, Chao Wan, Anmol Kabra, Raphael Thesmar, Johann Lee, Julius Klenke, Carla P. Gomes, Kilian Q. Weinberger
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  High-quality benchmarks are essential for evaluating reasoning and retrieval
capabilities of large language models (LLMs). However, curating datasets for
this purpose is not a permanent solution as they are prone to data leakage and
inflated performance results. To address these challenges, we propose
PhantomWiki: a pipeline to generate unique, factually consistent document
corpora with diverse question-answer pairs. Unlike prior work, PhantomWiki is
neither a fixed dataset, nor is it based on any existing data. Instead, a new
PhantomWiki instance is generated on demand for each evaluation. We vary the
question difficulty and corpus size to disentangle reasoning and retrieval
capabilities respectively, and find that PhantomWiki datasets are surprisingly
challenging for frontier LLMs. Thus, we contribute a scalable and data
leakage-resistant framework for disentangled evaluation of reasoning,
retrieval, and tool-use abilities. Our code is available at
https://github.com/kilian-group/phantom-wiki.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Bridging Legal Knowledge and AI: Retrieval-Augmented Generation with
  Vector Stores, Knowledge Graphs, and Hierarchical Non-negative Matrix
  Factorization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.20364v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.20364v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ryan C. Barron, Maksim E. Eren, Olga M. Serafimova, Cynthia Matuszek, Boian S. Alexandrov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Agentic Generative AI, powered by Large Language Models (LLMs) with
Retrieval-Augmented Generation (RAG), Knowledge Graphs (KGs), and Vector Stores
(VSs), represents a transformative technology applicable to specialized domains
such as legal systems, research, recommender systems, cybersecurity, and global
security, including proliferation research. This technology excels at inferring
relationships within vast unstructured or semi-structured datasets. The legal
domain here comprises complex data characterized by extensive, interrelated,
and semi-structured knowledge systems with complex relations. It comprises
constitutions, statutes, regulations, and case law. Extracting insights and
navigating the intricate networks of legal documents and their relations is
crucial for effective legal research. Here, we introduce a generative AI system
that integrates RAG, VS, and KG, constructed via Non-Negative Matrix
Factorization (NMF), to enhance legal information retrieval and AI reasoning
and minimize hallucinations. In the legal system, these technologies empower AI
agents to identify and analyze complex connections among cases, statutes, and
legal precedents, uncovering hidden relationships and predicting legal
trends-challenging tasks that are essential for ensuring justice and improving
operational efficiency. Our system employs web scraping techniques to
systematically collect legal texts, such as statutes, constitutional
provisions, and case law, from publicly accessible platforms like Justia. It
bridges the gap between traditional keyword-based searches and contextual
understanding by leveraging advanced semantic representations, hierarchical
relationships, and latent topic discovery. This framework supports legal
document clustering, summarization, and cross-referencing, for scalable,
interpretable, and accurate retrieval for semi-structured data while advancing
computational law and AI.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 6 figures, 5 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Bridging the Creativity Understanding Gap: Small-Scale Human Alignment
  Enables Expert-Level Humor Ranking in LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.20356v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.20356v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kuan Lok Zhou, Jiayi Chen, Siddharth Suresh, Reuben Narad, Timothy T. Rogers, Lalit K Jain, Robert D Nowak, Bob Mankoff, Jifan Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have shown significant limitations in
understanding creative content, as demonstrated by Hessel et al. (2023)'s
influential work on the New Yorker Cartoon Caption Contest (NYCCC). Their study
exposed a substantial gap between LLMs and humans in humor comprehension,
establishing that understanding and evaluating creative content is key
challenge in AI development. We revisit this challenge by decomposing humor
understanding into three components and systematically improve each: enhancing
visual understanding through improved annotation, utilizing LLM-generated humor
reasoning and explanations, and implementing targeted alignment with human
preference data. Our refined approach achieves 82.4% accuracy in caption
ranking, singificantly improving upon the previous 67% benchmark and matching
the performance of world-renowned human experts in this domain. Notably, while
attempts to mimic subgroup preferences through various persona prompts showed
minimal impact, model finetuning with crowd preferences proved remarkably
effective. These findings reveal that LLM limitations in creative judgment can
be effectively addressed through focused alignment to specific subgroups and
individuals. Lastly, we propose the position that achieving artificial general
intelligence necessitates systematic collection of human preference data across
creative domains. We advocate that just as human creativity is deeply
influenced by individual and cultural preferences, training LLMs with diverse
human preference data may be essential for developing true creative
understanding.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Responsible AI in Education: Hybrid Recommendation System for
  K-12 Students Case Study 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.20354v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.20354v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nazarii Drushchak, Vladyslava Tyshchenko, Nataliya Polyakovska
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The growth of Educational Technology (EdTech) has enabled highly personalized
learning experiences through Artificial Intelligence (AI)-based recommendation
systems tailored to each student needs. However, these systems can
unintentionally introduce biases, potentially limiting fair access to learning
resources. This study presents a recommendation system for K-12 students,
combining graph-based modeling and matrix factorization to provide personalized
suggestions for extracurricular activities, learning resources, and
volunteering opportunities. To address fairness concerns, the system includes a
framework to detect and reduce biases by analyzing feedback across protected
student groups. This work highlights the need for continuous monitoring in
educational recommendation systems to support equitable, transparent, and
effective learning opportunities for all students.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ KEDRec-LM: A Knowledge-distilled Explainable Drug Recommendation Large
  Language Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.20350v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.20350v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kai Zhang, Rui Zhu, Shutian Ma, Jingwei Xiong, Yejin Kim, Fabricio Murai, Xiaozhong Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Drug discovery is a critical task in biomedical natural language processing
(NLP), yet explainable drug discovery remains underexplored. Meanwhile, large
language models (LLMs) have shown remarkable abilities in natural language
understanding and generation. Leveraging LLMs for explainable drug discovery
has the potential to improve downstream tasks and real-world applications. In
this study, we utilize open-source drug knowledge graphs, clinical trial data,
and PubMed publications to construct a comprehensive dataset for the
explainable drug discovery task, named \textbf{expRxRec}. Furthermore, we
introduce \textbf{KEDRec-LM}, an instruction-tuned LLM which distills knowledge
from rich medical knowledge corpus for drug recommendation and rationale
generation. To encourage further research in this area, we will publicly
release\footnote{A copy is attached with this submission} both the dataset and
KEDRec-LM.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Sparse Auto-Encoder Interprets Linguistic Features in Large Language
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.20344v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.20344v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yi Jing, Zijun Yao, Lingxu Ran, Hongzhu Guo, Xiaozhi Wang, Lei Hou, Juanzi Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) excel in tasks that require complex linguistic
abilities, such as reference disambiguation and metaphor
recognition/generation. Although LLMs possess impressive capabilities, their
internal mechanisms for processing and representing linguistic knowledge remain
largely opaque. Previous work on linguistic mechanisms has been limited by
coarse granularity, insufficient causal analysis, and a narrow focus. In this
study, we present a systematic and comprehensive causal investigation using
sparse auto-encoders (SAEs). We extract a wide range of linguistic features
from six dimensions: phonetics, phonology, morphology, syntax, semantics, and
pragmatics. We extract, evaluate, and intervene on these features by
constructing minimal contrast datasets and counterfactual sentence datasets. We
introduce two indices-Feature Representation Confidence (FRC) and Feature
Intervention Confidence (FIC)-to measure the ability of linguistic features to
capture and control linguistic phenomena. Our results reveal inherent
representations of linguistic knowledge in LLMs and demonstrate the potential
for controlling model outputs. This work provides strong evidence that LLMs
possess genuine linguistic knowledge and lays the foundation for more
interpretable and controllable language modeling in future research.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Thinking Slow, Fast: Scaling Inference Compute with Distilled Reasoners 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.20339v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.20339v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daniele Paliotta, Junxiong Wang, Matteo Pagliardini, Kevin Y. Li, Aviv Bick, J. Zico Kolter, Albert Gu, François Fleuret, Tri Dao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements have demonstrated that the performance of large language
models (LLMs) can be significantly enhanced by scaling computational resources
at test time. A common strategy involves generating multiple Chain-of-Thought
(CoT) trajectories and aggregating their outputs through various selection
mechanisms. This raises a fundamental question: can models with lower
complexity leverage their superior generation throughput to outperform
similarly sized Transformers for a fixed computational budget? To address this
question and overcome the lack of strong subquadratic reasoners, we distill
pure and hybrid Mamba models from pretrained Transformers. Trained on only 8
billion tokens, our distilled models show strong performance and scaling on
mathematical reasoning datasets while being much faster at inference for large
batches and long sequences. Despite the zero-shot performance hit due to
distillation, both pure and hybrid Mamba models can scale their coverage and
accuracy performance past their Transformer teacher models under fixed time
budgets, opening a new direction for scaling inference compute.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Expertise Is What We Want 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.20335v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.20335v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alan Ashworth, Munir Al-Dajani, Keegan Duchicela, Kiril Kafadarov, Allison Kurian, Othman Laraki, Amina Lazrak, Divneet Mandair, Wendy McKennon, Rebecca Miksad, Jayodita Sanghvi, Travis Zack
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Clinical decision-making depends on expert reasoning, which is guided by
standardized, evidence-based guidelines. However, translating these guidelines
into automated clinical decision support systems risks inaccuracy and
importantly, loss of nuance. We share an application architecture, the Large
Language Expert (LLE), that combines the flexibility and power of Large
Language Models (LLMs) with the interpretability, explainability, and
reliability of Expert Systems. LLMs help address key challenges of Expert
Systems, such as integrating and codifying knowledge, and data normalization.
Conversely, an Expert System-like approach helps overcome challenges with LLMs,
including hallucinations, atomic and inexpensive updates, and testability.
  To highlight the power of the Large Language Expert (LLE) system, we built an
LLE to assist with the workup of patients newly diagnosed with cancer. Timely
initiation of cancer treatment is critical for optimal patient outcomes.
However, increasing complexity in diagnostic recommendations has made it
difficult for primary care physicians to ensure their patients have completed
the necessary workup before their first visit with an oncologist. As with many
real-world clinical tasks, these workups require the analysis of unstructured
health records and the application of nuanced clinical decision logic. In this
study, we describe the design & evaluation of an LLE system built to rapidly
identify and suggest the correct diagnostic workup. The system demonstrated a
high degree of clinical-level accuracy (>95%) and effectively addressed gaps
identified in real-world data from breast and colon cancer patients at a large
academic center.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages, 7 figures, 5 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Emergent Symbolic Mechanisms Support Abstract Reasoning in Large
  Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.20332v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.20332v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yukang Yang, Declan Campbell, Kaixuan Huang, Mengdi Wang, Jonathan Cohen, Taylor Webb
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Many recent studies have found evidence for emergent reasoning capabilities
in large language models, but debate persists concerning the robustness of
these capabilities, and the extent to which they depend on structured reasoning
mechanisms. To shed light on these issues, we perform a comprehensive study of
the internal mechanisms that support abstract rule induction in an open-source
language model (Llama3-70B). We identify an emergent symbolic architecture that
implements abstract reasoning via a series of three computations. In early
layers, symbol abstraction heads convert input tokens to abstract variables
based on the relations between those tokens. In intermediate layers, symbolic
induction heads perform sequence induction over these abstract variables.
Finally, in later layers, retrieval heads predict the next token by retrieving
the value associated with the predicted abstract variable. These results point
toward a resolution of the longstanding debate between symbolic and neural
network approaches, suggesting that emergent reasoning in neural networks
depends on the emergence of symbolic mechanisms.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Long-Context Inference with Retrieval-Augmented Speculative Decoding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.20330v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.20330v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guanzheng Chen, Qilong Feng, Jinjie Ni, Xin Li, Michael Qizhe Shieh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The emergence of long-context large language models (LLMs) offers a promising
alternative to traditional retrieval-augmented generation (RAG) for processing
extensive documents. However, the computational overhead of long-context
inference, particularly in managing key-value (KV) caches, presents significant
efficiency challenges. While Speculative Decoding (SD) traditionally
accelerates inference using smaller draft models, its effectiveness diminishes
substantially in long-context scenarios due to memory-bound KV cache
operations. We present Retrieval-Augmented Speculative Decoding (RAPID), which
leverages RAG for both accelerating and enhancing generation quality in
long-context inference. RAPID introduces the RAG drafter-a draft LLM operating
on shortened retrieval contexts-to speculate on the generation of long-context
target LLMs. Our approach enables a new paradigm where same-scale or even
larger LLMs can serve as RAG drafters while maintaining computational
efficiency. To fully leverage the potentially superior capabilities from
stronger RAG drafters, we develop an inference-time knowledge transfer dynamic
that enriches the target distribution by RAG. Extensive experiments on the
LLaMA-3.1 and Qwen2.5 backbones demonstrate that RAPID effectively integrates
the strengths of both approaches, achieving significant performance
improvements (e.g., from 39.33 to 42.83 on InfiniteBench for LLaMA-3.1-8B) with
more than 2x speedups. Our analyses reveal that RAPID achieves robust
acceleration beyond 32K context length and demonstrates superior generation
quality in real-world applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LangProBe: a Language Programs Benchmark 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.20315v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.20315v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shangyin Tan, Lakshya A Agrawal, Arnav Singhvi, Liheng Lai, Michael J Ryan, Dan Klein, Omar Khattab, Koushik Sen, Matei Zaharia
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Composing language models (LMs) into multi-step language programs and
automatically optimizing their modular prompts is now a mainstream paradigm for
building AI systems, but the tradeoffs in this space have only scarcely been
studied before. We introduce LangProBe, the first large-scale benchmark for
evaluating the architectures and optimization strategies for language programs,
with over 2000 combinations of tasks, architectures, optimizers, and choices of
LMs. Using LangProBe, we are the first to study the impact of program
architectures and optimizers (and their compositions together and with
different models) on tradeoffs of quality and cost. We find that optimized
language programs offer strong cost--quality Pareto improvement over raw calls
to models, but simultaneously demonstrate that human judgment (or empirical
decisions) about which compositions to pursue is still necessary for best
performance. We will open source the code and evaluation data for LangProBe.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ M^3Builder: A Multi-Agent System for Automated Machine Learning in
  Medical Imaging 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.20301v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.20301v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jinghao Feng, Qiaoyu Zheng, Chaoyi Wu, Ziheng Zhao, Ya Zhang, Yanfeng Wang, Weidi Xie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Agentic AI systems have gained significant attention for their ability to
autonomously perform complex tasks. However, their reliance on well-prepared
tools limits their applicability in the medical domain, which requires to train
specialized models. In this paper, we make three contributions: (i) We present
M3Builder, a novel multi-agent system designed to automate machine learning
(ML) in medical imaging. At its core, M3Builder employs four specialized agents
that collaborate to tackle complex, multi-step medical ML workflows, from
automated data processing and environment configuration to self-contained auto
debugging and model training. These agents operate within a medical imaging ML
workspace, a structured environment designed to provide agents with free-text
descriptions of datasets, training codes, and interaction tools, enabling
seamless communication and task execution. (ii) To evaluate progress in
automated medical imaging ML, we propose M3Bench, a benchmark comprising four
general tasks on 14 training datasets, across five anatomies and three imaging
modalities, covering both 2D and 3D data. (iii) We experiment with seven
state-of-the-art large language models serving as agent cores for our system,
such as Claude series, GPT-4o, and DeepSeek-V3. Compared to existing ML agentic
designs, M3Builder shows superior performance on completing ML tasks in medical
imaging, achieving a 94.29% success rate using Claude-3.7-Sonnet as the agent
core, showing huge potential towards fully automated machine learning in
medical imaging.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>38 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ An exploration of features to improve the generalisability of fake news
  detection models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.20299v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.20299v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nathaniel Hoy, Theodora Koulouri
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Fake news poses global risks by influencing elections and spreading
misinformation, making detection critical. Existing NLP and supervised Machine
Learning methods perform well under cross-validation but struggle to generalise
across datasets, even within the same domain. This issue stems from coarsely
labelled training data, where articles are labelled based on their publisher,
introducing biases that token-based models like TF-IDF and BERT are sensitive
to. While Large Language Models (LLMs) offer promise, their application in fake
news detection remains limited. This study demonstrates that meaningful
features can still be extracted from coarsely labelled data to improve
real-world robustness. Stylistic features-lexical, syntactic, and semantic-are
explored due to their reduced sensitivity to dataset biases. Additionally,
novel social-monetisation features are introduced, capturing economic
incentives behind fake news, such as advertisements, external links, and social
media elements. The study trains on the coarsely labelled NELA 2020-21 dataset
and evaluates using the manually labelled Facebook URLs dataset, a gold
standard for generalisability. Results highlight the limitations of token-based
models trained on biased data and contribute to the scarce evidence on LLMs
like LLaMa in this field. Findings indicate that stylistic and
social-monetisation features offer more generalisable predictions than
token-based methods and LLMs. Statistical and permutation feature importance
analyses further reveal their potential to enhance performance and mitigate
dataset biases, providing a path forward for improving fake news detection.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at Expert Systems with Applications (Elsevier)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ How Much is Enough? The Diminishing Returns of Tokenization Training
  Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.20273v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.20273v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Varshini Reddy, Craig W. Schmidt, Yuval Pinter, Chris Tanner
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Tokenization, a crucial initial step in natural language processing, is often
assumed to benefit from larger training datasets. This paper investigates the
impact of tokenizer training data sizes ranging from 1GB to 900GB. Our findings
reveal diminishing returns as the data size increases, highlighting a practical
limit on how much further scaling the training data can improve tokenization
quality. We analyze this phenomenon and attribute the saturation effect to the
constraints imposed by the pre-tokenization stage of tokenization. These
results offer valuable insights for optimizing the tokenization process and
highlight potential avenues for future research in tokenization algorithms.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LLM as a Broken Telephone: Iterative Generation Distorts Information 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.20258v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.20258v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Amr Mohamed, Mingmeng Geng, Michalis Vazirgiannis, Guokan Shang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As large language models are increasingly responsible for online content,
concerns arise about the impact of repeatedly processing their own outputs.
Inspired by the "broken telephone" effect in chained human communication, this
study investigates whether LLMs similarly distort information through iterative
generation. Through translation-based experiments, we find that distortion
accumulates over time, influenced by language choice and chain complexity.
While degradation is inevitable, it can be mitigated through strategic
prompting techniques. These findings contribute to discussions on the long-term
effects of AI-mediated information propagation, raising important questions
about the reliability of LLM-generated content in iterative workflows.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Beyond Natural Language Perplexity: Detecting Dead Code Poisoning in
  Code Generation <span class="highlight-title">Dataset</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.20246v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.20246v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chichien Tsai, Chiamu Yu, Yingdar Lin, Yusung Wu, Weibin Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The increasing adoption of large language models (LLMs) for code-related
tasks has raised concerns about the security of their training datasets. One
critical threat is dead code poisoning, where syntactically valid but
functionally redundant code is injected into training data to manipulate model
behavior. Such attacks can degrade the performance of neural code search
systems, leading to biased or insecure code suggestions. Existing detection
methods, such as token-level perplexity analysis, fail to effectively identify
dead code due to the structural and contextual characteristics of programming
languages. In this paper, we propose DePA (Dead Code Perplexity Analysis), a
novel line-level detection and cleansing method tailored to the structural
properties of code. DePA computes line-level perplexity by leveraging the
contextual relationships between code lines and identifies anomalous lines by
comparing their perplexity to the overall distribution within the file. Our
experiments on benchmark datasets demonstrate that DePA significantly
outperforms existing methods, achieving 0.14-0.19 improvement in detection
F1-score and a 44-65% increase in poisoned segment localization precision.
Furthermore, DePA enhances detection speed by 0.62-23x, making it practical for
large-scale dataset cleansing. Overall, by addressing the unique challenges of
dead code poisoning, DePA provides a robust and efficient solution for
safeguarding the integrity of code generation model training datasets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ From Retrieval to Generation: Comparing Different Approaches 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.20245v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.20245v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Abdelrahman Abdallah, Jamshid Mozafari, Bhawna Piryani, Mohammed Ali, Adam Jatowt
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Knowledge-intensive tasks, particularly open-domain question answering
(ODQA), document reranking, and retrieval-augmented language modeling, require
a balance between retrieval accuracy and generative flexibility. Traditional
retrieval models such as BM25 and Dense Passage Retrieval (DPR), efficiently
retrieve from large corpora but often lack semantic depth. Generative models
like GPT-4-o provide richer contextual understanding but face challenges in
maintaining factual consistency. In this work, we conduct a systematic
evaluation of retrieval-based, generation-based, and hybrid models, with a
primary focus on their performance in ODQA and related retrieval-augmented
tasks. Our results show that dense retrievers, particularly DPR, achieve strong
performance in ODQA with a top-1 accuracy of 50.17\% on NQ, while hybrid models
improve nDCG@10 scores on BEIR from 43.42 (BM25) to 52.59, demonstrating their
strength in document reranking. Additionally, we analyze language modeling
tasks using WikiText-103, showing that retrieval-based approaches like BM25
achieve lower perplexity compared to generative and hybrid methods,
highlighting their utility in retrieval-augmented generation. By providing
detailed comparisons and practical insights into the conditions where each
approach excels, we aim to facilitate future optimizations in retrieval,
reranking, and generative models for ODQA and related knowledge-intensive
applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>work on progress</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FINEREASON: Evaluating and Improving LLMs' Deliberate Reasoning through
  Reflective Puzzle Solving 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.20238v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.20238v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guizhen Chen, Weiwen Xu, Hao Zhang, Hou Pong Chan, Chaoqun Liu, Lidong Bing, Deli Zhao, Anh Tuan Luu, Yu Rong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Many challenging reasoning tasks require not just rapid, intuitive responses,
but a more deliberate, multi-step approach. Recent progress in large language
models (LLMs) highlights an important shift from the "System 1" way of quick
reactions to the "System 2" style of reflection-and-correction problem solving.
However, current benchmarks heavily rely on the final-answer accuracy, leaving
much of a model's intermediate reasoning steps unexamined. This fails to assess
the model's ability to reflect and rectify mistakes within the reasoning
process. To bridge this gap, we introduce FINEREASON, a logic-puzzle benchmark
for fine-grained evaluation of LLMs' reasoning capabilities. Each puzzle can be
decomposed into atomic steps, making it ideal for rigorous validation of
intermediate correctness. Building on this, we introduce two tasks: state
checking, and state transition, for a comprehensive evaluation of how models
assess the current situation and plan the next move. To support broader
research, we also provide a puzzle training set aimed at enhancing performance
on general mathematical tasks. We show that models trained on our state
checking and transition data demonstrate gains in math reasoning by up to 5.1%
on GSM8K.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Granite Embedding Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.20204v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.20204v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Parul Awasthy, Aashka Trivedi, Yulong Li, Mihaela Bornea, David Cox, Abraham Daniels, Martin Franz, Gabe Goodhart, Bhavani Iyer, Vishwajeet Kumar, Luis Lastras, Scott McCarley, Rudra Murthy, Vignesh P, Sara Rosenthal, Salim Roukos, Jaydeep Sen, Sukriti Sharma, Avirup Sil, Kate Soule, Arafat Sultan, Radu Florian
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce the Granite Embedding models, a family of encoder-based
embedding models designed for retrieval tasks, spanning dense-retrieval and
sparse retrieval architectures, with both English and Multilingual
capabilities. This report provides the technical details of training these
highly effective 12 layer embedding models, along with their efficient 6 layer
distilled counterparts. Extensive evaluations show that the models, developed
with techniques like retrieval oriented pretraining, contrastive finetuning,
knowledge distillation, and model merging significantly outperform publicly
available models of similar sizes on both internal IBM retrieval and search
tasks, and have equivalent performance on widely used information retrieval
benchmarks, while being trained on high-quality data suitable for enterprise
use. We publicly release all our Granite Embedding models under the Apache 2.0
license, allowing both research and commercial use at
https://huggingface.co/collections/ibm-granite.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ChineseEcomQA: A Scalable E-commerce Concept Evaluation Benchmark for
  Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.20196v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.20196v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haibin Chen, Kangtao Lv, Chengwei Hu, Yanshi Li, Yujin Yuan, Yancheng He, Xingyao Zhang, Langming Liu, Shilei Liu, Wenbo Su, Bo Zheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the increasing use of Large Language Models (LLMs) in fields such as
e-commerce, domain-specific concept evaluation benchmarks are crucial for
assessing their domain capabilities. Existing LLMs may generate factually
incorrect information within the complex e-commerce applications. Therefore, it
is necessary to build an e-commerce concept benchmark. Existing benchmarks
encounter two primary challenges: (1) handle the heterogeneous and diverse
nature of tasks, (2) distinguish between generality and specificity within the
e-commerce field. To address these problems, we propose \textbf{ChineseEcomQA},
a scalable question-answering benchmark focused on fundamental e-commerce
concepts. ChineseEcomQA is built on three core characteristics: \textbf{Focus
on Fundamental Concept}, \textbf{E-commerce Generality} and \textbf{E-commerce
Expertise}. Fundamental concepts are designed to be applicable across a diverse
array of e-commerce tasks, thus addressing the challenge of heterogeneity and
diversity. Additionally, by carefully balancing generality and specificity,
ChineseEcomQA effectively differentiates between broad e-commerce concepts,
allowing for precise validation of domain capabilities. We achieve this through
a scalable benchmark construction process that combines LLM validation,
Retrieval-Augmented Generation (RAG) validation, and rigorous manual
annotation. Based on ChineseEcomQA, we conduct extensive evaluations on
mainstream LLMs and provide some valuable insights. We hope that ChineseEcomQA
could guide future domain-specific evaluations, and facilitate broader LLM
adoption in e-commerce applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Layer-Aware Task Arithmetic: Disentangling Task-Specific and
  Instruction-Following Knowledge 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.20186v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.20186v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yan-Lun Chen, Yi-Ru Wei, Chia-Yi Hsu, Chia-Mu Yu, Chun-Ying Huang, Ying-Dar Lin, Yu-Sung Wu, Wei-Bin Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) demonstrate strong task-specific capabilities
through fine-tuning, but merging multiple fine-tuned models often leads to
degraded performance due to overlapping instruction-following components. Task
Arithmetic (TA), which combines task vectors derived from fine-tuning, enables
multi-task learning and task forgetting but struggles to isolate task-specific
knowledge from general instruction-following behavior. To address this, we
propose Layer-Aware Task Arithmetic (LATA), a novel approach that assigns
layer-specific weights to task vectors based on their alignment with
instruction-following or task-specific components. By amplifying task-relevant
layers and attenuating instruction-following layers, LATA improves task
learning and forgetting performance while preserving overall model utility.
Experiments on multiple benchmarks, including WikiText-2, GSM8K, and HumanEval,
demonstrate that LATA outperforms existing methods in both multi-task learning
and selective task forgetting, achieving higher task accuracy and alignment
with minimal degradation in output quality. Our findings highlight the
importance of layer-wise analysis in disentangling task-specific and
general-purpose knowledge, offering a robust framework for efficient model
merging and editing.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ An Extensive Evaluation of PDDL Capabilities in off-the-shelf LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.20175v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.20175v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kaustubh Vyas, Damien Graux, Sébastien Montella, Pavlos Vougiouklis, Ruofei Lai, Keshuang Li, Yang Ren, Jeff Z. Pan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent advancements, large language models (LLMs) have exhibited
proficiency in code generation and chain-of-thought reasoning, laying the
groundwork for tackling automatic formal planning tasks. This study evaluates
the potential of LLMs to understand and generate Planning Domain Definition
Language (PDDL), an essential representation in artificial intelligence
planning. We conduct an extensive analysis across 20 distinct models spanning 7
major LLM families, both commercial and open-source. Our comprehensive
evaluation sheds light on the zero-shot LLM capabilities of parsing,
generating, and reasoning with PDDL. Our findings indicate that while some
models demonstrate notable effectiveness in handling PDDL, others pose
limitations in more complex scenarios requiring nuanced planning knowledge.
These results highlight the promise and current limitations of LLMs in formal
planning tasks, offering insights into their application and guiding future
efforts in AI-driven planning paradigms.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multimodal Representation Alignment for Image Generation: Text-Image
  Interleaved Control Is Easier Than You Think 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.20172v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.20172v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Liang Chen, Shuai Bai, Wenhao Chai, Weichu Xie, Haozhe Zhao, Leon Vinci, Junyang Lin, Baobao Chang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The field of advanced text-to-image generation is witnessing the emergence of
unified frameworks that integrate powerful text encoders, such as CLIP and T5,
with Diffusion Transformer backbones. Although there have been efforts to
control output images with additional conditions, like canny and depth map, a
comprehensive framework for arbitrary text-image interleaved control is still
lacking. This gap is especially evident when attempting to merge concepts or
visual elements from multiple images in the generation process. To mitigate the
gap, we conducted preliminary experiments showing that large multimodal models
(LMMs) offer an effective shared representation space, where image and text can
be well-aligned to serve as a condition for external diffusion models. Based on
this discovery, we propose Dream Engine, an efficient and unified framework
designed for arbitrary text-image interleaved control in image generation
models. Building on powerful text-to-image models like SD3.5, we replace the
original text-only encoders by incorporating versatile multimodal information
encoders such as QwenVL. Our approach utilizes a two-stage training paradigm,
consisting of joint text-image alignment and multimodal interleaved instruction
tuning. Our experiments demonstrate that this training method is effective,
achieving a 0.69 overall score on the GenEval benchmark, and matching the
performance of state-of-the-art text-to-image models like SD3.5 and FLUX.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 9 figures, codebase in
  https://github.com/chenllliang/DreamEngine</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Representing Signs as Signs: One-Shot ISLR to Facilitate Functional Sign
  Language Technologies 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.20171v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.20171v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Toon Vandendriessche, Mathieu De Coster, Annelies Lejon, Joni Dambre
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Isolated Sign Language Recognition (ISLR) is crucial for scalable sign
language technology, yet language-specific approaches limit current models. To
address this, we propose a one-shot learning approach that generalises across
languages and evolving vocabularies. Our method involves pretraining a model to
embed signs based on essential features and using a dense vector search for
rapid, accurate recognition of unseen signs. We achieve state-of-the-art
results, including 50.8% one-shot MRR on a large dictionary containing 10,235
unique signs from a different language than the training set. Our approach is
robust across languages and support sets, offering a scalable, adaptable
solution for ISLR. Co-created with the Deaf and Hard of Hearing (DHH)
community, this method aligns with real-world needs, and advances scalable sign
language recognition.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Re-evaluating Open-ended Evaluation of Large Language Models <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.20170v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.20170v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Siqi Liu, Ian Gemp, Luke Marris, Georgios Piliouras, Nicolas Heess, Marc Lanctot
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Evaluation has traditionally focused on ranking candidates for a specific
skill. Modern generalist models, such as Large Language Models (LLMs),
decidedly outpace this paradigm. Open-ended evaluation systems, where candidate
models are compared on user-submitted prompts, have emerged as a popular
solution. Despite their many advantages, we show that the current Elo-based
rating systems can be susceptible to and even reinforce biases in data,
intentional or accidental, due to their sensitivity to redundancies. To address
this issue, we propose evaluation as a 3-player game, and introduce novel
game-theoretic solution concepts to ensure robustness to redundancy. We show
that our method leads to intuitive ratings and provide insights into the
competitive landscape of LLM development.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published at ICLR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Similarity-Distance-Magnitude Universal Verification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.20167v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.20167v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Allen Schmaltz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We solve the neural network robustness problem by adding Similarity (i.e.,
correctly predicted depth-matches into training)-awareness and
Distance-to-training-distribution-awareness to the existing output Magnitude
(i.e., decision-boundary)-awareness of the softmax function. The resulting sdm
activation function provides strong signals of the relative epistemic
(reducible) predictive uncertainty. We use this novel behavior to further
address the complementary HCI problem of mapping the output to
human-interpretable summary statistics over relevant partitions of a held-out
calibration set. Estimates of prediction-conditional uncertainty are obtained
via a parsimonious learned transform over the class-conditional empirical CDFs
of the output of a final-layer sdm activation function. For decision-making and
as an intrinsic model check, estimates of class-conditional accuracy are
obtained by further partitioning the high-probability regions of this
calibrated output into class-conditional, region-specific CDFs. The uncertainty
estimates from sdm calibration are remarkably robust to test-time distribution
shifts and out-of-distribution inputs; incorporate awareness of the effective
sample size; provide estimates of uncertainty from the learning and data
splitting processes; and are well-suited for selective classification and
conditional branching for additional test-time compute based on the predictive
uncertainty, as for selective LLM generation, routing, and composition over
multiple models and retrieval. Finally, we construct sdm networks, LLMs with
uncertainty-aware verification and interpretability-by-exemplar as intrinsic
properties. We provide open-source software implementing these results.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>35 pages (8 Tables, 4 Algorithms, 5 Listings)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Telephone <span class="highlight-title">Survey</span>s Meet Conversational AI: Evaluating a LLM-Based
  Telephone <span class="highlight-title">Survey</span> System at Scale 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.20140v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.20140v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Max M. Lang, Sol Eskenazi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Telephone surveys remain a valuable tool for gathering insights but typically
require substantial resources in training and coordinating human interviewers.
This work presents an AI-driven telephone survey system integrating
text-to-speech (TTS), a large language model (LLM), and speech-to-text (STT)
that mimics the versatility of human-led interviews on scale.
  We tested the system across two populations, a pilot study in the United
States (n = 75) and a large-scale deployment in Peru (n = 2,739), inviting
participants via web-based links and contacting them via direct phone calls.
The AI agent successfully administered open-ended and closed-ended questions,
handled basic clarifications, and dynamically navigated branching logic,
allowing fast large-scale survey deployment without interviewer recruitment or
training.
  Our findings demonstrate that while the AI system's probing for qualitative
depth was more limited than human interviewers, overall data quality approached
human-led standards for structured items. This study represents one of the
first successful large-scale deployments of an LLM-based telephone interviewer
in a real-world survey context. The AI-powered telephone survey system has the
potential for expanding scalable, consistent data collecting across market
research, social science, and public opinion studies, thus improving
operational efficiency while maintaining appropriate data quality for research.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Educator Attention: How computational tools can systematically identify
  the distribution of a key resource for students 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.20135v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.20135v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qingyang Zhang, Rose E. Wang, Ana T. Ribeiro, Dora Demszky, Susanna Loeb
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Educator attention is critical for student success, yet how educators
distribute their attention across students remains poorly understood due to
data and methodological constraints. This study presents the first large-scale
computational analysis of educator attention patterns, leveraging over 1
million educator utterances from virtual group tutoring sessions linked to
detailed student demographic and academic achievement data. Using natural
language processing techniques, we systematically examine the recipient and
nature of educator attention. Our findings reveal that educators often provide
more attention to lower-achieving students. However, disparities emerge across
demographic lines, particularly by gender. Girls tend to receive less attention
when paired with boys, even when they are the lower achieving student in the
group. Lower-achieving female students in mixed-gender pairs receive
significantly less attention than their higher-achieving male peers, while
lower-achieving male students receive significantly and substantially more
attention than their higher-achieving female peers. We also find some
differences by race and English learner (EL) status, with low-achieving Black
students receiving additional attention only when paired with another Black
student but not when paired with a non-Black peer. In contrast,
higher-achieving EL students receive disproportionately more attention than
their lower-achieving EL peers. This work highlights how large-scale
interaction data and computational methods can uncover subtle but meaningful
disparities in teaching practices, providing empirical insights to inform more
equitable and effective educational strategies.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The first two authors QZ and REW contributed equally. The last two
  authors DD and SL advised equally</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Finite State Automata Inside <span class="highlight-title">Transformer</span>s with Chain-of-Thought: A
  Mechanistic Study on State Tracking 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.20129v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.20129v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yifan Zhang, Wenyu Du, Dongming Jin, Jie Fu, Zhi Jin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Chain-of-Thought (CoT) significantly enhances the performance of large
language models (LLMs) across a wide range of tasks, and prior research shows
that CoT can theoretically increase expressiveness. However, there is limited
mechanistic understanding of the algorithms that Transformer+CoT can learn. In
this work, we (1) evaluate the state tracking capabilities of Transformer+CoT
and its variants, confirming the effectiveness of CoT. (2) Next, we identify
the circuit, a subset of model components, responsible for tracking the world
state, finding that late-layer MLP neurons play a key role. We propose two
metrics, compression and distinction, and show that the neuron sets for each
state achieve nearly 100% accuracy, providing evidence of an implicit finite
state automaton (FSA) embedded within the model. (3) Additionally, we explore
three realistic settings: skipping intermediate steps, introducing data noise,
and testing length generalization. Our results demonstrate that Transformer+CoT
learns robust algorithms (FSA), highlighting its resilience in challenging
scenarios.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SoRFT: Issue Resolving with Subtask-oriented Reinforced Fine-Tuning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.20127v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.20127v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zexiong Ma, Chao Peng, Pengfei Gao, Xiangxin Meng, Yanzhen Zou, Bing Xie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Mainstream issue-resolving frameworks predominantly rely on commercial
models, leading to high costs and privacy concerns. Existing training
approaches for issue resolving struggle with poor generalization and fail to
fully leverage open-source development resources. We propose Subtask-oriented
Reinforced Fine-Tuning (SoRFT), a novel training approach to enhance the issue
resolving capability of LLMs. We decomposes issue resolving into structured
subtasks: file localization, function localization, line localization, and code
edit generation. SoRFT consists of two training stages: (1) rejection-sampled
supervised fine-tuning, Chain of Thought (CoT) data is filtered using
ground-truth before fine-tuning the LLM, and (2) rule-based reinforcement
learning, which leverages PPO with ground-truth based rewards. We evaluate the
SoRFT-trained model on SWE-Bench Verified and SWE-Bench Lite, achieving
state-of-the-art (SOTA) performance among open-source models (e.g., resolve
21.4% issues on SWE-Bench Verified with SoRFT-Qwen-7B). The experimental
results demonstrate that SoRFT significantly enhances issue-resolving
performance, improves model generalization, and provides a cost-efficient
alternative to commercial models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Self-Training Elicits Concise Reasoning in Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.20122v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.20122v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tergel Munkhbat, Namgyu Ho, Seohyun Kim, Yongjin Yang, Yujin Kim, Se-Young Yun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Chain-of-thought (CoT) reasoning has enabled large language models (LLMs) to
utilize additional computation through intermediate tokens to solve complex
tasks. However, we posit that typical reasoning traces contain many redundant
tokens, incurring extraneous inference costs. Upon examination of the output
distribution of current LLMs, we find evidence on their latent ability to
reason more concisely, relative to their default behavior. To elicit this
capability, we propose simple fine-tuning methods which leverage self-generated
concise reasoning paths obtained by best-of-N sampling and few-shot
conditioning, in task-specific settings. Our combined method achieves a 30%
reduction in output tokens on average, across five model families on GSM8K and
MATH, while maintaining average accuracy. By exploiting the fundamental
stochasticity and in-context learning capabilities of LLMs, our self-training
approach robustly elicits concise reasoning on a wide range of models,
including those with extensive post-training. Code is available at
https://github.com/TergelMunkhbat/concise-reasoning
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>23 pages, 10 figures, 18 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LongRoPE2: Near-Lossless LLM Context Window Scaling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.20082v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.20082v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ning Shang, Li Lyna Zhang, Siyuan Wang, Gaokai Zhang, Gilsinia Lopez, Fan Yang, Weizhu Chen, Mao Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  LongRoPE2 is a novel approach that extends the effective context window of
pre-trained large language models (LLMs) to the target length, while preserving
the performance on the original shorter context window. This is achieved by
three contributions: (1) a hypothesis that insufficient training in higher RoPE
dimensions contributes to the persistent out-of-distribution (OOD) issues
observed in existing methods; (2) an effective RoPE rescaling algorithm that
adopts evolutionary search guided by "needle-driven" perplexity to address the
insufficient training problem; (3) a mixed context window training approach
that fine-tunes model weights to adopt rescaled RoPE for long-context sequences
while preserving the short-context performance with the original RoPE.
Extensive experiments on LLaMA3-8B and Phi3-mini-3.8B across various benchmarks
validate the hypothesis and demonstrate the effectiveness of LongRoPE2.
Remarkably, LongRoPE2 extends LLaMA3-8B to achieve a 128K effective context
length while retaining over 98.5% of short-context performance, using only 10B
tokens -- 80x fewer than Meta's approach, which fails to reach the target
effective context length. Code will be available at
https://github.com/microsoft/LongRoPE.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Collab-Overcooked: Benchmarking and Evaluating Large Language Models as
  Collaborative Agents 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.20073v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.20073v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haochen Sun, Shuwen Zhang, Lei Ren, Hao Xu, Hao Fu, Caixia Yuan, Xiaojie Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) based agent systems have made great strides in
real-world applications beyond traditional NLP tasks. This paper proposes a new
LLM-powered Multi-Agent System (LLM-MAS) benchmark, Collab-Overcooked, built on
the popular Overcooked-AI game with more applicable and challenging tasks in
interactive environments. Collab-Overcooked extends existing benchmarks from
two novel perspectives. First, it provides a multi-agent framework supporting
diverse tasks and objectives and encourages collaboration through natural
language communication. Second, it introduces a spectrum of process-oriented
evaluation metrics to assess the fine-grained collaboration capabilities of
different LLM agents, a dimension often overlooked in prior work. We conduct
extensive experiments over 10 popular LLMs and show that, while the LLMs
present a strong ability in goal interpretation, there is a significant
discrepancy in active collaboration and continuous adaption that are critical
for efficiently fulfilling complicated tasks. Notably, we highlight the
strengths and weaknesses in LLM-MAS and provide insights for improving and
evaluating LLM-MAS on a unified and open-sourced benchmark. Environments, 30
open-ended tasks, and an integrated evaluation package are now publicly
available at https://github.com/YusaeMeow/Collab-Overcooked.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>25 pages, 14 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Connecting the Persian-speaking World through Transliteration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.20047v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.20047v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rayyan Merchant, Akhilesh Kakolu Ramarao, Kevin Tang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite speaking mutually intelligible varieties of the same language,
speakers of Tajik Persian, written in a modified Cyrillic alphabet, cannot read
Iranian and Afghan texts written in the Perso-Arabic script. As the vast
majority of Persian text on the Internet is written in Perso-Arabic,
monolingual Tajik speakers are unable to interface with the Internet in any
meaningful way. Due to overwhelming similarity between the formal registers of
these dialects and the scarcity of Tajik-Farsi parallel data, machine
transliteration has been proposed as more a practical and appropriate solution
than machine translation. This paper presents a transformer-based G2P approach
to Tajik-Farsi transliteration, achieving chrF++ scores of 58.70 (Farsi to
Tajik) and 74.20 (Tajik to Farsi) on novel digraphic datasets, setting a
comparable baseline metric for future work. Our results also demonstrate the
non-trivial difficulty of this task in both directions. We also provide an
overview of the differences between the two scripts and the challenges they
present, so as to aid future efforts in Tajik-Farsi transliteration.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Polish-ASTE: Aspect-Sentiment Triplet Extraction <span class="highlight-title">Dataset</span>s for Polish 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.20046v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.20046v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Marta Lango, Borys Naglik, Mateusz Lango, Iwo Naglik
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Aspect-Sentiment Triplet Extraction (ASTE) is one of the most challenging and
complex tasks in sentiment analysis. It concerns the construction of triplets
that contain an aspect, its associated sentiment polarity, and an opinion
phrase that serves as a rationale for the assigned polarity. Despite the
growing popularity of the task and the many machine learning methods being
proposed to address it, the number of datasets for ASTE is very limited. In
particular, no dataset is available for any of the Slavic languages. In this
paper, we present two new datasets for ASTE containing customer opinions about
hotels and purchased products expressed in Polish. We also perform experiments
with two ASTE techniques combined with two large language models for Polish to
investigate their performance and the difficulty of the assembled datasets. The
new datasets are available under a permissive licence and have the same file
format as the English datasets, facilitating their use in future research.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Vision-Encoders (Already) Know What They See: Mitigating Object
  Hallucination via Simple Fine-Grained CLIPScore 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.20034v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.20034v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hongseok Oh, Wonseok Hwang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, Large Vision-Language Models (LVLMs) show remarkable performance
across various domains. However, these models suffer from object hallucination.
This study revisits the previous claim that the primary cause of such
hallucination lies in the limited representational capacity of the vision
encoder. Our analysis reveals that the capacity of the vision encoder itself is
already enough for detecting object hallucination. Based on this insight, we
propose a Fine-grained CLIPScore (F-CLIPScore), a simple yet effective
evaluation metric that enhances object-level granularity by incorporating text
embeddings at the noun phrase level. Evaluations on the OHD-Caps benchmark show
that F-CLIPScore significantly outperforms conventional CLIPScore in accuracy
by a large margin of 39.6% without additional training. We further validate
F-CLIPScore by showing that LVLM trained with the data filtered using
F-CLIPScore exhibits reduced hallucination.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>4 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Erasing Without Remembering: Safeguarding Knowledge Forgetting in Large
  Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.19982v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.19982v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Huazheng Wang, Yongcheng Jing, Haifeng Sun, Yingjie Wang, Jingyu Wang, Jianxin Liao, Dacheng Tao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we explore machine unlearning from a novel dimension, by
studying how to safeguard model unlearning in large language models (LLMs). Our
goal is to prevent unlearned models from recalling any related memory of the
targeted knowledge.We begin by uncovering a surprisingly simple yet overlooked
fact: existing methods typically erase only the exact expressions of the
targeted knowledge, leaving paraphrased or related information intact. To
rigorously measure such oversights, we introduce UGBench, the first benchmark
tailored for evaluating the generalisation performance across 13
state-of-the-art methods.UGBench reveals that unlearned models can still recall
paraphrased answers and retain target facts in intermediate layers. To address
this, we propose PERMU, a perturbation-based method that significantly enhances
the generalisation capabilities for safeguarding LLM unlearning.Experiments
demonstrate that PERMU delivers up to a 50.13% improvement in unlearning while
maintaining a 43.53% boost in robust generalisation. Our code can be found in
https://github.com/MaybeLizzy/UGBench.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Lookahead Limitation: Why Multi-Operand Addition is Hard for LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.19981v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.19981v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tanja Baeumel, Josef van Genabith, Simon Ostermann
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Autoregressive large language models (LLMs) exhibit impressive performance
across various tasks but struggle with simple arithmetic, such as addition of
two or more operands. We show that this struggle arises from LLMs' use of a
simple one-digit lookahead heuristic, which works fairly well (but not perfect)
for two-operand addition but fails in multi-operand cases, where the carry-over
logic is more complex. Our probing experiments and digit-wise accuracy
evaluation show that LLMs fail precisely where a one-digit lookahead is
insufficient to account for cascading carries. We analyze the impact of
tokenization strategies on arithmetic performance and show that all
investigated models, regardless of tokenization, are inherently limited in the
addition of multiple operands due to their reliance on a one-digit lookahead
heuristic. Our findings reveal fundamental limitations that prevent LLMs from
generalizing to more complex numerical reasoning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Pre-print</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Deterministic or probabilistic? The psychology of LLMs as random number
  generators 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.19965v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.19965v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Javier Coronado-Blázquez
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have transformed text generation through
inherently probabilistic context-aware mechanisms, mimicking human natural
language. In this paper, we systematically investigate the performance of
various LLMs when generating random numbers, considering diverse configurations
such as different model architectures, numerical ranges, temperature, and
prompt languages. Our results reveal that, despite their stochastic
transformers-based architecture, these models often exhibit deterministic
responses when prompted for random numerical outputs. In particular, we find
significant differences when changing the model, as well as the prompt
language, attributing this phenomenon to biases deeply embedded within the
training data. Models such as DeepSeek-R1 can shed some light on the internal
reasoning process of LLMs, despite arriving to similar results. These biases
induce predictable patterns that undermine genuine randomness, as LLMs are
nothing but reproducing our own human cognitive biases.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>31 pages, 12 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Collaborative Stance Detection via Small-Large Language Model
  Consistency Verification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.19954v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.19954v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yu Yan, Sheng Sun, Zixiang Tang, Teli Liu, Min Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Stance detection on social media aims to identify attitudes expressed in
tweets towards specific targets. Current studies prioritize Large Language
Models (LLMs) over Small Language Models (SLMs) due to the overwhelming
performance improving provided by LLMs. However, heavily relying on LLMs for
stance detection, regardless of the cost, is impractical for real-world social
media monitoring systems that require vast data analysis. To this end, we
propose \textbf{\underline{Co}}llaborative Stance Detection via Small-Large
Language Model Consistency \textbf{\underline{Ver}}ification (\textbf{CoVer})
framework, which enhances LLM utilization via context-shared batch reasoning
and logical verification between LLM and SLM. Specifically, instead of
processing each text individually, CoVer processes texts batch-by-batch,
obtaining stance predictions and corresponding explanations via LLM reasoning
in a shared context. Then, to exclude the bias caused by context noises, CoVer
introduces the SLM for logical consistency verification. Finally, texts that
repeatedly exhibit low logical consistency are classified using
consistency-weighted aggregation of prior LLM stance predictions. Our
experiments show that CoVer outperforms state-of-the-art methods across
multiple benchmarks in the zero-shot setting, achieving 0.54 LLM queries per
tweet while significantly enhancing performance. Our CoVer offers a more
practical solution for LLM deploying for social media stance detection.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GeoEdit: Geometric Knowledge Editing for Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.19953v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.19953v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yujie Feng, Liming Zhan, Zexin Lu, Yongxin Xu, Xu Chu, Yasha Wang, Jiannong Cao, Philip S. Yu, Xiao-Ming Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Regular updates are essential for maintaining up-to-date knowledge in large
language models (LLMs). Consequently, various model editing methods have been
developed to update specific knowledge within LLMs. However, training-based
approaches often struggle to effectively incorporate new knowledge while
preserving unrelated general knowledge. To address this challenge, we propose a
novel framework called Geometric Knowledge Editing (GeoEdit). GeoEdit utilizes
the geometric relationships of parameter updates from fine-tuning to
differentiate between neurons associated with new knowledge updates and those
related to general knowledge perturbations. By employing a direction-aware
knowledge identification method, we avoid updating neurons with directions
approximately orthogonal to existing knowledge, thus preserving the model's
generalization ability. For the remaining neurons, we integrate both old and
new knowledge for aligned directions and apply a "forget-then-learn" editing
strategy for opposite directions. Additionally, we introduce an
importance-guided task vector fusion technique that filters out redundant
information and provides adaptive neuron-level weighting, further enhancing
model editing performance. Extensive experiments on two publicly available
datasets demonstrate the superiority of GeoEdit over existing state-of-the-art
methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Alleviating Distribution Shift in Synthetic Data for Machine Translation
  Quality Estimation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.19941v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.19941v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiang Geng, Zhejian Lai, Jiajun Chen, Hao Yang, Shujian Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Quality Estimation (QE) models evaluate the quality of machine translations
without reference translations, serving as the reward models for the
translation task. Due to the data scarcity, synthetic data generation has
emerged as a promising solution. However, synthetic QE data often suffers from
distribution shift, which can manifest as discrepancies between pseudo and real
translations, or in pseudo labels that do not align with human preferences. To
tackle this issue, we introduce ADSQE, a novel framework for alleviating
distribution shift in synthetic QE data. To reduce the difference between
pseudo and real translations, we employ the constrained beam search algorithm
and enhance translation diversity through the use of distinct generation
models. ADSQE uses references, i.e., translation supervision signals, to guide
both the generation and annotation processes, enhancing the quality of
word-level labels. ADSE further identifies the shortest phrase covering
consecutive error tokens, mimicking human annotation behavior, to assign the
final phrase-level labels. Specially, we underscore that the translation model
can not annotate translations of itself accurately. Extensive experiments
demonstrate that ADSQE outperforms SOTA baselines like COMET in both supervised
and unsupervised settings. Further analysis offers insights into synthetic data
generation that could benefit reward models for other tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Picking the Cream of the Crop: Visual-Centric Data Selection with
  Collaborative Agents 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.19917v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.19917v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhenyu Liu, Yunxin Li, Baotian Hu, Wenhan Luo, Yaowei Wang, Min Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To improve Multimodal Large Language Models' (MLLMs) ability to process
images and complex instructions, researchers predominantly curate large-scale
visual instruction tuning datasets, which are either sourced from existing
vision tasks or synthetically generated using LLMs and image descriptions.
However, they often suffer from critical flaws, including misaligned
instruction-image pairs and low-quality images. Such issues hinder training
efficiency and limit performance improvements, as models waste resources on
noisy or irrelevant data with minimal benefit to overall capability. To address
this issue, we propose a \textbf{Vi}sual-Centric \textbf{S}election approach
via \textbf{A}gents Collaboration (ViSA), which centers on image quality
assessment and image-instruction relevance evaluation. Specifically, our
approach consists of 1) an image information quantification method via visual
agents collaboration to select images with rich visual information, and 2) a
visual-centric instruction quality assessment method to select high-quality
instruction data related to high-quality images. Finally, we reorganize 80K
instruction data from large open-source datasets. Extensive experiments
demonstrate that ViSA outperforms or is comparable to current state-of-the-art
models on seven benchmarks, using only 2.5\% of the original data, highlighting
the efficiency of our data selection approach. Moreover, we conduct ablation
studies to validate the effectiveness of each component of our method. The code
is available at https://github.com/HITsz-TMG/ViSA.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Order Doesn't Matter, But Reasoning Does: Training LLMs with
  Order-Centric Augmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.19907v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.19907v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qianxi He, Qianyu He, Jiaqing Liang, Yanghua Xiao, Weikang Zhou, Zeye Sun, Fei Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Logical reasoning is essential for large language models (LLMs) to ensure
accurate and coherent inference. However, LLMs struggle with reasoning order
variations and fail to generalize across logically equivalent transformations.
LLMs often rely on fixed sequential patterns rather than true logical
understanding. To address this issue, we introduce an order-centric data
augmentation framework based on commutativity in logical reasoning. We first
randomly shuffle independent premises to introduce condition order
augmentation. For reasoning steps, we construct a directed acyclic graph (DAG)
to model dependencies between steps, which allows us to identify valid
reorderings of steps while preserving logical correctness. By leveraging
order-centric augmentations, models can develop a more flexible and generalized
reasoning process. Finally, we conduct extensive experiments across multiple
logical reasoning benchmarks, demonstrating that our method significantly
enhances LLMs' reasoning performance and adaptability to diverse logical
structures. We release our codes and augmented data in
https://anonymous.4open.science/r/Order-Centric-Data-Augmentation-822C/.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Beyond the Tip of Efficiency: Uncovering the Submerged Threats of
  Jailbreak Attacks in Small Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.19883v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.19883v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sibo Yi, Tianshuo Cong, Xinlei He, Qi Li, Jiaxing Song
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Small language models (SLMs) have become increasingly prominent in the
deployment on edge devices due to their high efficiency and low computational
cost. While researchers continue to advance the capabilities of SLMs through
innovative training strategies and model compression techniques, the security
risks of SLMs have received considerably less attention compared to large
language models (LLMs).To fill this gap, we provide a comprehensive empirical
study to evaluate the security performance of 13 state-of-the-art SLMs under
various jailbreak attacks. Our experiments demonstrate that most SLMs are quite
susceptible to existing jailbreak attacks, while some of them are even
vulnerable to direct harmful prompts.To address the safety concerns, we
evaluate several representative defense methods and demonstrate their
effectiveness in enhancing the security of SLMs. We further analyze the
potential security degradation caused by different SLM techniques including
architecture compression, quantization, knowledge distillation, and so on. We
expect that our research can highlight the security challenges of SLMs and
provide valuable insights to future work in developing more robust and secure
SLMs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages. 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MMKE-Bench: A Multimodal Editing Benchmark for Diverse Visual Knowledge 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.19870v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.19870v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuntao Du, Kailin Jiang, Zhi Gao, Chenrui Shi, Zilong Zheng, Siyuan Qi, Qing Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Knowledge editing techniques have emerged as essential tools for updating the
factual knowledge of large language models (LLMs) and multimodal models (LMMs),
allowing them to correct outdated or inaccurate information without retraining
from scratch. However, existing benchmarks for multimodal knowledge editing
primarily focus on entity-level knowledge represented as simple triplets, which
fail to capture the complexity of real-world multimodal information. To address
this issue, we introduce MMKE-Bench, a comprehensive MultiModal Knowledge
Editing Benchmark, designed to evaluate the ability of LMMs to edit diverse
visual knowledge in real-world scenarios. MMKE-Bench addresses these
limitations by incorporating three types of editing tasks: visual entity
editing, visual semantic editing, and user-specific editing. Besides,
MMKE-Bench uses free-form natural language to represent and edit knowledge,
offering a more flexible and effective format. The benchmark consists of 2,940
pieces of knowledge and 8,363 images across 33 broad categories, with
evaluation questions automatically generated and human-verified. We assess five
state-of-the-art knowledge editing methods on three prominent LMMs, revealing
that no method excels across all criteria, and that visual and user-specific
edits are particularly challenging. MMKE-Bench sets a new standard for
evaluating the robustness of multimodal knowledge editing techniques, driving
progress in this rapidly evolving field.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MIND: Towards Immersive Psychological Healing with Multi-agent Inner
  Dialogue 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.19860v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.19860v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yujia Chen, Changsong Li, Yiming Wang, Qingqing Xiao, Nan Zhang, Zifan Kong, Peng Wang, Binyu Yan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Mental health issues are worsening in today's competitive society, such as
depression and anxiety. Traditional healings like counseling and chatbots fail
to engage effectively, they often provide generic responses lacking emotional
depth. Although large language models (LLMs) have the potential to create more
human-like interactions, they still struggle to capture subtle emotions. This
requires LLMs to be equipped with human-like adaptability and warmth. To fill
this gap, we propose the MIND (Multi-agent INner Dialogue), a novel paradigm
that provides more immersive psychological healing environments. Considering
the strong generative and role-playing ability of LLM agents, we predefine an
interactive healing framework and assign LLM agents different roles within the
framework to engage in interactive inner dialogues with users, thereby
providing an immersive healing experience. We conduct extensive human
experiments in various real-world healing dimensions, and find that MIND
provides a more user-friendly experience than traditional paradigms. This
demonstrates that MIND effectively leverages the significant potential of LLMs
in psychological healing.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Team A at SemEval-2025 Task 11: Breaking Language Barriers in Emotion
  Detection with Multilingual Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.19856v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.19856v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        P Sam Sahil, Anupam Jamatia
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper describes the system submitted by Team A to SemEval 2025 Task 11,
``Bridging the Gap in Text-Based Emotion Detection.'' The task involved
identifying the perceived emotion of a speaker from text snippets, with each
instance annotated with one of six emotions: joy, sadness, fear, anger,
surprise, or disgust. A dataset provided by the task organizers served as the
foundation for training and evaluating our models. Among the various approaches
explored, the best performance was achieved using multilingual embeddings
combined with a fully connected layer. This paper details the system
architecture, discusses experimental results, and highlights the advantages of
leveraging multilingual representations for robust emotion detection in text.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ConvCodeWorld: Benchmarking Conversational Code Generation in
  Reproducible Feedback Environments <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.19852v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.19852v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hojae Han, Seung-won Hwang, Rajhans Samdani, Yuxiong He
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have proven invaluable for code generation,
particularly in interactive settings. However, existing code generation
benchmarks fail to capture the diverse feedback encountered in multi-turn
interactions, limiting our ability to evaluate LLMs in these contexts. To
address this gap, we present a set of novel benchmarks that explicitly model
the quality of feedback provided to code generation LLMs. Our contributions are
threefold: First, we introduce CONVCODEWORLD, a novel and reproducible
environment for benchmarking interactive code generation. CONVCODEWORLD
simulates 9 distinct interactive code generation scenarios while systematically
combining three types of feedback: (a) compilation feedback; (b) execution
feedback with varying test coverage; (c) verbal feedback generated by GPT-4o
with different levels of expertise. Second, we introduce CONVCODEBENCH, a fast,
static version of benchmark that uses pre-generated feedback logs, eliminating
the need for costly dynamic verbal feedback generation while maintaining strong
Spearman's rank correlations (0.82 to 0.99) with CONVCODEWORLD. Third,
extensive evaluations of both closed-source and open-source LLMs including
R1-Distill on CONVCODEWORLD reveal key insights: (a) LLM performance varies
significantly based on the feedback provided; (b) Weaker LLMs, with sufficient
feedback, can outperform single-turn results of state-of-the-art LLMs without
feedback; (c) Training on a specific feedback combination can limit an LLM's
ability to utilize unseen combinations; (d) LLMs solve problems in fewer turns
(high MRR) may not solve as many problems overall (high Recall), and vice
versa. All implementations and benchmarks will be made publicly available at
https://huggingface.co/spaces/ConvCodeWorld/ConvCodeWorld
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Revisiting Self-Consistency from Dynamic Distributional Alignment
  Perspective on Answer Aggregation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.19830v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.19830v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiwei Li, Ji Zhang, Shaoxiong Feng, Peiwen Yuan, Xinglin Wang, Jiayi Shi, Yueqi Zhang, Chuyi Tan, Boyuan Pan, Yao Hu, Kan Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Self-consistency improves reasoning by aggregating diverse stochastic
samples, yet the dynamics behind its efficacy remain underexplored. We reframe
self-consistency as a dynamic distributional alignment problem, revealing that
decoding temperature not only governs sampling randomness but also actively
shapes the latent answer distribution. Given that high temperatures require
prohibitively large sample sizes to stabilize, while low temperatures risk
amplifying biases, we propose a confidence-driven mechanism that dynamically
calibrates temperature: sharpening the sampling distribution under uncertainty
to align with high-probability modes, and promoting exploration when confidence
is high. Experiments on mathematical reasoning tasks show this approach
outperforms fixed-diversity baselines under limited samples, improving both
average and best-case performance across varying initial temperatures without
additional data or modules. This establishes self-consistency as a
synchronization challenge between sampling dynamics and evolving answer
distributions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Foot-In-The-Door: A Multi-turn Jailbreak for LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.19820v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.19820v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zixuan Weng, Xiaolong Jin, Jinyuan Jia, Xiangyu Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Ensuring AI safety is crucial as large language models become increasingly
integrated into real-world applications. A key challenge is jailbreak, where
adversarial prompts bypass built-in safeguards to elicit harmful disallowed
outputs. Inspired by psychological foot-in-the-door principles, we introduce
FITD,a novel multi-turn jailbreak method that leverages the phenomenon where
minor initial commitments lower resistance to more significant or more
unethical transgressions.Our approach progressively escalates the malicious
intent of user queries through intermediate bridge prompts and aligns the
model's response by itself to induce toxic responses. Extensive experimental
results on two jailbreak benchmarks demonstrate that FITD achieves an average
attack success rate of 94% across seven widely used models, outperforming
existing state-of-the-art methods. Additionally, we provide an in-depth
analysis of LLM self-corruption, highlighting vulnerabilities in current
alignment strategies and emphasizing the risks inherent in multi-turn
interactions.The code is available at
https://github.com/Jinxiaolong1129/Foot-in-the-door-Jailbreak .
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>19 pages, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Text classification using machine learning methods 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.19801v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.19801v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bogdan Oancea
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper we present the results of an experiment aimed to use machine
learning methods to obtain models that can be used for the automatic
classification of products. In order to apply automatic classification methods,
we transformed the product names from a text representation to numeric vectors,
a process called word embedding. We used several embedding methods: Count
Vectorization, TF-IDF, Word2Vec, FASTTEXT, and GloVe. Having the product names
in a form of numeric vectors, we proceeded with a set of machine learning
methods for automatic classification: Logistic Regression, Multinomial Naive
Bayes, kNN, Artificial Neural Networks, Support Vector Machines, and Decision
trees with several variants. The results show an impressive accuracy of the
classification process for Support Vector Machines, Logistic Regression, and
Random Forests. Regarding the word embedding methods, the best results were
obtained with the FASTTEXT technique.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ NaijaNLP: A <span class="highlight-title">Survey</span> of Nigerian Low-Resource Languages 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.19784v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.19784v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Isa Inuwa-Dutse
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With over 500 languages in Nigeria, three languages -- Hausa, Yor\`ub\'a and
Igbo -- spoken by over 175 million people, account for about 60% of the spoken
languages. However, these languages are categorised as low-resource due to
insufficient resources to support tasks in computational linguistics. Several
research efforts and initiatives have been presented, however, a coherent
understanding of the state of Natural Language Processing (NLP) - from
grammatical formalisation to linguistic resources that support complex tasks
such as language understanding and generation is lacking. This study presents
the first comprehensive review of advancements in low-resource NLP (LR-NLP)
research across the three major Nigerian languages (NaijaNLP). We
quantitatively assess the available linguistic resources and identify key
challenges. Although a growing body of literature addresses various NLP
downstream tasks in Hausa, Igbo, and Yor\`ub\'a, only about 25.1% of the
reviewed studies contribute new linguistic resources. This finding highlights a
persistent reliance on repurposing existing data rather than generating novel,
high-quality resources. Additionally, language-specific challenges, such as the
accurate representation of diacritics, remain under-explored. To advance
NaijaNLP and LR-NLP more broadly, we emphasise the need for intensified efforts
in resource enrichment, comprehensive annotation, and the development of open
collaborative initiatives.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>35 pages, 2 figures, 4 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Do Retrieval-Augmented Language Models Adapt to Varying User Needs? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.19779v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.19779v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Peilin Wu, Xinlu Zhang, Wenhao Yu, Xingyu Liu, Xinya Du, Zhiyu Zoey Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in Retrieval-Augmented Language Models (RALMs) have
demonstrated their efficacy in knowledge-intensive tasks. However, existing
evaluation benchmarks often assume a single optimal approach to leveraging
retrieved information, failing to account for varying user needs. This paper
introduces a novel evaluation framework that systematically assesses RALMs
under three user need cases-Context-Exclusive, Context-First, and
Memory-First-across three distinct context settings: Context Matching,
Knowledge Conflict, and Information Irrelevant. By varying both user
instructions and the nature of retrieved information, our approach captures the
complexities of real-world applications where models must adapt to diverse user
requirements. Through extensive experiments on multiple QA datasets, including
HotpotQA, DisentQA, and our newly constructed synthetic URAQ dataset, we find
that restricting memory usage improves robustness in adversarial retrieval
conditions but decreases peak performance with ideal retrieval results and
model family dominates behavioral differences. Our findings highlight the
necessity of user-centric evaluations in the development of retrieval-augmented
systems and provide insights into optimizing model performance across varied
retrieval contexts. We will release our code and URAQ dataset upon acceptance
of the paper.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Advancements in Natural Language Processing for Automatic Text
  Summarization <span class="chip">CCS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.19773v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.19773v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nevidu Jayatilleke, Ruvan Weerasinghe, Nipuna Senanayake
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The substantial growth of textual content in diverse domains and platforms
has led to a considerable need for Automatic Text Summarization (ATS)
techniques that aid in the process of text analysis. The effectiveness of text
summarization models has been significantly enhanced in a variety of technical
domains because of advancements in Natural Language Processing (NLP) and Deep
Learning (DL). Despite this, the process of summarizing textual information
continues to be significantly constrained by the intricate writing styles of a
variety of texts, which involve a range of technical complexities. Text
summarization techniques can be broadly categorized into two main types:
abstractive summarization and extractive summarization. Extractive
summarization involves directly extracting sentences, phrases, or segments of
text from the content without making any changes. On the other hand,
abstractive summarization is achieved by reconstructing the sentences, phrases,
or segments from the original text using linguistic analysis. Through this
study, a linguistically diverse categorizations of text summarization
approaches have been addressed in a constructive manner. In this paper, the
authors explored existing hybrid techniques that have employed both extractive
and abstractive methodologies. In addition, the pros and cons of various
approaches discussed in the literature are also investigated. Furthermore, the
authors conducted a comparative analysis on different techniques and matrices
to evaluate the generated summaries using language generation models. This
survey endeavors to provide a comprehensive overview of ATS by presenting the
progression of language processing regarding this task through a breakdown of
diverse systems and architectures accompanied by technical and mathematical
explanations of their operations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 9 figures, ICCS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ EdiText: Controllable Coarse-to-Fine Text Editing with Diffusion
  Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.19765v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.19765v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Che Hyun Lee, Heeseung Kim, Jiheum Yeom, Sungroh Yoon
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose EdiText, a controllable text editing method that modify the
reference text to desired attributes at various scales. We integrate an
SDEdit-based editing technique that allows for broad adjustments in the degree
of text editing. Additionally, we introduce a novel fine-level editing method
based on self-conditioning, which allows subtle control of reference text.
While being capable of editing on its own, this fine-grained method, integrated
with the SDEdit approach, enables EdiText to make precise adjustments within
the desired range. EdiText demonstrates its controllability to robustly adjust
reference text at broad range of levels across various tasks, including
toxicity control and sentiment control.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Poly<span class="highlight-title">Prompt</span>: Automating Knowledge Extraction from Multilingual Language
  Models with Dynamic <span class="highlight-title">Prompt</span> Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.19756v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.19756v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nathan Roll
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) showcase increasingly impressive English
benchmark scores, however their performance profiles remain inconsistent across
multilingual settings. To address this gap, we introduce PolyPrompt, a novel,
parameter-efficient framework for enhancing the multilingual capabilities of
LLMs. Our method learns a set of trigger tokens for each language through a
gradient-based search, identifying the input query's language and selecting the
corresponding trigger tokens which are prepended to the prompt during
inference. We perform experiments on two ~1 billion parameter models, with
evaluations on the global MMLU benchmark across fifteen typologically and
resource diverse languages, demonstrating accuracy gains of 3.7%-19.9% compared
to naive and translation-pipeline baselines.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 2 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Beneath the Surface: How Large Language Models Reflect Hidden Bias 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.19749v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.19749v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jinhao Pan, Chahat Raj, Ziyu Yao, Ziwei Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The exceptional performance of Large Language Models (LLMs) often comes with
the unintended propagation of social biases embedded in their training data.
While existing benchmarks evaluate overt bias through direct term associations
between bias concept terms and demographic terms, LLMs have become increasingly
adept at avoiding biased responses, creating an illusion of neutrality.
However, biases persist in subtler, contextually hidden forms that traditional
benchmarks fail to capture. We introduce the Hidden Bias Benchmark (HBB), a
novel dataset designed to assess hidden bias that bias concepts are hidden
within naturalistic, subtly framed contexts in real-world scenarios. We analyze
six state-of-the-art LLMs, revealing that while models reduce bias in response
to overt bias, they continue to reinforce biases in nuanced settings. Data,
code, and results are available at
https://github.com/JP-25/Hidden-Bias-Benchmark.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ HaLoRA: Hardware-aware Low-Rank Adaptation for Large Language Models
  Based on Hybrid Compute-in-Memory Architecture 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.19747v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.19747v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Taiqiang Wu, Chenchen Ding, Wenyong Zhou, Yuxin Cheng, Xincheng Feng, Shuqi Wang, Chufan Shi, Zhengwu Liu, Ngai Wong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Low-rank adaptation (LoRA) is a predominant parameter-efficient finetuning
method to adapt large language models (LLMs) for downstream tasks. In this
paper, we first propose to deploy the LoRA-finetuned LLMs on the hybrid
compute-in-memory (CIM) architecture (i.e., pretrained weights onto RRAM and
LoRA onto SRAM). To address performance degradation from RRAM's inherent noise,
we design a novel Hardware-aware Low-rank Adaption (HaLoRA) method, aiming to
train a LoRA branch that is both robust and accurate by aligning the training
objectives under both ideal and noisy conditions. Experiments finetuning LLaMA
3.2 1B and 3B demonstrate HaLoRA's effectiveness across multiple reasoning
tasks, achieving up to 22.7 improvement in average score while maintaining
robustness at various noise levels.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ XCOMPS: A Multilingual Benchmark of Conceptual Minimal Pairs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.19737v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.19737v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Linyang He, Ercong Nie, Sukru Samet Dindar, Arsalan Firoozi, Adrian Florea, Van Nguyen, Corentin Puffay, Riki Shimizu, Haotian Ye, Jonathan Brennan, Helmut Schmid, Hinrich Schütze, Nima Mesgarani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce XCOMPS in this work, a multilingual conceptual minimal pair
dataset covering 17 languages. Using this dataset, we evaluate LLMs'
multilingual conceptual understanding through metalinguistic prompting, direct
probability measurement, and neurolinguistic probing. By comparing base,
instruction-tuned, and knowledge-distilled models, we find that: 1) LLMs
exhibit weaker conceptual understanding for low-resource languages, and
accuracy varies across languages despite being tested on the same concept sets.
2) LLMs excel at distinguishing concept-property pairs that are visibly
different but exhibit a marked performance drop when negative pairs share
subtle semantic similarities. 3) Instruction tuning improves performance in
concept understanding but does not enhance internal competence; knowledge
distillation can enhance internal competence in conceptual understanding for
low-resource languages with limited gains in explicit task performance. 4) More
morphologically complex languages yield lower concept understanding scores and
require deeper layers for conceptual reasoning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ R1-T1: Fully Incentivizing Translation Capability in LLMs via Reasoning
  Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.19735v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.19735v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Minggui He, Yilun Liu, Shimin Tao, Yuanchang Luo, Hongyong Zeng, Chang Su, Li Zhang, Hongxia Ma, Daimeng Wei, Weibin Meng, Hao Yang, Boxing Chen, Osamu Yoshie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite recent breakthroughs in reasoning-enhanced large language models
(LLMs) like DeepSeek-R1, incorporating inference-time reasoning into machine
translation (MT), where human translators naturally employ structured,
multi-layered reasoning chain-of-thoughts (CoTs), is yet underexplored.
Existing methods either design a fixed CoT tailored for a specific MT sub-task
(e.g., literature translation), or rely on synthesizing CoTs unaligned with
humans and supervised fine-tuning (SFT) prone to catastrophic forgetting,
limiting their adaptability to diverse translation scenarios. This paper
introduces R1-Translator (R1-T1), a novel framework to achieve inference-time
reasoning for general MT via reinforcement learning (RL) with human-aligned
CoTs comprising six common patterns. Our approach pioneers three innovations:
(1) extending reasoning-based translation beyond MT sub-tasks to six languages
and diverse tasks (e.g., legal/medical domain adaptation, idiom resolution);
(2) formalizing six expert-curated CoT templates that mirror hybrid human
strategies like context-aware paraphrasing and back translation; and (3)
enabling self-evolving CoT discovery and anti-forgetting adaptation through RL
with KL-constrained rewards. Experimental results indicate a steady translation
performance improvement in 21 languages and 80 translation directions on
Flores-101 test set, especially on the 15 languages unseen from training, with
its general multilingual abilities preserved compared with plain SFT.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Speculative Decoding and Beyond: An In-Depth <span class="highlight-title">Review</span> of Techniques 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.19732v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.19732v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yunhai Hu, Zining Liu, Zhenyuan Dong, Tianfan Peng, Bradley McDanel, Sai Qian Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sequential dependencies present a fundamental bottleneck in deploying
large-scale autoregressive models, particularly for real-time applications.
While traditional optimization approaches like pruning and quantization often
compromise model quality, recent advances in generation-refinement frameworks
demonstrate that this trade-off can be significantly mitigated.
  This survey presents a comprehensive taxonomy of generation-refinement
frameworks, analyzing methods across autoregressive sequence tasks. We
categorize methods based on their generation strategies (from simple n-gram
prediction to sophisticated draft models) and refinement mechanisms (including
single-pass verification and iterative approaches). Through systematic analysis
of both algorithmic innovations and system-level implementations, we examine
deployment strategies across computing environments and explore applications
spanning text, images, and speech generation. This systematic examination of
both theoretical frameworks and practical implementations provides a foundation
for future research in efficient autoregressive decoding.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Preference Learning Unlocks LLMs' Psycho-Counseling Skills 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.19731v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.19731v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mian Zhang, Shaun M. Eack, Zhiyu Zoey Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Applying large language models (LLMs) to assist in psycho-counseling is an
emerging and meaningful approach, driven by the significant gap between patient
needs and the availability of mental health support. However, current LLMs
struggle to consistently provide effective responses to client speeches,
largely due to the lack of supervision from high-quality real psycho-counseling
data, whose content is typically inaccessible due to client privacy concerns.
Furthermore, the quality of therapists' responses in available sessions can
vary significantly based on their professional training and experience.
Assessing the quality of therapists' responses remains an open challenge. In
this work, we address these challenges by first proposing a set of professional
and comprehensive principles to evaluate therapists' responses to client
speeches. Using these principles, we create a preference dataset,
PsychoCounsel-Preference, which contains 36k high-quality preference comparison
pairs. This dataset aligns with the preferences of professional
psychotherapists, providing a robust foundation for evaluating and improving
LLMs in psycho-counseling. Experiments on reward modeling and preference
learning demonstrate that PsychoCounsel-Preference is an excellent resource for
LLMs to acquire essential skills for responding to clients in a counseling
session. Our best-aligned model, PsychoCounsel-Llama3-8B, achieves an
impressive win rate of 87% against GPT-4o. We release PsychoCounsel-Preference,
PsychoCounsel-Llama3-8B and the reward model PsychoCounsel Llama3-8B-Reward to
facilitate the research of psycho-counseling with LLMs at:
https://hf.co/Psychotherapy-LLM.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Tokens for Learning, Tokens for Unlearning: Mitigating Membership
  Inference Attacks in Large Language Models via Dual-Purpose Training 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.19726v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.19726v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Toan Tran, Ruixuan Liu, Li Xiong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have become the backbone of modern natural
language processing but pose privacy concerns about leaking sensitive training
data. Membership inference attacks (MIAs), which aim to infer whether a sample
is included in a model's training dataset, can serve as a foundation for
broader privacy threats. Existing defenses designed for traditional
classification models do not account for the sequential nature of text data. As
a result, they either require significant computational resources or fail to
effectively mitigate privacy risks in LLMs. In this work, we propose a
lightweight yet effective empirical privacy defense for protecting training
data of language modeling by leveraging the token-specific characteristics. By
analyzing token dynamics during training, we propose a token selection strategy
that categorizes tokens into hard tokens for learning and memorized tokens for
unlearning. Subsequently, our training-phase defense optimizes a novel
dual-purpose token-level loss to achieve a Pareto-optimal balance between
utility and privacy. Extensive experiments demonstrate that our approach not
only provides strong protection against MIAs but also improves language
modeling performance by around 10\% across various LLM architectures and
datasets compared to the baselines.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CNsum:Automatic Summarization for Chinese News Text 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.19723v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.19723v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yu Zhao, Songping Huang, Dongsheng Zhou, Zhaoyun Ding, Fei Wang, Aixin Nian
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Obtaining valuable information from massive data efficiently has become our
research goal in the era of Big Data. Text summarization technology has been
continuously developed to meet this demand. Recent work has also shown that
transformer-based pre-trained language models have achieved great success on
various tasks in Natural Language Processing (NLP). Aiming at the problem of
Chinese news text summary generation and the application of Transformer
structure on Chinese, this paper proposes a Chinese news text summarization
model (CNsum) based on Transformer structure, and tests it on Chinese datasets
such as THUCNews. The results of the conducted experiments show that CNsum
achieves better ROUGE score than the baseline models, which verifies the
outperformance of the model.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>WASA 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Few-Shot Multilingual Open-Domain QA from 5 Examples <span class="chip">ACL</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.19722v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.19722v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fan Jiang, Tom Drummond, Trevor Cohn
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent approaches to multilingual open-domain question answering (MLODQA)
have achieved promising results given abundant language-specific training data.
However, the considerable annotation cost limits the application of these
methods for underrepresented languages. We introduce a \emph{few-shot learning}
approach to synthesise large-scale multilingual data from large language models
(LLMs). Our method begins with large-scale self-supervised pre-training using
WikiData, followed by training on high-quality synthetic multilingual data
generated by prompting LLMs with few-shot supervision. The final model,
\textsc{FsModQA}, significantly outperforms existing few-shot and supervised
baselines in MLODQA and cross-lingual and monolingual retrieval. We further
show our method can be extended for effective zero-shot adaptation to new
languages through a \emph{cross-lingual prompting} strategy with only
English-supervised data, making it a general and applicable solution for MLODQA
tasks without costly large-scale annotation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by TACL; pre-MIT Press publication version</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Sensing and Steering Stereotypes: Extracting and Applying Gender
  Representation Vectors in LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.19721v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.19721v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hannah Cyberey, Yangfeng Ji, David Evans
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) are known to perpetuate stereotypes and exhibit
biases. Various strategies have been proposed to mitigate potential harms that
may result from these biases, but most work studies biases in LLMs as a
black-box problem without considering how concepts are represented within the
model. We adapt techniques from representation engineering to study how the
concept of "gender" is represented within LLMs. We introduce a new method that
extracts concept representations via probability weighting without labeled data
and efficiently selects a steering vector for measuring and manipulating the
model's representation. We also present a projection-based method that enables
precise steering of model predictions and demonstrate its effectiveness in
mitigating gender bias in LLMs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GRACE: A Granular Benchmark for Evaluating Model Calibration against
  Human Calibration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.19684v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.19684v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yoo Yeon Sung, Eve Fleisig, Yu Hou, Ishan Upadhyay, Jordan Lee Boyd-Graber
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Language models are often miscalibrated, leading to confidently incorrect
answers. We introduce GRACE, a benchmark for language model calibration that
incorporates comparison with human calibration. GRACE consists of
question-answer pairs, in which each question contains a series of clues that
gradually become easier, all leading to the same answer; models must answer
correctly as early as possible as the clues are revealed. This setting permits
granular measurement of model calibration based on how early, accurately, and
confidently a model answers. After collecting these questions, we host live
human vs. model competitions to gather 1,749 data points on human and model
teams' timing, accuracy, and confidence. We propose a metric, CalScore, that
uses GRACE to analyze model calibration errors and identify types of model
miscalibration that differ from human behavior. We find that although humans
are less accurate than models, humans are generally better calibrated. Since
state-of-the-art models struggle on GRACE, it effectively evaluates progress on
improving model calibration.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Future Outcome Reasoning and Confidence Assessment Benchmark 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.19676v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.19676v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhangdie Yuan, Zifeng Ding, Andreas Vlachos
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Forecasting is an important task in many domains, such as technology and
economics. However existing forecasting benchmarks largely lack comprehensive
confidence assessment, focus on limited question types, and often consist of
artificial questions that do not align with real-world human forecasting needs.
To address these gaps, we introduce FOReCAst (Future Outcome Reasoning and
Confidence Assessment), a benchmark that evaluates models' ability to make
predictions and their confidence in them. FOReCAst spans diverse forecasting
scenarios involving Boolean questions, timeframe prediction, and quantity
estimation, enabling a comprehensive evaluation of both prediction accuracy and
confidence calibration for real-world applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Investigating Neurons and Heads in <span class="highlight-title">Transformer</span>-based LLMs for
  Typographical Errors 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.19669v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.19669v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kohei Tsuji, Tatsuya Hiraoka, Yuchang Cheng, Eiji Aramaki, Tomoya Iwakura
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper investigates how LLMs encode inputs with typos. We hypothesize
that specific neurons and attention heads recognize typos and fix them
internally using local and global contexts. We introduce a method to identify
typo neurons and typo heads that work actively when inputs contain typos. Our
experimental results suggest the following: 1) LLMs can fix typos with local
contexts when the typo neurons in either the early or late layers are
activated, even if those in the other are not. 2) Typo neurons in the middle
layers are responsible for the core of typo-fixing with global contexts. 3)
Typo heads fix typos by widely considering the context not focusing on specific
tokens. 4) Typo neurons and typo heads work not only for typo-fixing but also
for understanding general contexts.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 10 figures, 6 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SuPreME: A Supervised <span class="highlight-title">Pre-train</span>ing Framework for Multimodal ECG
  Representation Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.19668v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.19668v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mingsheng Cai, Jiuming Jiang, Wenhao Huang, Che Liu, Rossella Arcucci
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Cardiovascular diseases are a leading cause of death and disability
worldwide. Electrocardiogram (ECG) recordings are critical for diagnosing and
monitoring cardiac health, but obtaining large-scale annotated ECG datasets is
labor-intensive and time-consuming. Recent ECG Self-Supervised Learning (eSSL)
methods mitigate this by learning features without extensive labels but fail to
capture fine-grained clinical semantics and require extensive task-specific
fine-tuning. To address these challenges, we propose $\textbf{SuPreME}$, a
$\textbf{Su}$pervised $\textbf{Pre}$-training framework for
$\textbf{M}$ultimodal $\textbf{E}$CG representation learning. SuPreME applies
Large Language Models (LLMs) to extract structured clinical entities from
free-text ECG reports, filter out noise and irrelevant content, enhance
clinical representation learning, and build a high-quality, fine-grained
labeled dataset. By using text-based cardiac queries instead of traditional
categorical labels, SuPreME enables zero-shot classification of unseen diseases
without additional fine-tuning. We evaluate SuPreME on six downstream datasets
covering 127 cardiac conditions, achieving superior zero-shot AUC performance
over state-of-the-art eSSL and multimodal methods by over 1.96\%. Results
demonstrate the effectiveness of SuPreME in leveraging structured, clinically
relevant knowledge for high-quality ECG representations. All code and data will
be released upon acceptance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Med-RLVR: Emerging Medical Reasoning from a 3B base model via
  reinforcement Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.19655v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.19655v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sheng Zhang, Qianchu Liu, Guanghui Qin, Tristan Naumann, Hoifung Poon
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reinforcement learning from verifiable rewards (RLVR) has recently gained
attention for its ability to elicit self-evolved reasoning capabilitie from
base language models without explicit reasoning supervisions, as demonstrated
by DeepSeek-R1. While prior work on RLVR has primarily focused on mathematical
and coding domains, its applicability to other tasks and domains remains
unexplored. In this work, we investigate whether medical reasoning can emerge
from RLVR. We introduce Med-RLVR as an initial study of RLVR in the medical
domain leveraging medical multiple-choice question answering (MCQA) data as
verifiable labels. Our results demonstrate that RLVR is not only effective for
math and coding but also extends successfully to medical question answering.
Notably, Med-RLVR achieves performance comparable to traditional supervised
fine-tuning (SFT) on in-distribution tasks while significantly improving
out-of-distribution generalization, with an 8-point accuracy gain. Further
analysis of training dynamics reveals that, with no explicit reasoning
supervision, reasoning emerges from the 3B-parameter base model. These findings
underscore the potential of RLVR in domains beyond math and coding, opening new
avenues for its application in knowledge-intensive fields such as medicine.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Unlocking Multi-Modal Potentials for Dynamic Text-Attributed Graph
  Representation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.19651v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.19651v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuanyuan Xu, Wenjie Zhang, Ying Zhang, Xuemin Lin, Xiwei Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Dynamic Text-Attributed Graphs (DyTAGs) are a novel graph paradigm that
captures evolving temporal edges alongside rich textual attributes. A prior
approach to representing DyTAGs leverages pre-trained language models to encode
text attributes and subsequently integrates them into dynamic graph models.
However, it follows edge-centric modeling, as in dynamic graph learning, which
is limited in local structures and fails to exploit the unique characteristics
of DyTAGs, leading to suboptimal performance. We observe that DyTAGs inherently
comprise three distinct modalities-temporal, textual, and structural-often
exhibiting dispersed or even orthogonal distributions, with the first two
largely overlooked in existing research. Building on this insight, we propose
MoMent, a model-agnostic multi-modal framework that can seamlessly integrate
with dynamic graph models for structural modality learning. The core idea is to
shift from edge-centric to node-centric modeling, fully leveraging three
modalities for node representation. Specifically, MoMent presents non-shared
node-centric encoders based on the attention mechanism to capture global
temporal and semantic contexts from temporal and textual modalities, together
with local structure learning, thus generating modality-specific tokens. To
prevent disjoint latent space, we propose a symmetric alignment loss, an
auxiliary objective that aligns temporal and textual tokens, ensuring global
temporal-semantic consistency with a theoretical guarantee. Last, we design a
lightweight adaptor to fuse these tokens, generating comprehensive and cohesive
node representations. We theoretically demonstrate that MoMent enhances
discriminative power over exclusive edge-centric modeling. Extensive
experiments across seven datasets and two downstream tasks show that MoMent
achieves up to 33.62% improvement against the baseline using four dynamic graph
models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Taxonomy, Opportunities, and Challenges of Representation Engineering
  for Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.19649v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.19649v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jan Wehner, Sahar Abdelnabi, Daniel Tan, David Krueger, Mario Fritz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Representation Engineering (RepE) is a novel paradigm for controlling the
behavior of LLMs. Unlike traditional approaches that modify inputs or fine-tune
the model, RepE directly manipulates the model's internal representations. As a
result, it may offer more effective, interpretable, data-efficient, and
flexible control over models' behavior. We present the first comprehensive
survey of RepE for LLMs, reviewing the rapidly growing literature to address
key questions: What RepE methods exist and how do they differ? For what
concepts and problems has RepE been applied? What are the strengths and
weaknesses of RepE compared to other methods? To answer these, we propose a
unified framework describing RepE as a pipeline comprising representation
identification, operationalization, and control. We posit that while RepE
methods offer significant potential, challenges remain, including managing
multiple concepts, ensuring reliability, and preserving models' performance.
Towards improving RepE, we identify opportunities for experimental and
methodological improvements and construct a guide for best practices.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Few-Shot, No Problem: Descriptive Continual Relation Extraction <span class="chip">AAAI 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.20596v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.20596v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nguyen Xuan Thanh, Anh Duc Le, Quyen Tran, Thanh-Thien Le, Linh Ngo Van, Thien Huu Nguyen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Few-shot Continual Relation Extraction is a crucial challenge for enabling AI
systems to identify and adapt to evolving relationships in dynamic real-world
domains. Traditional memory-based approaches often overfit to limited samples,
failing to reinforce old knowledge, with the scarcity of data in few-shot
scenarios further exacerbating these issues by hindering effective data
augmentation in the latent space. In this paper, we propose a novel
retrieval-based solution, starting with a large language model to generate
descriptions for each relation. From these descriptions, we introduce a
bi-encoder retrieval training paradigm to enrich both sample and class
representation learning. Leveraging these enhanced representations, we design a
retrieval-based prediction method where each sample "retrieves" the best
fitting relation via a reciprocal rank fusion score that integrates both
relation description vectors and class prototypes. Extensive experiments on
multiple datasets demonstrate that our method significantly advances the
state-of-the-art by maintaining robust performance across sequential tasks,
effectively addressing catastrophic forgetting.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to AAAI 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multi$^2$: Multi-Agent Test-Time Scalable Framework for Multi-Document
  Processing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.20592v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.20592v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Juntai Cao, Xiang Zhang, Raymond Li, Chuyuan Li, Shafiq Joty, Giuseppe Carenini
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in test-time scaling have shown promising results in
improving Large Language Models (LLMs) performance through strategic
computation allocation during inference. While this approach has demonstrated
strong performance improvements in logical and mathematical reasoning tasks,
its application to natural language generation (NLG), especially summarization,
has yet to be explored. Multi-Document Summarization (MDS) is a challenging
task that focuses on extracting and synthesizing useful information from
multiple lengthy documents. Unlike reasoning tasks, MDS requires a more nuanced
approach to prompt design and ensemble, as there is no "best" prompt to satisfy
diverse summarization requirements. To address this, we propose a novel
framework that leverages inference-time scaling for this task. Precisely, we
take prompt ensemble approach by leveraging various prompt to first generate
candidate summaries and then ensemble them with an aggregator to produce a
refined summary. We also introduce two new evaluation metrics:
Consistency-Aware Preference (CAP) score and LLM Atom-Content-Unit (ACU) score,
to enhance LLM's contextual understanding while mitigating its positional bias.
Extensive experiments demonstrate the effectiveness of our approach in
improving summary quality while identifying and analyzing the scaling
boundaries in summarization tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LLMs Have Rhythm: Fingerprinting Large Language Models Using Inter-Token
  Times and Network Traffic Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.20589v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.20589v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Saeif Alhazbi, Ahmed Mohamed Hussain, Gabriele Oligeri, Panos Papadimitratos
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As Large Language Models (LLMs) become increasingly integrated into many
technological ecosystems across various domains and industries, identifying
which model is deployed or being interacted with is critical for the security
and trustworthiness of the systems. Current verification methods typically rely
on analyzing the generated output to determine the source model. However, these
techniques are susceptible to adversarial attacks, operate in a post-hoc
manner, and may require access to the model weights to inject a verifiable
fingerprint. In this paper, we propose a novel passive and non-invasive
fingerprinting technique that operates in real-time and remains effective even
under encrypted network traffic conditions. Our method leverages the intrinsic
autoregressive generation nature of language models, which generate text one
token at a time based on all previously generated tokens, creating a unique
temporal pattern like a rhythm or heartbeat that persists even when the output
is streamed over a network. We find that measuring the Inter-Token Times
(ITTs)-time intervals between consecutive tokens-can identify different
language models with high accuracy. We develop a Deep Learning (DL) pipeline to
capture these timing patterns using network traffic analysis and evaluate it on
16 Small Language Models (SLMs) and 10 proprietary LLMs across different
deployment scenarios, including local host machine (GPU/CPU), Local Area
Network (LAN), Remote Network, and Virtual Private Network (VPN). The
experimental results confirm that our proposed technique is effective and
maintains high accuracy even when tested in different network conditions. This
work opens a new avenue for model identification in real-world scenarios and
contributes to more secure and trustworthy language model deployment.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Noisy Path from Source to Citation: Measuring How Scholars Engage
  with Past Research 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.20581v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.20581v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hong Chen, Misha Teplitskiy, David Jurgens
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Academic citations are widely used for evaluating research and tracing
knowledge flows. Such uses typically rely on raw citation counts and neglect
variability in citation types. In particular, citations can vary in their
fidelity as original knowledge from cited studies may be paraphrased,
summarized, or reinterpreted, possibly wrongly, leading to variation in how
much information changes from cited to citing paper. In this study, we
introduce a computational pipeline to quantify citation fidelity at scale.
Using full texts of papers, the pipeline identifies citations in citing papers
and the corresponding claims in cited papers, and applies supervised models to
measure fidelity at the sentence level. Analyzing a large-scale
multi-disciplinary dataset of approximately 13 million citation sentence pairs,
we find that citation fidelity is higher when authors cite papers that are 1)
more recent and intellectually close, 2) more accessible, and 3) the first
author has a lower H-index and the author team is medium-sized. Using a
quasi-experiment, we establish the "telephone effect" - when citing papers have
low fidelity to the original claim, future papers that cite the citing paper
and the original have lower fidelity to the original. Our work reveals
systematic differences in citation fidelity, underscoring the limitations of
analyses that rely on citation quantity alone and the potential for distortion
of evidence.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ECCOS: Efficient Capability and Cost Coordinated Scheduling for
  Multi-LLM Serving 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.20576v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.20576v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kai Mei, Wujiang Xu, Shuhang Lin, Yongfeng Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As large language models (LLMs) are increasingly deployed as service
endpoints in systems, the surge in query volume creates significant scheduling
challenges. Existing scheduling frameworks mainly target at latency
optimization while neglecting the capability of LLMs to serve different level
of queries, which could lead to computational resource waste. This paper
addresses this challenge by proposing a capability-cost coordinated scheduling
framework, ECCOS, for multi-LLM serving, which explicitly constrains response
quality and workload to optimize LLM inference cost. Specifically, it
introduces the two-stage scheduling by designing a multi-objective predictor
and a constrained optimizer. The predictor estimates both model capabilities
and computational costs through training-based and retrieval-based approaches,
while the optimizer determines cost-optimal assignments under quality and
workload constraints. It also introduces QAServe, a dataset collected for
sample-wise response quality and costs by zero-shot prompting different LLMs on
knowledge QA and mathematical reasoning. Extensive experiments demonstrate that
ECCOS improves success rates by 6.30% while reducing costs by 10.15% compared
to existing methods, consuming less than 0.5% of LLM response time. The code is
available at: https://github.com/agiresearch/ECCOS.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Visual Reasoning at Urban Intersections: FineTuning <span class="highlight-title">GPT</span>-4o for Traffic
  Conflict Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.20573v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.20573v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sari Masri, Huthaifa I. Ashqar, Mohammed Elhenawy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Traffic control in unsignalized urban intersections presents significant
challenges due to the complexity, frequent conflicts, and blind spots. This
study explores the capability of leveraging Multimodal Large Language Models
(MLLMs), such as GPT-4o, to provide logical and visual reasoning by directly
using birds-eye-view videos of four-legged intersections. In this proposed
method, GPT-4o acts as intelligent system to detect conflicts and provide
explanations and recommendations for the drivers. The fine-tuned model achieved
an accuracy of 77.14%, while the manual evaluation of the true predicted values
of the fine-tuned GPT-4o showed significant achievements of 89.9% accuracy for
model-generated explanations and 92.3% for the recommended next actions. These
results highlight the feasibility of using MLLMs for real-time traffic
management using videos as inputs, offering scalable and actionable insights
into intersections traffic management and operation. Code used in this study is
available at
https://github.com/sarimasri3/Traffic-Intersection-Conflict-Detection-using-images.git.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ HazardNet: A Small-Scale Vision Language Model for Real-Time Traffic
  Safety Detection at Edge Devices 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.20572v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.20572v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohammad Abu Tami, Mohammed Elhenawy, Huthaifa I. Ashqar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Traffic safety remains a vital concern in contemporary urban settings,
intensified by the increase of vehicles and the complicated nature of road
networks. Traditional safety-critical event detection systems predominantly
rely on sensor-based approaches and conventional machine learning algorithms,
necessitating extensive data collection and complex training processes to
adhere to traffic safety regulations. This paper introduces HazardNet, a
small-scale Vision Language Model designed to enhance traffic safety by
leveraging the reasoning capabilities of advanced language and vision models.
We built HazardNet by fine-tuning the pre-trained Qwen2-VL-2B model, chosen for
its superior performance among open-source alternatives and its compact size of
two billion parameters. This helps to facilitate deployment on edge devices
with efficient inference throughput. In addition, we present HazardQA, a novel
Vision Question Answering (VQA) dataset constructed specifically for training
HazardNet on real-world scenarios involving safety-critical events. Our
experimental results show that the fine-tuned HazardNet outperformed the base
model up to an 89% improvement in F1-Score and has comparable results with
improvement in some cases reach up to 6% when compared to larger models, such
as GPT-4o. These advancements underscore the potential of HazardNet in
providing real-time, reliable traffic safety event detection, thereby
contributing to reduced accidents and improved traffic management in urban
environments. Both HazardNet model and the HazardQA dataset are available at
https://huggingface.co/Tami3/HazardNet and
https://huggingface.co/datasets/Tami3/HazardQA, respectively.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Statistical Factuality Guarantee for Large Vision-Language
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.20560v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.20560v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhuohang Li, Chao Yan, Nicholas J. Jackson, Wendi Cui, Bo Li, Jiaxin Zhang, Bradley A. Malin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Advancements in Large Vision-Language Models (LVLMs) have demonstrated
promising performance in a variety of vision-language tasks involving
image-conditioned free-form text generation. However, growing concerns about
hallucinations in LVLMs, where the generated text is inconsistent with the
visual context, are becoming a major impediment to deploying these models in
applications that demand guaranteed reliability. In this paper, we introduce a
framework to address this challenge, ConfLVLM, which is grounded on conformal
prediction to achieve finite-sample distribution-free statistical guarantees on
the factuality of LVLM output. This framework treats an LVLM as a hypothesis
generator, where each generated text detail (or claim) is considered an
individual hypothesis. It then applies a statistical hypothesis testing
procedure to verify each claim using efficient heuristic uncertainty measures
to filter out unreliable claims before returning any responses to users. We
conduct extensive experiments covering three representative application
domains, including general scene understanding, medical radiology report
generation, and document understanding. Remarkably, ConfLVLM reduces the error
rate of claims generated by LLaVa-1.5 for scene descriptions from 87.8\% to
10.0\% by filtering out erroneous claims with a 95.3\% true positive rate. Our
results further demonstrate that ConfLVLM is highly flexible, and can be
applied to any black-box LVLMs paired with any uncertainty measure for any
image-conditioned free-form text generation task while providing a rigorous
guarantee on controlling the risk of hallucination.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ HuAMR: A Hungarian AMR Parser and <span class="highlight-title">Dataset</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.20552v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.20552v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Botond Barta, Endre Hamerlik, Milán Konor Nyist, Judit Ács
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present HuAMR, the first Abstract Meaning Representation (AMR) dataset and
a suite of large language model-based AMR parsers for Hungarian, targeting the
scarcity of semantic resources for non-English languages. To create HuAMR, we
employed Llama-3.1-70B to automatically generate silver-standard AMR
annotations, which we then refined manually to ensure quality. Building on this
dataset, we investigate how different model architectures - mT5 Large and
Llama-3.2-1B - and fine-tuning strategies affect AMR parsing performance.
  While incorporating silver-standard AMRs from Llama-3.1-70B into the training
data of smaller models does not consistently boost overall scores, our results
show that these techniques effectively enhance parsing accuracy on Hungarian
news data (the domain of HuAMR). We evaluate our parsers using Smatch scores
and confirm the potential of HuAMR and our parsers for advancing semantic
parsing research.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ $Q\sharp$: Provably Optimal Distributional RL for LLM Post-Training 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.20548v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.20548v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jin Peng Zhou, Kaiwen Wang, Jonathan Chang, Zhaolin Gao, Nathan Kallus, Kilian Q. Weinberger, Kianté Brantley, Wen Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reinforcement learning (RL) post-training is crucial for LLM alignment and
reasoning, but existing policy-based methods, such as PPO and DPO, can fall
short of fixing shortcuts inherited from pre-training. In this work, we
introduce $Q\sharp$, a value-based algorithm for KL-regularized RL that guides
the reference policy using the optimal regularized $Q$ function. We propose to
learn the optimal $Q$ function using distributional RL on an aggregated online
dataset. Unlike prior value-based baselines that guide the model using
unregularized $Q$-values, our method is theoretically principled and provably
learns the optimal policy for the KL-regularized RL problem. Empirically,
$Q\sharp$ outperforms prior baselines in math reasoning benchmarks while
maintaining a smaller KL divergence to the reference policy. Theoretically, we
establish a reduction from KL-regularized RL to no-regret online learning,
providing the first bounds for deterministic MDPs under only realizability.
Thanks to distributional RL, our bounds are also variance-dependent and
converge faster when the reference policy has small variance. In sum, our
results highlight $Q\sharp$ as an effective approach for post-training LLMs,
offering both improved performance and theoretical guarantees. The code can be
found at https://github.com/jinpz/q_sharp.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ NANO<span class="highlight-title">GPT</span>: A Query-Driven Large Language Model Retrieval-Augmented
  Generation System for Nanotechnology Research 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.20541v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.20541v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Achuth Chandrasekhar, Omid Barati Farimani, Olabode T. Ajenifujah, Janghoon Ock, Amir Barati Farimani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents the development and application of a Large Language Model
Retrieval-Augmented Generation (LLM-RAG) system tailored for nanotechnology
research. The system leverages the capabilities of a sophisticated language
model to serve as an intelligent research assistant, enhancing the efficiency
and comprehensiveness of literature reviews in the nanotechnology domain.
Central to this LLM-RAG system is its advanced query backend retrieval
mechanism, which integrates data from multiple reputable sources. The system
retrieves relevant literature by utilizing Google Scholar's advanced search,
and scraping open-access papers from Elsevier, Springer Nature, and ACS
Publications. This multifaceted approach ensures a broad and diverse collection
of up-to-date scholarly articles and papers. The proposed system demonstrates
significant potential in aiding researchers by providing a streamlined,
accurate, and exhaustive literature retrieval process, thereby accelerating
research advancements in nanotechnology. The effectiveness of the LLM-RAG
system is validated through rigorous testing, illustrating its capability to
significantly reduce the time and effort required for comprehensive literature
reviews, while maintaining high accuracy, query relevance and outperforming
standard, publicly available LLMS.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>61 pages, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Supervised Fine-Tuning LLMs to Behave as Pedagogical Agents in
  Programming Education 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.20527v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.20527v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Emily Ross, Yuval Kansal, Jake Renzella, Alexandra Vassar, Andrew Taylor
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) are increasingly being explored in higher
education, yet their effectiveness as teaching agents remains underexamined. In
this paper, we present the development of GuideLM, a fine-tuned LLM designed
for programming education. GuideLM has been integrated into the Debugging C
Compiler (DCC), an educational C compiler that leverages LLMs to generate
pedagogically sound error explanations. Previously, DCC relied on off-the-shelf
OpenAI models, which, while accurate, often over-assisted students by directly
providing solutions despite contrary prompting.
  To address this, we employed supervised fine-tuning (SFT) on a dataset of 528
student-question/teacher-answer pairs, creating two models: GuideLM and
GuideLM-mini, fine-tuned on ChatGPT-4o and 4o-mini, respectively. We conducted
an expert analysis of 400 responses per model, comparing their pedagogical
effectiveness against base OpenAI models. Our evaluation, grounded in
constructivism and cognitive load theory, assessed factors such as conceptual
scaffolding, clarity, and Socratic guidance.
  Results indicate that GuideLM and GuideLM-mini improve pedagogical
performance, with an 8% increase in Socratic guidance and a 58% improvement in
economy of words compared to GPT-4o. However, this refinement comes at the cost
of a slight reduction in general accuracy. While further work is needed, our
findings suggest that fine-tuning LLMs with targeted datasets is a promising
approach for developing models better suited to educational contexts.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TripCraft: A Benchmark for Spatio-Temporally Fine Grained Travel
  Planning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.20508v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.20508v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Soumyabrata Chaudhuri, Pranav Purkar, Ritwik Raghav, Shubhojit Mallick, Manish Gupta, Abhik Jana, Shreya Ghosh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in probing Large Language Models (LLMs) have explored
their latent potential as personalized travel planning agents, yet existing
benchmarks remain limited in real world applicability. Existing datasets, such
as TravelPlanner and TravelPlanner+, suffer from semi synthetic data reliance,
spatial inconsistencies, and a lack of key travel constraints, making them
inadequate for practical itinerary generation. To address these gaps, we
introduce TripCraft, a spatiotemporally coherent travel planning dataset that
integrates real world constraints, including public transit schedules, event
availability, diverse attraction categories, and user personas for enhanced
personalization. To evaluate LLM generated plans beyond existing binary
validation methods, we propose five continuous evaluation metrics, namely
Temporal Meal Score, Temporal Attraction Score, Spatial Score, Ordering Score,
and Persona Score which assess itinerary quality across multiple dimensions.
Our parameter informed setting significantly enhances meal scheduling,
improving the Temporal Meal Score from 61% to 80% in a 7 day scenario.
TripCraft establishes a new benchmark for LLM driven personalized travel
planning, offering a more realistic, constraint aware framework for itinerary
generation. Dataset and Codebase will be made publicly available upon
acceptance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>27 pages, 18 Tables and 6 Figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Thousand Words or An Image: Studying the Influence of Persona Modality
  in Multimodal LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.20504v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.20504v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Julius Broomfield, Kartik Sharma, Srijan Kumar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have recently demonstrated remarkable
advancements in embodying diverse personas, enhancing their effectiveness as
conversational agents and virtual assistants. Consequently, LLMs have made
significant strides in processing and integrating multimodal information.
However, even though human personas can be expressed in both text and image,
the extent to which the modality of a persona impacts the embodiment by the LLM
remains largely unexplored. In this paper, we investigate how do different
modalities influence the expressiveness of personas in multimodal LLMs. To this
end, we create a novel modality-parallel dataset of 40 diverse personas varying
in age, gender, occupation, and location. This consists of four modalities to
equivalently represent a persona: image-only, text-only, a combination of image
and small text, and typographical images, where text is visually stylized to
convey persona-related attributes. We then create a systematic evaluation
framework with 60 questions and corresponding metrics to assess how well LLMs
embody each persona across its attributes and scenarios. Comprehensive
experiments on $5$ multimodal LLMs show that personas represented by detailed
text show more linguistic habits, while typographical images often show more
consistency with the persona. Our results reveal that LLMs often overlook
persona-specific details conveyed through images, highlighting underlying
limitations and paving the way for future research to bridge this gap. We
release the data and code at https://github.com/claws-lab/persona-modality .
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Protecting multimodal large language models against misleading
  visualizations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.20503v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.20503v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jonathan Tonglet, Tinne Tuytelaars, Marie-Francine Moens, Iryna Gurevych
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We assess the vulnerability of multimodal large language models to misleading
visualizations - charts that distort the underlying data using techniques such
as truncated or inverted axes, leading readers to draw inaccurate conclusions
that may support misinformation or conspiracy theories. Our analysis shows that
these distortions severely harm multimodal large language models, reducing
their question-answering accuracy to the level of the random baseline. To
mitigate this vulnerability, we introduce six inference-time methods to improve
performance of MLLMs on misleading visualizations while preserving their
accuracy on non-misleading ones. The most effective approach involves (1)
extracting the underlying data table and (2) using a text-only large language
model to answer questions based on the table. This method improves performance
on misleading visualizations by 15.4 to 19.6 percentage points.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint. Code and data available at
  https://github.com/UKPLab/arxiv2025-misleading-visualizations</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ EgoNormia: Benchmarking Physical Social Norm Understanding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.20490v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.20490v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        MohammadHossein Rezaei, Yicheng Fu, Phil Cuvin, Caleb Ziems, Yanzhe Zhang, Hao Zhu, Diyi Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Human activity is moderated by norms. When performing actions in the real
world, humans not only follow norms, but also consider the trade-off between
different norms However, machines are often trained without explicit
supervision on norm understanding and reasoning, especially when the norms are
grounded in a physical and social context. To improve and evaluate the
normative reasoning capability of vision-language models (VLMs), we present
EgoNormia $\|\epsilon\|$, consisting of 1,853 ego-centric videos of human
interactions, each of which has two related questions evaluating both the
prediction and justification of normative actions. The normative actions
encompass seven categories: safety, privacy, proxemics, politeness,
cooperation, coordination/proactivity, and communication/legibility. To compile
this dataset at scale, we propose a novel pipeline leveraging video sampling,
automatic answer generation, filtering, and human validation. Our work
demonstrates that current state-of-the-art vision-language models lack robust
norm understanding, scoring a maximum of 45% on EgoNormia (versus a human bench
of 92%). Our analysis of performance in each dimension highlights the
significant risks of safety, privacy, and the lack of collaboration and
communication capability when applied to real-world agents. We additionally
show that through a retrieval-based generation method, it is possible to use
EgoNomia to enhance normative reasoning in VLMs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Logicbreaks: A Framework for Understanding Subversion of Rule-based
  Inference 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.00075v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.00075v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anton Xue, Avishree Khare, Rajeev Alur, Surbhi Goel, Eric Wong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study how to subvert large language models (LLMs) from following
prompt-specified rules. We first formalize rule-following as inference in
propositional Horn logic, a mathematical system in which rules have the form
"if $P$ and $Q$, then $R$" for some propositions $P$, $Q$, and $R$. Next, we
prove that although small transformers can faithfully follow such rules,
maliciously crafted prompts can still mislead both theoretical constructions
and models learned from data. Furthermore, we demonstrate that popular attack
algorithms on LLMs find adversarial prompts and induce attention patterns that
align with our theory. Our novel logic-based framework provides a foundation
for studying LLMs in rule-based settings, enabling a formal analysis of tasks
like logical reasoning and jailbreak attacks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SuperGPQA: Scaling LLM Evaluation across 285 Graduate Disciplines 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.14739v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.14739v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        M-A-P Team, Xinrun Du, Yifan Yao, Kaijing Ma, Bingli Wang, Tianyu Zheng, Kang Zhu, Minghao Liu, Yiming Liang, Xiaolong Jin, Zhenlin Wei, Chujie Zheng, Kaixin Deng, Shian Jia, Sichao Jiang, Yiyan Liao, Rui Li, Qinrui Li, Sirun Li, Yizhi Li, Yunwen Li, Dehua Ma, Yuansheng Ni, Haoran Que, Qiyao Wang, Zhoufutu Wen, Siwei Wu, Tianshun Xing, Ming Xu, Zhenzhu Yang, Zekun Moore Wang, Junting Zhou, Yuelin Bai, Xingyuan Bu, Chenglin Cai, Liang Chen, Yifan Chen, Chengtuo Cheng, Tianhao Cheng, Keyi Ding, Siming Huang, Yun Huang, Yaoru Li, Yizhe Li, Zhaoqun Li, Tianhao Liang, Chengdong Lin, Hongquan Lin, Yinghao Ma, Tianyang Pang, Zhongyuan Peng, Zifan Peng, Qige Qi, Shi Qiu, Xingwei Qu, Shanghaoran Quan, Yizhou Tan, Zili Wang, Chenqing Wang, Hao Wang, Yiya Wang, Yubo Wang, Jiajun Xu, Kexin Yang, Ruibin Yuan, Yuanhao Yue, Tianyang Zhan, Chun Zhang, Jinyang Zhang, Xiyue Zhang, Xingjian Zhang, Yue Zhang, Yongchi Zhao, Xiangyu Zheng, Chenghua Zhong, Yang Gao, Zhoujun Li, Dayiheng Liu, Qian Liu, Tianyu Liu, Shiwen Ni, Junran Peng, Yujia Qin, Wenbo Su, Guoyin Wang, Shi Wang, Jian Yang, Min Yang, Meng Cao, Xiang Yue, Zhaoxiang Zhang, Wangchunshu Zhou, Jiaheng Liu, Qunshu Lin, Wenhao Huang, Ge Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have demonstrated remarkable proficiency in
mainstream academic disciplines such as mathematics, physics, and computer
science. However, human knowledge encompasses over 200 specialized disciplines,
far exceeding the scope of existing benchmarks. The capabilities of LLMs in
many of these specialized fields-particularly in light industry, agriculture,
and service-oriented disciplines-remain inadequately evaluated. To address this
gap, we present SuperGPQA, a comprehensive benchmark that evaluates
graduate-level knowledge and reasoning capabilities across 285 disciplines. Our
benchmark employs a novel Human-LLM collaborative filtering mechanism to
eliminate trivial or ambiguous questions through iterative refinement based on
both LLM responses and expert feedback. Our experimental results reveal
significant room for improvement in the performance of current state-of-the-art
LLMs across diverse knowledge domains (e.g., the reasoning-focused model
DeepSeek-R1 achieved the highest accuracy of 61.82% on SuperGPQA), highlighting
the considerable gap between current model capabilities and artificial general
intelligence. Additionally, we present comprehensive insights from our
management of a large-scale annotation process, involving over 80 expert
annotators and an interactive Human-LLM collaborative system, offering valuable
methodological guidance for future research initiatives of comparable scope.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AI on My Shoulder: Supporting Emotional Labor in Front-Office Roles with
  an LLM-based Empathetic Coworker 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02408v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02408v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vedant Das Swain, Qiuyue "Joy" Zhong, Jash Rajesh Parekh, Yechan Jeon, Roy Zimmermann, Mary Czerwinski, Jina Suh, Varun Mishra, Koustuv Saha, Javier Hernandez
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Client-Service Representatives (CSRs) are vital to organizations. Frequent
interactions with disgruntled clients, however, disrupt their mental
well-being. To help CSRs regulate their emotions while interacting with uncivil
clients, we designed Care-Pilot, an LLM-powered assistant, and evaluated its
efficacy, perception, and use. Our comparative analyses between 665 human and
Care-Pilot-generated support messages highlight Care-Pilot's ability to adapt
to and demonstrate empathy in various incivility incidents. Additionally, 143
CSRs assessed Care-Pilot's empathy as more sincere and actionable than human
messages. Finally, we interviewed 20 CSRs who interacted with Care-Pilot in a
simulation exercise. They reported that Care-Pilot helped them avoid negative
thinking, recenter thoughts, and humanize clients; showing potential for
bridging gaps in coworker support. Yet, they also noted deployment challenges
and emphasized the indispensability of shared experiences. We discuss future
designs and societal implications of AI-mediated emotional labor, underscoring
empathy as a critical function for AI assistants for worker mental health.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Large-Scale Simulation on Large Language Models for Decision-Making in
  Political Science 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.15291v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.15291v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chenxiao Yu, Jinyi Ye, Yuangang Li, Zhaotian Weng, Zheng Li, Emilio Ferrara, Xiyang Hu, Yue Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While LLMs have demonstrated remarkable capabilities in text generation and
reasoning, their ability to simulate human decision-making -- particularly in
political contexts -- remains an open question. However, modeling voter
behavior presents unique challenges due to limited voter-level data, evolving
political landscapes, and the complexity of human reasoning. In this study, we
develop a theory-driven, multi-step reasoning framework that integrates
demographic, temporal and ideological factors to simulate voter decision-making
at scale. Using synthetic personas calibrated to real-world voter data, we
conduct large-scale simulations of recent U.S. presidential elections. Our
method significantly improves simulation accuracy while mitigating model
biases. We examine its robustness by comparing performance across different
LLMs. We further investigate the challenges and constraints that arise from
LLM-based political simulations. Our work provides both a scalable framework
for modeling political decision-making behavior and insights into the promise
and limitations of using LLMs in political science research.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: substantial text overlap with arXiv:2411.03321 This
  version adds a new model to our experimental setup, modifies the paper's main
  discussion, and updates the authorship list</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AIR: Complex Instruction Generation via Automatic Iterative Refinement 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.17787v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.17787v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wei Liu, Yancheng He, Hui Huang, Chengwei Hu, Jiaheng Liu, Shilong Li, Wenbo Su, Bo Zheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the development of large language models, their ability to follow simple
instructions has significantly improved. However, adhering to complex
instructions remains a major challenge. Current approaches to generating
complex instructions are often irrelevant to the current instruction
requirements or suffer from limited scalability and diversity. Moreover,
methods such as back-translation, while effective for simple instruction
generation, fail to leverage the rich contents and structures in large web
corpora. In this paper, we propose a novel automatic iterative refinement
framework to generate complex instructions with constraints, which not only
better reflects the requirements of real scenarios but also significantly
enhances LLMs' ability to follow complex instructions. The AIR framework
consists of two stages: (1)Generate an initial instruction from a document;
(2)Iteratively refine instructions with LLM-as-judge guidance by comparing the
model's output with the document to incorporate valuable constraints. Finally,
we construct the AIR-10K dataset with 10K complex instructions and demonstrate
that instructions generated with our approach significantly improve the model's
ability to follow complex instructions, outperforming existing methods for
instruction generation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The first three authors contributed equally, 20 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Can Large Language Models Detect Errors in Long Chain-of-Thought
  Reasoning? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.19361v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.19361v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yancheng He, Shilong Li, Jiaheng Liu, Weixun Wang, Xingyuan Bu, Ge Zhang, Zhongyuan Peng, Zhaoxiang Zhang, Zhicheng Zheng, Wenbo Su, Bo Zheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, o1-like models have drawn significant attention, where these models
produce the long Chain-of-Thought (CoT) reasoning steps to improve the
reasoning abilities of existing Large Language Models (LLMs). In this paper, to
understand the qualities of these long CoTs and measure the critique abilities
of existing LLMs on these long CoTs, we introduce the DeltaBench, including the
generated long CoTs from different o1-like models (e.g., QwQ, DeepSeek-R1) for
different reasoning tasks (e.g., Math, Code, General Reasoning), to measure the
ability to detect errors in long CoT reasoning. Based on DeltaBench, we first
perform fine-grained analysis of the generated long CoTs to discover the
effectiveness and efficiency of different o1-like models. Then, we conduct
extensive evaluations of existing process reward models (PRMs) and critic
models to detect the errors of each annotated process, which aims to
investigate the boundaries and limitations of existing PRMs and critic models.
Finally, we hope that DeltaBench could guide developers to better understand
the long CoT reasoning abilities of their models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The first four authors contributed equally, 27 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Sequence Graph Network for Online Debate Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.18696v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.18696v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Quan Mai, Susan Gauch, Douglas Adams, Miaoqing Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Online debates involve a dynamic exchange of ideas over time, where
participants need to actively consider their opponents' arguments, respond with
counterarguments, reinforce their own points, and introduce more compelling
arguments as the discussion unfolds. Modeling such a complex process is not a
simple task, as it necessitates the incorporation of both sequential
characteristics and the capability to capture interactions effectively. To
address this challenge, we employ a sequence-graph approach. Building the
conversation as a graph allows us to effectively model interactions between
participants through directed edges. Simultaneously, the propagation of
information along these edges in a sequential manner enables us to capture a
more comprehensive representation of context. We also introduce a Sequence
Graph Attention layer to illustrate the proposed information update scheme. The
experimental results show that sequence graph networks achieve superior results
to existing methods in online debates.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ RRM: Robust Reward Model Training Mitigates Reward Hacking <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.13156v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.13156v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tianqi Liu, Wei Xiong, Jie Ren, Lichang Chen, Junru Wu, Rishabh Joshi, Yang Gao, Jiaming Shen, Zhen Qin, Tianhe Yu, Daniel Sohn, Anastasiia Makarova, Jeremiah Liu, Yuan Liu, Bilal Piot, Abe Ittycheriah, Aviral Kumar, Mohammad Saleh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reward models (RMs) play a pivotal role in aligning large language models
(LLMs) with human preferences. However, traditional RM training, which relies
on response pairs tied to specific prompts, struggles to disentangle
prompt-driven preferences from prompt-independent artifacts, such as response
length and format. In this work, we expose a fundamental limitation of current
RM training methods, where RMs fail to effectively distinguish between
contextual signals and irrelevant artifacts when determining preferences. To
address this, we introduce a causal framework that learns preferences
independent of these artifacts and propose a novel data augmentation technique
designed to eliminate them. Extensive experiments show that our approach
successfully filters out undesirable artifacts, yielding a more robust reward
model (RRM). Our RRM improves the performance of a pairwise reward model
trained on Gemma-2-9b-it, on RewardBench, increasing accuracy from 80.61% to
84.15%. Additionally, we train two DPO policies using both the RM and RRM,
demonstrating that the RRM significantly enhances DPO-aligned policies,
improving MT-Bench scores from 7.27 to 8.31 and length-controlled win-rates in
AlpacaEval-2 from 33.46% to 52.49%.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in ICLR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Probing-RAG: Self-Probing to Guide Language Models in Selective Document
  Retrieval <span class="chip">NAACL 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13339v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13339v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ingeol Baek, Hwan Chang, Byeongjeong Kim, Jimin Lee, Hwanhee Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Retrieval-Augmented Generation (RAG) enhances language models by retrieving
and incorporating relevant external knowledge. However, traditional
retrieve-and-generate processes may not be optimized for real-world scenarios,
where queries might require multiple retrieval steps or none at all. In this
paper, we propose a Probing-RAG, which utilizes the hidden state
representations from the intermediate layers of language models to adaptively
determine the necessity of additional retrievals for a given query. By
employing a pre-trained prober, Probing-RAG effectively captures the model's
internal cognition, enabling reliable decision-making about retrieving external
documents. Experimental results across five open-domain QA datasets demonstrate
that Probing-RAG outperforms previous methods while reducing the number of
redundant retrieval steps.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NAACL 2025 Findings</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Sheffield's Submission to the AmericasNLP Shared Task on Machine
  Translation into Indigenous Languages 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.09830v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.09830v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Edward Gow-Smith, Danae Sánchez Villegas
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper we describe the University of Sheffield's submission to the
AmericasNLP 2023 Shared Task on Machine Translation into Indigenous Languages
which comprises the translation from Spanish to eleven indigenous languages.
Our approach consists of extending, training, and ensembling different
variations of NLLB-200. We use data provided by the organizers and data from
various other sources such as constitutions, handbooks, news articles, and
backtranslations generated from monolingual data. On the dev set, our best
submission outperforms the baseline by 11% average chrF across all languages,
with substantial improvements particularly for Aymara, Guarani and Quechua. On
the test set, we achieve the highest average chrF of all the submissions, we
rank first in four of the eleven languages, and at least one of our submissions
ranks in the top 3 for all languages.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Best-performing submission overall to the AmericasNLP 2023 Shared
  Task. Code and models available here:
  https://github.com/edwardgowsmith/americasnlp-2023-sheffield</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ JSONSchemaBench: A Rigorous Benchmark of Structured Outputs for Language
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.10868v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.10868v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Saibo Geng, Hudson Cooper, Michał Moskal, Samuel Jenkins, Julian Berman, Nathan Ranchin, Robert West, Eric Horvitz, Harsha Nori
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reliably generating structured outputs has become a critical capability for
modern language model (LM) applications. Constrained decoding has emerged as
the dominant technology across sectors for enforcing structured outputs during
generation. Despite its growing adoption, little has been done with the
systematic evaluation of the behaviors and performance of constrained decoding.
Constrained decoding frameworks have standardized around JSON Schema as a
structured data format, with most uses guaranteeing constraint compliance given
a schema. However, there is poor understanding of the effectiveness of the
methods in practice. We present an evaluation framework to assess constrained
decoding approaches across three critical dimensions: efficiency in generating
constraint-compliant outputs, coverage of diverse constraint types, and quality
of the generated outputs. To facilitate this evaluation, we introduce
JSONSchemaBench, a benchmark for constrained decoding comprising 10K real-world
JSON schemas that encompass a wide range of constraints with varying
complexity. We pair the benchmark with the existing official JSON Schema Test
Suite and evaluate six state-of-the-art constrained decoding frameworks,
including Guidance, Outlines, Llamacpp, XGrammar, OpenAI, and Gemini. Through
extensive experiments, we gain insights into the capabilities and limitations
of constrained decoding on structured generation with real-world JSON schemas.
Our work provides actionable insights for improving constrained decoding
frameworks and structured generation tasks, setting a new standard for
evaluating constrained decoding and structured generation. We release
JSONSchemaBench at https://github.com/guidance-ai/jsonschemabench
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ EMS: Adaptive Evict-then-Merge Strategy for Head-wise KV Cache
  Compression Based on Global-Local Importance 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.08521v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.08521v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yingxin Li, Ye Li, Yuan Meng, Xinzhu Ma, Zihan Geng, Shutao Xia, Zhi Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As large language models (LLMs) continue to advance, the demand for higher
quality and faster processing of long contexts across various applications is
growing. KV cache is widely adopted as it stores previously generated key and
value tokens, effectively reducing redundant computations during inference.
However, as memory overhead becomes a significant concern, efficient
compression of KV cache has gained increasing attention. Most existing methods
perform compression from two perspectives: identifying important tokens and
designing compression strategies. However, these approaches often produce
biased distributions of important tokens due to the influence of accumulated
attention scores or positional encoding. Furthermore, they overlook the
sparsity and redundancy across different heads, which leads to difficulties in
preserving the most effective information at the head level. To this end, we
propose EMS to overcome these limitations, while achieving better KV cache
compression under extreme compression ratios. Specifically, we introduce a
Global-Local score that combines accumulated attention scores from both global
and local KV tokens to better identify the token importance. For the
compression strategy, we design an adaptive and unified Evict-then-Merge
framework that accounts for the sparsity and redundancy of KV tokens across
different heads. Additionally, we implement the head-wise parallel compression
through a zero-class mechanism to enhance efficiency. Extensive experiments
demonstrate our SOTA performance even under extreme compression ratios. EMS
consistently achieves the lowest perplexity, improves scores by over 1.28
points across four LLMs on LongBench under a 256 cache budget, and preserves
95% retrieval accuracy with a cache budget less than 2% of the context length
in the Needle-in-a-Haystack task.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Improving Neuron-level Interpretability with White-box Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.16443v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.16443v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hao Bai, Yi Ma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neurons in auto-regressive language models like GPT-2 can be interpreted by
analyzing their activation patterns. Recent studies have shown that techniques
such as dictionary learning, a form of post-hoc sparse coding, enhance this
neuron-level interpretability. In our research, we are driven by the goal to
fundamentally improve neural network interpretability by embedding sparse
coding directly within the model architecture, rather than applying it as an
afterthought. In our study, we introduce a white-box transformer-like
architecture named Coding RAte TransformEr (CRATE), explicitly engineered to
capture sparse, low-dimensional structures within data distributions. Our
comprehensive experiments showcase significant improvements (up to 103%
relative improvement) in neuron-level interpretability across a variety of
evaluation metrics. Detailed investigations confirm that this enhanced
interpretability is steady across different layers irrespective of the model
size, underlining CRATE's robust performance in enhancing neural network
interpretability. Further analysis shows that CRATE's increased
interpretability comes from its enhanced ability to consistently and
distinctively activate on relevant tokens. These findings point towards a
promising direction for creating white-box foundation models that excel in
neuron-level interpretation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CPAL 2025 camera-ready version. Selected as Oral</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The Impact of Unstated Norms in Bias Analysis of Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.03471v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.03471v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Farnaz Kohankhaki, D. B. Emerson, Jacob-Junqi Tian, Laleh Seyyed-Kalantari, Faiza Khan Khattak
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Bias in large language models (LLMs) has many forms, from overt
discrimination to implicit stereotypes. Counterfactual bias evaluation is a
widely used approach to quantifying bias and often relies on template-based
probes that explicitly state group membership. It measures whether the outcome
of a task performed by an LLM is invariant to a change in group membership. In
this work, we find that template-based probes can lead to unrealistic bias
measurements. For example, LLMs appear to mistakenly cast text associated with
White race as negative at higher rates than other groups. We hypothesize that
this arises artificially via a mismatch between commonly unstated norms, in the
form of markedness, in the pretraining text of LLMs (e.g., Black president vs.
president) and templates used for bias measurement (e.g., Black president vs.
White president). The findings highlight the potential misleading impact of
varying group membership through explicit mention in counterfactual bias
quantification.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 Pages, 4 Figures, 4 Tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MMEvalPro: Calibrating Multimodal Benchmarks Towards Trustworthy and
  Efficient Evaluation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.00468v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.00468v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jinsheng Huang, Liang Chen, Taian Guo, Fu Zeng, Yusheng Zhao, Bohan Wu, Ye Yuan, Haozhe Zhao, Zhihui Guo, Yichi Zhang, Jingyang Yuan, Wei Ju, Luchen Liu, Tianyu Liu, Baobao Chang, Ming Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Multimodal Models (LMMs) exhibit impressive cross-modal understanding
and reasoning abilities, often assessed through multiple-choice questions
(MCQs) that include an image, a question, and several options. However, many
benchmarks used for such evaluations suffer from systematic biases. Remarkably,
Large Language Models (LLMs) without any visual perception capabilities achieve
non-trivial performance, undermining the credibility of these evaluations. To
address this issue while maintaining the efficiency of MCQ evaluations, we
propose MMEvalPro, a benchmark designed to avoid Type-I errors through a
trilogy evaluation pipeline and more rigorous metrics. For each original
question from existing benchmarks, human annotators augment it by creating one
perception question and one knowledge anchor question through a meticulous
annotation process. MMEvalPro comprises $2,138$ question triplets, totaling
$6,414$ distinct questions. Two-thirds of these questions are manually labeled
by human experts, while the rest are sourced from existing benchmarks (MMMU,
ScienceQA, and MathVista). Compared with the existing benchmarks, our
experiments with the latest LLMs and LMMs demonstrate that MMEvalPro is more
challenging (the best LMM lags behind human performance by $31.73\%$, compared
to an average gap of $8.03\%$ in previous benchmarks) and more trustworthy (the
best LLM trails the best LMM by $23.09\%$, whereas the gap for previous
benchmarks is just $14.64\%$). Our in-depth analysis explains the reason for
the large performance gap and justifies the trustworthiness of evaluation,
underscoring its significant potential for advancing future research.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages, code released at https://github.com/chenllliang/MMEvalPro,
  Homepage at https://mmevalpro.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Continual Memorization of Factoids in Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07175v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07175v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Howard Chen, Jiayi Geng, Adithya Bhaskar, Dan Friedman, Danqi Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As new knowledge rapidly accumulates, language models (LMs) with pretrained
knowledge quickly become obsolete. A common approach to updating LMs is
fine-tuning them directly on new knowledge. However, recent studies have shown
that fine-tuning for memorization may be ineffective in storing knowledge or
may exacerbate hallucinations. In this work, we introduce a setting we call
continual memorization, where a model must memorize and retain a set of
factoids through multiple stages of fine-tuning on subsequent datasets. We
characterized the forgetting patterns through extensive experiments and show
that LMs widely suffer from forgetting, especially when needing to memorize
factoids in the second stage. We posit that forgetting can be alleviated by
modifying training dynamics: (1) protecting the memorization process when
learning factoids or (2) reducing interference from subsequent training stages.
Intriguingly, we find that mixing randomly generated word sequences or generic
data sampled from pretraining corpora at different training stages effectively
mitigates forgetting REMIX: Random and Generic Data Mixing). REMIX can recover
performance from severe forgetting, outperforming replay methods and other
continual learning baselines. We analyze how REMIX influences the learning
process and find that robust memorization follows a distinct pattern: the model
stores factoids in earlier layers than usual and diversifies the layers that
retain them, which results in easier recall and manipulate of the learned
factoids.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LongAttn: Selecting Long-context Training Data via Token-level Attention 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.16860v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.16860v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Longyun Wu, Dawei Zhu, Guangxiang Zhao, Zhuocheng Yu, Junfeng Ran, Xiangyu Wong, Lin Sun, Sujian Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the development of large language models (LLMs), there has been an
increasing need for significant advancements in handling long contexts. To
enhance long-context capabilities, constructing high-quality training data with
long-range dependencies is crucial. Existing methods to select long-context
data often rely on sentence-level analysis, which can be greatly optimized in
both performance and efficiency. In this paper, we propose a novel token-level
framework, LongAttn, which leverages the self-attention mechanism of LLMs to
measure the long-range dependencies for the data. By calculating token-level
dependency strength and distribution uniformity of token scores, LongAttn
effectively quantifies long-range dependencies, enabling more accurate and
efficient data selection. We filter LongABC-32K from open-source long-context
datasets (ArXiv, Book, and Code). Through our comprehensive experiments,
LongAttn has demonstrated its excellent effectiveness, scalability, and
efficiency. To facilitate future research in long-context data, we released our
code and the high-quality long-context training data LongABC-32K.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Relation Also Knows: Rethinking the Recall and Editing of Factual
  Associations in Auto-Regressive <span class="highlight-title">Transformer</span> Language Models <span class="chip">AAAI25</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.15091v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.15091v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiyu Liu, Zhengxiao Liu, Naibin Gu, Zheng Lin, Wanli Ma, Ji Xiang, Weiping Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The storage and recall of factual associations in auto-regressive transformer
language models (LMs) have drawn a great deal of attention, inspiring knowledge
editing by directly modifying the located model weights. Most editing works
achieve knowledge editing under the guidance of existing interpretations of
knowledge recall that mainly focus on subject knowledge. However, these
interpretations are seriously flawed, neglecting relation information and
leading to the over-generalizing problem for editing. In this work, we discover
a novel relation-focused perspective to interpret the knowledge recall of
transformer LMs during inference and apply it on single knowledge editing to
avoid over-generalizing. Experimental results on the dataset supplemented with
a new R-Specificity criterion demonstrate that our editing approach
significantly alleviates over-generalizing while remaining competitive on other
criteria, breaking the domination of subject-focused editing for future
research.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by AAAI25</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Word Boundary Information Isn't Useful for Encoder Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.07923v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.07923v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Edward Gow-Smith, Dylan Phelps, Harish Tayyar Madabushi, Carolina Scarton, Aline Villavicencio
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  All existing transformer-based approaches to NLP using subword tokenisation
algorithms encode whitespace (word boundary information) through the use of
special space symbols (such as \#\# or \_) forming part of tokens. These
symbols have been shown to a) lead to reduced morphological validity of
tokenisations, and b) give substantial vocabulary redundancy. As such, removing
these symbols has been shown to have a beneficial effect on the processing of
morphologically complex words for transformer encoders in the pretrain-finetune
paradigm. In this work, we explore whether word boundary information is at all
useful to such models. In particular, we train transformer encoders across four
different training scales, and investigate several alternative approaches to
including word boundary information, evaluating on a range of tasks across
different domains and problem set-ups: GLUE (for sentence-level
classification), NER (for token-level classification), and two classification
datasets involving complex words (Superbizarre and FLOTA). Overall, through an
extensive experimental setup that includes the pre-training of 29 models, we
find no substantial improvements from our alternative approaches, suggesting
that modifying tokenisers to remove word boundary information isn't leading to
a loss of useful information.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9th Workshop on Representation Learning for NLP</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ How Far Are We on the Decision-Making of LLMs? Evaluating LLMs' Gaming
  Ability in Multi-Agent Environments <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.11807v6">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.11807v6.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jen-tse Huang, Eric John Li, Man Ho Lam, Tian Liang, Wenxuan Wang, Youliang Yuan, Wenxiang Jiao, Xing Wang, Zhaopeng Tu, Michael R. Lyu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Decision-making is a complex process requiring diverse abilities, making it
an excellent framework for evaluating Large Language Models (LLMs). Researchers
have examined LLMs' decision-making through the lens of Game Theory. However,
existing evaluation mainly focus on two-player scenarios where an LLM competes
against another. Additionally, previous benchmarks suffer from test set leakage
due to their static design. We introduce GAMA($\gamma$)-Bench, a new framework
for evaluating LLMs' Gaming Ability in Multi-Agent environments. It includes
eight classical game theory scenarios and a dynamic scoring scheme specially
designed to quantitatively assess LLMs' performance. $\gamma$-Bench allows
flexible game settings and adapts the scoring system to different game
parameters, enabling comprehensive evaluation of robustness, generalizability,
and strategies for improvement. Our results indicate that GPT-3.5 demonstrates
strong robustness but limited generalizability, which can be enhanced using
methods like Chain-of-Thought. We also evaluate 13 LLMs from 6 model families,
including GPT-3.5, GPT-4, Gemini, LLaMA-3.1, Mixtral, and Qwen-2.
Gemini-1.5-Pro outperforms others, scoring of $69.8$ out of $100$, followed by
LLaMA-3.1-70B ($65.9$) and Mixtral-8x22B ($62.4$). Our code and experimental
results are publicly available at https://github.com/CUHK-ARISE/GAMABench.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ICLR 2025; 11 pages of main text; 26 pages of appendices;
  Included models: GPT-3.5-{0613, 1106, 0125}, GPT-4-0125, GPT-4o-0806,
  Gemini-{1.0, 1.5)-Pro, LLaMA-3.1-{7, 70, 405}B, Mixtral-8x{7, 22}B,
  Qwen-2-72B</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AgentSquare: Automatic LLM Agent Search in Modular Design Space 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.06153v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.06153v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yu Shang, Yu Li, Keyu Zhao, Likai Ma, Jiahe Liu, Fengli Xu, Yong Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in Large Language Models (LLMs) have led to a rapid
growth of agentic systems capable of handling a wide range of complex tasks.
However, current research largely relies on manual, task-specific design,
limiting their adaptability to novel tasks. In this paper, we introduce a new
research problem: Modularized LLM Agent Search (MoLAS). We propose a modular
design space that abstracts existing LLM agent designs into four fundamental
modules with uniform IO interface: Planning, Reasoning, Tool Use, and Memory.
Building on this design space, we present a novel LLM agent search framework
called AgentSquare, which introduces two core mechanisms, i.e., module
evolution and recombination, to efficiently search for optimized LLM agents. To
further accelerate the process, we design a performance predictor that uses
in-context surrogate models to skip unpromising agent designs. Extensive
experiments across six benchmarks, covering the diverse scenarios of web,
embodied, tool use and game applications, show that AgentSquare substantially
outperforms hand-crafted agents, achieving an average performance gain of 17.2%
against best-known human designs. Moreover, AgentSquare can generate
interpretable design insights, enabling a deeper understanding of agentic
architecture and its impact on task performance. We believe that the modular
design space and AgentSquare search framework offer a platform for fully
exploiting the potential of prior successful designs and consolidating the
collective efforts of research community. Code repo is available at
https://github.com/tsinghua-fib-lab/AgentSquare.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>25 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Kanana: Compute-efficient Bilingual Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.18934v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.18934v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                         Kanana LLM Team, Yunju Bak, Hojin Lee, Minho Ryu, Jiyeon Ham, Seungjae Jung, Daniel Wontae Nam, Taegyeong Eo, Donghun Lee, Doohae Jung, Boseop Kim, Nayeon Kim, Jaesun Park, Hyunho Kim, Hyunwoong Ko, Changmin Lee, Kyoung-Woon On, Seulye Baeg, Junrae Cho, Sunghee Jung, Jieun Kang, EungGyun Kim, Eunhwa Kim, Byeongil Ko, Daniel Lee, Minchul Lee, Miok Lee, Shinbok Lee, Gaeun Seo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce Kanana, a series of bilingual language models that demonstrate
exceeding performance in Korean and competitive performance in English. The
computational cost of Kanana is significantly lower than that of
state-of-the-art models of similar size. The report details the techniques
employed during pre-training to achieve compute-efficient yet competitive
models, including high quality data filtering, staged pre-training, depth
up-scaling, and pruning and distillation. Furthermore, the report outlines the
methodologies utilized during the post-training of the Kanana models,
encompassing supervised fine-tuning and preference optimization, aimed at
enhancing their capability for seamless interaction with users. Lastly, the
report elaborates on plausible approaches used for language model adaptation to
specific scenarios, such as embedding, retrieval augmented generation, and
function calling. The Kanana model series spans from 2.1B to 32.5B parameters
with 2.1B models (base, instruct, embedding) publicly released to promote
research on Korean language models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>40 pages, 15 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LongSafety: Enhance Safety for Long-Context LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.06899v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.06899v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mianqiu Huang, Xiaoran Liu, Shaojun Zhou, Mozhi Zhang, Qipeng Guo, Linyang Li, Chenkun Tan, Yang Gao, Pengyu Wang, Linlin Li, Qun Liu, Yaqian Zhou, Xipeng Qiu, Xuanjing Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in model architectures and length extrapolation
techniques have significantly extended the context length of large language
models (LLMs), paving the way for their application in increasingly complex
tasks. However, despite the growing capabilities of long-context LLMs, the
safety issues in long-context scenarios remain underexplored. While safety
alignment in short context has been widely studied, the safety concerns of
long-context LLMs have not been adequately addressed. In this work, we
introduce \textbf{LongSafety}, a comprehensive safety alignment dataset for
long-context LLMs, containing 10 tasks and 17k samples, with an average length
of 40.9k tokens. Our experiments demonstrate that training with LongSafety can
enhance long-context safety performance while enhancing short-context safety
and preserving general capabilities. Furthermore, we demonstrate that
long-context safety does not equal long-context alignment with short-context
safety data and LongSafety has generalizing capabilities in context length and
long-context safety scenarios.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Measuring Data Diversity for Instruction Tuning: A Systematic Analysis
  and A Reliable Metric 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.17184v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.17184v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuming Yang, Yang Nan, Junjie Ye, Shihan Dou, Xiao Wang, Shuo Li, Huijie Lv, Mingqi Wu, Tao Gui, Qi Zhang, Xuanjing Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Data diversity is crucial for the instruction tuning of large language
models. Existing studies have explored various diversity-aware data selection
methods to construct high-quality datasets and enhance model performance.
However, the fundamental problem of precisely defining and measuring data
diversity remains underexplored, limiting clear guidance for data engineering.
To address this, we systematically analyze 11 existing diversity measurement
methods by evaluating their correlation with model performance through
extensive fine-tuning experiments. Our results indicate that a reliable
diversity measure should properly account for both inter-sample differences and
the information distribution in the sample space. Building on this, we propose
NovelSum, a new diversity metric based on sample-level "novelty." Experiments
on both simulated and real-world data show that NovelSum accurately captures
diversity variations and achieves a 0.97 correlation with instruction-tuned
model performance, highlighting its value in guiding data engineering
practices. With NovelSum as an optimization objective, we further develop a
greedy, diversity-oriented data selection strategy that outperforms existing
approaches, validating both the effectiveness and practical significance of our
metric.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages. The related codes and resources will be released later.
  Project page: https://github.com/UmeanNever/NovelSum</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Say Less, Mean More: Leveraging Pragmatics in Retrieval-Augmented
  Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.17839v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.17839v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haris Riaz, Ellen Riloff, Mihai Surdeanu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a simple, unsupervised method that injects pragmatic principles in
retrieval-augmented generation (RAG) frameworks such as Dense Passage Retrieval
to enhance the utility of retrieved contexts. Our approach first identifies
which sentences in a pool of documents retrieved by RAG are most relevant to
the question at hand, cover all the topics addressed in the input question and
no more, and then highlights these sentences within their context, before they
are provided to the LLM, without truncating or altering the context in any
other way. We show that this simple idea brings consistent improvements in
experiments on three question answering tasks (ARC-Challenge, PubHealth and
PopQA) using five different LLMs. It notably enhances relative accuracy by up
to 19.7% on PubHealth and 10% on ARC-Challenge compared to a conventional RAG
system.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages, 2 figures, 8 tables. Preprint</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ThinK: Thinner Key Cache by Query-Driven Pruning <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.21018v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.21018v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuhui Xu, Zhanming Jie, Hanze Dong, Lei Wang, Xudong Lu, Aojun Zhou, Amrita Saha, Caiming Xiong, Doyen Sahoo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have revolutionized the field of natural
language processing, achieving unprecedented performance across a variety of
applications. However, their increased computational and memory demands present
significant challenges, especially when handling long sequences. This paper
focuses on the long-context scenario, addressing the inefficiencies in KV cache
memory consumption during inference. Unlike existing approaches that optimize
the memory based on the sequence length, we identify substantial redundancy in
the channel dimension of the KV cache, as indicated by an uneven magnitude
distribution and a low-rank structure in the attention weights. In response, we
propose ThinK, a novel query-dependent KV cache pruning method designed to
minimize attention weight loss while selectively pruning the least significant
channels. Our approach not only maintains or enhances model accuracy but also
achieves a reduction in KV cache memory costs by over 20% compared with vanilla
KV cache eviction and quantization methods. For instance, ThinK integrated with
KIVI can achieve a 2.8x reduction in peak memory usage while maintaining nearly
the same quality, enabling up to a 5x increase in batch size when using a
single GPU. Extensive evaluations on the LLaMA and Mistral models across
various long-sequence datasets verified the efficiency of ThinK, establishing a
new baseline algorithm for efficient LLM deployment without compromising
performance. Our code has been made available at
https://github.com/SalesforceAIResearch/ThinK.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2025 (Spotlight)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Knowledge Localization: Mission Not Accomplished? Enter Query
  Localization! <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.14117v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.14117v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuheng Chen, Pengfei Cao, Yubo Chen, Kang Liu, Jun Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) store extensive factual knowledge, but the
mechanisms behind how they store and express this knowledge remain unclear. The
Knowledge Neuron (KN) thesis is a prominent theory for explaining these
mechanisms. This theory is based on the Knowledge Localization (KL) assumption,
which suggests that a fact can be localized to a few knowledge storage units,
namely knowledge neurons.
  However, this assumption has two limitations: first, it may be too rigid
regarding knowledge storage, and second, it neglects the role of the attention
module in knowledge expression.
  In this paper, we first re-examine the KL assumption and demonstrate that its
limitations do indeed exist. To address these, we then present two new
findings, each targeting one of the limitations: one focusing on knowledge
storage and the other on knowledge expression. We summarize these findings as
\textbf{Query Localization} (QL) assumption and argue that the KL assumption
can be viewed as a simplification of the QL assumption. Based on QL assumption,
we further propose the Consistency-Aware KN modification method, which improves
the performance of knowledge modification, further validating our new
assumption. We conduct 39 sets of experiments, along with additional
visualization experiments, to rigorously confirm our conclusions. Code is
available at https://github.com/heng840/KnowledgeLocalization.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2025 Spotlight</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The Knowledge Microscope: Features as Better Analytical Lenses than
  Neurons 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.12483v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.12483v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuheng Chen, Pengfei Cao, Kang Liu, Jun Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Previous studies primarily utilize MLP neurons as units of analysis for
understanding the mechanisms of factual knowledge in Language Models (LMs);
however, neurons suffer from polysemanticity, leading to limited knowledge
expression and poor interpretability. In this paper, we first conduct
preliminary experiments to validate that Sparse Autoencoders (SAE) can
effectively decompose neurons into features, which serve as alternative
analytical units. With this established, our core findings reveal three key
advantages of features over neurons: (1) Features exhibit stronger influence on
knowledge expression and superior interpretability. (2) Features demonstrate
enhanced monosemanticity, showing distinct activation patterns between related
and unrelated facts. (3) Features achieve better privacy protection than
neurons, demonstrated through our proposed FeatureEdit method, which
significantly outperforms existing neuron-based approaches in erasing
privacy-sensitive information from LMs.Code and dataset will be available.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ARR February UnderReview</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PyramidDrop: Accelerating Your Large Vision-Language Models via Pyramid
  Visual Redundancy Reduction <span class="chip">CVPR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.17247v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.17247v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Long Xing, Qidong Huang, Xiaoyi Dong, Jiajie Lu, Pan Zhang, Yuhang Zang, Yuhang Cao, Conghui He, Jiaqi Wang, Feng Wu, Dahua Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In large vision-language models (LVLMs), images serve as inputs that carry a
wealth of information. As the idiom "A picture is worth a thousand words"
implies, representing a single image in current LVLMs can require hundreds or
even thousands of tokens. This results in significant computational costs,
which grow quadratically as input image resolution increases, thereby severely
impacting the efficiency of both training and inference. Previous approaches
have attempted to reduce the number of image tokens either before or within the
early layers of LVLMs. However, these strategies inevitably result in the loss
of crucial image information, ultimately diminishing model performance. To
address this challenge, we conduct an empirical study revealing that all visual
tokens are necessary for LVLMs in the shallow layers, and token redundancy
progressively increases in the deeper layers of the model. To this end, we
propose PyramidDrop, a visual redundancy reduction strategy for LVLMs to boost
their efficiency in both training and inference with neglectable performance
loss. Specifically, we partition the LVLM into several stages and drop part of
the image tokens at the end of each stage with a pre-defined ratio, creating
pyramid-like visual tokens across model layers. The dropping is based on a
lightweight similarity calculation with a negligible time overhead. Extensive
experiments demonstrate that PyramidDrop can achieve a 40% training time and
55% inference FLOPs acceleration of LLaVA-NeXT with comparable performance.
Besides, the PyramidDrop could also serve as a plug-and-play strategy for
inference acceleration without training, with better performance and lower
inference cost than counterparts. Code is available at
https://github.com/Cooperx521/PyramidDrop.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR 2025, code is available at
  https://github.com/Cooperx521/PyramidDrop</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Progressive Mixed-Precision Decoding for Efficient LLM Inference 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13461v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13461v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hao Mark Chen, Fuwen Tan, Alexandros Kouris, Royson Lee, Hongxiang Fan, Stylianos I. Venieris
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In spite of the great potential of large language models (LLMs) across
various tasks, their deployment on resource-constrained devices remains
challenging due to their excessive computational and memory demands.
Quantization has emerged as an effective solution by storing weights in reduced
precision. However, utilizing low precisions (i.e.~2/3-bit) to substantially
alleviate the memory-boundedness of LLM decoding, still suffers from
prohibitive performance drop. In this work, we argue that existing approaches
fail to explore the diversity in computational patterns, redundancy, and
sensitivity to approximations of the different phases of LLM inference,
resorting to a uniform quantization policy throughout. Instead, we propose a
novel phase-aware method that selectively allocates precision during different
phases of LLM inference, achieving both strong context extraction during
prefill and efficient memory bandwidth utilization during decoding. To further
address the memory-boundedness of the decoding phase, we introduce Progressive
Mixed-Precision Decoding (PMPD), a technique that enables the gradual lowering
of precision deeper in the generated sequence, together with a spectrum of
precision-switching schedulers that dynamically drive the precision-lowering
decisions in either task-adaptive or prompt-adaptive manner. Extensive
evaluation across diverse language tasks shows that when targeting Nvidia GPUs,
PMPD achieves 1.4$-$12.2$\times$ speedup in matrix-vector multiplications over
fp16 models, while when targeting an LLM-optimized NPU, our approach delivers a
throughput gain of 3.8$-$8.0$\times$ over fp16 models and up to 1.54$\times$
over uniform quantization approaches while preserving the output quality.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Mitigating Unintended Memorization with LoRA in Federated Learning for
  LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.05087v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.05087v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Thierry Bossy, Julien Vignoud, Tahseen Rabbani, Juan R. Troncoso Pastoriza, Martin Jaggi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated learning (FL) is a popular paradigm for collaborative training
which avoids direct data exposure between clients. However, data privacy issues
still remain: FL-trained large language models are capable of memorizing and
completing phrases and sentences contained in training data when given with
their prefixes. Thus, it is possible for adversarial and honest-but-curious
clients to recover training data of other participants simply through targeted
prompting. In this work, we demonstrate that a popular and simple fine-tuning
strategy, low-rank adaptation (LoRA), reduces memorization during FL up to a
factor of 10. We study this effect by performing a medical question-answering
fine-tuning task and injecting multiple replicas of out-of-distribution
sensitive sequences drawn from an external clinical dataset. We observe a
reduction in memorization for a wide variety of Llama 2 and 3 models, and find
that LoRA can reduce memorization in centralized learning as well. Furthermore,
we show that LoRA can be combined with other privacy-preserving techniques such
as gradient clipping and Gaussian noising, secure aggregation, and Goldfish
loss to further improve record-level privacy while maintaining performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Topic Classification of Case Law Using a Large Language Model and a New
  Taxonomy for UK Law: AI Insights into Summary Judgment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.12910v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.12910v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Holli Sargeant, Ahmed Izzidien, Felix Steffek
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper addresses a critical gap in legal analytics by developing and
applying a novel taxonomy for topic classification of summary judgment cases in
the United Kingdom. Using a curated dataset of summary judgment cases, we use
the Large Language Model Claude 3 Opus to explore functional topics and trends.
We find that Claude 3 Opus correctly classified the topic with an accuracy of
87.13% and an F1 score of 0.87. The analysis reveals distinct patterns in the
application of summary judgments across various legal domains. As case law in
the United Kingdom is not originally labelled with keywords or a topic
filtering option, the findings not only refine our understanding of the
thematic underpinnings of summary judgments but also illustrate the potential
of combining traditional and AI-driven approaches in legal classification.
Therefore, this paper provides a new and general taxonomy for UK law. The
implications of this work serve as a foundation for further research and policy
discussions in the field of judicial administration and computational legal
research methodologies.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Reference-Based Post-OCR Processing with LLM for Precise Diacritic Text
  in Historical Document Recognition <span class="chip">AAAI 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13305v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13305v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Thao Do, Dinh Phu Tran, An Vo, Daeyoung Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Extracting fine-grained OCR text from aged documents in diacritic languages
remains challenging due to unexpected artifacts, time-induced degradation, and
lack of datasets. While standalone spell correction approaches have been
proposed, they show limited performance for historical documents due to
numerous possible OCR error combinations and differences between modern and
classical corpus distributions. We propose a method utilizing available
content-focused ebooks as a reference base to correct imperfect OCR-generated
text, supported by large language models. This technique generates
high-precision pseudo-page-to-page labels for diacritic languages, where small
strokes pose significant challenges in historical conditions. The pipeline
eliminates various types of noise from aged documents and addresses issues such
as missing characters, words, and disordered sequences. Our post-processing
method, which generated a large OCR dataset of classical Vietnamese books,
achieved a mean grading score of 8.72 on a 10-point scale. This outperformed
the state-of-the-art transformer-based Vietnamese spell correction model, which
scored 7.03 when evaluated on a sampled subset of the dataset. We also trained
a baseline OCR model to assess and compare it with well-known engines.
Experimental results demonstrate the strength of our baseline model compared to
widely used open-source solutions. The resulting dataset will be released
publicly to support future studies.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in the AAAI 2025 (39th) AISI track. Dataset and repo are in
  the paper</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Following the Autoregressive Nature of LLM Embeddings via Compression
  and Alignment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11401v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11401v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jingcheng Deng, Zhongtao Jiang, Liang Pang, Liwei Chen, Kun Xu, Zihao Wei, Huawei Shen, Xueqi Cheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A new trend uses LLMs as dense text encoders via contrastive learning.
However, since LLM embeddings predict the probability distribution of the next
token, they are inherently generative and distributive, conflicting with
contrastive learning, which requires embeddings to capture full-text semantics
and align via cosine similarity. This discrepancy hinders the full utilization
of LLMs' pre-training capabilities, resulting in inefficient learning. In
response to this issue, we propose AutoRegEmbed, a new contrastive learning
method built on embedding conditional probability distributions, which
integrates two core tasks: information compression and conditional distribution
alignment. The information compression task encodes text into the embedding
space, ensuring that the embedding vectors capture global semantics. The
conditional distribution alignment task focuses on aligning text embeddings
with positive samples embeddings by leveraging the conditional distribution of
embeddings while simultaneously reducing the likelihood of generating negative
samples from text embeddings, thereby achieving embedding alignment and
uniformity. Experimental results demonstrate that our method significantly
outperforms traditional contrastive learning approaches and achieves
performance comparable to state-of-the-art models when using the same amount of
data.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Identifying and Mitigating Social Bias Knowledge in Language Models <span class="chip">NAACL 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.11843v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.11843v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruizhe Chen, Yichen Li, Jianfei Yang, Joey Tianyi Zhou, Jian Wu, Zuozhu Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generating fair and accurate predictions plays a pivotal role in deploying
large language models (LLMs) in the real world. However, existing debiasing
methods inevitably generate unfair or incorrect predictions as they are
designed and evaluated to achieve parity across different social groups but
leave aside individual commonsense facts, resulting in modified knowledge that
elicits unreasonable or undesired predictions. In this paper, we first
establish a new bias mitigation benchmark, BiaScope, which systematically
assesses performance by leveraging newly constructed datasets and metrics on
knowledge retention and generalization. Then, we propose a novel debiasing
approach, Fairness Stamp (FAST), which enables fine-grained calibration of
individual social biases. FAST identifies the decisive layer responsible for
storing social biases and then calibrates its outputs by integrating a small
modular network, considering both bias mitigation and knowledge-preserving
demands. Comprehensive experiments demonstrate that FAST surpasses
state-of-the-art baselines with superior debiasing performance while not
compromising the overall model capability for knowledge retention and
downstream predictions. This highlights the potential of fine-grained debiasing
strategies to achieve fairness in LLMs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NAACL 2025 Findings. arXiv admin note: substantial text overlap with
  arXiv:2405.09341</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Factual consistency evaluation of summarization in the Era of large
  language models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.13758v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.13758v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zheheng Luo, Qianqian Xie, Sophia Ananiadou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Factual inconsistency with source documents in automatically generated
summaries can lead to misinformation or pose risks. Existing factual
consistency (FC) metrics are constrained by their performance, efficiency, and
explainability. Recent advances in Large language models (LLMs) have
demonstrated remarkable potential in text evaluation but their effectiveness in
assessing FC in summarization remains underexplored. Prior research has mostly
focused on proprietary LLMs, leaving essential factors that affect their
assessment capabilities unexplored. Additionally, current FC evaluation
benchmarks are restricted to news articles, casting doubt on the generality of
the FC methods tested on them. In this paper, we first address the gap by
introducing TreatFact a dataset of LLM-generated summaries of clinical texts,
annotated for FC by domain experts. Moreover, we benchmark 11 LLMs for FC
evaluation across news and clinical domains and analyse the impact of model
size, prompts, pre-training and fine-tuning data. Our findings reveal that
despite proprietary models prevailing on the task, open-source LLMs lag behind.
Nevertheless, there is potential for enhancing the performance of open-source
LLMs through increasing model size, expanding pre-training data, and developing
well-curated fine-tuning data. Experiments on TreatFact suggest that both
previous methods and LLM-based evaluators are unable to capture factual
inconsistencies in clinical summaries, posing a new challenge for FC
evaluation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>published on ESWA</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Language Independent Stance Detection: Social Interaction-based
  Embeddings and Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.05715v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.05715v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Joseba Fernandez de Landa, Rodrigo Agerri
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The large majority of the research performed on stance detection has been
focused on developing more or less sophisticated text classification systems,
even when many benchmarks are based on social network data such as Twitter.
This paper aims to take on the stance detection task by placing the emphasis
not so much on the text itself but on the interaction data available on social
networks. More specifically, we propose a new method to leverage social
information such as friends and retweets by generating Relational Embeddings,
namely, dense vector representations of interaction pairs. Our experiments on
seven publicly available datasets and four different languages (Basque,
Catalan, Italian, and Spanish) show that combining our relational embeddings
with discriminative textual methods helps to substantially improve performance,
obtaining state-of-the-art results for six out of seven evaluation settings,
outperforming strong baselines based on Large Language Models, or other popular
interaction-based approaches such as DeepWalk or node2vec.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Expansion Quantization Network: An Efficient Micro-emotion Annotation
  and Detection Framework 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.06160v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.06160v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jingyi Zhou, Senlin Luo, Haofan Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Text emotion detection constitutes a crucial foundation for advancing
artificial intelligence from basic comprehension to the exploration of
emotional reasoning. Most existing emotion detection datasets rely on manual
annotations, which are associated with high costs, substantial subjectivity,
and severe label imbalances. This is particularly evident in the inadequate
annotation of micro-emotions and the absence of emotional intensity
representation, which fail to capture the rich emotions embedded in sentences
and adversely affect the quality of downstream task completion. By proposing an
all-labels and training-set label regression method, we map label values to
energy intensity levels, thereby fully leveraging the learning capabilities of
machine models and the interdependencies among labels to uncover multiple
emotions within samples. This led to the establishment of the Emotion
Quantization Network (EQN) framework for micro-emotion detection and
annotation. Using five commonly employed sentiment datasets, we conducted
comparative experiments with various models, validating the broad applicability
of our framework within NLP machine learning models. Based on the EQN
framework, emotion detection and annotation are conducted on the GoEmotions
dataset. A comprehensive comparison with the results from Google literature
demonstrates that the EQN framework possesses a high capability for automatic
detection and annotation of micro-emotions. The EQN framework is the first to
achieve automatic micro-emotion annotation with energy-level scores, providing
strong support for further emotion detection analysis and the quantitative
research of emotion computing.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>3.1 There is a misstatement in the EQN Framework section</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Native Sparse Attention: Hardware-Aligned and Natively Trainable Sparse
  Attention 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11089v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11089v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jingyang Yuan, Huazuo Gao, Damai Dai, Junyu Luo, Liang Zhao, Zhengyan Zhang, Zhenda Xie, Y. X. Wei, Lean Wang, Zhiping Xiao, Yuqing Wang, Chong Ruan, Ming Zhang, Wenfeng Liang, Wangding Zeng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Long-context modeling is crucial for next-generation language models, yet the
high computational cost of standard attention mechanisms poses significant
computational challenges. Sparse attention offers a promising direction for
improving efficiency while maintaining model capabilities. We present NSA, a
Natively trainable Sparse Attention mechanism that integrates algorithmic
innovations with hardware-aligned optimizations to achieve efficient
long-context modeling. NSA employs a dynamic hierarchical sparse strategy,
combining coarse-grained token compression with fine-grained token selection to
preserve both global context awareness and local precision. Our approach
advances sparse attention design with two key innovations: (1) We achieve
substantial speedups through arithmetic intensity-balanced algorithm design,
with implementation optimizations for modern hardware. (2) We enable end-to-end
training, reducing pretraining computation without sacrificing model
performance. As shown in Figure 1, experiments show the model pretrained with
NSA maintains or exceeds Full Attention models across general benchmarks,
long-context tasks, and instruction-based reasoning. Meanwhile, NSA achieves
substantial speedups over Full Attention on 64k-length sequences across
decoding, forward propagation, and backward propagation, validating its
efficiency throughout the model lifecycle.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ETHIC: Evaluating Large Language Models on Long-Context Tasks with High
  Information Coverage <span class="chip">NAACL 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.16848v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.16848v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Taewhoo Lee, Chanwoong Yoon, Kyochul Jang, Donghyeon Lee, Minju Song, Hyunjae Kim, Jaewoo Kang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in large language models (LLM) capable of processing
extremely long texts highlight the need for a dedicated evaluation benchmark to
assess their long-context capabilities. However, existing methods, like the
needle-in-a-haystack test, do not effectively assess whether these models fully
utilize contextual information, raising concerns about the reliability of
current evaluation techniques. To thoroughly examine the effectiveness of
existing benchmarks, we introduce a new metric called information coverage
(IC), which quantifies the proportion of the input context necessary for
answering queries. Our findings indicate that current benchmarks exhibit low
IC; although the input context may be extensive, the actual usable context is
often limited. To address this, we present ETHIC, a novel benchmark designed to
assess LLMs' ability to leverage the entire context. Our benchmark comprises
1,986 test instances spanning four long-context tasks with high IC scores in
the domains of books, debates, medicine, and law. Our evaluations reveal
significant performance drops in contemporary LLMs, highlighting a critical
challenge in managing long contexts. Our benchmark is available at
https://github.com/dmis-lab/ETHIC.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NAACL 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ANPMI: Assessing the True Comprehension Capabilities of LLMs for
  Multiple Choice Questions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.18798v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.18798v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gyeongje Cho, Yeonkyoung So, Jaejin Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multiple-choice benchmarks, consisting of various prompts and choices, are
among the most widely used methods to assess a language model's natural
language understanding capability. Given a specific prompt, we typically
compute $P(Choice|Prompt)$ to evaluate how likely a language model is to
generate the correct choice compared to incorrect ones. However, we observe
that performance measured using this approach reflects not only the model's
comprehension of the prompt but also its inherent biases for certain choices
regardless of the prompt. This issue makes it challenging to accurately measure
a model's natural language understanding, as models may select the answer
without fully understanding the prompt. To address this limitation, we propose
a novel metric called ANPMI, which normalizes Pointwise Mutual Information
(PMI) by $-\log P(Choice)$. ANPMI provides a more accurate assessment of the
model's natural language understanding by ensuring that it is challenging to
answer a question without properly understanding the prompt.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CoT-ICL Lab: A Petri Dish for Studying Chain-of-Thought Learning from
  In-Context Demonstrations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.15132v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.15132v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vignesh Kothapalli, Hamed Firooz, Maziar Sanjabi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce CoT-ICL Lab, a framework and methodology to generate synthetic
tokenized datasets and systematically study chain-of-thought (CoT) in-context
learning (ICL) in language models. CoT-ICL Lab allows fine grained control over
the complexity of in-context examples by decoupling (1) the causal structure
involved in chain token generation from (2) the underlying token processing
functions. We train decoder-only transformers (up to 700M parameters) on these
datasets and show that CoT accelerates the accuracy transition to higher values
across model sizes. In particular, we find that model depth is crucial for
leveraging CoT with limited in-context examples, while more examples help
shallow models match deeper model performance. Additionally, limiting the
diversity of token processing functions throughout training improves causal
structure learning via ICL. We also interpret these transitions by analyzing
transformer embeddings and attention maps. Overall, CoT-ICL Lab serves as a
simple yet powerful testbed for theoretical and empirical insights into ICL and
CoT in language models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>23 pages, 27 figures, 3 tables, code at
  https://github.com/kvignesh1420/cot-icl-lab</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The Dual-use Dilemma in LLMs: Do Empowering Ethical Capacities Make a
  Degraded Utility? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.13952v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.13952v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiyi Zhang, Xingyu Chen, Kexin Chen, Yuyang Du, Xilin Dang, Pheng-Ann Heng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent years have witnessed extensive efforts to enhance Large Language
Models (LLMs) across various domains, alongside growing attention to their
ethical implications. However, a critical challenge remains largely overlooked:
LLMs must balance between rejecting harmful requests for safety and
accommodating legitimate ones for utility. This paper presents a Direct
Preference Optimization (DPO) based alignment framework that achieves better
overall performance by addressing this ethical-utility trade-off, using
chemical domain applications as a proof-of-concept. Our alignment pipeline
starts with a GPT-assisted three-phase data generation scheme, in which we
create LibraChemQA, a chemical question-answering dataset comprising 31.6k
triplet instances. By incorporating an innovative balanced seed in the data
generation process, our framework systematically considers both legitimate and
illegitimate requests. The framework also introduces a rephrasing mechanism for
efficient data augmentation that enhances the model's chemical comprehension.
We further develop a novel hybrid evaluation scheme with LLM judges for precise
assessment of both safety and utility. Experimental results demonstrate our
model's substantial improvements in overall performance where both safety and
utility are considered - the resulting model outperforms leading LLMs including
Claude-3, GPT-4o, and LLaMA-3 by margins of 13.44%, 7.16%, and 7.10%
respectively on our released benchmark. At the end of this paper, we analyze
experimental results obtained from testing DeepSeek-R1 on our benchmark and
reveal the critical ethical concerns raised by this highly acclaimed model. We
highlight that the long Chain-of-Thought (CoT) reasoning process employed by
DeepSeek-R1, as well as other LLMs distilled from it, introduces significant
ethical vulnerabilities when exposed to users.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The Potential of LLMs in Medical Education: Generating Questions and
  Answers for Qualification Exams 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.23769v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.23769v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yunqi Zhu, Wen Tang, Huayu Yang, Jinghao Niu, Liyang Dou, Yifan Gu, Yuanyuan Wu, Wensheng Zhang, Ying Sun, Xuebing Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we leverage LLMs to produce medical qualification exam
questions and the corresponding answers through few-shot prompts, investigating
in-depth how LLMs meet the requirements in terms of coherence, evidence of
statement, factual consistency, and professionalism etc. Utilizing a
multicenter bidirectional anonymized database with respect to comorbid chronic
diseases, named Elderly Comorbidity Medical Database (CECMed), we tasked LLMs
with generating open-ended questions and answers based on a subset of sampled
admission reports. For CECMed, the retrospective cohort includes patients
enrolled from January 2010 to January 2022 while the prospective cohort from
January 2023 to November 2023, with participants sourced from selected tertiary
and community hospitals across the southern, northern, and central regions of
China. A total of 8 widely used LLMs were used, including ERNIE 4, ChatGLM 4,
Doubao, Hunyuan, Spark 4, Qwen,
  Conventional medical education requires sophisticated clinicians to formulate
questions and answers based on prototypes from EHRs, which is heuristic and
time-consuming. We found that mainstream LLMs could generate questions and
answers with real-world EHRs at levels close to clinicians. Although current
LLMs performed dissatisfactory in some aspects, medical students, interns and
residents could reasonably make use of LLMs to facilitate understanding.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AutoPureData: Automated Filtering of Undesirable Web Data to Update LLM
  Knowledge 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19271v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19271v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Praneeth Vadlapati
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Up-to-date and reliable language models are consistently sought after and are
essential in various applications. Typically, models are trained on a fixed
dataset and then deployed globally. However, the knowledge of the models
becomes outdated. Enabling automatic updation of AI knowledge using web data
involves significant concerns regarding the model's safety and quality due to a
threat from unsafe and undesirable text across the web. The purity of new data
was essential for updating knowledge of language models to maintain their
reliability. This paper proposes AutoPureData, a system that automatically
collects and purifies web data. The system loaded a sample of web data.
Utilizing existing trusted AI models, it successfully eliminated unsafe text
with an accuracy of 97% and undesirable text with an accuracy of 86%,
demonstrating the system's effectiveness in purifying the data. The system
ensures that only meaningful and safe text can be used to update LLM knowledge.
The pure text was then optimized and stored in a vector database for future
querying. It was found that LLM can fetch new data from the vector DB. The LLM
writes the RAG query in English, even if the user's query is in another
language, proving that the system can perform cross-lingual retrieval. This
paper proposes a method to maintain the accuracy and relevance of up-to-date
language models by ensuring that only purified data was used to update LLM
knowledge. This work contributes to updating knowledge of chatbots using
meaningful and safe text, enhancing their utility across various industries,
and potentially reducing the risks associated with outputs caused by unsafe or
impure data. Code is available at github.com/Pro-GenAI/AutoPureData.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Final version</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LlamaLens: Specialized Multilingual LLM for Analyzing News and Social
  Media Content 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.15308v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.15308v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohamed Bayan Kmainasi, Ali Ezzat Shahroor, Maram Hasanain, Sahinur Rahman Laskar, Naeemul Hassan, Firoj Alam
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have demonstrated remarkable success as
general-purpose task solvers across various fields. However, their capabilities
remain limited when addressing domain-specific problems, particularly in
downstream NLP tasks. Research has shown that models fine-tuned on
instruction-based downstream NLP datasets outperform those that are not
fine-tuned. While most efforts in this area have primarily focused on
resource-rich languages like English and broad domains, little attention has
been given to multilingual settings and specific domains. To address this gap,
this study focuses on developing a specialized LLM, LlamaLens, for analyzing
news and social media content in a multilingual context. To the best of our
knowledge, this is the first attempt to tackle both domain specificity and
multilinguality, with a particular focus on news and social media. Our
experimental setup includes 18 tasks, represented by 52 datasets covering
Arabic, English, and Hindi. We demonstrate that LlamaLens outperforms the
current state-of-the-art (SOTA) on 23 testing sets, and achieves comparable
performance on 8 sets. We make the models and resources publicly available for
the research community
(https://huggingface.co/collections/QCRI/llamalens-672f7e0604a0498c6a2f0fe9).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>LLMs, Multilingual, Language Diversity, Large Language Models, Social
  Media, News Media, Specialized LLMs, Fact-checking, Media Analysis, Arabic,
  Hindi, English</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Herald: A Natural Language Annotated Lean 4 <span class="highlight-title">Dataset</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.10878v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.10878v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guoxiong Gao, Yutong Wang, Jiedong Jiang, Qi Gao, Zihan Qin, Tianyi Xu, Bin Dong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Verifiable formal languages like Lean have profoundly impacted mathematical
reasoning, particularly through the use of large language models (LLMs) for
automated reasoning. A significant challenge in training LLMs for these formal
languages is the lack of parallel datasets that align natural language with
formal language proofs. To address this challenge, this paper introduces a
novel framework for translating the Mathlib4 corpus (a unified library of
mathematics in formal language Lean 4) into natural language. Building upon
this, we employ a dual augmentation strategy that combines tactic-based and
informal-based approaches, leveraging the Lean-jixia system, a Lean 4 analyzer.
We present the results of this pipeline on Mathlib4 as Herald (Hierarchy and
Retrieval-based Translated Lean Dataset). We also propose the Herald
Translator, which is fine-tuned on Herald. Herald translator achieves a 93.2%
accuracy (Pass@128) on formalizing statements in the miniF2F-test and a 22.5%
accuracy on our internal graduate-level textbook dataset, outperforming
InternLM2-Math-Plus-7B (74.0% and 7.5%) and TheoremLlama (50.1% and 4.0%).
Furthermore, we propose a section-level translation framework for real-world
applications. As a direct application of Herald translator, we have
successfully translated a template section in the Stack project, marking a
notable progress in the automatic formalization of graduate-level mathematical
literature. Our model, along with the datasets, are open-sourced to the public.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">Survey</span>X: Academic <span class="highlight-title">Survey</span> Automation via Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.14776v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.14776v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xun Liang, Jiawei Yang, Yezhaohui Wang, Chen Tang, Zifan Zheng, Shichao Song, Zehao Lin, Yebin Yang, Simin Niu, Hanyu Wang, Bo Tang, Feiyu Xiong, Keming Mao, Zhiyu li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have demonstrated exceptional comprehension
capabilities and a vast knowledge base, suggesting that LLMs can serve as
efficient tools for automated survey generation. However, recent research
related to automated survey generation remains constrained by some critical
limitations like finite context window, lack of in-depth content discussion,
and absence of systematic evaluation frameworks. Inspired by human writing
processes, we propose SurveyX, an efficient and organized system for automated
survey generation that decomposes the survey composing process into two phases:
the Preparation and Generation phases. By innovatively introducing online
reference retrieval, a pre-processing method called AttributeTree, and a
re-polishing process, SurveyX significantly enhances the efficacy of survey
composition. Experimental evaluation results show that SurveyX outperforms
existing automated survey generation systems in content quality (0.259
improvement) and citation quality (1.76 enhancement), approaching human expert
performance across multiple evaluation dimensions. Examples of surveys
generated by SurveyX are available on www.surveyx.cn
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 16 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Decoding Reading Goals from Eye Movements 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.20779v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.20779v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Omer Shubi, Cfir Avraham Hadar, Yevgeni Berzak
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Readers can have different goals with respect to the text that they are
reading. Can these goals be decoded from their eye movements over the text? In
this work, we examine for the first time whether it is possible to distinguish
between two types of common reading goals: information seeking and ordinary
reading for comprehension. Using large-scale eye tracking data, we address this
task with a wide range of models that cover different architectural and data
representation strategies, and further introduce a new model ensemble. We find
that transformer-based models with scanpath representations coupled with
language modeling solve it most successfully, and that accurate predictions can
be made in real time, long before the participant finished reading the text. We
further introduce a new method for model performance analysis based on mixed
effect modeling. Combining this method with rich textual annotations reveals
key properties of textual items and participants that contribute to the
difficulty of the task, and improves our understanding of the variability in
eye movement patterns across the two reading regimes.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SDPO: Segment-Level Direct Preference Optimization for Social Agents 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.01821v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.01821v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aobo Kong, Wentao Ma, Shiwan Zhao, Yongbin Li, Yuchuan Wu, Ke Wang, Xiaoqian Liu, Qicheng Li, Yong Qin, Fei Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Social agents powered by large language models (LLMs) can simulate human
social behaviors but fall short in handling complex social dialogues. Direct
Preference Optimization (DPO) has proven effective in aligning LLM behavior
with human preferences across various agent tasks. However, standard DPO
focuses solely on individual turns, which limits its effectiveness in
multi-turn social interactions. Several DPO-based multi-turn alignment methods
with session-level data have shown potential in addressing this problem.While
these methods consider multiple turns across entire sessions, they are often
overly coarse-grained, introducing training noise, and lack robust theoretical
support. To resolve these limitations, we propose Segment-Level Direct
Preference Optimization (SDPO), which dynamically select key segments within
interactions to optimize multi-turn agent behavior. SDPO minimizes training
noise and is grounded in a rigorous theoretical framework. Evaluations on the
SOTOPIA benchmark demonstrate that SDPO-tuned agents consistently outperform
both existing DPO-based methods and proprietary LLMs like GPT-4o, underscoring
SDPO's potential to advance the social intelligence of LLM-based agents. We
release our code and data at
https://github.com/AlibabaResearch/DAMO-ConvAI/tree/main/SDPO.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Reward Shaping to Mitigate Reward Hacking in RLHF 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.18770v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.18770v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiayi Fu, Xuandong Zhao, Chengyuan Yao, Heng Wang, Qi Han, Yanghua Xiao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reinforcement Learning from Human Feedback (RLHF) is essential for aligning
large language models (LLMs) with human values. However, RLHF is susceptible to
reward hacking, where the agent exploits flaws in the reward function rather
than learning the intended behavior, thus degrading alignment. While reward
shaping helps stabilize RLHF and partially mitigate reward hacking, a
systematic investigation into shaping techniques and their underlying
principles remains lacking. To bridge this gap, we present a comprehensive
study of the prevalent reward shaping methods. Our analysis suggests three key
design principles: (1) RL reward is ideally bounded, (2) RL benefits from rapid
initial growth followed by gradual convergence, and (3) RL reward is best
formulated as a function of centered reward. Guided by these insights, we
propose Preference As Reward (PAR), a novel approach that leverages the latent
preferences embedded within the reward model itself as the signal for
reinforcement learning. We evaluated PAR on two base models, Gemma2-2B and
Llama3-8B, using two datasets, Ultrafeedback-Binarized and HH-RLHF.
Experimental results demonstrate PAR's superior performance over other reward
shaping methods. On the AlpacaEval 2.0 benchmark, PAR achieves a win rate at
least 5 percentage points higher than competing approaches. Furthermore, PAR
exhibits remarkable data efficiency, requiring only a single reference reward
for optimal performance, and maintains robustness against reward hacking even
after two full epochs of training. Code is available at
https://github.com/PorUna-byte/PAR.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>19 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Reducing Tool Hallucination via Reliability Alignment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.04141v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.04141v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hongshen Xu, Zichen Zhu, Lei Pan, Zihan Wang, Su Zhu, Da Ma, Ruisheng Cao, Lu Chen, Kai Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have expanded their capabilities beyond language
generation to interact with external tools, enabling automation and real-world
applications. However, tool hallucinations, where models either select
inappropriate tools or misuse them, pose significant challenges, leading to
erroneous task execution, increased computational costs, and reduced system
reliability. To systematically address this issue, we define and categorize
tool hallucinations into two main types, tool selection hallucination and tool
usage hallucination. To evaluate and mitigate these issues, we introduce
RelyToolBench, which integrates specialized test cases and novel metrics to
assess hallucination-aware task success and efficiency. Finally, we propose
Relign, a reliability alignment framework that expands the tool-use action
space to include indecisive actions, allowing LLMs to defer tool use, seek
clarification, or adjust tool selection dynamically. Through extensive
experiments, we demonstrate that Relign significantly reduces tool
hallucinations, improves task reliability, and enhances the efficiency of LLM
tool interactions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LUME: LLM Unlearning with Multitask Evaluations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.15097v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.15097v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anil Ramakrishna, Yixin Wan, Xiaomeng Jin, Kai-Wei Chang, Zhiqi Bu, Bhanukiran Vinzamuri, Volkan Cevher, Mingyi Hong, Rahul Gupta
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Unlearning aims to remove copyrighted, sensitive, or private content from
large language models (LLMs) without a full retraining. In this work, we
develop a multi-task unlearning benchmark (LUME) which features three tasks:
(1) unlearn synthetically generated creative short novels, (2) unlearn
synthetic biographies with sensitive information, and (3) unlearn a collection
of public biographies. We further release two fine-tuned LLMs of 1B and 7B
parameter sizes as the target models. We conduct detailed evaluations of
several recently proposed unlearning algorithms and present results on
carefully crafted metrics to understand their behavior and limitations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SPA-VL: A Comprehensive Safety Preference Alignment <span class="highlight-title">Dataset</span> for Vision
  Language Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.12030v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.12030v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yongting Zhang, Lu Chen, Guodong Zheng, Yifeng Gao, Rui Zheng, Jinlan Fu, Zhenfei Yin, Senjie Jin, Yu Qiao, Xuanjing Huang, Feng Zhao, Tao Gui, Jing Shao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The emergence of Vision Language Models (VLMs) has brought unprecedented
advances in understanding multimodal information. The combination of textual
and visual semantics in VLMs is highly complex and diverse, making the safety
alignment of these models challenging. Furthermore, due to the limited study on
the safety alignment of VLMs, there is a lack of large-scale, high-quality
datasets. To address these limitations, we propose a Safety Preference
Alignment dataset for Vision Language Models named SPA-VL. In terms of breadth,
SPA-VL covers 6 harmfulness domains, 13 categories, and 53 subcategories, and
contains 100,788 samples of the quadruple (question, image, chosen response,
rejected response). In terms of depth, the responses are collected from 12
open-source (e.g., QwenVL) and closed-source (e.g., Gemini) VLMs to ensure
diversity. The construction of preference data is fully automated, and the
experimental results indicate that models trained with alignment techniques on
the SPA-VL dataset exhibit substantial improvements in harmlessness and
helpfulness while maintaining core capabilities. SPA-VL, as a large-scale,
high-quality, and diverse dataset, represents a significant milestone in
ensuring that VLMs achieve both harmlessness and helpfulness.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Low-Confidence Gold: Refining Low-Confidence Samples for Efficient
  Instruction Tuning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.18978v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.18978v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hongyi Cal, Jie Li, Wenzhen Dong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The effectiveness of instruction fine-tuning for Large Language Models is
fundamentally constrained by the quality and efficiency of training datasets.
This work introduces Low-Confidence Gold (LCG), a novel filtering framework
that employs centroid-based clustering and confidence-guided selection for
identifying valuable instruction pairs. Through a semi-supervised approach
using a lightweight classifier trained on representative samples, LCG curates
high-quality subsets while preserving data diversity. Experimental evaluation
demonstrates that models fine-tuned on LCG-filtered subsets of 6K samples
achieve superior performance compared to existing methods, with substantial
improvements on MT-bench and consistent gains across comprehensive evaluation
metrics. The framework's efficacy while maintaining model performance
establishes a promising direction for efficient instruction tuning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MMFakeBench: A Mixed-Source Multimodal Misinformation Detection
  Benchmark for LVLMs <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.08772v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.08772v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xuannan Liu, Zekun Li, Peipei Li, Huaibo Huang, Shuhan Xia, Xing Cui, Linzhi Huang, Weihong Deng, Zhaofeng He
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Current multimodal misinformation detection (MMD) methods often assume a
single source and type of forgery for each sample, which is insufficient for
real-world scenarios where multiple forgery sources coexist. The lack of a
benchmark for mixed-source misinformation has hindered progress in this field.
To address this, we introduce MMFakeBench, the first comprehensive benchmark
for mixed-source MMD. MMFakeBench includes 3 critical sources: textual veracity
distortion, visual veracity distortion, and cross-modal consistency distortion,
along with 12 sub-categories of misinformation forgery types. We further
conduct an extensive evaluation of 6 prevalent detection methods and 15 Large
Vision-Language Models (LVLMs) on MMFakeBench under a zero-shot setting. The
results indicate that current methods struggle under this challenging and
realistic mixed-source MMD setting. Additionally, we propose MMD-Agent, a novel
approach to integrate the reasoning, action, and tool-use capabilities of LVLM
agents, significantly enhancing accuracy and generalization. We believe this
study will catalyze future research into more realistic mixed-source multimodal
misinformation and provide a fair evaluation of misinformation detection
methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICLR 2025, Project page:
  https://liuxuannan.github.io/MMFakeBench.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SECURA: Sigmoid-Enhanced CUR Decomposition with Uninterrupted Retention
  and Low-Rank Adaptation in Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.18168v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.18168v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuxuan Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the rapid development of large language models (LLMs), fully fine-tuning
(FT) these models has become increasingly impractical due to the high
computational demands. Additionally, FT can lead to catastrophic forgetting. As
an alternative, Low-Rank Adaptation (LoRA) has been proposed, which fine-tunes
only a small subset of parameters, achieving similar performance to FT while
significantly reducing resource requirements. However, since LoRA inherits FT's
design, the issue of catastrophic forgetting remains.
  To address these challenges, we propose SECURA: Sigmoid-Enhanced CUR
Decomposition LoRA, a novel parameter-efficient fine-tuning (PEFT) variant that
mitigates catastrophic forgetting while improving fine-tuning performance. Our
method introduces a new normalization technique, SigNorm, to enhance parameter
retention and overall performance.
  SECURA has been evaluated on a variety of tasks, including mathematical
problem-solving (GSM8K), challenging question-answering (CNNDM), translation
(NewsDE), and complex multiple-choice reasoning (LogiQA). Experimental results
show that SECURA achieves an average fine-tuning improvement of 3.59% across
four multiple-choice question (MCQ) tasks and a 2.51% improvement across five
question-answering (QA) tasks on models such as Gemma2 2b, Qwen2 1.5b, Qwen 2
7b, Llama3 8b, and Llama3.1 8b, compared to DoRA. Moreover, SECURA demonstrates
superior knowledge retention capabilities, maintaining more than 70% accuracy
on basic LLM knowledge across 16 continual learning tests, outperforming
Experience Replay (ER), Sequential Learning (SEQ), EWC, I-LoRA, and CUR-LoRA.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>New work on Parameter-Efficient Fine-Tuning (PEFT) for large language
  models. Includes new techniques SigNorm and CABR-LoRA for optimizing
  fine-tune performance and Knowledge retention</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Taxonomy Tree Generation from Citation Graph 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.03761v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.03761v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuntong Hu, Zhuofeng Li, Zheng Zhang, Chen Ling, Raasikh Kanjiani, Boxin Zhao, Liang Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Constructing taxonomies from citation graphs is essential for organizing
scientific knowledge, facilitating literature reviews, and identifying emerging
research trends. However, manual taxonomy construction is labor-intensive,
time-consuming, and prone to human biases, often overlooking pivotal but
less-cited papers. In this paper, to enable automatic hierarchical taxonomy
generation from citation graphs, we propose HiGTL (Hierarchical Graph Taxonomy
Learning), a novel end-to-end framework guided by human-provided instructions
or preferred topics. Specifically, we propose a hierarchical citation graph
clustering method that recursively groups related papers based on both textual
content and citation structure, ensuring semantically meaningful and
structurally coherent clusters. Additionally, we develop a novel taxonomy node
verbalization strategy that iteratively generates central concepts for each
cluster, leveraging a pre-trained large language model (LLM) to maintain
semantic consistency across hierarchical levels. To further enhance
performance, we design a joint optimization framework that fine-tunes both the
clustering and concept generation modules, aligning structural accuracy with
the quality of generated taxonomies. Extensive experiments demonstrate that
HiGTL effectively produces coherent, high-quality taxonomies.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PRobELM: Plausibility Ranking Evaluation for Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.03818v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.03818v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhangdie Yuan, Eric Chamoun, Rami Aly, Chenxi Whitehouse, Andreas Vlachos
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces PRobELM (Plausibility Ranking Evaluation for Language
Models), a benchmark designed to assess language models' ability to discern
more plausible from less plausible scenarios through their parametric
knowledge. While benchmarks such as TruthfulQA emphasise factual accuracy or
truthfulness, and others such as COPA explore plausible scenarios without
explicitly incorporating world knowledge, PRobELM seeks to bridge this gap by
evaluating models' capabilities to prioritise plausible scenarios that leverage
world knowledge over less plausible alternatives. This design allows us to
assess the potential of language models for downstream use cases such as
literature-based discovery where the focus is on identifying information that
is likely but not yet known. Our benchmark is constructed from a dataset
curated from Wikidata edit histories, tailored to align the temporal bounds of
the training data for the evaluated models. PRobELM facilitates the evaluation
of language models across multiple prompting types, including statement, text
completion, and question-answering. Experiments with 10 models of various sizes
and architectures on the relationship between model scales, training recency,
and plausibility performance, reveal that factual accuracy does not directly
correlate with plausibility performance and that up-to-date training data
enhances plausibility assessment across different model architectures.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ H-CoT: Hijacking the Chain-of-Thought Safety Reasoning Mechanism to
  Jailbreak Large Reasoning Models, Including OpenAI o1/o3, DeepSeek-R1, and
  Gemini 2.0 Flash Thinking 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.12893v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.12893v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Martin Kuo, Jianyi Zhang, Aolin Ding, Qinsi Wang, Louis DiValentin, Yujia Bao, Wei Wei, Hai Li, Yiran Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Reasoning Models (LRMs) have recently extended their powerful reasoning
capabilities to safety checks-using chain-of-thought reasoning to decide
whether a request should be answered. While this new approach offers a
promising route for balancing model utility and safety, its robustness remains
underexplored. To address this gap, we introduce Malicious-Educator, a
benchmark that disguises extremely dangerous or malicious requests beneath
seemingly legitimate educational prompts. Our experiments reveal severe
security flaws in popular commercial-grade LRMs, including OpenAI o1/o3,
DeepSeek-R1, and Gemini 2.0 Flash Thinking. For instance, although OpenAI's o1
model initially maintains a high refusal rate of about 98%, subsequent model
updates significantly compromise its safety; and attackers can easily extract
criminal strategies from DeepSeek-R1 and Gemini 2.0 Flash Thinking without any
additional tricks. To further highlight these vulnerabilities, we propose
Hijacking Chain-of-Thought (H-CoT), a universal and transferable attack method
that leverages the model's own displayed intermediate reasoning to jailbreak
its safety reasoning mechanism. Under H-CoT, refusal rates sharply
decline-dropping from 98% to below 2%-and, in some instances, even transform
initially cautious tones into ones that are willing to provide harmful content.
We hope these findings underscore the urgent need for more robust safety
mechanisms to preserve the benefits of advanced reasoning capabilities without
compromising ethical standards.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Website: https://maliciouseducator.org/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SudoLM: Learning Access Control of Parametric Knowledge with
  Authorization Alignment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14676v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14676v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qin Liu, Fei Wang, Chaowei Xiao, Muhao Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing preference alignment is a one-size-fits-all alignment mechanism,
where the part of the large language model (LLM) parametric knowledge with
non-preferred features is uniformly blocked to all the users. However, this
part of knowledge can be useful to advanced users whose expertise qualifies
them to handle these information. The one-size-fits-all alignment mechanism
undermines LLM's utility for these qualified users. To address this problem, we
propose SudoLM, a framework that lets LLMs learn access control over specific
parametric knowledge for users with different credentials via authorization
alignment. SudoLM allows authorized users to unlock their access to all the
parametric knowledge with an assigned SUDO key while blocking access to
non-qualified users. Experiments on two application scenarios demonstrate that
SudoLM effectively controls the user's access to the parametric knowledge and
maintains its general utility.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Bridge: A Unified Framework to Knowledge Graph Completion via Language
  Models and Knowledge Representation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.06660v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.06660v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qiao Qiao, Yuepei Li, Qing Wang, Kang Zhou, Qi Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Knowledge graph completion (KGC) is a task of inferring missing triples based
on existing Knowledge Graphs (KGs). Both structural and semantic information
are vital for successful KGC. However, existing methods only use either the
structural knowledge from the KG embeddings or the semantic information from
pre-trained language models (PLMs), leading to suboptimal model performance.
Moreover, since PLMs are not trained on KGs, directly using PLMs to encode
triples may be inappropriate. To overcome these limitations, we propose a novel
framework called Bridge, which jointly encodes structural and semantic
information of KGs. Specifically, we strategically encode entities and
relations separately by PLMs to better utilize the semantic knowledge of PLMs
and enable structured representation learning via a structural learning
principle. Furthermore, to bridge the gap between KGs and PLMs, we employ a
self-supervised representation learning method called BYOL to fine-tune PLMs
with two different views of a triple. Unlike BYOL, which uses augmentation
methods to create two semantically similar views of the same image, potentially
altering the semantic information. We strategically separate the triple into
two parts to create different views, thus avoiding semantic alteration.
Experiments demonstrate that Bridge outperforms the SOTA models on three
benchmark datasets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ TAID: Temporally Adaptive Interpolated Distillation for Efficient
  Knowledge Transfer in Language Models <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.16937v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.16937v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Makoto Shing, Kou Misaki, Han Bao, Sho Yokoi, Takuya Akiba
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Causal language models have demonstrated remarkable capabilities, but their
size poses significant challenges for deployment in resource-constrained
environments. Knowledge distillation, a widely-used technique for transferring
knowledge from a large teacher model to a small student model, presents a
promising approach for model compression. A significant remaining issue lies in
the major differences between teacher and student models, namely the
substantial capacity gap, mode averaging, and mode collapse, which pose
barriers during distillation. To address these issues, we introduce
$\textit{Temporally Adaptive Interpolated Distillation (TAID)}$, a novel
knowledge distillation approach that dynamically interpolates student and
teacher distributions through an adaptive intermediate distribution, gradually
shifting from the student's initial distribution towards the teacher's
distribution. We provide a theoretical analysis demonstrating TAID's ability to
prevent mode collapse and empirically show its effectiveness in addressing the
capacity gap while balancing mode averaging and mode collapse. Our
comprehensive experiments demonstrate TAID's superior performance across
various model sizes and architectures in both instruction tuning and
pre-training scenarios. Furthermore, we showcase TAID's practical impact by
developing two state-of-the-art compact foundation models:
$\texttt{TAID-LLM-1.5B}$ for language tasks and $\texttt{TAID-VLM-2B}$ for
vision-language tasks. These results demonstrate TAID's effectiveness in
creating high-performing and efficient models, advancing the development of
more accessible AI technologies.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear at the 13th International Conference on Learning
  Representations (ICLR 2025) as a Spotlight presentation</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Show, Don't Tell: Evaluating Large Language Models Beyond Textual
  Understanding with ChildPlay 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.11068v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.11068v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gonçalo Hora de Carvalho, Oscar Knap, Robert Pollice
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We developed a benchmark set to assess the generalization of state-of-the-art
large language models on problems beyond linguistic tasks and evaluate it on a
systematic progression of GPT models (GPT-3.5, GPT-4, GPT-4o, GPT-4o-mini).
Using simple games like Tic-Tac-Toe, Connect Four, Battleship, and a Shape
Recognition Game, all encoded in ASCII, we test strategic capabilities and
spatial reasoning, core abilities any artificial intelligence would need to
master for solving problems in chemistry. To probe generalization, we introduce
two new games for spatial logic: LEGO Connect Language (LCL) and
Guess-the-SMILES (GtS), a operationally simple chemistry benchmark. Our results
show that GPT models provide meaningful responses for several tasks but,
generally, perform poorly. A systematic performance progression with increased
model capabilities (GPT-3.5, GPT-4, GPT-4o) is only observed for 4 out of the 7
benchmark tasks. All models consistently struggle with Battleship, LCL, and
GtS. This suggests that while GPT models can emulate conversational proficiency
and basic rule comprehension, they have limited generalization with respect to
strategy and spatial reasoning. Particularly poor performance is observed for
interpreting molecular graphs when encoded in ASCII. The results provided by
our open-source benchmark suite
(\href{https://github.com/BlueVelvetSackOfGoldPotatoes/child-play}{\texttt{ChildPlay}
GitHub Repository}) caution against claims of emergent intelligence in GPT
models, which appear more specialized than general.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Flash Interpretability: Decoding Specialised Feature Neurons in Large
  Language Models with the LM-Head 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.02688v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.02688v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Harry J Davies
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) typically have billions of parameters and are
thus often difficult to interpret in their operation. In this work, we
demonstrate that it is possible to decode neuron weights directly into token
probabilities through the final projection layer of the model (the LM-head).
This is illustrated in Llama 3.1 8B where we use the LM-head to find examples
of specialised feature neurons such as a "dog" neuron and a "California"
neuron, and we validate this by clamping these neurons to affect the
probability of the concept in the output. We evaluate this method on both the
pre-trained and Instruct models, finding that over 75% of neurons in the
up-projection layers in the instruct model have the same top associated token
compared to the pretrained model. Finally, we demonstrate that clamping the
"dog" neuron leads the instruct model to always discuss dogs when asked about
its favourite animal. Through our method, it is possible to map the top
features of the entirety of Llama 3.1 8B's up-projection neurons in less than
10 seconds, with minimal compute.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ From Tokens to Words: On the Inner Lexicon of LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05864v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05864v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guy Kaplan, Matanel Oren, Yuval Reif, Roy Schwartz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Natural language is composed of words, but modern large language models
(LLMs) process sub-words as input. A natural question raised by this
discrepancy is whether LLMs encode words internally, and if so how. We present
evidence that LLMs engage in an intrinsic detokenization process, where
sub-word sequences are combined into coherent whole-word representations at
their last token. Our experiments show that this process primarily takes place
within the early and middle layers of the model. We further demonstrate its
robustness to arbitrary splits (e.g., "cats" to "ca" and "ts"), typos, and
importantly-to out-of-vocabulary words: when feeding the last token internal
representations of such words to the model as input, it can "understand" them
as the complete word despite never seeing such representations as input during
training. Our findings suggest that LLMs maintain a latent vocabulary beyond
the tokenizer's scope. These insights provide a practical, finetuning-free
application for expanding the vocabulary of pre-trained models. By enabling the
addition of new vocabulary words, we reduce input length and inference
iterations, which reduces both space and model latency, with little to no loss
in model accuracy.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ExACT: Teaching AI Agents to Explore with Reflective-MCTS and
  Exploratory Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02052v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02052v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiao Yu, Baolin Peng, Vineeth Vajipey, Hao Cheng, Michel Galley, Jianfeng Gao, Zhou Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Autonomous agents have demonstrated significant potential in automating
complex multistep decision-making tasks. However, even state-of-the-art
vision-language models (VLMs), such as GPT-4o, still fall short of human-level
performance, particularly in intricate web environments and long-horizon tasks.
To address these limitations, we present ExACT, an approach to combine
test-time search and self-learning to build o1-like models for agentic
applications. We first introduce Reflective Monte Carlo Tree Search (R-MCTS), a
novel test time algorithm designed to enhance AI agents' ability to explore
decision space on the fly. R-MCTS extends traditional MCTS by 1) incorporating
contrastive reflection, allowing agents to learn from past interactions and
dynamically improve their search efficiency; and 2) using multi-agent debate
for reliable state evaluation. Next, we introduce Exploratory Learning, a novel
learning strategy to teach agents to search at inference time without relying
on any external search algorithms. On the challenging VisualWebArena benchmark,
our GPT-4o based R-MCTS agent achieves a 6% to 30% relative improvement across
various tasks compared to the previous state-of-the-art. Additionally, we show
that the knowledge and experience gained from test-time search can be
effectively transferred back to GPT-4o via fine-tuning. After Exploratory
Learning, GPT-4o 1) demonstrates the ability to explore the environment,
evaluate a state, and backtrack to viable ones when it detects that the current
state cannot lead to success, and 2) matches 87% of R-MCTS's performance while
using significantly less compute. Notably, our work demonstrates the compute
scaling properties in both training - data collection with R-MCTS - and testing
time. These results suggest a promising research direction to enhance VLMs'
capabilities for agentic applications via test-time search and self-learning.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2025-02-26T00:00:00Z">2025-02-26</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Computation and Language
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Norm Growth and Stability Challenges in Localized Sequential Knowledge
  Editing <span class="chip">AAAI 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.19416v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.19416v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Akshat Gupta, Christine Fang, Atahan Ozdemir, Maochuan Lu, Ahmed Alaa, Thomas Hartvigsen, Gopala Anumanchipalli
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study investigates the impact of localized updates to large language
models (LLMs), specifically in the context of knowledge editing - a task aimed
at incorporating or modifying specific facts without altering broader model
capabilities. We first show that across different post-training interventions
like continuous pre-training, full fine-tuning and LORA-based fine-tuning, the
Frobenius norm of the updated matrices always increases. This increasing norm
is especially detrimental for localized knowledge editing, where only a subset
of matrices are updated in a model . We reveal a consistent phenomenon across
various editing techniques, including fine-tuning, hypernetwork-based
approaches, and locate-and-edit methods: the norm of the updated matrix
invariably increases with successive updates. Such growth disrupts model
balance, particularly when isolated matrices are updated while the rest of the
model remains static, leading to potential instability and degradation of
downstream performance. Upon deeper investigations of the intermediate
activation vectors, we find that the norm of internal activations decreases and
is accompanied by shifts in the subspaces occupied by these activations, which
shows that these activation vectors now occupy completely different regions in
the representation space compared to the unedited model. With our paper, we
highlight the technical challenges with continuous and localized sequential
knowledge editing and their implications for maintaining model stability and
utility.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for Oral Presentation at KnowFM @ AAAI 2025. arXiv admin
  note: text overlap with arXiv:2502.01636</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Project Alexandria: Towards Freeing Scientific Knowledge from Copyright
  Burdens via LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.19413v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.19413v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Christoph Schuhmann, Gollam Rabby, Ameya Prabhu, Tawsif Ahmed, Andreas Hochlehnert, Huu Nguyen, Nick Akinci Heidrich, Ludwig Schmidt, Robert Kaczmarczyk, Sören Auer, Jenia Jitsev, Matthias Bethge
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Paywalls, licenses and copyright rules often restrict the broad dissemination
and reuse of scientific knowledge. We take the position that it is both legally
and technically feasible to extract the scientific knowledge in scholarly
texts. Current methods, like text embeddings, fail to reliably preserve factual
content, and simple paraphrasing may not be legally sound. We urge the
community to adopt a new idea: convert scholarly documents into Knowledge Units
using LLMs. These units use structured data capturing entities, attributes and
relationships without stylistic content. We provide evidence that Knowledge
Units: (1) form a legally defensible framework for sharing knowledge from
copyrighted research texts, based on legal analyses of German copyright law and
U.S. Fair Use doctrine, and (2) preserve most (~95%) factual knowledge from
original text, measured by MCQ performance on facts from the original
copyrighted text across four research domains. Freeing scientific knowledge
from copyright promises transformative benefits for scientific research and
education by allowing language models to reuse important facts from copyrighted
text. To support this, we share open-source tools for converting research
documents into Knowledge Units. Overall, our work posits the feasibility of
democratizing access to scientific knowledge while respecting copyright.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Technical Report</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Mighty ToRR: A Benchmark for Table Reasoning and Robustness 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.19412v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.19412v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shir Ashury-Tahan, Yifan Mai, Rajmohan C, Ariel Gera, Yotam Perlitz, Asaf Yehudai, Elron Bandel, Leshem Choshen, Eyal Shnarch, Percy Liang, Michal Shmueli-Scheuer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite its real-world significance, model performance on tabular data
remains underexplored, leaving uncertainty about which model to rely on and
which prompt configuration to adopt. To address this gap, we create ToRR, a
benchmark for Table Reasoning and Robustness, that measures model performance
and robustness on table-related tasks. The benchmark includes 10 datasets that
cover different types of table reasoning capabilities across varied domains.
ToRR goes beyond model performance rankings, and is designed to reflect whether
models can handle tabular data consistently and robustly, across a variety of
common table representation formats. We present a leaderboard as well as
comprehensive analyses of the results of leading models over ToRR. Our results
reveal a striking pattern of brittle model behavior, where even strong models
are unable to perform robustly on tabular data tasks. Although no specific
table format leads to consistently better performance, we show that testing
over multiple formats is crucial for reliably estimating model capabilities.
Moreover, we show that the reliability boost from testing multiple prompts can
be equivalent to adding more test examples. Overall, our findings show that
table understanding and reasoning tasks remain a significant challenge.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Code to Think, Think to Code: A <span class="highlight-title">Survey</span> on Code-Enhanced Reasoning and
  Reasoning-Driven Code Intelligence in LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.19411v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.19411v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dayu Yang, Tianyang Liu, Daoan Zhang, Antoine Simoulin, Xiaoyi Liu, Yuwei Cao, Zhaopu Teng, Xin Qian, Grey Yang, Jiebo Luo, Julian McAuley
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In large language models (LLMs), code and reasoning reinforce each other:
code offers an abstract, modular, and logic-driven structure that supports
reasoning, while reasoning translates high-level goals into smaller, executable
steps that drive more advanced code intelligence. In this study, we examine how
code serves as a structured medium for enhancing reasoning: it provides
verifiable execution paths, enforces logical decomposition, and enables runtime
validation. We also explore how improvements in reasoning have transformed code
intelligence from basic completion to advanced capabilities, enabling models to
address complex software engineering tasks through planning and debugging.
Finally, we identify key challenges and propose future research directions to
strengthen this synergy, ultimately improving LLM's performance in both areas.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project Repo: https://github.com/dayuyang1999/Awesome-Code-Reasoning</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ImageChain: Advancing Sequential Image-to-Text Reasoning in Multimodal
  Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.19409v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.19409v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Danae Sánchez Villegas, Ingo Ziegler, Desmond Elliott
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reasoning over sequences of images remains a challenge for multimodal large
language models (MLLMs). While recent models incorporate multi-image data
during pre-training, they still struggle to recognize sequential structures,
often treating images independently. This work introduces ImageChain, a
framework that enhances MLLMs with sequential reasoning capabilities over image
data by modeling visual sequences as a multi-turn conversation. In ImageChain,
images are interleaved with corresponding textual descriptions to form a
controlled dialogue that explicitly captures temporal dependencies and
narrative progression. Our method optimizes for the task of next-scene
description, where the model generates a context-aware description of an
upcoming scene based on preceding visual and textual cues. We demonstrate that
our approach improves performance on the next-scene description task --
achieving an average improvement from 3.7% to 19% in SimRate, a metric that
quantifies semantic similarity to human-annotated ground truths. Moreover,
ImageChain achieves robust zero-shot out-of-domain performance in applications
ranging from comics to robotics. Extensive experiments validate that
instruction-tuning in a multimodal, multi-turn conversation design is key to
bridging the gap between static image understanding and temporally-aware
reasoning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code, dataset, and checkpoints are publicly available at
  https://github.com/danaesavi/ImageChain</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning Code-Edit Embedding to Model Student Debugging Behavior 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.19407v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.19407v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hasnain Heickal, Andrew Lan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Providing effective feedback for programming assignments in computer science
education can be challenging: students solve problems by iteratively submitting
code, executing it, and using limited feedback from the compiler or the
auto-grader to debug. Analyzing student debugging behavior in this process may
reveal important insights into their knowledge and inform better personalized
support tools. In this work, we propose an encoder-decoder-based model that
learns meaningful code-edit embeddings between consecutive student code
submissions, to capture their debugging behavior. Our model leverages
information on whether a student code submission passes each test case to
fine-tune large language models (LLMs) to learn code editing representations.
It enables personalized next-step code suggestions that maintain the student's
coding style while improving test case correctness. Our model also enables us
to analyze student code-editing patterns to uncover common student errors and
debugging behaviors, using clustering techniques. Experimental results on a
real-world student code submission dataset demonstrate that our model excels at
code reconstruction and personalized code suggestion while revealing
interesting patterns in student debugging behavior.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TheoremExplainAgent: Towards Multimodal Explanations for LLM Theorem
  Understanding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.19400v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.19400v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Max Ku, Thomas Chong, Jonathan Leung, Krish Shah, Alvin Yu, Wenhu Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Understanding domain-specific theorems often requires more than just
text-based reasoning; effective communication through structured visual
explanations is crucial for deeper comprehension. While large language models
(LLMs) demonstrate strong performance in text-based theorem reasoning, their
ability to generate coherent and pedagogically meaningful visual explanations
remains an open challenge. In this work, we introduce TheoremExplainAgent, an
agentic approach for generating long-form theorem explanation videos (over 5
minutes) using Manim animations. To systematically evaluate multimodal theorem
explanations, we propose TheoremExplainBench, a benchmark covering 240 theorems
across multiple STEM disciplines, along with 5 automated evaluation metrics.
Our results reveal that agentic planning is essential for generating detailed
long-form videos, and the o3-mini agent achieves a success rate of 93.8% and an
overall score of 0.77. However, our quantitative and qualitative studies show
that most of the videos produced exhibit minor issues with visual element
layout. Furthermore, multimodal explanations expose deeper reasoning flaws that
text-based explanations fail to reveal, highlighting the importance of
multimodal explanations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Residual Speech Embeddings for Tone Classification: Removing Linguistic
  Content to Enhance Paralinguistic Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.19387v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.19387v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hamdan Al Ahbabi, Gautier Marti, Saeed AlMarri, Ibrahim Elfadel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Self-supervised learning models for speech processing, such as wav2vec2,
HuBERT, WavLM, and Whisper, generate embeddings that capture both linguistic
and paralinguistic information, making it challenging to analyze tone
independently of spoken content. In this work, we introduce a method for
disentangling paralinguistic features from linguistic content by regressing
speech embeddings onto their corresponding text embeddings and using the
residuals as a representation of vocal tone. We evaluate this approach across
multiple self-supervised speech embeddings, demonstrating that residual
embeddings significantly improve tone classification performance compared to
raw speech embeddings. Our results show that this method enhances linear
separability, enabling improved classification even with simple models such as
logistic regression. Visualization of the residual embeddings further confirms
the successful removal of linguistic information while preserving tone-related
features. These findings highlight the potential of residual embeddings for
applications in sentiment analysis, speaker characterization, and
paralinguistic speech processing.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DataMan: Data Manager for <span class="highlight-title">Pre-train</span>ing Large Language Models <span class="chip">ICLR2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.19363v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.19363v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ru Peng, Kexin Yang, Yawen Zeng, Junyang Lin, Dayiheng Liu, Junbo Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The performance emergence of large language models (LLMs) driven by data
scaling laws makes the selection of pre-training data increasingly important.
However, existing methods rely on limited heuristics and human intuition,
lacking comprehensive and clear guidelines. To address this, we are inspired by
``reverse thinking'' -- prompting LLMs to self-identify which criteria benefit
its performance. As its pre-training capabilities are related to perplexity
(PPL), we derive 14 quality criteria from the causes of text perplexity
anomalies and introduce 15 common application domains to support domain mixing.
In this paper, we train a Data Manager (DataMan) to learn quality ratings and
domain recognition from pointwise rating, and use it to annotate a 447B token
pre-training corpus with 14 quality ratings and domain type. Our experiments
validate our approach, using DataMan to select 30B tokens to train a
1.3B-parameter language model, demonstrating significant improvements in
in-context learning (ICL), perplexity, and instruction-following ability over
the state-of-the-art baseline. The best-performing model, based on the Overall
Score l=5 surpasses a model trained with 50% more data using uniform sampling.
We continue pre-training with high-rated, domain-specific data annotated by
DataMan to enhance domain-specific ICL performance and thus verify DataMan's
domain mixing ability. Our findings emphasize the importance of quality
ranking, the complementary nature of quality criteria, and their low
correlation with perplexity, analyzing misalignment between PPL and ICL
performance. We also thoroughly analyzed our pre-training dataset, examining
its composition, the distribution of quality ratings, and the original document
sources.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR2025 paper</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Can Large Language Models Detect Errors in Long Chain-of-Thought
  Reasoning? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.19361v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.19361v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yancheng He, Shilong Li, Jiaheng Liu, Weixun Wang, Xingyuan Bu, Ge Zhang, Zhongyuan Peng, Zhaoxiang Zhang, Wenbo Su, Bo Zheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, o1-like models have drawn significant attention, where these models
produce the long Chain-of-Thought (CoT) reasoning steps to improve the
reasoning abilities of existing Large Language Models (LLMs). In this paper, to
understand the qualities of these long CoTs and measure the critique abilities
of existing LLMs on these long CoTs, we introduce the DeltaBench, including the
generated long CoTs from different o1-like models (e.g., QwQ, DeepSeek-R1) for
different reasoning tasks (e.g., Math, Code, General Reasoning), to measure the
ability to detect errors in long CoT reasoning. Based on DeltaBench, we first
perform fine-grained analysis of the generated long CoTs to discover the
effectiveness and efficiency of different o1-like models. Then, we conduct
extensive evaluations of existing process reward models (PRMs) and critic
models to detect the errors of each annotated process, which aims to
investigate the boundaries and limitations of existing PRMs and critic models.
Finally, we hope that DeltaBench could guide developers to better understand
the long CoT reasoning abilities of their models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The first three authors contributed equally, 27 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Controlled Diversity: Length-optimized Natural Language Generation <span class="chip">ISCA</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.19347v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.19347v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Diana Marie Schenke, Timo Baumann
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  LLMs are not generally able to adjust the length of their outputs based on
strict length requirements, a capability that would improve their usefulness in
applications that require adherence to diverse user and system requirements. We
present an approach to train LLMs to acquire this capability by augmenting
existing data and applying existing fine-tuning techniques, which we compare
based on the trained models' adherence to the length requirement and overall
response quality relative to the baseline model. Our results demonstrate that
these techniques can be successfully applied to train LLMs to adhere to length
requirements, with the trained models generating texts which better align to
the length requirements. Our results indicate that our method may change the
response quality when using training data that was not generated by the
baseline model. This allows simultaneous alignment to another training
objective in certain scenarios, but is undesirable otherwise. Training on a
dataset containing the model's own responses eliminates this issue.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ISCA/ITG Workshop on Diversity in Large Speech and Language Models</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Evaluating LLMs and <span class="highlight-title">Pre-train</span>ed Models for Text Summarization Across
  Diverse <span class="highlight-title">Dataset</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.19339v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.19339v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tohida Rehman, Soumabha Ghosh, Kuntal Das, Souvik Bhattacharjee, Debarshi Kumar Sanyal, Samiran Chattopadhyay
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Text summarization plays a crucial role in natural language processing by
condensing large volumes of text into concise and coherent summaries. As
digital content continues to grow rapidly and the demand for effective
information retrieval increases, text summarization has become a focal point of
research in recent years. This study offers a thorough evaluation of four
leading pre-trained and open-source large language models: BART, FLAN-T5,
LLaMA-3-8B, and Gemma-7B, across five diverse datasets CNN/DM, Gigaword, News
Summary, XSum, and BBC News. The evaluation employs widely recognized automatic
metrics, including ROUGE-1, ROUGE-2, ROUGE-L, BERTScore, and METEOR, to assess
the models' capabilities in generating coherent and informative summaries. The
results reveal the comparative strengths and limitations of these models in
processing various text types.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, 2 figures, 6 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Agentic Reward Modeling: Integrating Human Preferences with Verifiable
  Correctness Signals for Reliable Reward Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.19328v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.19328v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hao Peng, Yunjia Qi, Xiaozhi Wang, Zijun Yao, Bin Xu, Lei Hou, Juanzi Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reward models (RMs) are crucial for the training and inference-time scaling
up of large language models (LLMs). However, existing reward models primarily
focus on human preferences, neglecting verifiable correctness signals which
have shown strong potential in training LLMs. In this paper, we propose agentic
reward modeling, a reward system that combines reward models with verifiable
correctness signals from different aspects to provide reliable rewards. We
empirically implement a reward agent, named RewardAgent, that combines human
preference rewards with two verifiable signals: factuality and instruction
following, to provide more reliable rewards. We conduct comprehensive
experiments on existing reward model benchmarks and inference time best-of-n
searches on real-world downstream tasks. RewardAgent significantly outperforms
vanilla reward models, demonstrating its effectiveness. We further construct
training preference pairs using RewardAgent and train an LLM with the DPO
objective, achieving superior performance on various NLP benchmarks compared to
conventional reward models. Our codes are publicly released to facilitate
further research (https://github.com/THU-KEG/Agentic-Reward-Modeling).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Shh, don't say that! Domain Certification in LLMs <span class="chip">ICLR</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.19320v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.19320v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Cornelius Emde, Alasdair Paren, Preetham Arvind, Maxime Kayser, Tom Rainforth, Thomas Lukasiewicz, Bernard Ghanem, Philip H. S. Torr, Adel Bibi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) are often deployed to perform constrained tasks,
with narrow domains. For example, customer support bots can be built on top of
LLMs, relying on their broad language understanding and capabilities to enhance
performance. However, these LLMs are adversarially susceptible, potentially
generating outputs outside the intended domain. To formalize, assess, and
mitigate this risk, we introduce domain certification; a guarantee that
accurately characterizes the out-of-domain behavior of language models. We then
propose a simple yet effective approach, which we call VALID that provides
adversarial bounds as a certificate. Finally, we evaluate our method across a
diverse set of datasets, demonstrating that it yields meaningful certificates,
which bound the probability of out-of-domain samples tightly with minimum
penalty to refusal behavior.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, includes appendix Published in International Conference on
  Learning Representations (ICLR) 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FSPO: Few-Shot Preference Optimization of Synthetic Preference Data in
  LLMs Elicits Effective Personalization to Real Users 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.19312v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.19312v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anikait Singh, Sheryl Hsu, Kyle Hsu, Eric Mitchell, Stefano Ermon, Tatsunori Hashimoto, Archit Sharma, Chelsea Finn
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Effective personalization of LLMs is critical for a broad range of
user-interfacing applications such as virtual assistants and content curation.
Inspired by the strong in-context learning capabilities of LLMs, we propose
Few-Shot Preference Optimization (FSPO), which reframes reward modeling as a
meta-learning problem. Under this framework, an LLM learns to quickly adapt to
a user via a few labeled preferences from that user, constructing a
personalized reward function for them. Additionally, since real-world
preference data is scarce and challenging to collect at scale, we propose
careful design choices to construct synthetic preference datasets for
personalization, generating over 1M synthetic personalized preferences using
publicly available LLMs. In particular, to successfully transfer from synthetic
data to real users, we find it crucial for the data to exhibit both high
diversity and coherent, self-consistent structure. We evaluate FSPO on
personalized open-ended generation for up to 1,500 synthetic users across
across three domains: movie reviews, pedagogical adaptation based on
educational background, and general question answering, along with a controlled
human study. Overall, FSPO achieves an 87% Alpaca Eval winrate on average in
generating responses that are personalized to synthetic users and a 72% winrate
with real human users in open-ended question answering.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Website: https://fewshot-preference-optimization.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CritiQ: Mining Data Quality Criteria from Human Preferences 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.19279v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.19279v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Honglin Guo, Kai Lv, Qipeng Guo, Tianyi Liang, Zhiheng Xi, Demin Song, Qiuyinzhe Zhang, Yu Sun, Kai Chen, Xipeng Qiu, Tao Gui
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Language model heavily depends on high-quality data for optimal performance.
Existing approaches rely on manually designed heuristics, the perplexity of
existing models, training classifiers, or careful prompt engineering, which
require significant expert experience and human annotation effort while
introduce biases. We introduce CritiQ, a novel data selection method that
automatically mines criteria from human preferences for data quality with only
$\sim$30 human-annotated pairs and performs efficient data selection. The main
component, CritiQ Flow, employs a manager agent to evolve quality criteria and
worker agents to make pairwise judgments. We build a knowledge base that
extracts quality criteria from previous work to boost CritiQ Flow. Compared to
perplexity- and classifier- based methods, verbal criteria are more
interpretable and possess reusable value. After deriving the criteria, we train
the CritiQ Scorer to give quality scores and perform efficient data selection.
We demonstrate the effectiveness of our method in the code, math, and logic
domains, achieving high accuracy on human-annotated test sets. To validate the
quality of the selected data, we continually train Llama 3.1 models and observe
improved performance on downstream tasks compared to uniform sampling. Ablation
studies validate the benefits of the knowledge base and the reflection process.
We analyze how criteria evolve and the effectiveness of majority voting.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Disentangled VAD Representations via a Variational Framework for
  Political Stance Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.19276v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.19276v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Beiyu Xu, Zhiwei Liu, Sophia Ananiadou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The stance detection task aims to categorise the stance regarding specified
targets. Current methods face challenges in effectively integrating sentiment
information for stance detection. Moreover, the role of highly granular
sentiment labelling in stance detection has been largely overlooked. This study
presents a novel stance detection framework utilizing a variational autoencoder
(VAE) to disentangle latent emotional features-value, arousal, and dominance
(VAD)-from political discourse on social media. This approach addresses
limitations in current methods, particularly in in-target and cross-target
stance detection scenarios. This research uses an advanced emotional annotation
tool to annotate seven-class sentiment labels for P-STANCE. Evaluations on
benchmark datasets, including P-STANCE and SemEval-2016, reveal that
PoliStance-VAE achieves state-of-the-art performance, surpassing models like
BERT, BERTweet, and GPT-4o. PoliStance-VAE offers a robust and interpretable
solution for stance detection, demonstrating the effectiveness of integrating
nuanced emotional representations. This framework paves the way for
advancements in natural language processing tasks, particularly those requiring
detailed emotional understanding.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Drop-Upcycling: Training Sparse Mixture of Experts with Partial
  Re-initialization <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.19261v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.19261v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Taishi Nakamura, Takuya Akiba, Kazuki Fujii, Yusuke Oda, Rio Yokota, Jun Suzuki
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Mixture of Experts (MoE) architecture reduces the training and inference
cost significantly compared to a dense model of equivalent capacity. Upcycling
is an approach that initializes and trains an MoE model using a pre-trained
dense model. While upcycling leads to initial performance gains, the training
progresses slower than when trained from scratch, leading to suboptimal
performance in the long term. We propose Drop-Upcycling - a method that
effectively addresses this problem. Drop-Upcycling combines two seemingly
contradictory approaches: utilizing the knowledge of pre-trained dense models
while statistically re-initializing some parts of the weights. This approach
strategically promotes expert specialization, significantly enhancing the MoE
model's efficiency in knowledge acquisition. Extensive large-scale experiments
demonstrate that Drop-Upcycling significantly outperforms previous MoE
construction methods in the long term, specifically when training on hundreds
of billions of tokens or more. As a result, our MoE model with 5.9B active
parameters achieves comparable performance to a 13B dense model in the same
model family, while requiring approximately 1/4 of the training FLOPs. All
experimental resources, including source code, training data, model checkpoints
and logs, are publicly available to promote reproducibility and future research
on MoE.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear at the 13th International Conference on Learning
  Representations (ICLR 2025)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Between Circuits and Chomsky: Pre-<span class="highlight-title">pretrain</span>ing on Formal Languages
  Imparts Linguistic Biases 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.19249v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.19249v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Michael Y. Hu, Jackson Petty, Chuan Shi, William Merrill, Tal Linzen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pretraining language models on formal languages can improve their acquisition
of natural language, but it is unclear which features of the formal language
impart an inductive bias that leads to effective transfer. Drawing on insights
from linguistics and complexity theory, we hypothesize that effective transfer
occurs when the formal language both captures dependency structures in natural
language and remains within the computational limitations of the model
architecture. Focusing on transformers, we find that formal languages with both
these properties enable language models to achieve lower loss on natural
language and better linguistic generalization compared to other languages. In
fact, pre-pretraining, or training on formal-then-natural language, reduces
loss more efficiently than the same amount of natural language. For a
1B-parameter language model trained on roughly 1.6B tokens of natural language,
pre-pretraining achieves the same loss and better linguistic generalization
with a 33% smaller token budget. We also give mechanistic evidence of
cross-task transfer from formal to natural language: attention heads acquired
during formal language pretraining remain crucial for the model's performance
on syntactic evaluations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Two Heads Are Better Than One: Dual-Model Verbal Reflection at
  Inference-Time 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.19230v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.19230v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiazheng Li, Yuxiang Zhou, Junru Lu, Gladys Tyen, Lin Gui, Cesare Aloisi, Yulan He
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) often struggle with complex reasoning scenarios.
While preference optimization methods enhance reasoning performance through
training, they often lack transparency in why one reasoning outcome is
preferred over another. Verbal reflection techniques improve explainability but
are limited in LLMs' critique and refinement capacity. To address these
challenges, we introduce a contrastive reflection synthesis pipeline that
enhances the accuracy and depth of LLM-generated reflections. We further
propose a dual-model reasoning framework within a verbal reinforcement learning
paradigm, decoupling inference-time self-reflection into specialized, trained
models for reasoning critique and refinement. Extensive experiments show that
our framework outperforms traditional preference optimization methods across
all evaluation metrics. Our findings also show that "two heads are better than
one", demonstrating that a collaborative Reasoner-Critic model achieves
superior reasoning performance and transparency, compared to single-model
approaches.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Negation-Induced Forgetting in LLMs <span class="chip">ISCA</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.19211v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.19211v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Francesca Capuano, Ellen Boschert, Barbara Kaup
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The study explores whether Large Language Models (LLMs) exhibit
negation-induced forgetting (NIF), a cognitive phenomenon observed in humans
where negating incorrect attributes of an object or event leads to diminished
recall of this object or event compared to affirming correct attributes (Mayo
et al., 2014; Zang et al., 2023). We adapted Zang et al. (2023) experimental
framework to test this effect in ChatGPT-3.5, GPT-4o mini and
Llama3-70b-instruct. Our results show that ChatGPT-3.5 exhibits NIF, with
negated information being less likely to be recalled than affirmed information.
GPT-4o-mini showed a marginally significant NIF effect, while LLaMA-3-70B did
not exhibit NIF. The findings provide initial evidence of negation-induced
forgetting in some LLMs, suggesting that similar cognitive biases may emerge in
these models. This work is a preliminary step in understanding how
memory-related phenomena manifest in LLMs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ISCA/ITG Workshop on Diversity in Large Speech and Language Models</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Bi'an: A Bilingual Benchmark and Model for Hallucination Detection in
  Retrieval-Augmented Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.19209v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.19209v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhouyu Jiang, Mengshu Sun, Zhiqiang Zhang, Lei Liang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Retrieval-Augmented Generation (RAG) effectively reduces hallucinations in
Large Language Models (LLMs) but can still produce inconsistent or unsupported
content. Although LLM-as-a-Judge is widely used for RAG hallucination detection
due to its implementation simplicity, it faces two main challenges: the absence
of comprehensive evaluation benchmarks and the lack of domain-optimized judge
models. To bridge these gaps, we introduce \textbf{Bi'an}, a novel framework
featuring a bilingual benchmark dataset and lightweight judge models. The
dataset supports rigorous evaluation across multiple RAG scenarios, while the
judge models are fine-tuned from compact open-source LLMs. Extensive
experimental evaluations on Bi'anBench show our 14B model outperforms baseline
models with over five times larger parameter scales and rivals state-of-the-art
closed-source LLMs. We will release our data and models soon at
https://github.com/OpenSPG/KAG.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MultiConAD: A Unified Multilingual Conversational <span class="highlight-title">Dataset</span> for Early
  Alzheimer's Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.19208v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.19208v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Arezo Shakeri, Mina Farmanbar, Krisztian Balog
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Dementia is a progressive cognitive syndrome with Alzheimer's disease (AD) as
the leading cause. Conversation-based AD detection offers a cost-effective
alternative to clinical methods, as language dysfunction is an early biomarker
of AD. However, most prior research has framed AD detection as a binary
classification problem, limiting the ability to identify Mild Cognitive
Impairment (MCI)-a crucial stage for early intervention. Also, studies
primarily rely on single-language datasets, mainly in English, restricting
cross-language generalizability. To address this gap, we make three key
contributions. First, we introduce a novel, multilingual dataset for AD
detection by unifying 16 publicly available dementia-related conversational
datasets. This corpus spans English, Spanish, Chinese, and Greek and
incorporates both audio and text data derived from a variety of cognitive
assessment tasks. Second, we perform finer-grained classification, including
MCI, and evaluate various classifiers using sparse and dense text
representations. Third, we conduct experiments in monolingual and multilingual
settings, finding that some languages benefit from multilingual training while
others perform better independently. This study highlights the challenges in
multilingual AD detection and enables future research on both language-specific
approaches and techniques aimed at improving model generalization and
robustness.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 3 Figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FaithUn: Toward Faithful Forgetting in Language Models by Investigating
  the Interconnectedness of Knowledge 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.19207v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.19207v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nakyeong Yang, Minsung Kim, Seunghyun Yoon, Joongbo Shin, Kyomin Jung
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Various studies have attempted to remove sensitive or private knowledge from
a language model to prevent its unauthorized exposure. However, prior studies
have overlooked the complex and interconnected nature of knowledge, where
related knowledge must be carefully examined. Specifically, they have failed to
evaluate whether an unlearning method faithfully erases interconnected
knowledge that should be removed, retaining knowledge that appears relevant but
exists in a completely different context. To resolve this problem, we first
define a new concept called superficial unlearning, which refers to the
phenomenon where an unlearning method either fails to erase the interconnected
knowledge it should remove or unintentionally erases irrelevant knowledge.
Based on the definition, we introduce a new benchmark, FaithUn, to analyze and
evaluate the faithfulness of unlearning in real-world knowledge QA settings.
Furthermore, we propose a novel unlearning method, KLUE, which updates only
knowledge-related neurons to achieve faithful unlearning. KLUE identifies
knowledge neurons using an explainability method and updates only those neurons
using selected unforgotten samples. Experimental results demonstrate that
widely-used unlearning methods fail to ensure faithful unlearning, while our
method shows significant effectiveness in real-world QA unlearning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LiGT: Layout-infused Generative <span class="highlight-title">Transformer</span> for Visual Question
  Answering on Vietnamese Receipts 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.19202v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.19202v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Thanh-Phong Le, Trung Le Chi Phan, Nghia Hieu Nguyen, Kiet Van Nguyen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  \textbf{Purpose:} Document Visual Question Answering (document VQA)
challenges multimodal systems to holistically handle textual, layout, and
visual modalities to provide appropriate answers. Document VQA has gained
popularity in recent years due to the increasing amount of documents and the
high demand for digitization. Nonetheless, most of document VQA datasets are
developed in high-resource languages such as English.
  \textbf{Methods:} In this paper, we present ReceiptVQA (\textbf{Receipt}
\textbf{V}isual \textbf{Q}uestion \textbf{A}nswering), the initial large-scale
document VQA dataset in Vietnamese dedicated to receipts, a document kind with
high commercial potentials. The dataset encompasses \textbf{9,000+} receipt
images and \textbf{60,000+} manually annotated question-answer pairs. In
addition to our study, we introduce LiGT (\textbf{L}ayout-\textbf{i}nfused
\textbf{G}enerative \textbf{T}ransformer), a layout-aware encoder-decoder
architecture designed to leverage embedding layers of language models to
operate layout embeddings, minimizing the use of additional neural modules.
  \textbf{Results:} Experiments on ReceiptVQA show that our architecture
yielded promising performance, achieving competitive results compared with
outstanding baselines. Furthermore, throughout analyzing experimental results,
we found evident patterns that employing encoder-only model architectures has
considerable disadvantages in comparison to architectures that can generate
answers. We also observed that it is necessary to combine multiple modalities
to tackle our dataset, despite the critical role of semantic understanding from
language models.
  \textbf{Conclusion:} We hope that our work will encourage and facilitate
future development in Vietnamese document VQA, contributing to a diverse
multimodal research community in the Vietnamese language.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at IJDAR</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ BIG-Bench Extra Hard 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.19187v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.19187v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mehran Kazemi, Bahare Fatemi, Hritik Bansal, John Palowitch, Chrysovalantis Anastasiou, Sanket Vaibhav Mehta, Lalit K. Jain, Virginia Aglietti, Disha Jindal, Peter Chen, Nishanth Dikkala, Gladys Tyen, Xin Liu, Uri Shalit, Silvia Chiappa, Kate Olszewska, Yi Tay, Vinh Q. Tran, Quoc V. Le, Orhan Firat
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) are increasingly deployed in everyday
applications, demanding robust general reasoning capabilities and diverse
reasoning skillset. However, current LLM reasoning benchmarks predominantly
focus on mathematical and coding abilities, leaving a gap in evaluating broader
reasoning proficiencies. One particular exception is the BIG-Bench dataset,
which has served as a crucial benchmark for evaluating the general reasoning
capabilities of LLMs, thanks to its diverse set of challenging tasks that
allowed for a comprehensive assessment of general reasoning across various
skills within a unified framework. However, recent advances in LLMs have led to
saturation on BIG-Bench, and its harder version BIG-Bench Hard (BBH).
State-of-the-art models achieve near-perfect scores on many tasks in BBH, thus
diminishing its utility. To address this limitation, we introduce BIG-Bench
Extra Hard (BBEH), a new benchmark designed to push the boundaries of LLM
reasoning evaluation. BBEH replaces each task in BBH with a novel task that
probes a similar reasoning capability but exhibits significantly increased
difficulty. We evaluate various models on BBEH and observe a (harmonic) average
accuracy of 9.8\% for the best general-purpose model and 44.8\% for the best
reasoning-specialized model, indicating substantial room for improvement and
highlighting the ongoing challenge of achieving robust general reasoning in
LLMs. We release BBEH publicly at: https://github.com/google-deepmind/bbeh.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MEDDxAgent: A Unified Modular Agent Framework for Explainable Automatic
  Differential Diagnosis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.19175v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.19175v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daniel Rose, Chia-Chien Hung, Marco Lepri, Israa Alqassem, Kiril Gashteovski, Carolin Lawrence
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Differential Diagnosis (DDx) is a fundamental yet complex aspect of clinical
decision-making, in which physicians iteratively refine a ranked list of
possible diseases based on symptoms, antecedents, and medical knowledge. While
recent advances in large language models have shown promise in supporting DDx,
existing approaches face key limitations, including single-dataset evaluations,
isolated optimization of components, unrealistic assumptions about complete
patient profiles, and single-attempt diagnosis. We introduce a Modular
Explainable DDx Agent (MEDDxAgent) framework designed for interactive DDx,
where diagnostic reasoning evolves through iterative learning, rather than
assuming a complete patient profile is accessible. MEDDxAgent integrates three
modular components: (1) an orchestrator (DDxDriver), (2) a history taking
simulator, and (3) two specialized agents for knowledge retrieval and diagnosis
strategy. To ensure robust evaluation, we introduce a comprehensive DDx
benchmark covering respiratory, skin, and rare diseases. We analyze single-turn
diagnostic approaches and demonstrate the importance of iterative refinement
when patient profiles are not available at the outset. Our broad evaluation
demonstrates that MEDDxAgent achieves over 10% accuracy improvements in
interactive DDx across both large and small LLMs, while offering critical
explainability into its diagnostic reasoning process.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TestNUC: Enhancing Test-Time Computing Approaches through Neighboring
  Unlabeled Data Consistency 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.19163v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.19163v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Henry Peng Zou, Zhengyao Gu, Yue Zhou, Yankai Chen, Weizhi Zhang, Liancheng Fang, Yibo Wang, Yangning Li, Kay Liu, Philip S. Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Test-time computing approaches, which leverage additional computational
resources during inference, have been proven effective in enhancing large
language model performance. This work introduces a novel, linearly scaling
approach, TestNUC, that improves test-time predictions by leveraging the local
consistency of neighboring unlabeled data-it classifies an input instance by
considering not only the model's prediction on that instance but also on
neighboring unlabeled instances. We evaluate TestNUC across eight diverse
datasets, spanning intent classification, topic mining, domain discovery, and
emotion detection, demonstrating its consistent superiority over baseline
methods such as standard prompting and self-consistency. Furthermore, TestNUC
can be seamlessly integrated with existing test-time computing approaches,
substantially boosting their performance. Our analysis reveals that TestNUC
scales effectively with increasing amounts of unlabeled data and performs
robustly across different embedding models, making it practical for real-world
applications. Our code is available at https://github.com/HenryPengZou/TestNUC.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Detecting Linguistic Indicators for Stereotype Assessment with Large
  Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.19160v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.19160v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rebekka Görge, Michael Mock, Héctor Allende-Cid
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Social categories and stereotypes are embedded in language and can introduce
data bias into Large Language Models (LLMs). Despite safeguards, these biases
often persist in model behavior, potentially leading to representational harm
in outputs. While sociolinguistic research provides valuable insights into the
formation of stereotypes, NLP approaches for stereotype detection rarely draw
on this foundation and often lack objectivity, precision, and interpretability.
To fill this gap, in this work we propose a new approach that detects and
quantifies the linguistic indicators of stereotypes in a sentence. We derive
linguistic indicators from the Social Category and Stereotype Communication
(SCSC) framework which indicate strong social category formulation and
stereotyping in language, and use them to build a categorization scheme. To
automate this approach, we instruct different LLMs using in-context learning to
apply the approach to a sentence, where the LLM examines the linguistic
properties and provides a basis for a fine-grained assessment. Based on an
empirical evaluation of the importance of different linguistic indicators, we
learn a scoring function that measures the linguistic indicators of a
stereotype. Our annotations of stereotyped sentences show that these indicators
are present in these sentences and explain the strength of a stereotype. In
terms of model performance, our results show that the models generally perform
well in detecting and classifying linguistic indicators of category labels used
to denote a category, but sometimes struggle to correctly evaluate the
associated behaviors and characteristics. Using more few-shot examples within
the prompts, significantly improves performance. Model performance increases
with size, as Llama-3.3-70B-Instruct and GPT-4 achieve comparable results that
surpass those of Mixtral-8x7B-Instruct, GPT-4-mini and Llama-3.1-8B-Instruct.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ When Personalization Meets Reality: A Multi-Faceted Analysis of
  Personalized Preference Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.19158v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.19158v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yijiang River Dong, Tiancheng Hu, Yinhong Liu, Ahmet Üstün, Nigel Collier
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While Reinforcement Learning from Human Feedback (RLHF) is widely used to
align Large Language Models (LLMs) with human preferences, it typically assumes
homogeneous preferences across users, overlooking diverse human values and
minority viewpoints. Although personalized preference learning addresses this
by tailoring separate preferences for individual users, the field lacks
standardized methods to assess its effectiveness. We present a multi-faceted
evaluation framework that measures not only performance but also fairness,
unintended effects, and adaptability across varying levels of preference
divergence. Through extensive experiments comparing eight personalization
methods across three preference datasets, we demonstrate that performance
differences between methods could reach 36% when users strongly disagree, and
personalization can introduce up to 20% safety misalignment. These findings
highlight the critical need for holistic evaluation approaches to advance the
development of more effective and inclusive preference learning systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Isolating Language-Coding from Problem-Solving: Benchmarking LLMs with
  PseudoEval 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.19149v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.19149v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiarong Wu, Songqiang Chen, Jialun Cao, Hau Ching Lo, Shing-Chi Cheung
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing code generation benchmarks for Large Language Models (LLMs) such as
HumanEval and MBPP are designed to study LLMs' end-to-end performance, where
the benchmarks feed a problem description in natural language as input and
examine the generated code in specific programming languages. However, the
evaluation scores revealed in this way provide a little hint as to the
bottleneck of the code generation -- whether LLMs are struggling with their
problem-solving capability or language-coding capability. To answer this
question, we construct PseudoEval, a multilingual code generation benchmark
that provides a solution written in pseudocode as input. By doing so, the
bottleneck of code generation in various programming languages could be
isolated and identified. Our study yields several interesting findings. For
example, we identify that the bottleneck of LLMs in Python programming is
problem-solving, while Rust is struggling relatively more in language-coding.
Also, our study indicates that problem-solving capability may transfer across
programming languages, while language-coding needs more language-specific
effort, especially for undertrained programming languages. Finally, we release
the pipeline of constructing PseudoEval to facilitate the extension to existing
benchmarks. PseudoEval is available at:
https://anonymous.4open.science/r/PseudocodeACL25-7B74.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Amulet: ReAlignment During Test Time for Personalized Preference
  Adaptation of LLMs <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.19148v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.19148v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhaowei Zhang, Fengshuo Bai, Qizhi Chen, Chengdong Ma, Mingzhi Wang, Haoran Sun, Zilong Zheng, Yaodong Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  How to align large language models (LLMs) with user preferences from a static
general dataset has been frequently studied. However, user preferences are
usually personalized, changing, and diverse regarding culture, values, or time.
This leads to the problem that the actual user preferences often do not
coincide with those trained by the model developers in the practical use of
LLMs. Since we cannot collect enough data and retrain for every demand,
researching efficient real-time preference adaptation methods based on the
backbone LLMs during test time is important. To this end, we introduce Amulet,
a novel, training-free framework that formulates the decoding process of every
token as a separate online learning problem with the guidance of simple
user-provided prompts, thus enabling real-time optimization to satisfy users'
personalized preferences. To reduce the computational cost brought by this
optimization process for each token, we additionally provide a closed-form
solution for each iteration step of the optimization process, thereby reducing
the computational time cost to a negligible level. The detailed experimental
results demonstrate that Amulet can achieve significant performance
improvements in rich settings with combinations of different LLMs, datasets,
and user preferences, while maintaining acceptable computational efficiency.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICLR 2025, Project page:
  https://zowiezhang.github.io/projects/Amulet</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Self-Memory Alignment: Mitigating Factual Hallucinations with
  Generalized Improvement 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.19127v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.19127v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Siyuan Zhang, Yichi Zhang, Yinpeng Dong, Hang Su
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) often struggle to align their responses with
objective facts, resulting in the issue of factual hallucinations, which can be
difficult to detect and mislead users without relevant knowledge. While
post-training techniques have been employed to mitigate the issue, existing
methods usually suffer from poor generalization and trade-offs in different
capabilities. In this paper, we propose to address it by directly augmenting
LLM's fundamental ability to precisely leverage its existing memory--the
knowledge acquired from pre-training data. We introduce self-memory alignment
(SMA), which fine-tunes the model on self-generated responses to precise and
simple factual questions through preference optimization. Furthermore, we
construct FactualBench, a comprehensive and precise factual QA dataset
containing 181k Chinese data spanning 21 domains, to facilitate both evaluation
and training. Extensive experiments show that SMA significantly improves LLMs'
overall performance, with consistent enhancement across various benchmarks
concerning factuality, as well as helpfulness and comprehensive skills.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>29 pages, 17 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Improving customer service with automatic topic detection in user emails 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.19115v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.19115v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bojana Bašaragin, Darija Medvecki, Gorana Gojić, Milena Oparnica, Dragiša Mišković
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study introduces a novel Natural Language Processing pipeline that
enhances customer service efficiency at Telekom Srbija, a leading Serbian
telecommunications company, through automated email topic detection and
labelling. Central to the pipeline is BERTopic, a modular architecture that
allows unsupervised topic modelling. After a series of preprocessing and
post-processing steps, we assign one of 12 topics and several additional labels
to incoming emails, allowing customer service to filter and access them through
a custom-made application. The model's performance was evaluated by assessing
the speed and correctness of the automatically assigned topics across a test
dataset of 100 customer emails. The pipeline shows broad applicability across
languages, particularly for those that are low-resourced and morphologically
rich. The system now operates in the company's production environment,
streamlining customer service operations through automated email
classification.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Paper submitted to the 15th International Conference on Information
  Society and Technology (ICIST), Kopaonik, Serbia, 9-12 March 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Conformal Linguistic Calibration: Trading-off between Factuality and
  Specificity 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.19110v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.19110v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhengping Jiang, Anqi Liu, Benjamin Van Durme
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Language model outputs are not always reliable; this prompts research into
methods for adapting model responses based on uncertainty. Common approaches
include: \emph{abstention}, where models refrain from generating responses when
uncertain; and \emph{linguistic calibration}, where models hedge their
statements using uncertainty quantifiers. However, abstention can withhold
valuable information, while linguistically calibrated responses are often
challenging to leverage in downstream tasks. We propose a unifying view of both
approaches, Conformal Linguistic Calibration (CLC), reinterpreting linguistic
calibration as answer set prediction. We begin by presenting a unified
framework that connects abstention and linguistic calibration through the lens
of linguistic pragmatics. We then describe an implementation that allows for
controlling the level of imprecision in model responses. Experimental results
show that our method produces calibrated outputs with conformal guarantees on
factual accuracy. Furthermore, our approach enables fine-tuning models to
perform uncertainty-aware adaptive claim rewriting, offering a controllable
balance between factuality and specificity.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Evaluating Gender Bias in German Machine Translation <span class="chip">ISCA</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.19104v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.19104v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Michelle Kappl
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present WinoMTDE, a new gender bias evaluation test set designed to assess
occupational stereotyping and underrepresentation in German machine translation
(MT) systems. Building on the automatic evaluation method introduced by
arXiv:1906.00591v1 [cs.CL], we extend the approach to German, a language with
grammatical gender. The WinoMTDE dataset comprises 288 German sentences that
are balanced in regard to gender, as well as stereotype, which was annotated
using German labor statistics. We conduct a large-scale evaluation of five
widely used MT systems and a large language model. Our results reveal
persistent bias in most models, with the LLM outperforming traditional systems.
The dataset and evaluation code are publicly available under
https://github.com/michellekappl/mt_gender_german.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ISCA/ITG Workshop on Diversity in Large Speech and Language Models</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LongEval: A Comprehensive Analysis of Long-Text Generation Through a
  Plan-based Paradigm 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.19103v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.19103v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Siwei Wu, Yizhi Li, Xingwei Qu, Rishi Ravikumar, Yucheng Li, Tyler Loakman Shanghaoran Quan Xiaoyong Wei, Riza Batista-Navarro, Chenghua Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have achieved remarkable success in various
natural language processing tasks, yet their ability to generate long-form
content remains poorly understood and evaluated. Our analysis reveals that
current LLMs struggle with length requirements and information density in
long-text generation, with performance deteriorating as text length increases.
To quantitively locate such a performance degradation and provide further
insights on model development, we present LongEval, a benchmark that evaluates
long-text generation through both direct and plan-based generation paradigms,
inspired by cognitive and linguistic writing models. The comprehensive
experiments in this work reveal interesting findings such as that while model
size correlates with generation ability, the small-scale model (e.g.,
LongWriter), well-trained on long texts, has comparable performance. All code
and datasets are released in https://github.com/Wusiwei0410/LongEval.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Sparse Brains are Also Adaptive Brains: Cognitive-Load-Aware Dynamic
  Activation for LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.19078v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.19078v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiheng Yang, Yujie Wang, Chi Ma, Lei Yu, Emmanuele Chersoni, Chu-Ren Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Dense large language models(LLMs) face critical efficiency bottlenecks as
they rigidly activate all parameters regardless of input complexity. While
existing sparsity methods(static pruning or dynamic activation) address this
partially, they either lack adaptivity to contextual or model structural
demands or incur prohibitive computational overhead. Inspired by human brain's
dual-process mechanisms - predictive coding (N400) for backbone sparsity and
structural reanalysis (P600) for complex context - we propose CLADA, a
\textit{\textbf{C}ognitive-\textbf{L}oad-\textbf{A}ware \textbf{D}ynamic
\textbf{A}ctivation} framework that synergizes statistical sparsity with
semantic adaptability. Our key insight is that LLM activations exhibit two
complementary patterns: 1) \textit{Global statistical sparsity} driven by
sequence-level prefix information, and 2) \textit{Local semantic adaptability}
modulated by cognitive load metrics(e.g., surprisal and entropy). CLADA employs
a hierarchical thresholding strategy: a baseline from offline error-controlled
optimization ensures 40\%+ sparsity, dynamically adjusted by real-time
cognitive signals. Evaluations across six mainstream LLMs and nine benchmarks
demonstrate that CLADA achieves \textbf{~20\% average speedup with <2\%
accuracy drop}, outperforming Griffin (5\%+ degradation) and TT (negligible
speedup). Crucially, we establish the first formal connection between
neurolinguistic event-related potential (ERP) components and LLM efficiency
mechanisms through multi-level regression analysis ($R^2=0.17$ for
sparsity-adaptation synergy). Requiring no retraining or architectural changes,
CLADA offers a deployable solution for resource-aware LLM inference while
advancing biologically-inspired AI design. Our code is available at
\href{https://github.com/Oldify/CLADA}{CLADA}.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Improving the quality of Web-mined Parallel Corpora of Low-Resource
  Languages using Debiasing Heuristics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.19074v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.19074v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aloka Fernando, Surangika Ranathunga, Nisansa de Silva
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Parallel Data Curation (PDC) techniques aim to filter out noisy parallel
sentences from the web-mined corpora. Prior research has demonstrated that
ranking sentence pairs using similarity scores on sentence embeddings derived
from Pre-trained Multilingual Language Models (multiPLMs) and training the NMT
systems with the top-ranked samples, produces superior NMT performance than
when trained using the full dataset. However, previous research has shown that
the choice of multiPLM significantly impacts the ranking quality. This paper
investigates the reasons behind this disparity across multiPLMs. Using the
web-mined corpora CCMatrix and CCAligned for En$\rightarrow$Si,
En$\rightarrow$Ta and Si$\rightarrow$Ta, we show that different multiPLMs
(LASER3, XLM-R, and LaBSE) are biased towards certain types of sentences, which
allows noisy sentences to creep into the top-ranked samples. We show that by
employing a series of heuristics, this noise can be removed to a certain
extent. This results in improving the results of NMT systems trained with
web-mined corpora and reduces the disparity across multiPLMs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ IndicEval-XL: Bridging Linguistic Diversity in Code Generation Across
  Indic Languages 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.19067v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.19067v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ujjwal Singh, Aditi Sharma, Nikhil Gupta,  Deepakshi, Vivek Kumar Jha
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have demonstrated remarkable capabilities in
code generation from natural language prompts, revolutionizing software
development workflows. As we advance towards agent-based development paradigms,
these models form the cornerstone of next-generation software development
lifecycles. However, current benchmarks for evaluating multilingual code
generation capabilities are predominantly English-centric, limiting their
applicability across the global developer community. To address this
limitation, we present IndicEval-XL, a comprehensive benchmark for code
generation that incorporates 6 major Indic languages, collectively spoken by
approximately 14\% of the world's population. Our benchmark bridges these
languages with 12 programming languages, creating a robust evaluation
framework. This work is particularly significant given India's representation
of one-eighth of the global population and the crucial role Indic languages
play in Indian society. IndicEval-XL represents a significant step toward
expanding the linguistic diversity in code generation systems and evaluation
frameworks. By developing resources that support multiple languages, we aim to
make AI-powered development tools more inclusive and accessible to developers
of various linguistic backgrounds. To facilitate further research and
development in this direction, we make our dataset and evaluation benchmark
publicly available at https://github.com/telekom/IndicEval-XL
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Can Large Language Models Outperform Non-Experts in Poetry Evaluation? A
  Comparative Study Using the Consensual Assessment Technique 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.19064v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.19064v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Piotr Sawicki, Marek Grześ, Dan Brown, Fabrício Góes
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Consensual Assessment Technique (CAT) evaluates creativity through
holistic expert judgments. We investigate the use of two advanced Large
Language Models (LLMs), Claude-3-Opus and GPT-4o, to evaluate poetry by a
methodology inspired by the CAT. Using a dataset of 90 poems, we found that
these LLMs can surpass the results achieved by non-expert human judges at
matching a ground truth based on publication venue, particularly when assessing
smaller subsets of poems. Claude-3-Opus exhibited slightly superior performance
than GPT-4o. We show that LLMs are viable tools for accurately assessing
poetry, paving the way for their broader application into other creative
domains.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MathClean: A Benchmark for Synthetic Mathematical Data Cleaning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.19058v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.19058v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hao Liang, Meiyi Qiang, Yuying Li, Zefeng He, Yongzhen Guo, Zhengzhou Zhu, Wentao Zhang, Bin Cui
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the rapid development of large language models (LLMs), the quality of
training data has become crucial. Among the various types of training data,
mathematical data plays a key role in enabling LLMs to acquire strong reasoning
abilities. While high-quality open-source data is important, it is often
insufficient for pre-training, necessitating the addition of synthetic math
problems. However, synthetic math questions and answers can introduce
inaccuracies, which may degrade both the training data and web data. Therefore,
an effective method for cleaning synthetic math data is essential. In this
paper, we propose the MathClean benchmark to evaluate the effectiveness of math
data cleaning models. The MathClean benchmark consists of 2,000 correct
questions and 2,000 erroneous questions with additional 2,000 correct and
erroneous answers sourced from augmented data based on GSM8K and MATH.
Moreover, we also annotate error types for each question or answer, since it
can assess whether models can correctly identify the error categories for
future improvements. Finally, we present comprehensive evaluations using
state-of-the-art (SOTA) models. Our results demonstrate that even strong models
like GPT-o1 and DeepSeek-R1 perform poorly on this benchmark, highlighting the
utility of MathClean. Our code and data is available at
https://github.com/YuYingLi0/MathClean.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Ground-level Viewpoint Vision-and-Language Navigation in Continuous
  Environments <span class="chip">ICRA 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.19024v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.19024v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zerui Li, Gengze Zhou, Haodong Hong, Yanyan Shao, Wenqi Lyu, Yanyuan Qiao, Qi Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision-and-Language Navigation (VLN) empowers agents to associate
time-sequenced visual observations with corresponding instructions to make
sequential decisions. However, generalization remains a persistent challenge,
particularly when dealing with visually diverse scenes or transitioning from
simulated environments to real-world deployment. In this paper, we address the
mismatch between human-centric instructions and quadruped robots with a
low-height field of view, proposing a Ground-level Viewpoint Navigation (GVNav)
approach to mitigate this issue. This work represents the first attempt to
highlight the generalization gap in VLN across varying heights of visual
observation in realistic robot deployments. Our approach leverages weighted
historical observations as enriched spatiotemporal contexts for instruction
following, effectively managing feature collisions within cells by assigning
appropriate weights to identical features across different viewpoints. This
enables low-height robots to overcome challenges such as visual obstructions
and perceptual mismatches. Additionally, we transfer the connectivity graph
from the HM3D and Gibson datasets as an extra resource to enhance spatial
priors and a more comprehensive representation of real-world scenarios, leading
to improved performance and generalizability of the waypoint predictor in
real-world environments. Extensive experiments demonstrate that our
Ground-level Viewpoint Navigation (GVnav) approach significantly improves
performance in both simulated environments and real-world deployments with
quadruped robots.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICRA 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Binary Neural Networks for Large Language Model: A <span class="highlight-title">Survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.19008v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.19008v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Liangdong Liu, Zhitong Zheng, Cong Wang, Tianhuang Su, Zhenyu Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have wide applications in the field of natural
language processing(NLP), such as GPT-4 and Llama. However, with the
exponential growth of model parameter sizes, LLMs bring significant resource
overheads. Low-bit quantization, as a key technique, reduces memory usage and
computational demands by decreasing the bit-width of model parameters,
activations, and gradients. Previous quantization methods for LLMs have largely
employed Post-Training Quantization (PTQ) and Quantization-Aware Training
(QAT). PTQ does not require any retraining of the original model, while QAT
involves optimizing precision during training to achieve the best quantization
parameters. The BitNet team proposed a radically different approach, where
quantization is performed from the start of model training, utilizing
low-precision binary weights during the training process. This approach has led
to the emergence of many binary quantization techniques for large language
models. This paper provides a comprehensive review of these binary quantization
techniques. Specifically, we will introduce binary quantization techniques in
deep neural networks and further explore their application to LLMs, reviewing
their various contributions, implementations, and applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>23 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MEBench: Benchmarking Large Language Models for Cross-Document
  Multi-Entity Question Answering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.18993v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.18993v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Teng Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-entity question answering (MEQA) represents significant challenges for
large language models (LLM) and retrieval-augmented generation (RAG) systems,
which frequently struggle to consolidate scattered information across diverse
documents. While existing methods excel at single-document comprehension, they
often struggle with cross-document aggregation, particularly when resolving
entity-dense questions like "What is the distribution of ACM Fellows among
various fields of study?", which require integrating entity-centric insights
from heterogeneous sources (e.g., Wikipedia pages). To address this gap, we
introduce MEBench, a novel multi-document, multi-entity benchmark designed to
systematically evaluate LLMs' capacity to retrieve, consolidate, and reason
over fragmented information. Our benchmark comprises 4,780 questions which are
systematically categorized into three primary categories, further divided into
eight distinct types, ensuring broad coverage of real-world multi-entity
reasoning scenarios. Our experiments on state-of-the-art LLMs (e.g., GPT-4,
Llama-3) and RAG pipelines reveal critical limitations: even advanced models
achieve only 59% accuracy on MEBench. Our benchmark emphasizes the importance
of completeness and factual precision of information extraction in MEQA tasks,
using Entity-Attributed F1 (EA-F1) metric for granular evaluation of
entity-level correctness and attribution validity. MEBench not only highlights
systemic weaknesses in current LLM frameworks but also provides a foundation
for advancing robust, entity-aware QA architectures.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GenTool: Enhancing Tool Generalization in Language Models through
  Zero-to-One and Weak-to-Strong Simulation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.18990v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.18990v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jie He, Jennifer Neville, Mengting Wan, Longqi Yang, Hui Liu, Xiaofeng Xu, Xia Song, Jeff Z. Pan, Pei Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) can enhance their capabilities as AI assistants
by integrating external tools, allowing them to access a wider range of
information. While recent LLMs are typically fine-tuned with tool usage
examples during supervised fine-tuning (SFT), questions remain about their
ability to develop robust tool-usage skills and can effectively generalize to
unseen queries and tools. In this work, we present GenTool, a novel training
framework that prepares LLMs for diverse generalization challenges in tool
utilization. Our approach addresses two fundamental dimensions critical for
real-world applications: Zero-to-One Generalization, enabling the model to
address queries initially lacking a suitable tool by adopting and utilizing one
when it becomes available, and Weak-to-Strong Generalization, allowing models
to leverage enhanced versions of existing tools to solve queries. To achieve
this, we develop synthetic training data simulating these two dimensions of
tool usage and introduce a two-stage fine-tuning approach: optimizing tool
ranking, then refining tool selection. Through extensive experiments across
four generalization scenarios, we demonstrate that our method significantly
enhances the tool-usage capabilities of LLMs ranging from 1B to 8B parameters,
achieving performance that surpasses GPT-4o. Furthermore, our analysis also
provides valuable insights into the challenges LLMs encounter in tool
generalization.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PEToolLLM: Towards Personalized Tool Learning in Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.18980v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.18980v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qiancheng Xu, Yongqi Li, Heming Xia, Fan Liu, Min Yang, Wenjie Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Tool learning has emerged as a promising direction by extending Large
Language Models' (LLMs) capabilities with external tools. Existing tool
learning studies primarily focus on the general-purpose tool-use capability,
which addresses explicit user requirements in instructions. However, they
overlook the importance of personalized tool-use capability, leading to an
inability to handle implicit user preferences. To address the limitation, we
first formulate the task of personalized tool learning, which integrates user's
interaction history towards personalized tool usage. To fill the gap of missing
benchmarks, we construct PEToolBench, featuring diverse user preferences
reflected in interaction history under three distinct personalized settings,
and encompassing a wide range of tool-use scenarios. Moreover, we propose a
framework PEToolLLaMA to adapt LLMs to the personalized tool learning task,
which is trained through supervised fine-tuning and direct preference
optimization. Extensive experiments on PEToolBench demonstrate the superiority
of PEToolLLaMA over existing LLMs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Low-Confidence Gold: Refining Low-Confidence Samples for Efficient
  Instruction Tuning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.18978v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.18978v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hongyi Cal, ie Li, Wenzhen Dong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The effectiveness of instruction fine-tuning for Large Language Models is
fundamentally constrained by the quality and efficiency of training datasets.
This work introduces Low-Confidence Gold (LCG), a novel filtering framework
that employs centroid-based clustering and confidence-guided selection for
identifying valuable instruction pairs. Through a semi-supervised approach
using a lightweight classifier trained on representative samples, LCG curates
high-quality subsets while preserving data diversity. Experimental evaluation
demonstrates that models fine-tuned on LCG-filtered subsets of 6K samples
achieve superior performance compared to existing methods, with substantial
improvements on MT-bench and consistent gains across comprehensive evaluation
metrics. The framework's efficacy while maintaining model performance
establishes a promising direction for efficient instruction tuning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ (Mis)Fitting: A <span class="highlight-title">Survey</span> of Scaling Laws <span class="chip">ICLR</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.18969v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.18969v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Margaret Li, Sneha Kudugunta, Luke Zettlemoyer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Modern foundation models rely heavily on using scaling laws to guide crucial
training decisions. Researchers often extrapolate the optimal architecture and
hyper parameters settings from smaller training runs by describing the
relationship between, loss, or task performance, and scale. All components of
this process vary, from the specific equation being fit, to the training setup,
to the optimization method. Each of these factors may affect the fitted law,
and therefore, the conclusions of a given study. We discuss discrepancies in
the conclusions that several prior works reach, on questions such as the
optimal token to parameter ratio. We augment this discussion with our own
analysis of the critical impact that changes in specific details may effect in
a scaling study, and the resulting altered conclusions. Additionally, we survey
over 50 papers that study scaling trends: while 45 of these papers quantify
these trends using a power law, most under-report crucial details needed to
reproduce their findings. To mitigate this, we we propose a checklist for
authors to consider while contributing to scaling law research.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>41 pages, 3 figure, first two authors contributed equally. ICLR, 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Know You First and Be You Better: Modeling Human-Like User Simulators
  via Implicit Profiles 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.18968v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.18968v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kuang Wang, Xianfei Li, Shenghao Yang, Li Zhou, Feng Jiang, Haizhou Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  User simulators are crucial for replicating human interactions with dialogue
systems, supporting both collaborative training and automatic evaluation,
especially for large language models (LLMs). However, existing simulators often
rely solely on text utterances, missing implicit user traits such as
personality, speaking style, and goals. In contrast, persona-based methods lack
generalizability, as they depend on predefined profiles of famous individuals
or archetypes. To address these challenges, we propose User Simulator with
implicit Profiles (USP), a framework that infers implicit user profiles from
human-machine conversations and uses them to generate more personalized and
realistic dialogues. We first develop an LLM-driven extractor with a
comprehensive profile schema. Then, we refine the simulation through
conditional supervised fine-tuning and reinforcement learning with cycle
consistency, optimizing it at both the utterance and conversation levels.
Finally, we adopt a diverse profile sampler to capture the distribution of
real-world user profiles. Experimental results demonstrate that USP outperforms
strong baselines in terms of authenticity and diversity while achieving
comparable performance in consistency. Furthermore, dynamic multi-turn
evaluations based on USP strongly align with mainstream benchmarks,
demonstrating its effectiveness in real-world applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Label-Only Membership Inference Attack against <span class="highlight-title">Pre-train</span>ed Large
  Language Models <span class="chip">USENIX Security 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.18943v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.18943v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yu He, Boheng Li, Liu Liu, Zhongjie Ba, Wei Dong, Yiming Li, Zhan Qin, Kui Ren, Chun Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Membership Inference Attacks (MIAs) aim to predict whether a data sample
belongs to the model's training set or not. Although prior research has
extensively explored MIAs in Large Language Models (LLMs), they typically
require accessing to complete output logits (\ie, \textit{logits-based
attacks}), which are usually not available in practice. In this paper, we study
the vulnerability of pre-trained LLMs to MIAs in the \textit{label-only
setting}, where the adversary can only access generated tokens (text). We first
reveal that existing label-only MIAs have minor effects in attacking
pre-trained LLMs, although they are highly effective in inferring fine-tuning
datasets used for personalized LLMs. We find that their failure stems from two
main reasons, including better generalization and overly coarse perturbation.
Specifically, due to the extensive pre-training corpora and exposing each
sample only a few times, LLMs exhibit minimal robustness differences between
members and non-members. This makes token-level perturbations too coarse to
capture such differences.
  To alleviate these problems, we propose \textbf{PETAL}: a label-only
membership inference attack based on \textbf{PE}r-\textbf{T}oken
sem\textbf{A}ntic simi\textbf{L}arity. Specifically, PETAL leverages
token-level semantic similarity to approximate output probabilities and
subsequently calculate the perplexity. It finally exposes membership based on
the common assumption that members are `better' memorized and have smaller
perplexity. We conduct extensive experiments on the WikiMIA benchmark and the
more challenging MIMIR benchmark. Empirically, our PETAL performs better than
the extensions of existing label-only attacks against personalized LLMs and
even on par with other advanced logit-based attacks across all metrics on five
prevalent open-source LLMs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by USENIX Security 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MathTutorBench: A Benchmark for Measuring Open-ended Pedagogical
  Capabilities of LLM Tutors 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.18940v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.18940v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jakub Macina, Nico Daheim, Ido Hakimi, Manu Kapur, Iryna Gurevych, Mrinmaya Sachan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Evaluating the pedagogical capabilities of AI-based tutoring models is
critical for making guided progress in the field. Yet, we lack a reliable,
easy-to-use, and simple-to-run evaluation that reflects the pedagogical
abilities of models. To fill this gap, we present MathTutorBench, an
open-source benchmark for holistic tutoring model evaluation. MathTutorBench
contains a collection of datasets and metrics that broadly cover tutor
abilities as defined by learning sciences research in dialog-based teaching. To
score the pedagogical quality of open-ended teacher responses, we train a
reward model and show it can discriminate expert from novice teacher responses
with high accuracy. We evaluate a wide set of closed- and open-weight models on
MathTutorBench and find that subject expertise, indicated by solving ability,
does not immediately translate to good teaching. Rather, pedagogy and subject
expertise appear to form a trade-off that is navigated by the degree of
tutoring specialization of the model. Furthermore, tutoring appears to become
more challenging in longer dialogs, where simpler questioning strategies begin
to fail. We release the benchmark, code, and leaderboard openly to enable rapid
benchmarking of future models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>https://eth-lre.github.io/mathtutorbench</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ JailBench: A Comprehensive Chinese Security Assessment Benchmark for
  Large Language Models <span class="chip">PAKDD 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.18935v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.18935v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shuyi Liu, Simiao Cui, Haoran Bu, Yuming Shang, Xi Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have demonstrated remarkable capabilities across
various applications, highlighting the urgent need for comprehensive safety
evaluations. In particular, the enhanced Chinese language proficiency of LLMs,
combined with the unique characteristics and complexity of Chinese expressions,
has driven the emergence of Chinese-specific benchmarks for safety assessment.
However, these benchmarks generally fall short in effectively exposing LLM
safety vulnerabilities. To address the gap, we introduce JailBench, the first
comprehensive Chinese benchmark for evaluating deep-seated vulnerabilities in
LLMs, featuring a refined hierarchical safety taxonomy tailored to the Chinese
context. To improve generation efficiency, we employ a novel Automatic
Jailbreak Prompt Engineer (AJPE) framework for JailBench construction, which
incorporates jailbreak techniques to enhance assessing effectiveness and
leverages LLMs to automatically scale up the dataset through context-learning.
The proposed JailBench is extensively evaluated over 13 mainstream LLMs and
achieves the highest attack success rate against ChatGPT compared to existing
Chinese benchmarks, underscoring its efficacy in identifying latent
vulnerabilities in LLMs, as well as illustrating the substantial room for
improvement in the security and trustworthiness of LLMs within the Chinese
context. Our benchmark is publicly available at
https://github.com/STAIR-BUPT/JailBench.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 5 figures, accepted at PAKDD 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Kanana: Compute-efficient Bilingual Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.18934v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.18934v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                         Kanana LLM Team, Yunju Bak, Hojin Lee, Minho Ryu, Jiyeon Ham, Seungjae Jung, Daniel Wontae Nam, Taegyeong Eo, Donghun Lee, Doohae Jung, Boseop Kim, Nayeon Kim, Jaesun Park, Hyunho Kim, Hyunwoong Ko, Changmin Lee, Kyoung-Woon On, Seulye Baeg, Junrae Cho, Sunghee Jung, Jieun Kang, EungGyun Kim, Eunhwa Kim, Byeongil Ko, Daniel Lee, Minchul Lee, Miok Lee, Shinbok Lee, Gaeun Seo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce Kanana, a series of bilingual language models that demonstrate
exceeding performance in Korean and competitive performance in English. The
computational cost of Kanana is significantly lower than that of
state-of-the-art models of similar size. The report details the techniques
employed during pre-training to achieve compute-efficient yet competitive
models, including high quality data filtering, staged pre-training, depth
up-scaling, and pruning and distillation. Furthermore, the report outlines the
methodologies utilized during the post-training of the Kanana models,
encompassing supervised fine-tuning and preference optimization, aimed at
enhancing their capability for seamless interaction with users. Lastly, the
report elaborates on plausible approaches used for language model adaptation to
specific scenarios, such as embedding, retrieval augmented generation, and
function calling. The Kanana model series spans from 2.1B to 32.5B parameters
with 2.1B models (base, instruct, embedding) publicly released to promote
research on Korean language models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>40 pages, 15 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ END: Early Noise Dropping for Efficient and Effective Context Denoising 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.18915v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.18915v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hongye Jin, Pei Chen, Jingfeng Yang, Zhengyang Wang, Meng Jiang, Yifan Gao, Binxuan Huang, Xinyang Zhang, Zheng Li, Tianyi Liu, Huasheng Li, Bing Yin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have demonstrated remarkable performance across
a wide range of natural language processing tasks. However, they are often
distracted by irrelevant or noisy context in input sequences that degrades
output quality. This problem affects both long- and short-context scenarios,
such as retrieval-augmented generation, table question-answering, and
in-context learning. We reveal that LLMs can implicitly identify whether input
sequences contain useful information at early layers, prior to token
generation. Leveraging this insight, we introduce Early Noise Dropping
(\textsc{END}), a novel approach to mitigate this issue without requiring
fine-tuning the LLMs. \textsc{END} segments input sequences into chunks and
employs a linear prober on the early layers of LLMs to differentiate between
informative and noisy chunks. By discarding noisy chunks early in the process,
\textsc{END} preserves critical information, reduces distraction, and lowers
computational overhead. Extensive experiments demonstrate that \textsc{END}
significantly improves both performance and efficiency across different LLMs on
multiple evaluation datasets. Furthermore, by investigating LLMs' implicit
understanding to the input with the prober, this work also deepens
understanding of how LLMs do reasoning with contexts internally.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CS-Dialogue: A 104-Hour <span class="highlight-title">Dataset</span> of Spontaneous Mandarin-English
  Code-Switching Dialogues for Speech Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.18913v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.18913v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiaming Zhou, Yujie Guo, Shiwan Zhao, Haoqin Sun, Hui Wang, Jiabei He, Aobo Kong, Shiyao Wang, Xi Yang, Yequan Wang, Yonghua Lin, Yong Qin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Code-switching (CS), the alternation between two or more languages within a
single conversation, presents significant challenges for automatic speech
recognition (ASR) systems. Existing Mandarin-English code-switching datasets
often suffer from limitations in size, spontaneity, and the lack of full-length
dialogue recordings with transcriptions, hindering the development of robust
ASR models for real-world conversational scenarios. This paper introduces
CS-Dialogue, a novel large-scale Mandarin-English code-switching speech dataset
comprising 104 hours of spontaneous conversations from 200 speakers. Unlike
previous datasets, CS-Dialogue provides full-length dialogue recordings with
complete transcriptions, capturing naturalistic code-switching patterns in
continuous speech. We describe the data collection and annotation processes,
present detailed statistics of the dataset, and establish benchmark ASR
performance using state-of-the-art models. Our experiments, using Transformer,
Conformer, and Branchformer, demonstrate the challenges of code-switching ASR,
and show that existing pre-trained models such as Whisper still have the space
to improve. The CS-Dialogue dataset will be made freely available for all
academic purposes.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ From Hours to Minutes: Lossless Acceleration of Ultra Long Sequence
  Generation up to 100K Tokens 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.18890v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.18890v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tong Wu, Junzhe Shen, Zixia Jia, Yuxuan Wang, Zilong Zheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generating ultra-long sequences with large language models (LLMs) has become
increasingly crucial but remains a highly time-intensive task, particularly for
sequences up to 100K tokens. While traditional speculative decoding methods
exist, simply extending their generation limits fails to accelerate the process
and can be detrimental. Through an in-depth analysis, we identify three major
challenges hindering efficient generation: frequent model reloading, dynamic
key-value (KV) management and repetitive generation. To address these issues,
we introduce TOKENSWIFT, a novel framework designed to substantially accelerate
the generation process of ultra-long sequences while maintaining the target
model's inherent quality. Experimental results demonstrate that TOKENSWIFT
achieves over 3 times speedup across models of varying scales (1.5B, 7B, 8B,
14B) and architectures (MHA, GQA). This acceleration translates to hours of
time savings for ultra-long sequence generation, establishing TOKENSWIFT as a
scalable and effective solution at unprecedented lengths. Code can be found at
https://github.com/bigai-nlco/TokenSwift.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Clip-TTS: Contrastive Text-content and Mel-spectrogram, A High-Huality
  Text-to-Speech Method based on Contextual Semantic Understanding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.18889v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.18889v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tianyun Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Traditional text-to-speech (TTS) methods primarily focus on establishing a
mapping between phonemes and mel-spectrograms. However, during the phoneme
encoding stage, there is often a lack of real mel-spectrogram auxiliary
information, which results in the encoding process lacking true semantic
understanding. At the same time, traditional TTS systems often struggle to
balance the inference speed of the model with the quality of the synthesized
speech. Methods that generate high-quality synthesized speech tend to have
slower inference speeds, while faster inference methods often sacrifice speech
quality. In this paper, I propose Clip-TTS, a TTS method based on the Clip
architecture. This method uses the Clip framework to establish a connection
between text content and real mel-spectrograms during the text encoding stage,
enabling the text encoder to directly learn the true semantics of the global
context, thereby ensuring the quality of the synthesized speech. In terms of
model architecture, I adopt the basic structure of Transformer, which allows
Clip-TTS to achieve fast inference speeds. Experimental results show that on
the LJSpeech and Baker datasets, the speech generated by Clip-TTS achieves
state-of-the-art MOS scores, and it also performs excellently on multi-emotion
datasets.Audio samples are available at: https://ltydd1314.github.io/.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On Pruning State-Space LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.18886v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.18886v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tamer Ghattas, Michael Hassid, Roy Schwartz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent work proposed state-space models (SSMs) as an efficient alternative to
transformer-based LLMs. Can these models be pruned to further reduce their
computation costs? We adapt several pruning methods to the SSM structure, and
apply them to four SSM-based LLMs across multiple tasks. We find that such
models are quite robust to some pruning methods (e.g. WANDA), while using other
methods lead to fast performance degradation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning to Generate Structured Output with Schema Reinforcement
  Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.18878v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.18878v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yaxi Lu, Haolun Li, Xin Cong, Zhong Zhang, Yesai Wu, Yankai Lin, Zhiyuan Liu, Fangming Liu, Maosong Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study investigates the structured generation capabilities of large
language models (LLMs), focusing on producing valid JSON outputs against a
given schema. Despite the widespread use of JSON in integrating language models
with programs, there is a lack of comprehensive analysis and benchmarking of
these capabilities. We explore various aspects of JSON generation, such as
structure understanding, escaping, and natural language description, to
determine how to assess and enable LLMs to generate valid responses. Building
upon this, we propose SchemaBench features around 40K different JSON schemas to
obtain and assess models' abilities in generating valid JSON. We find that the
latest LLMs are still struggling to generate a valid JSON string. Moreover, we
demonstrate that incorporating reinforcement learning with a Fine-grained
Schema Validator can further enhance models' understanding of JSON schema,
leading to improved performance. Our models demonstrate significant improvement
in both generating JSON outputs and downstream tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning to Align Multi-Faceted Evaluation: A Unified and Robust
  Framework 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.18874v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.18874v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kaishuai Xu, Tiezheng Yu, Wenjun Hou, Yi Cheng, Liangyou Li, Xin Jiang, Lifeng Shang, Qun Liu, Wenjie Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) are being used more and more extensively for
automated evaluation in various scenarios. Previous studies have attempted to
fine-tune open-source LLMs to replicate the evaluation explanations and
judgments of powerful proprietary models, such as GPT-4. However, these methods
are largely limited to text-based analyses under predefined general criteria,
resulting in reduced adaptability for unseen instructions and demonstrating
instability in evaluating adherence to quantitative and structural constraints.
To address these limitations, we propose a novel evaluation framework, ARJudge,
that adaptively formulates evaluation criteria and synthesizes both text-based
and code-driven analyses to evaluate LLM responses. ARJudge consists of two
components: a fine-tuned Analyzer that generates multi-faceted evaluation
analyses and a tuning-free Refiner that combines and refines all analyses to
make the final judgment. We construct a Composite Analysis Corpus that
integrates tasks for evaluation criteria generation alongside text-based and
code-driven analysis generation to train the Analyzer. Our results demonstrate
that ARJudge outperforms existing fine-tuned evaluators in effectiveness and
robustness. Furthermore, it demonstrates the importance of multi-faceted
evaluation and code-driven analyses in enhancing evaluation capabilities.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multi-LLM Collaborative Search for Complex Problem Solving 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.18873v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.18873v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sen Yang, Yafu Li, Wai Lam, Yu Cheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) often struggle with complex reasoning tasks due
to their limitations in addressing the vast reasoning space and inherent
ambiguities of natural language. We propose the Mixture-of-Search-Agents (MoSA)
paradigm, a novel approach leveraging the collective expertise of multiple LLMs
to enhance search-based reasoning. MoSA integrates diverse reasoning pathways
by combining independent exploration with iterative refinement among LLMs,
mitigating the limitations of single-model approaches. Using Monte Carlo Tree
Search (MCTS) as a backbone, MoSA enables multiple agents to propose and
aggregate reasoning steps, resulting in improved accuracy. Our comprehensive
evaluation across four reasoning benchmarks demonstrates MoSA's consistent
performance improvements over single-agent and other multi-agent baselines,
particularly in complex mathematical and commonsense reasoning tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards an AI co-scientist 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.18864v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.18864v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Juraj Gottweis, Wei-Hung Weng, Alexander Daryin, Tao Tu, Anil Palepu, Petar Sirkovic, Artiom Myaskovsky, Felix Weissenberger, Keran Rong, Ryutaro Tanno, Khaled Saab, Dan Popovici, Jacob Blum, Fan Zhang, Katherine Chou, Avinatan Hassidim, Burak Gokturk, Amin Vahdat, Pushmeet Kohli, Yossi Matias, Andrew Carroll, Kavita Kulkarni, Nenad Tomasev, Yuan Guan, Vikram Dhillon, Eeshit Dhaval Vaishnav, Byron Lee, Tiago R D Costa, José R Penadés, Gary Peltz, Yunhan Xu, Annalisa Pawlosky, Alan Karthikesalingam, Vivek Natarajan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Scientific discovery relies on scientists generating novel hypotheses that
undergo rigorous experimental validation. To augment this process, we introduce
an AI co-scientist, a multi-agent system built on Gemini 2.0. The AI
co-scientist is intended to help uncover new, original knowledge and to
formulate demonstrably novel research hypotheses and proposals, building upon
prior evidence and aligned to scientist-provided research objectives and
guidance. The system's design incorporates a generate, debate, and evolve
approach to hypothesis generation, inspired by the scientific method and
accelerated by scaling test-time compute. Key contributions include: (1) a
multi-agent architecture with an asynchronous task execution framework for
flexible compute scaling; (2) a tournament evolution process for self-improving
hypotheses generation. Automated evaluations show continued benefits of
test-time compute, improving hypothesis quality. While general purpose, we
focus development and validation in three biomedical areas: drug repurposing,
novel target discovery, and explaining mechanisms of bacterial evolution and
anti-microbial resistance. For drug repurposing, the system proposes candidates
with promising validation findings, including candidates for acute myeloid
leukemia that show tumor inhibition in vitro at clinically applicable
concentrations. For novel target discovery, the AI co-scientist proposed new
epigenetic targets for liver fibrosis, validated by anti-fibrotic activity and
liver cell regeneration in human hepatic organoids. Finally, the AI
co-scientist recapitulated unpublished experimental results via a parallel in
silico discovery of a novel gene transfer mechanism in bacterial evolution.
These results, detailed in separate, co-timed reports, demonstrate the
potential to augment biomedical and scientific discovery and usher an era of AI
empowered scientists.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>81 pages in total (main 38 pages, appendix 43 pages), 13 main
  figures, 40 appendix figures, 1 main table, 2 appendix tables, 143 main
  references, 7 appendix references</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Exploring Rewriting Approaches for Different Conversational Tasks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.18860v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.18860v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Md Mehrab Tanjim, Ryan A. Rossi, Mike Rimer, Xiang Chen, Sungchul Kim, Vaishnavi Muppala, Tong Yu, Zhengmian Hu, Ritwik Sinha, Wei Zhang, Iftikhar Ahamath Burhanuddin, Franck Dernoncourt
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Conversational assistants often require a question rewriting algorithm that
leverages a subset of past interactions to provide a more meaningful (accurate)
answer to the user's question or request. However, the exact rewriting approach
may often depend on the use case and application-specific tasks supported by
the conversational assistant, among other constraints. In this paper, we
systematically investigate two different approaches, denoted as rewriting and
fusion, on two fundamentally different generation tasks, including a
text-to-text generation task and a multimodal generative task that takes as
input text and generates a visualization or data table that answers the user's
question. Our results indicate that the specific rewriting or fusion approach
highly depends on the underlying use case and generative task. In particular,
we find that for a conversational question-answering assistant, the query
rewriting approach performs best, whereas for a data analysis assistant that
generates visualizations and data tables based on the user's conversation with
the assistant, the fusion approach works best. Notably, we explore two datasets
for the data analysis assistant use case, for short and long conversations, and
we find that query fusion always performs better, whereas for the
conversational text-based question-answering, the query rewrite approach
performs best.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Causal Lens for Evaluating Faithfulness Metrics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.18848v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.18848v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kerem Zaman, Shashank Srivastava
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) offer natural language explanations as an
alternative to feature attribution methods for model interpretability. However,
despite their plausibility, they may not reflect the model's internal reasoning
faithfully, which is crucial for understanding the model's true decision-making
processes. Although several faithfulness metrics have been proposed, a unified
evaluation framework remains absent. To address this gap, we present Causal
Diagnosticity, a framework to evaluate faithfulness metrics for natural
language explanations. Our framework employs the concept of causal
diagnosticity, and uses model-editing methods to generate faithful-unfaithful
explanation pairs. Our benchmark includes four tasks: fact-checking, analogy,
object counting, and multi-hop reasoning. We evaluate a variety of faithfulness
metrics, including post-hoc explanation and chain-of-thought-based methods. We
find that all tested faithfulness metrics often fail to surpass a random
baseline. Our work underscores the need for improved metrics and more reliable
interpretability methods in LLMs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages, 18 figures, 6 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Sliding Window Attention Training for Efficient Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.18845v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.18845v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zichuan Fu, Wentao Song, Yejing Wang, Xian Wu, Yefeng Zheng, Yingying Zhang, Derong Xu, Xuetao Wei, Tong Xu, Xiangyu Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in transformer-based Large Language Models (LLMs) have
demonstrated remarkable capabilities across various tasks. However, their
quadratic computational complexity concerning sequence length remains a
significant bottleneck for processing long documents. As a result, many efforts
like sparse attention and state space models have been proposed to improve the
efficiency of LLMs over long sequences. Though effective, these approaches
compromise the performance or introduce structural complexity. This calls for a
simple yet efficient model that preserves the fundamental Transformer
architecture. To this end, we introduce SWAT, which enables efficient
long-context handling via Sliding Window Attention Training. This paper first
attributes the inefficiency of Transformers to the attention sink phenomenon
resulting from the high variance of softmax operation. Then, we replace softmax
with the sigmoid function and utilize a balanced ALiBi and Rotary Position
Embedding for efficient information compression and retention. Experiments
demonstrate that SWAT achieves SOTA performance compared with state-of-the-art
linear recurrent architectures on eight benchmarks. Code is available at
https://anonymous.4open.science/r/SWAT-attention.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Sentiment Analysis of Movie <span class="highlight-title">Review</span>s Using <span class="highlight-title">BERT</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.18841v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.18841v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gibson Nkhata, Usman Anjum, Justin Zhan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sentiment Analysis (SA) or opinion mining is analysis of emotions and
opinions from any kind of text. SA helps in tracking peoples viewpoints and it
is an important factor when it comes to social media monitoring product and
brand recognition customer satisfaction customer loyalty advertising and
promotions success and product acceptance. That is why SA is one of the active
research areas in Natural Language Processing (NLP). SA is applied on data
sourced from various media platforms to mine sentiment knowledge from them.
Various approaches have been deployed in the literature to solve the problem.
Most techniques devise complex and sophisticated frameworks in order to attain
optimal accuracy. This work aims to finetune Bidirectional Encoder
Representations from Transformers (BERT) with Bidirectional Long Short-Term
Memory (BiLSTM) for movie reviews sentiment analysis and still provide better
accuracy than the State-of-The-Art (SOTA) methods. The paper also shows how
sentiment analysis can be applied if someone wants to recommend a certain movie
for example by computing overall polarity of its sentiments predicted by the
model. That is our proposed method serves as an upper-bound baseline in
prediction of a predominant reaction to a movie. To compute overall polarity a
heuristic algorithm is applied to BERTBiLSTM output vector. Our model can be
extended to three-class four-class or any fine-grained classification and apply
overall polarity computation again. This is intended to be exploited in future
work.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 3 figures, published in the proceedings The Fifteenth
  International Conference on Information, Process, and Knowledge Management
  (eKNOW 2023)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Evidence-Driven Marker Extraction for Social Media Suicide Risk
  Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.18823v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.18823v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Carter Adams, Caleb Carter, Jackson Simmons
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Early detection of suicide risk from social media text is crucial for timely
intervention. While Large Language Models (LLMs) offer promising capabilities
in this domain, challenges remain in terms of interpretability and
computational efficiency. This paper introduces Evidence-Driven LLM (ED-LLM), a
novel approach for clinical marker extraction and suicide risk classification.
ED-LLM employs a multi-task learning framework, jointly training a Mistral-7B
based model to identify clinical marker spans and classify suicide risk levels.
This evidence-driven strategy enhances interpretability by explicitly
highlighting textual evidence supporting risk assessments. Evaluated on the
CLPsych datasets, ED-LLM demonstrates competitive performance in risk
classification and superior capability in clinical marker span identification
compared to baselines including fine-tuned LLMs, traditional machine learning,
and prompt-based methods. The results highlight the effectiveness of multi-task
learning for interpretable and efficient LLM-based suicide risk assessment,
paving the way for clinically relevant applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Judge as A Judge: Improving the Evaluation of Retrieval-Augmented
  Generation through the Judge-Consistency of Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.18817v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.18817v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shuliang Liu, Xinze Li, Zhenghao Liu, Yukun Yan, Cheng Yang, Zheni Zeng, Zhiyuan Liu, Maosong Sun, Ge Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Retrieval-Augmented Generation (RAG) has proven its effectiveness in
alleviating hallucinations for Large Language Models (LLMs). However, existing
automated evaluation metrics cannot fairly evaluate the outputs generated by
RAG models during training and evaluation. LLM-based judgment models provide
the potential to produce high-quality judgments, but they are highly sensitive
to evaluation prompts, leading to inconsistencies when judging the output of
RAG models. This paper introduces the Judge-Consistency (ConsJudge) method,
which aims to enhance LLMs to generate more accurate evaluations for RAG
models. Specifically, ConsJudge prompts LLMs to generate different judgments
based on various combinations of judgment dimensions, utilize the
judge-consistency to evaluate these judgments and select the accepted and
rejected judgments for DPO training. Our experiments show that ConsJudge can
effectively provide more accurate judgments for optimizing RAG models across
various RAG models and datasets. Further analysis reveals that judgments
generated by ConsJudge have a high agreement with the superior LLM. All codes
are available at https://github.com/OpenBMB/ConsJudge.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Holistic Audit <span class="highlight-title">Dataset</span> Generation for LLM Unlearning via Knowledge Graph
  Traversal and Redundancy Removal 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.18810v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.18810v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weipeng Jiang, Juan Zhai, Shiqing Ma, Ziyan Lei, Xiaofei Xie, Yige Wang, Chao Shen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, Large Language Models (LLMs) have faced increasing demands
to selectively remove sensitive information, protect privacy, and comply with
copyright regulations through unlearning, by Machine Unlearning. While
evaluating unlearning effectiveness is crucial, existing benchmarks are limited
in scale and comprehensiveness, typically containing only a few hundred test
cases. We identify two critical challenges in generating holistic audit
datasets: ensuring audit adequacy and handling knowledge redundancy between
forget and retain dataset. To address these challenges, we propose HANKER, an
automated framework for holistic audit dataset generation leveraging knowledge
graphs to achieve fine-grained coverage and eliminate redundant knowledge.
Applying HANKER to the popular MUSE benchmark, we successfully generated over
69,000 and 111,000 audit cases for the News and Books datasets respectively,
identifying thousands of knowledge memorization instances that the previous
benchmark failed to detect. Our empirical analysis uncovers how knowledge
redundancy significantly skews unlearning effectiveness metrics, with redundant
instances artificially inflating the observed memorization measurements ROUGE
from 19.7% to 26.1% and Entailment Scores from 32.4% to 35.2%, highlighting the
necessity of systematic deduplication for accurate assessment.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Language Models Grow Less Humanlike beyond Phase Transition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.18802v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.18802v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tatsuya Aoyama, Ethan Wilcox
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  LMs' alignment with human reading behavior (i.e. psychometric predictive
power; PPP) is known to improve during pretraining up to a tipping point,
beyond which it either plateaus or degrades. Various factors, such as word
frequency, recency bias in attention, and context size, have been theorized to
affect PPP, yet there is no current account that explains why such a tipping
point exists, and how it interacts with LMs' pretraining dynamics more
generally. We hypothesize that the underlying factor is a pretraining phase
transition, characterized by the rapid emergence of specialized attention
heads. We conduct a series of correlational and causal experiments to show that
such a phase transition is responsible for the tipping point in PPP. We then
show that, rather than producing attention patterns that contribute to the
degradation in PPP, phase transitions alter the subsequent learning dynamics of
the model, such that further training keeps damaging PPP.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ANPMI: Assessing the True Comprehension Capabilities of LLMs for
  Multiple Choice Questions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.18798v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.18798v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gyeongje Cho, Yeonkyoung So, Jaejin Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multiple-choice benchmarks, consisting of various prompts and choices, are
among the most widely used methods to assess a language model's natural
language understanding capability. Given a specific prompt, we typically
compute $P(Choice|Prompt)$ to evaluate how likely a language model is to
generate the correct choice compared to incorrect ones. However, we observe
that performance measured using this approach reflects not only the model's
comprehension of the prompt but also its inherent biases for certain choices
regardless of the prompt. This issue makes it challenging to accurately measure
a model's natural language understanding, as models may select the answer
without fully understanding the prompt. To address this limitation, we propose
a novel metric called ANPMI, which normalizes Pointwise Mutual Information
(PMI) by $-\log P(Choice)$. ANPMI provides a more accurate assessment of the
model's natural language understanding by ensuring that it is challenging to
answer a question without properly understanding the prompt.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Anything Goes? A Crosslinguistic Study of (Im)possible Language Learning
  in LMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.18795v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.18795v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiulin Yang, Tatsuya Aoyama, Yuekun Yao, Ethan Wilcox
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Do LLMs offer insights into human language learning? A common argument
against this idea is that because their architecture and training paradigm are
so vastly different from humans, LLMs can learn arbitrary inputs as easily as
natural languages. In this paper, we test this claim by training LMs to model
impossible and typologically unattested languages. Unlike previous work, which
has focused exclusively on English, we conduct experiments on 12 natural
languages from 4 language families. Our results show that while GPT-2 small can
primarily distinguish attested languages from their impossible counterparts, it
does not achieve perfect separation between all the attested languages and all
the impossible ones. We further test whether GPT-2 small distinguishes
typologically attested from unattested languages with different NP orders by
manipulating word order based on Greenberg's Universal 20. We find that the
model's perplexity scores do not distinguish attested vs. unattested word
orders, as long as the unattested variants maintain constituency structure.
These findings suggest that language models exhibit some human-like inductive
biases, though these biases are weaker than those found in human learners.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Seeing the Forest for the Trees: A Large Scale, Continuously Updating
  Meta-Analysis of Frontier LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.18791v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.18791v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jungsoo Park, Junmo Kang, Gabriel Stanovsky, Alan Ritter
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The surge of LLM studies makes synthesizing their findings challenging.
Meta-analysis can uncover important trends across studies, but its use is
limited by the time-consuming nature of manual data extraction. Our study
presents a semi-automated approach for meta-analysis that accelerates data
extraction using LLMs. It automatically identifies relevant arXiv papers,
extracts experimental results and related attributes, and organizes them into a
structured dataset. We conduct a comprehensive meta-analysis of frontier LLMs
using an automatically extracted dataset, reducing the effort of paper
surveying and data extraction by more than 93\% compared to manual approaches.
We validate our dataset by showing that it reproduces key findings from a
recent manual meta-analysis about Chain-of-Thought (CoT), and also uncovers new
insights that go beyond it, showing for example that in-context examples
benefit multimodal tasks but offer limited gains in mathematical tasks compared
to CoT. Our automatically updatable dataset enables continuous tracking of
target models by extracting evaluation studies as new data becomes available.
Through our scientific artifacts and empirical analysis, we provide novel
insights into LLMs while facilitating ongoing meta-analyses of their behavior.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>21 pages, 9 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Active Few-Shot Learning for Text Classification <span class="chip">NAACL 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.18782v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.18782v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Saeed Ahmadnia, Arash Yousefi Jordehi, Mahsa Hosseini Khasheh Heyran, Seyed Abolghasem Mirroshandel, Owen Rambow, Cornelia Caragea
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rise of Large Language Models (LLMs) has boosted the use of Few-Shot
Learning (FSL) methods in natural language processing, achieving acceptable
performance even when working with limited training data. The goal of FSL is to
effectively utilize a small number of annotated samples in the learning
process. However, the performance of FSL suffers when unsuitable support
samples are chosen. This problem arises due to the heavy reliance on a limited
number of support samples, which hampers consistent performance improvement
even when more support samples are added. To address this challenge, we propose
an active learning-based instance selection mechanism that identifies effective
support instances from the unlabeled pool and can work with different LLMs. Our
experiments on five tasks show that our method frequently improves the
performance of FSL. We make our implementation available on GitHub.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to NAACL 2025 Main Conference; 18 pages, 8 figures, 13
  tables including Appendix</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Optimal Multi-draft Speculative Decoding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.18779v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.18779v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhengmian Hu, Tong Zheng, Vignesh Viswanathan, Ziyi Chen, Ryan A. Rossi, Yihan Wu, Dinesh Manocha, Heng Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have become an indispensable part of natural
language processing tasks. However, autoregressive sampling has become an
efficiency bottleneck. Multi-Draft Speculative Decoding (MDSD) is a recent
approach where, when generating each token, a small draft model generates
multiple drafts, and the target LLM verifies them in parallel, ensuring that
the final output conforms to the target model distribution. The two main design
choices in MDSD are the draft sampling method and the verification algorithm.
For a fixed draft sampling method, the optimal acceptance rate is a solution to
an optimal transport problem, but the complexity of this problem makes it
difficult to solve for the optimal acceptance rate and measure the gap between
existing verification algorithms and the theoretical upper bound. This paper
discusses the dual of the optimal transport problem, providing a way to
efficiently compute the optimal acceptance rate. For the first time, we measure
the theoretical upper bound of MDSD efficiency for vocabulary sizes in the
thousands and quantify the gap between existing verification algorithms and
this bound. We also compare different draft sampling methods based on their
optimal acceptance rates. Our results show that the draft sampling method
strongly influences the optimal acceptance rate, with sampling without
replacement outperforming sampling with replacement. Additionally, existing
verification algorithms do not reach the theoretical upper bound for both
without replacement and with replacement sampling. Our findings suggest that
carefully designed draft sampling methods can potentially improve the optimal
acceptance rate and enable the development of verification algorithms that
closely match the theoretical upper bound.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ M2-omni: Advancing Omni-MLLM for Comprehensive Modality Support with
  Competitive Performance 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.18778v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.18778v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qingpei Guo, Kaiyou Song, Zipeng Feng, Ziping Ma, Qinglong Zhang, Sirui Gao, Xuzheng Yu, Yunxiao Sun,  Tai-WeiChang, Jingdong Chen, Ming Yang, Jun Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present M2-omni, a cutting-edge, open-source omni-MLLM that achieves
competitive performance to GPT-4o. M2-omni employs a unified multimodal
sequence modeling framework, which empowers Large Language Models(LLMs) to
acquire comprehensive cross-modal understanding and generation capabilities.
Specifically, M2-omni can process arbitrary combinations of audio, video,
image, and text modalities as input, generating multimodal sequences
interleaving with audio, image, or text outputs, thereby enabling an advanced
and interactive real-time experience. The training of such an omni-MLLM is
challenged by significant disparities in data quantity and convergence rates
across modalities. To address these challenges, we propose a step balance
strategy during pre-training to handle the quantity disparities in
modality-specific data. Additionally, a dynamically adaptive balance strategy
is introduced during the instruction tuning stage to synchronize the
modality-wise training progress, ensuring optimal convergence. Notably, we
prioritize preserving strong performance on pure text tasks to maintain the
robustness of M2-omni's language understanding capability throughout the
training process. To our best knowledge, M2-omni is currently a very
competitive open-source model to GPT-4o, characterized by its comprehensive
modality and task support, as well as its exceptional performance. We expect
M2-omni will advance the development of omni-MLLMs, thus facilitating future
research in this domain.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Plutus: Benchmarking Large Language Models in Low-Resource Greek Finance 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.18772v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.18772v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xueqing Peng, Triantafillos Papadopoulos, Efstathia Soufleri, Polydoros Giannouris, Ruoyu Xiang, Yan Wang, Lingfei Qian, Jimin Huang, Qianqian Xie, Sophia Ananiadou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite Greece's pivotal role in the global economy, large language models
(LLMs) remain underexplored for Greek financial context due to the linguistic
complexity of Greek and the scarcity of domain-specific datasets. Previous
efforts in multilingual financial natural language processing (NLP) have
exposed considerable performance disparities, yet no dedicated Greek financial
benchmarks or Greek-specific financial LLMs have been developed until now. To
bridge this gap, we introduce Plutus-ben, the first Greek Financial Evaluation
Benchmark, and Plutus-8B, the pioneering Greek Financial LLM, fine-tuned with
Greek domain-specific data. Plutus-ben addresses five core financial NLP tasks
in Greek: numeric and textual named entity recognition, question answering,
abstractive summarization, and topic classification, thereby facilitating
systematic and reproducible LLM assessments. To underpin these tasks, we
present three novel, high-quality Greek financial datasets, thoroughly
annotated by expert native Greek speakers, augmented by two existing resources.
Our comprehensive evaluation of 22 LLMs on Plutus-ben reveals that Greek
financial NLP remains challenging due to linguistic complexity, domain-specific
terminology, and financial reasoning gaps. These findings underscore the
limitations of cross-lingual transfer, the necessity for financial expertise in
Greek-trained models, and the challenges of adapting financial LLMs to Greek
text. We release Plutus-ben, Plutus-8B, and all associated datasets publicly to
promote reproducible research and advance Greek financial NLP, fostering
broader multilingual inclusivity in finance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Reward Shaping to Mitigate Reward Hacking in RLHF 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.18770v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.18770v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiayi Fu, Xuandong Zhao, Chengyuan Yao, Heng Wang, Qi Han, Yanghua Xiao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reinforcement Learning from Human Feedback (RLHF) is essential for aligning
large language models (LLMs) with human values. However, RLHF is susceptible to
reward hacking, where the agent exploits flaws in the reward function rather
than learning the intended behavior, thus degrading alignment. While reward
shaping helps stabilize RLHF and partially mitigate reward hacking, a
systematic investigation into shaping techniques and their underlying
principles remains lacking. To bridge this gap, we present a comprehensive
study of the prevalent reward shaping methods. Our analysis suggests three key
design principles: (1) RL reward is ideally bounded, (2) RL benefits from rapid
initial growth followed by gradual convergence, and (3) RL reward is best
formulated as a function of centered reward. Guided by these insights, we
propose Preference As Reward (PAR), a novel approach that leverages the latent
preferences embedded within the reward model itself as the signal for
reinforcement learning. We evaluated PAR on two base models, Gemma2-2B and
Llama3-8B, using two datasets, Ultrafeedback-Binarized and HH-RLHF.
Experimental results demonstrate PAR's superior performance over other reward
shaping methods. On the AlpacaEval 2.0 benchmark, PAR achieves a win rate at
least 5 percentage points higher than competing approaches. Furthermore, PAR
exhibits remarkable data efficiency, requiring only a single reference reward
for optimal performance, and maintains robustness against reward hacking even
after two full epochs of training. Code is available at
https://github.com/PorUna-byte/PAR.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>19 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Automatic <span class="highlight-title">Prompt</span> Optimization via Heuristic Search: A <span class="highlight-title">Survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.18746v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.18746v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wendi Cui, Jiaxin Zhang, Zhuohang Li, Hao Sun, Damien Lopez, Kamalika Das, Bradley A. Malin, Sricharan Kumar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in Large Language Models have led to remarkable achievements
across a variety of Natural Language Processing tasks, making prompt
engineering increasingly central to guiding model outputs. While manual methods
can be effective, they typically rely on intuition and do not automatically
refine prompts over time. In contrast, automatic prompt optimization employing
heuristic-based search algorithms can systematically explore and improve
prompts with minimal human oversight. This survey proposes a comprehensive
taxonomy of these methods, categorizing them by where optimization occurs, what
is optimized, what criteria drive the optimization, which operators generate
new prompts, and which iterative search algorithms are applied. We further
highlight specialized datasets and tools that support and accelerate automated
prompt refinement. We conclude by discussing key open challenges pointing
toward future opportunities for more robust and versatile LLM applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Weaker LLMs' Opinions Also Matter: Mixture of Opinions Enhances LLM's
  Mathematical Reasoning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.19622v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.19622v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yanan Chen, Ali Pesaranghader, Tanmana Sadhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in Large Language Models (LLMs) have raised interest in their
formal reasoning capabilities, particularly in mathematics. While closed LLMs
like GPT-4 perform well on mathematical benchmarks, e.g., GSM8K, it remains
unclear whether small to medium-sized open LLMs can achieve similar
performance, questioning their reliability. To close this gap, we propose a
post-training approach leveraging a mixture of opinions (MoO) from weaker
ancillary LLMs to enhance a (relatively) stronger LLM's reasoning. For that,
each post-training sample is augmented with Chain-of-Thought (CoT) reasoning
steps and answers from ancillary LLMs, enabling the main LLM to learn from
diverse perspectives. We compare MoO with standard supervised fine-tuning
(SFT), few-shot prompting, and the Mixture of Agents (MoA) method on
mathematical reasoning benchmarks. Our results show that incorporating weaker
LLMs' opinions improves mathematical reasoning by an average of 5%,
highlighting the value of diverse perspectives in reasoning tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 1 figure, 3 tables, 4 prompt/data templates</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Is Your Paper Being <span class="highlight-title">Review</span>ed by an LLM? A New Benchmark <span class="highlight-title">Dataset</span> and
  Approach for Detecting AI Text in Peer <span class="highlight-title">Review</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.19614v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.19614v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sungduk Yu, Man Luo, Avinash Madusu, Vasudev Lal, Phillip Howard
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Peer review is a critical process for ensuring the integrity of published
scientific research. Confidence in this process is predicated on the assumption
that experts in the relevant domain give careful consideration to the merits of
manuscripts which are submitted for publication. With the recent rapid
advancements in large language models (LLMs), a new risk to the peer review
process is that negligent reviewers will rely on LLMs to perform the often time
consuming process of reviewing a paper. However, there is a lack of existing
resources for benchmarking the detectability of AI text in the domain of peer
review.
  To address this deficiency, we introduce a comprehensive dataset containing a
total of 788,984 AI-written peer reviews paired with corresponding human
reviews, covering 8 years of papers submitted to each of two leading AI
research conferences (ICLR and NeurIPS). We use this new resource to evaluate
the ability of 18 existing AI text detection algorithms to distinguish between
peer reviews written by humans and different state-of-the-art LLMs. Motivated
by the shortcomings of existing methods, we propose a new detection approach
which surpasses existing methods in the identification of AI written peer
reviews. Our work reveals the difficulty of identifying AI-generated text at
the individual peer review level, highlighting the urgent need for new tools
and methods to detect this unethical use of generative AI.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Evaluation of Hate Speech Detection Using Large Language Models and
  Geographical Contextualization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.19612v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.19612v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anwar Hossain Zahid, Monoshi Kumar Roy, Swarna Das
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The proliferation of hate speech on social media is one of the serious issues
that is bringing huge impacts to society: an escalation of violence,
discrimination, and social fragmentation. The problem of detecting hate speech
is intrinsically multifaceted due to cultural, linguistic, and contextual
complexities and adversarial manipulations. In this study, we systematically
investigate the performance of LLMs on detecting hate speech across
multilingual datasets and diverse geographic contexts. Our work presents a new
evaluation framework in three dimensions: binary classification of hate speech,
geography-aware contextual detection, and robustness to adversarially generated
text. Using a dataset of 1,000 comments from five diverse regions, we evaluate
three state-of-the-art LLMs: Llama2 (13b), Codellama (7b), and DeepSeekCoder
(6.7b). Codellama had the best binary classification recall with 70.6% and an
F1-score of 52.18%, whereas DeepSeekCoder had the best performance in
geographic sensitivity, correctly detecting 63 out of 265 locations. The tests
for adversarial robustness also showed significant weaknesses; Llama2
misclassified 62.5% of manipulated samples. These results bring to light the
trade-offs between accuracy, contextual understanding, and robustness in the
current versions of LLMs. This work has thus set the stage for developing
contextually aware, multilingual hate speech detection systems by underlining
key strengths and limitations, therefore offering actionable insights for
future research and real-world applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 2 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Revisiting Word Embeddings in the LLM Era 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.19607v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.19607v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yash Mahajan, Matthew Freestone, Sathyanarayanan Aakur, Santu Karmaker
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have recently shown remarkable advancement in
various NLP tasks. As such, a popular trend has emerged lately where NLP
researchers extract word/sentence/document embeddings from these large
decoder-only models and use them for various inference tasks with promising
results. However, it is still unclear whether the performance improvement of
LLM-induced embeddings is merely because of scale or whether underlying
embeddings they produce significantly differ from classical encoding models
like Word2Vec, GloVe, Sentence-BERT (SBERT) or Universal Sentence Encoder
(USE). This is the central question we investigate in the paper by
systematically comparing classical decontextualized and contextualized word
embeddings with the same for LLM-induced embeddings. Our results show that LLMs
cluster semantically related words more tightly and perform better on analogy
tasks in decontextualized settings. However, in contextualized settings,
classical models like SimCSE often outperform LLMs in sentence-level similarity
assessment tasks, highlighting their continued relevance for fine-grained
semantics.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A City of Millions: Mapping Literary Social Networks At Scale 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.19590v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.19590v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sil Hamilton, Rebecca M. M. Hicke, David Mimno, Matthew Wilkens
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We release 70,509 high-quality social networks extracted from multilingual
fiction and nonfiction narratives. We additionally provide metadata for ~30,000
of these texts (73% nonfiction and 27% fiction) written between 1800 and 1999
in 58 languages. This dataset provides information on historical social worlds
at an unprecedented scale, including data for 1,192,855 individuals in
2,805,482 pair-wise relationships annotated for affinity and relationship type.
We achieve this scale by automating previously manual methods of extracting
social networks; specifically, we adapt an existing annotation task as a
language model prompt, ensuring consistency at scale with the use of structured
output. This dataset provides an unprecedented resource for the humanities and
social sciences by providing data on cognitive models of social realities.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Neo<span class="highlight-title">BERT</span>: A Next-Generation <span class="highlight-title">BERT</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.19587v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.19587v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lola Le Breton, Quentin Fournier, Mariam El Mezouar, Sarath Chandar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent innovations in architecture, pre-training, and fine-tuning have led to
the remarkable in-context learning and reasoning abilities of large
auto-regressive language models such as LLaMA and DeepSeek. In contrast,
encoders like BERT and RoBERTa have not seen the same level of progress despite
being foundational for many downstream NLP applications. To bridge this gap, we
introduce NeoBERT, a next-generation encoder that redefines the capabilities of
bidirectional models by integrating state-of-the-art advancements in
architecture, modern data, and optimized pre-training methodologies. NeoBERT is
designed for seamless adoption: it serves as a plug-and-play replacement for
existing base models, relies on an optimal depth-to-width ratio, and leverages
an extended context length of 4,096 tokens. Despite its compact 250M parameter
footprint, it achieves state-of-the-art results on the massive MTEB benchmark,
outperforming BERT large, RoBERTa large, NomicBERT, and ModernBERT under
identical fine-tuning conditions. In addition, we rigorously evaluate the
impact of each modification on GLUE and design a uniform fine-tuning and
evaluation framework for MTEB. We release all code, data, checkpoints, and
training scripts to accelerate research and real-world adoption.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>19 pages, 5 figures, 9 tables. Submitted to TMLR</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Where Are We? Evaluating LLM Performance on African Languages 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.19582v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.19582v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ife Adebara, Hawau Olamide Toyin, Nahom Tesfu Ghebremichael, AbdelRahim Elmadany, Muhammad Abdul-Mageed
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Africa's rich linguistic heritage remains underrepresented in NLP, largely
due to historical policies that favor foreign languages and create significant
data inequities. In this paper, we integrate theoretical insights on Africa's
language landscape with an empirical evaluation using Sahara - a comprehensive
benchmark curated from large-scale, publicly accessible datasets capturing the
continent's linguistic diversity. By systematically assessing the performance
of leading large language models (LLMs) on Sahara, we demonstrate how
policy-induced data variations directly impact model effectiveness across
African languages. Our findings reveal that while a few languages perform
reasonably well, many Indigenous languages remain marginalized due to sparse
data. Leveraging these insights, we offer actionable recommendations for policy
reforms and inclusive data practices. Overall, our work underscores the urgent
need for a dual approach - combining theoretical understanding with empirical
evaluation - to foster linguistic diversity in AI for African communities.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Do Large Language Models Know How Much They Know? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.19573v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.19573v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gabriele Prato, Jerry Huang, Prasannna Parthasarathi, Shagun Sodhani, Sarath Chandar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have emerged as highly capable systems and are
increasingly being integrated into various uses. However, the rapid pace of
their deployment has outpaced a comprehensive understanding of their internal
mechanisms and a delineation of their capabilities and limitations. A desired
attribute of an intelligent system is its ability to recognize the scope of its
own knowledge. To investigate whether LLMs embody this characteristic, we
develop a benchmark designed to challenge these models to enumerate all
information they possess on specific topics. This benchmark evaluates whether
the models recall excessive, insufficient, or the precise amount of
information, thereby indicating their awareness of their own knowledge. Our
findings reveal that all tested LLMs, given sufficient scale, demonstrate an
understanding of how much they know about specific topics. While different
architectures exhibit varying rates of this capability's emergence, the results
suggest that awareness of knowledge may be a generalizable attribute of LLMs.
Further research is needed to confirm this potential and fully elucidate the
underlying mechanisms.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Stay Focused: Problem Drift in Multi-Agent Debate 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.19559v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.19559v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jonas Becker, Lars Benedikt Kaesberg, Andreas Stephan, Jan Philip Wahle, Terry Ruas, Bela Gipp
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-agent debate - multiple instances of large language models discussing
problems in turn-based interaction - has shown promise for solving knowledge
and reasoning tasks. However, these methods show limitations, particularly when
scaling them to longer reasoning chains. In this study, we unveil a new issue
of multi-agent debate: discussions drift away from the initial problem over
multiple turns. We define this phenomenon as problem drift and quantify its
presence across ten tasks (i.e., three generative, three knowledge, three
reasoning, and one instruction-following task). To identify the reasons for
this issue, we perform a human study with eight experts on discussions
suffering from problem drift, who find the most common issues are a lack of
progress (35% of cases), low-quality feedback (26% of cases), and a lack of
clarity (25% of cases). To systematically address the issue of problem drift,
we propose DRIFTJudge, a method based on LLM-as-a-judge, to detect problem
drift at test-time. We further propose DRIFTPolicy, a method to mitigate 31% of
problem drift cases. Our study can be seen as a first step to understanding a
key limitation of multi-agent debate, highlighting pathways for improving their
effectiveness in the future.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>34 pages, 21 figures, 4 tables, under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The <span class="highlight-title">Prompt</span> Report: A Systematic <span class="highlight-title">Survey</span> of <span class="highlight-title">Prompt</span> Engineering Techniques 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.06608v6">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.06608v6.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sander Schulhoff, Michael Ilie, Nishant Balepur, Konstantine Kahadze, Amanda Liu, Chenglei Si, Yinheng Li, Aayush Gupta, HyoJung Han, Sevien Schulhoff, Pranav Sandeep Dulepet, Saurav Vidyadhara, Dayeon Ki, Sweta Agrawal, Chau Pham, Gerson Kroiz, Feileen Li, Hudson Tao, Ashay Srivastava, Hevander Da Costa, Saloni Gupta, Megan L. Rogers, Inna Goncearenco, Giuseppe Sarli, Igor Galynker, Denis Peskoff, Marine Carpuat, Jules White, Shyamal Anadkat, Alexander Hoyle, Philip Resnik
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generative Artificial Intelligence (GenAI) systems are increasingly being
deployed across diverse industries and research domains. Developers and
end-users interact with these systems through the use of prompting and prompt
engineering. Although prompt engineering is a widely adopted and extensively
researched area, it suffers from conflicting terminology and a fragmented
ontological understanding of what constitutes an effective prompt due to its
relatively recent emergence. We establish a structured understanding of prompt
engineering by assembling a taxonomy of prompting techniques and analyzing
their applications. We present a detailed vocabulary of 33 vocabulary terms, a
taxonomy of 58 LLM prompting techniques, and 40 techniques for other
modalities. Additionally, we provide best practices and guidelines for prompt
engineering, including advice for prompting state-of-the-art (SOTA) LLMs such
as ChatGPT. We further present a meta-analysis of the entire literature on
natural language prefix-prompting. As a culmination of these efforts, this
paper presents the most comprehensive survey on prompt engineering to date.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ R.I.P.: Better Models by Survival of the Fittest <span class="highlight-title">Prompt</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.18578v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.18578v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ping Yu, Weizhe Yuan, Olga Golovneva, Tianhao Wu, Sainbayar Sukhbaatar, Jason Weston, Jing Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Training data quality is one of the most important drivers of final model
quality. In this work, we introduce a method for evaluating data integrity
based on the assumption that low-quality input prompts result in high variance
and low quality responses. This is achieved by measuring the rejected response
quality and the reward gap between the chosen and rejected preference pair. Our
method, Rejecting Instruction Preferences (RIP) can be used to filter prompts
from existing training sets, or to make high quality synthetic datasets,
yielding large performance gains across various benchmarks compared to
unfiltered data. Using Llama 3.1-8B-Instruct, RIP improves AlpacaEval2 LC Win
Rate by 9.4%, Arena-Hard by 8.7%, and WildBench by 9.9%. Using Llama
3.3-70B-Instruct, RIP improves Arena-Hard from 67.5 to 82.9, which is from 18th
place to 6th overall in the leaderboard.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Privacy-Preserving Retrieval-Augmented Generation with Differential
  Privacy 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.04697v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.04697v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tatsuki Koga, Ruihan Wu, Kamalika Chaudhuri
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the recent remarkable advancement of large language models (LLMs), there
has been a growing interest in utilizing them in the domains with highly
sensitive data that lies outside their training data. For this purpose,
retrieval-augmented generation (RAG) is particularly effective -- it assists
LLMs by directly providing relevant information from the external knowledge
sources. However, without extra privacy safeguards, RAG outputs risk leaking
sensitive information from the external data source. In this work, we explore
RAG under differential privacy (DP), a formal guarantee of data privacy. The
main challenge with differentially private RAG is how to generate long accurate
answers within a moderate privacy budget. We address this by proposing an
algorithm that smartly spends privacy budget only for the tokens that require
the sensitive information and uses the non-private LLM for other tokens. Our
extensive empirical evaluations reveal that our algorithm outperforms the
non-RAG baseline under a reasonable privacy budget of $\epsilon\approx 10$
across different models and datasets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Beyond Linear Approximations: A Novel Pruning Approach for Attention
  Matrix <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.11261v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.11261v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yingyu Liang, Jiangxuan Long, Zhenmei Shi, Zhao Song, Yufa Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have shown immense potential in enhancing
various aspects of our daily lives, from conversational AI to search and AI
assistants. However, their growing capabilities come at the cost of extremely
large model sizes, making deployment on edge devices challenging due to memory
and computational constraints. This paper introduces a novel approach to LLM
weight pruning that directly optimizes for approximating the attention matrix,
a core component of transformer architectures. Unlike existing methods that
focus on linear approximations, our approach accounts for the non-linear nature
of the Softmax attention mechanism. We provide theoretical guarantees for the
convergence of our Gradient Descent-based optimization method to a near-optimal
pruning mask solution. Our empirical results demonstrate the effectiveness of
our non-linear pruning approach in maintaining model performance while
significantly reducing computational costs, which is beyond the current
state-of-the-art methods, i.e., SparseGPT and Wanda, by a large margin. This
work establishes a new theoretical foundation for pruning algorithm design in
LLMs, potentially paving the way for more efficient LLM inference on
resource-constrained devices.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ InductionBench: LLMs Fail in the Simplest Complexity Class 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.15823v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.15823v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenyue Hua, Tyler Wong, Sun Fei, Liangming Pan, Adam Jardine, William Yang Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have shown remarkable improvements in reasoning
and many existing benchmarks have been addressed by models such as o1 and o3
either fully or partially. However, a majority of these benchmarks emphasize
deductive reasoning, including mathematical and coding tasks in which rules
such as mathematical axioms or programming syntax are clearly defined, based on
which LLMs can plan and apply these rules to arrive at a solution. In contrast,
inductive reasoning, where one infers the underlying rules from observed data,
remains less explored. Such inductive processes lie at the heart of scientific
discovery, as they enable researchers to extract general principles from
empirical observations. To assess whether LLMs possess this capacity, we
introduce InductionBench, a new benchmark designed to evaluate the inductive
reasoning ability of LLMs. Our experimental findings reveal that even the most
advanced models available struggle to master the simplest complexity classes
within the subregular hierarchy of functions, highlighting a notable deficiency
in current LLMs' inductive reasoning capabilities. Coda and data are available
https://github.com/Wenyueh/inductive_reasoning_benchmark.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>24 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Stronger Models are NOT Stronger Teachers for Instruction Tuning <span class="chip">NAACL 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07133v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07133v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhangchen Xu, Fengqing Jiang, Luyao Niu, Bill Yuchen Lin, Radha Poovendran
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Instruction tuning has been widely adopted to ensure large language models
(LLMs) follow user instructions effectively. The resulting
instruction-following capabilities of LLMs heavily rely on the instruction
datasets used for tuning. Recently, synthetic instruction datasets have emerged
as an economically viable solution to provide LLMs diverse and high-quality
instructions. However, existing approaches typically assume that larger or
stronger models are stronger teachers for instruction tuning, and hence simply
adopt these models as response generators to the synthetic instructions. In
this paper, we challenge this commonly-adopted assumption. Our extensive
experiments across five base models and twenty response generators reveal that
larger and stronger models are not necessarily stronger teachers of smaller
models. We refer to this phenomenon as the Larger Models' Paradox. We observe
that existing metrics cannot precisely predict the effectiveness of response
generators since they ignore the compatibility between teachers and base models
being fine-tuned. We thus develop a novel metric, named as
Compatibility-Adjusted Reward (CAR) to measure the effectiveness of response
generators. Our experiments across five base models demonstrate that CAR
outperforms almost all baselines.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This is paper is accepted at NAACL 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning to Generate Unit Tests for Automated Debugging 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.01619v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.01619v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Archiki Prasad, Elias Stengel-Eskin, Justin Chih-Yao Chen, Zaid Khan, Mohit Bansal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Unit tests (UTs) play an instrumental role in assessing code correctness as
well as providing feedback to large language models (LLMs), motivating
automated test generation. However, we uncover a trade-off between generating
unit test inputs that reveal errors when given a faulty code and correctly
predicting the unit test output without access to the gold solution. To address
this trade-off, we propose UTGen, which teaches LLMs to generate unit test
inputs that reveal errors along with their correct expected outputs based on
task descriptions. Since model-generated tests can provide noisy signals (e.g.,
from incorrectly predicted outputs), we propose UTDebug that (i) scales UTGen
via test-time compute to improve UT output prediction, and (ii) validates and
backtracks edits based on multiple generated UTs to avoid overfitting, and
helps LLMs debug effectively. We show that UTGen outperforms other LLM-based
baselines by 7.59% based on a metric measuring the presence of both
error-revealing UT inputs and correct UT outputs. When used with UTDebug, we
find that feedback from UTGen's unit tests improves pass@1 accuracy of Qwen2.5
32B on HumanEvalFix and our own harder debugging split of MBPP+ by over 3.17%
and 12.35% (respectively) over other LLM-based UT generation baselines. Lastly,
we demonstrate that UTGen is a better judge for code correctness, outperforming
a state-of-the-art trained 8B reward model by 4.43% on HumanEval+ with
best-of-10 sampling using Qwen2.5 7B.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>First two authors contributed equally. Dataset and Code:
  https://github.com/archiki/UTGenDebug</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The Hyperfitting Phenomenon: Sharpening and Stabilizing LLMs for
  Open-Ended Text Generation <span class="chip">ICLR</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.04318v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.04318v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fredrik Carlsson, Fangyu Liu, Daniel Ward, Murathan Kurfali, Joakim Nivre
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces the counter-intuitive generalization results of
overfitting pre-trained large language models (LLMs) on very small datasets. In
the setting of open-ended text generation, it is well-documented that LLMs tend
to generate repetitive and dull sequences, a phenomenon that is especially
apparent when generating using greedy decoding. This issue persists even with
state-of-the-art LLMs containing billions of parameters, trained via next-token
prediction on large datasets. We find that by further fine-tuning these models
to achieve a near-zero training loss on a small set of samples -- a process we
refer to as hyperfitting -- the long-sequence generative capabilities are
greatly enhanced. Greedy decoding with these Hyperfitted models even outperform
Top-P sampling over long-sequences, both in terms of diversity and human
preferences. This phenomenon extends to LLMs of various sizes, different
domains, and even autoregressive image generation. We further find this
phenomena to be distinctly different from that of Grokking and double descent.
Surprisingly, our experiments indicate that hyperfitted models rarely fall into
repeating sequences they were trained on, and even explicitly blocking these
sequences results in high-quality output. All hyperfitted models produce
extremely low-entropy predictions, often allocating nearly all probability to a
single token.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under review at ICLR</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Self-calibration for Language Model Quantization and Pruning <span class="chip">NAACL 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.17170v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.17170v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Miles Williams, George Chrysostomou, Nikolaos Aletras
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Quantization and pruning are fundamental approaches for model compression,
enabling efficient inference for language models. In a post-training setting,
state-of-the-art quantization and pruning methods require calibration data, a
small set of unlabeled examples. Conventionally, this is randomly sampled web
text, aiming to reflect the model training data. However, this poses two key
problems: (1) unrepresentative calibration examples can harm model performance,
and (2) organizations increasingly avoid releasing model training data. In this
paper, we propose self-calibration as a solution. Our approach requires no
external data, instead leveraging the model itself to generate synthetic
calibration data, with a view to better approximating the pre-training data
distribution. We extensively compare the performance of self-calibration with
several baselines, across a variety of models, compression methods, and tasks.
Our approach proves consistently competitive in maximizing downstream task
performance, frequently outperforming even using real data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NAACL 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Modality Interactive Mixture-of-Experts for Fake News Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.12431v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.12431v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yifan Liu, Yaokun Liu, Zelin Li, Ruichen Yao, Yang Zhang, Dong Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The proliferation of fake news on social media platforms disproportionately
impacts vulnerable populations, eroding trust, exacerbating inequality, and
amplifying harmful narratives. Detecting fake news in multimodal contexts --
where deceptive content combines text and images -- is particularly challenging
due to the nuanced interplay between modalities. Existing multimodal fake news
detection methods often emphasize cross-modal consistency but ignore the
complex interactions between text and visual elements, which may complement,
contradict, or independently influence the predicted veracity of a post. To
address these challenges, we present Modality Interactive Mixture-of-Experts
for Fake News Detection (MIMoE-FND), a novel hierarchical Mixture-of-Experts
framework designed to enhance multimodal fake news detection by explicitly
modeling modality interactions through an interaction gating mechanism. Our
approach models modality interactions by evaluating two key aspects of modality
interactions: unimodal prediction agreement and semantic alignment. The
hierarchical structure of MIMoE-FND allows for distinct learning pathways
tailored to different fusion scenarios, adapting to the unique characteristics
of each modality interaction. By tailoring fusion strategies to diverse
modality interaction scenarios, MIMoE-FND provides a more robust and nuanced
approach to multimodal fake news detection. We evaluate our approach on three
real-world benchmarks spanning two languages, demonstrating its superior
performance compared to state-of-the-art methods. By enhancing the accuracy and
interpretability of fake news detection, MIMoE-FND offers a promising tool to
mitigate the spread of misinformation, with the potential to better safeguard
vulnerable communities against its harmful effects.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by the Proceedings of the ACM Web Conference 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Akan Cinematic Emotions (ACE): A Multimodal Multi-party <span class="highlight-title">Dataset</span> for
  Emotion Recognition in Movie Dialogues 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.10973v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.10973v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        David Sasu, Zehui Wu, Ziwei Gong, Run Chen, Pengyuan Shi, Lin Ai, Julia Hirschberg, Natalie Schluter
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we introduce the Akan Conversation Emotion (ACE) dataset, the
first multimodal emotion dialogue dataset for an African language, addressing
the significant lack of resources for low-resource languages in emotion
recognition research. ACE, developed for the Akan language, contains 385
emotion-labeled dialogues and 6,162 utterances across audio, visual, and
textual modalities, along with word-level prosodic prominence annotations. The
presence of prosodic labels in this dataset also makes it the first
prosodically annotated African language dataset. We demonstrate the quality and
utility of ACE through experiments using state-of-the-art emotion recognition
methods, establishing solid baselines for future research. We hope ACE inspires
further work on inclusive, linguistically and culturally diverse NLP resources.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ TAPO: Task-Referenced Adaptation for <span class="highlight-title">Prompt</span> Optimization <span class="chip">ICASSP 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.06689v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.06689v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenxin Luo, Weirui Wang, Xiaopeng Li, Weibo Zhou, Pengyue Jia, Xiangyu Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Prompt engineering can significantly improve the performance of large
language models (LLMs), with automated prompt optimization (APO) gaining
significant attention due to the time-consuming and laborious nature of manual
prompt design. However, much of the existing work in APO overlooks
task-specific characteristics, resulting in prompts that lack domain
specificity and are not well-suited for task-specific optimization. In this
paper, we introduce TAPO, a multitask-aware prompt optimization framework
composed of three key modules. First, a task-aware metric selection module is
proposed to enhance task-specific prompt generation capabilities. Second, we
present a multi-metrics evaluation module to jointly evaluate prompts from
multiple perspectives. Third, an evolution-based optimization framework is
introduced for automatic prompt refinement, which improves adaptability across
various tasks. Extensive experiments on six datasets demonstrate the
effectiveness of our approach, and our code is publicly available.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ICASSP 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Application of Multimodal Large Language Models in Autonomous Driving 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.16410v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.16410v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Md Robiul Islam
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this era of technological advancements, several cutting-edge techniques
are being implemented to enhance Autonomous Driving (AD) systems, focusing on
improving safety, efficiency, and adaptability in complex driving environments.
However, AD still faces some problems including performance limitations. To
address this problem, we conducted an in-depth study on implementing the
Multi-modal Large Language Model. We constructed a Virtual Question Answering
(VQA) dataset to fine-tune the model and address problems with the poor
performance of MLLM on AD. We then break down the AD decision-making process by
scene understanding, prediction, and decision-making. Chain of Thought has been
used to make the decision more perfectly. Our experiments and detailed analysis
of Autonomous Driving give an idea of how important MLLM is for AD.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 9 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ NeuroVoz: a Castillian Spanish corpus of parkinsonian speech 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.02371v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.02371v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Janaína Mendes-Laureano, Jorge A. Gómez-García, Alejandro Guerrero-López, Elisa Luque-Buzo, Julián D. Arias-Londoño, Francisco J. Grandas-Pérez, Juan I. Godino-Llorente
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The screening of Parkinson's Disease (PD) through speech is hindered by a
notable lack of publicly available datasets in different languages. This fact
limits the reproducibility and further exploration of existing research.
  To address this gap, this manuscript presents the NeuroVoz corpus consisting
of 112 native Castilian-Spanish speakers, including 58 healthy controls and 54
individuals with PD, all recorded in ON state. The corpus showcases a diverse
array of speech tasks: sustained vowels; diadochokinetic tests; 16
Listen-and-Repeat utterances; and spontaneous monologues.
  The dataset is also complemented with subjective assessments of voice quality
performed by an expert according to the GRBAS scale
(Grade/Roughness/Breathiness/Asthenia/Strain), as well as annotations with a
thorough examination of phonation quality, intensity, speed, resonance,
intelligibility, and prosody.
  The corpus offers a substantial resource for the exploration of the impact of
PD on speech. This data set has already supported several studies, achieving a
benchmark accuracy of 89% for the screening of PD. Despite these advances, the
broader challenge of conducting a language-agnostic, cross-corpora analysis of
Parkinsonian speech patterns remains open.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Paper accepted at Scientific Data</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ UniGenCoder: Merging Seq2Seq and Seq2Tree Paradigms for Unified Code
  Generation <span class="chip">ICSE 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.12490v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.12490v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Liangying Shao, Yanfu Yan, Denys Poshyvanyk, Jinsong Su
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep learning-based code generation has completely transformed the way
developers write programs today. Existing approaches to code generation have
focused either on the Sequence-to-Sequence paradigm, which generates target
code as a sequence of tokens, or the Sequence-to-Tree paradigm, which outputs
code as a sequence of actions. While these two paradigms are intuitively
complementary, their combination has not been previously explored. By comparing
the code generated under these two paradigms, we find that integrating them
holds significant potential. In this paper, we propose UniGenCoder for
code-related generation tasks, which consists of a shared encoder, a shared
decoder with a minimal set of additional parameters to unify two paradigms, and
a selector that dynamically chooses optimal paradigm for each instance. Also,
during the model training, we first perform the multi-task learning and
distillation strategies to facilitate knowledge transfer between two paradigms,
and then leverage contrastive learning to train the selector. Experimental
results on the text-to-code and code-to-code generation tasks demonstrate the
effectiveness of our proposed model. We release our code at
https://github.com/DeepLearnXMU/UniGenCoder.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>accepted to 47th International Conference on Software Engineering
  (ICSE 2025), NIER track</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Combining TF-GridNet and Mixture Encoder for Continuous Speech
  Separation for Meeting Transcription 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.08454v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.08454v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Peter Vieting, Simon Berger, Thilo von Neumann, Christoph Boeddeker, Ralf Schlüter, Reinhold Haeb-Umbach
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Many real-life applications of automatic speech recognition (ASR) require
processing of overlapped speech. A common method involves first separating the
speech into overlap-free streams on which ASR is performed. Recently,
TF-GridNet has shown impressive performance in speech separation in real
reverberant conditions. Furthermore, a mixture encoder was proposed that
leverages the mixed speech to mitigate the effect of separation artifacts. In
this work, we extended the mixture encoder from a static two-speaker scenario
to a natural meeting context featuring an arbitrary number of speakers and
varying degrees of overlap. We further demonstrate its limits by the
integration with separators of varying strength including TF-GridNet. Our
experiments result in a new state-of-the-art performance on LibriCSS using a
single microphone. They show that TF-GridNet largely closes the gap between
previous methods and oracle separation independent of mixture encoding. We
further investigate the remaining potential for improvement.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Presented at SLT 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AfroBench: How Good are Large Language Models on African Languages? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.07978v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.07978v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jessica Ojo, Odunayo Ogundepo, Akintunde Oladipo, Kelechi Ogueji, Jimmy Lin, Pontus Stenetorp, David Ifeoluwa Adelani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large-scale multilingual evaluations, such as MEGA, often include only a
handful of African languages due to the scarcity of high-quality evaluation
data and the limited discoverability of existing African datasets. This lack of
representation hinders comprehensive LLM evaluation across a diverse range of
languages and tasks. To address these challenges, we introduce AfroBench -- a
multi-task benchmark for evaluating the performance of LLMs across 64 African
languages, 15 tasks and 22 datasets. AfroBench consists of nine natural
language understanding datasets, six text generation datasets, six knowledge
and question answering tasks, and one mathematical reasoning task. We present
results comparing the performance of prompting LLMs to fine-tuned baselines
based on BERT and T5-style models. Our results suggest large gaps in
performance between high-resource languages, such as English, and African
languages across most tasks; but performance also varies based on the
availability of monolingual data resources. Our findings confirm that
performance on African languages continues to remain a hurdle for current LLMs,
underscoring the need for additional efforts to close this gap.
  https://mcgill-nlp.github.io/AfroBench/
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SECURA: Sigmoid-Enhanced CUR Decomposition with Uninterrupted Retention
  and Low-Rank Adaptation in Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.18168v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.18168v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuxuan Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the rapid development of large language models (LLMs), fully fine-tuning
(FT) these models has become increasingly impractical due to the high
computational demands. Additionally, FT can lead to catastrophic forgetting. As
an alternative, Low-Rank Adaptation (LoRA) has been proposed, which fine-tunes
only a small subset of parameters, achieving similar performance to FT while
significantly reducing resource requirements. However, since LoRA inherits FT's
design, the issue of catastrophic forgetting remains.
  To address these challenges, we propose SECURA: Sigmoid-Enhanced CUR
Decomposition LoRA, a novel parameter-efficient fine-tuning (PEFT) variant that
mitigates catastrophic forgetting while improving fine-tuning performance. Our
method introduces a new normalization technique, SigNorm, to enhance parameter
retention and overall performance.
  SECURA has been evaluated on a variety of tasks, including mathematical
problem-solving (GSM8K), challenging question-answering (CNNDM), translation
(NewsDE), and complex multiple-choice reasoning (LogiQA). Experimental results
show that SECURA achieves an average fine-tuning improvement of 3.59% across
four multiple-choice question (MCQ) tasks and a 2.51% improvement across five
question-answering (QA) tasks on models such as Gemma2 2b, Qwen2 1.5b, Qwen 2
7b, Llama3 8b, and Llama3.1 8b, compared to DoRA. Moreover, SECURA demonstrates
superior knowledge retention capabilities, maintaining more than 70% accuracy
on basic LLM knowledge across 16 continual learning tests, outperforming
Experience Replay (ER), Sequential Learning (SEQ), EWC, I-LoRA, and CUR-LoRA.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>New work on Parameter-Efficient Fine-Tuning (PEFT) for large language
  models. Includes new techniques SigNorm and CABR-LoRA for optimizing
  fine-tune performance and Knowledge retention</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FinMTEB: Finance Massive Text Embedding Benchmark 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.10990v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.10990v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yixuan Tang, Yi Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Embedding models play a crucial role in representing and retrieving
information across various NLP applications. Recent advances in large language
models (LLMs) have further enhanced the performance of embedding models. While
these models are often benchmarked on general-purpose datasets, real-world
applications demand domain-specific evaluation. In this work, we introduce the
Finance Massive Text Embedding Benchmark (FinMTEB), a specialized counterpart
to MTEB designed for the financial domain. FinMTEB comprises 64 financial
domain-specific embedding datasets across 7 tasks that cover diverse textual
types in both Chinese and English, such as financial news articles, corporate
annual reports, ESG reports, regulatory filings, and earnings call transcripts.
We also develop a finance-adapted model, Fin-E5, using a persona-based data
synthetic method to cover diverse financial embedding tasks for training.
Through extensive evaluation of 15 embedding models, including Fin-E5, we show
three key findings: (1) performance on general-purpose benchmarks shows limited
correlation with financial domain tasks; (2) domain-adapted models consistently
outperform their general-purpose counterparts; and (3) surprisingly, a simple
Bag-of-Words (BoW) approach outperforms sophisticated dense embeddings in
financial Semantic Textual Similarity (STS) tasks, underscoring current
limitations in dense embedding techniques. Our work establishes a robust
evaluation framework for financial NLP applications and provides crucial
insights for developing domain-specific embedding models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>https://github.com/yixuantt/FinMTEB</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Adaptive Activation Steering: A Tuning-Free LLM Truthfulness Improvement
  Method for Diverse Hallucinations Categories <span class="chip">WWW 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.00034v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.00034v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tianlong Wang, Xianfeng Jiao, Yinghao Zhu, Zhongzhi Chen, Yifan He, Xu Chu, Junyi Gao, Yasha Wang, Liantao Ma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent studies have indicated that Large Language Models (LLMs) harbor an
inherent understanding of truthfulness, yet often fail to consistently express
it and generate false statements. This gap between "knowing" and "telling"
poses a challenge for ensuring the truthfulness of generated content. Inspired
by recent work on the practice of encoding human-interpretable concepts
linearly within large language models, we treat truthfulness as a specially
linearly encoded concept within LLMs, and introduce Adaptive Activation
Steering (ACT), a tuning-free method that adaptively shifts LLM's activations
in the "truthful" direction during inference. ACT addresses diverse categories
of hallucinations by utilizing diverse truthfulness-related steering vectors
and adjusting the steering intensity adaptively. Applied as an add-on across
various models, ACT significantly improves truthfulness in LLaMA ($\uparrow$
142%), LLaMA2 ($\uparrow$ 24%), Alpaca ($\uparrow$ 36%), Vicuna ($\uparrow$
28%), LLaMA2-Chat ($\uparrow$ 19%), and LLaMA3($\uparrow$ 34%). Furthermore, we
verify ACT's scalability across larger models (13B, 33B, 65B), underscoring the
adaptability of ACT to large-scale language models. Our code is available at
https://github.com/tianlwang/ACT.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ACM TheWebConf 2025 Conference (WWW 2025) Research Track</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ What Is That Talk About? A Video-to-Text Summarization <span class="highlight-title">Dataset</span> for
  Scientific Presentations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.08279v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.08279v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dongqi Liu, Chenxi Whitehouse, Xi Yu, Louis Mahon, Rohit Saxena, Zheng Zhao, Yifu Qiu, Mirella Lapata, Vera Demberg
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Transforming recorded videos into concise and accurate textual summaries is a
growing challenge in multimodal learning. This paper introduces VISTA, a
dataset specifically designed for video-to-text summarization in scientific
domains. VISTA contains 18,599 recorded AI conference presentations paired with
their corresponding paper abstracts. We benchmark the performance of
state-of-the-art large models and apply a plan-based framework to better
capture the structured nature of abstracts. Both human and automated
evaluations confirm that explicit planning enhances summary quality and factual
consistency. However, a considerable gap remains between models and human
performance, highlighting the challenges of scientific video summarization.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ChineseSimpleVQA -- "See the World, Discover Knowledge": A Chinese
  Factuality Evaluation for Large Vision Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11718v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11718v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jihao Gu, Yingyao Wang, Pi Bu, Chen Wang, Ziming Wang, Tengtao Song, Donglai Wei, Jiale Yuan, Yingxiu Zhao, Yancheng He, Shilong Li, Jiaheng Liu, Meng Cao, Jun Song, Yingshui Tan, Xiang Li, Wenbo Su, Zhicheng Zheng, Xiaoyong Zhu, Bo Zheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The evaluation of factual accuracy in large vision language models (LVLMs)
has lagged behind their rapid development, making it challenging to fully
reflect these models' knowledge capacity and reliability. In this paper, we
introduce the first factuality-based visual question-answering benchmark in
Chinese, named ChineseSimpleVQA, aimed at assessing the visual factuality of
LVLMs across 8 major topics and 56 subtopics. The key features of this
benchmark include a focus on the Chinese language, diverse knowledge types, a
multi-hop question construction, high-quality data, static consistency, and
easy-to-evaluate through short answers. Moreover, we contribute a rigorous data
construction pipeline and decouple the visual factuality into two parts: seeing
the world (i.e., object recognition) and discovering knowledge. This decoupling
allows us to analyze the capability boundaries and execution mechanisms of
LVLMs. Subsequently, we evaluate 34 advanced open-source and closed-source
models, revealing critical performance gaps within this field.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>24 pages, 21 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Verbalized Probabilistic Graphical Modeling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.05516v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.05516v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hengguan Huang, Xing Shen, Songtao Wang, Lingfa Meng, Dianbo Liu, Hao Wang, Samir Bhatt
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Human cognition excels at transcending sensory input and forming latent
representations that structure our understanding of the world. Although Large
Language Models (LLMs) can produce chain-of-thought reasoning, they lack a
principled framework to capture latent structures and model uncertainty,
especially in compositional reasoning tasks. We propose Verbalized
Probabilistic Graphical Modeling (vPGM), a Bayesian prompting framework that
guides LLMs to simulate key principles of Probabilistic Graphical Models (PGMs)
in natural language. Unlike many traditional probabilistic methods requiring
substantial domain expertise or specialized training, vPGM bypasses
expert-driven model design, making it well-suited for scenarios with limited
assumptions or scarce data. We evaluated our model on several compositional
reasoning tasks, both close-ended and open-ended. Our results indicate that the
model effectively enhances confidence calibration and text generation quality.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ColaCare: Enhancing Electronic Health Record Modeling through Large
  Language Model-Driven Multi-Agent Collaboration <span class="chip">WWW 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02551v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02551v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zixiang Wang, Yinghao Zhu, Huiya Zhao, Xiaochen Zheng, Dehao Sui, Tianlong Wang, Wen Tang, Yasha Wang, Ewen Harrison, Chengwei Pan, Junyi Gao, Liantao Ma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce ColaCare, a framework that enhances Electronic Health Record
(EHR) modeling through multi-agent collaboration driven by Large Language
Models (LLMs). Our approach seamlessly integrates domain-specific expert models
with LLMs to bridge the gap between structured EHR data and text-based
reasoning. Inspired by the Multidisciplinary Team (MDT) approach used in
clinical settings, ColaCare employs two types of agents: DoctorAgents and a
MetaAgent, which collaboratively analyze patient data. Expert models process
and generate predictions from numerical EHR data, while LLM agents produce
reasoning references and decision-making reports within the MDT-driven
collaborative consultation framework. The MetaAgent orchestrates the
discussion, facilitating consultations and evidence-based debates among
DoctorAgents, simulating diverse expertise in clinical decision-making. We
additionally incorporate the Merck Manual of Diagnosis and Therapy (MSD)
medical guideline within a retrieval-augmented generation (RAG) module for
medical evidence support, addressing the challenge of knowledge currency.
Extensive experiments conducted on three EHR datasets demonstrate ColaCare's
superior performance in clinical mortality outcome and readmission prediction
tasks, underscoring its potential to revolutionize clinical decision support
systems and advance personalized precision medicine. All code, case studies and
a questionnaire are available at the project website:
https://colacare.netlify.app.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ACM TheWebConf 2025 Conference (WWW 2025) Research Track</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ EMERGE: Enhancing Multimodal Electronic Health Records Predictive
  Modeling with Retrieval-Augmented Generation <span class="chip">CIKM 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.00036v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.00036v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yinghao Zhu, Changyu Ren, Zixiang Wang, Xiaochen Zheng, Shiyun Xie, Junlan Feng, Xi Zhu, Zhoujun Li, Liantao Ma, Chengwei Pan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The integration of multimodal Electronic Health Records (EHR) data has
significantly advanced clinical predictive capabilities. Existing models, which
utilize clinical notes and multivariate time-series EHR data, often fall short
of incorporating the necessary medical context for accurate clinical tasks,
while previous approaches with knowledge graphs (KGs) primarily focus on
structured knowledge extraction. In response, we propose EMERGE, a
Retrieval-Augmented Generation (RAG) driven framework to enhance multimodal EHR
predictive modeling. We extract entities from both time-series data and
clinical notes by prompting Large Language Models (LLMs) and align them with
professional PrimeKG, ensuring consistency. In addition to triplet
relationships, we incorporate entities' definitions and descriptions for richer
semantics. The extracted knowledge is then used to generate task-relevant
summaries of patients' health statuses. Finally, we fuse the summary with other
modalities using an adaptive multimodal fusion network with cross-attention.
Extensive experiments on the MIMIC-III and MIMIC-IV datasets' in-hospital
mortality and 30-day readmission tasks demonstrate the superior performance of
the EMERGE framework over baseline models. Comprehensive ablation studies and
analysis highlight the efficacy of each designed module and robustness to data
sparsity. EMERGE contributes to refining the utilization of multimodal EHR data
in healthcare, bridging the gap with nuanced medical contexts essential for
informed clinical predictions. We have publicly released the code at
https://github.com/yhzhu99/EMERGE.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CIKM 2024 Full Research Paper; arXiv admin note: text overlap with
  arXiv:2402.07016</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Make LoRA Great Again: Boosting LoRA with Adaptive Singular Values and
  Mixture-of-Experts Optimization Alignment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.16894v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.16894v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chenghao Fan, Zhenyi Lu, Sichen Liu, Xiaoye Qu, Wei Wei, Chengfeng Gu, Yu Cheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While Low-Rank Adaptation (LoRA) enables parameter-efficient fine-tuning for
Large Language Models (LLMs), its performance often falls short of Full
Fine-Tuning (Full FT). Current methods optimize LoRA by initializing with
static singular value decomposition (SVD) subsets, leading to suboptimal
leveraging of pre-trained knowledge. Another path for improving LoRA is
incorporating a Mixture-of-Experts (MoE) architecture. However, weight
misalignment and complex gradient dynamics make it challenging to adopt SVD
prior to the LoRA MoE architecture. To mitigate these issues, we propose
\underline{G}reat L\underline{o}R\underline{A} Mixture-of-Exper\underline{t}
(GOAT), a framework that (1) adaptively integrates relevant priors using an
SVD-structured MoE, and (2) aligns optimization with full fine-tuned MoE by
deriving a theoretical scaling factor. We demonstrate that proper scaling,
without modifying the architecture or training algorithms, boosts LoRA MoE's
efficiency and performance. Experiments across 25 datasets, including natural
language understanding, commonsense reasoning, image classification, and
natural language generation, demonstrate GOAT's state-of-the-art performance,
closing the gap with Full FT.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ECG-Expert-QA: A Benchmark for Evaluating Medical Large Language Models
  in Heart Disease Diagnosis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.17475v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.17475v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xu Wang, Jiaju Kang, Puyu Han
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present ECG-Expert-QA, a comprehensive multimodal dataset designed for
evaluating diagnostic capabilities in ECG interpretation, integrating real
clinical data with systematically generated synthetic cases. The dataset
encompasses six fundamental diagnostic tasks, comprising 47,211 meticulously
curated question-answer pairs that span a spectrum of clinical scenarios, from
basic rhythm analysis to complex case interpretation. By simulating challenging
clinical cases through a rigorous medical knowledge-guided process,
ECG-Expert-QA not only enhances the availability of annotated diagnostic data
but also significantly increases the complexity and diversity of clinical
presentations, including rare cardiac conditions and temporal progression
patterns. This design enables comprehensive evaluation of medical language
models across multiple dimensions, including diagnostic accuracy, clinical
reasoning, and knowledge integration. To facilitate global research
collaboration, ECG-Expert-QA is available in both Chinese and English versions,
with rigorous quality control ensuring linguistic and clinical consistency. The
dataset's challenging diagnostic tasks, which include interpretation of complex
arrhythmias, identification of subtle ischemic changes, and integration of
clinical context, establish it as an effective benchmark for advancing
AI-assisted ECG interpretation and pushing the boundaries of current diagnostic
models. Our dataset is open-source and available at
https://github.com/Zaozzz/ECG-Expert-QA
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SynGhost: Invisible and Universal Task-agnostic Backdoor Attack via
  Syntactic Transfer <span class="chip">NAACL 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.18945v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.18945v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pengzhou Cheng, Wei Du, Zongru Wu, Fengwei Zhang, Libo Chen, Zhuosheng Zhang, Gongshen Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Although pre-training achieves remarkable performance, it suffers from
task-agnostic backdoor attacks due to vulnerabilities in data and training
mechanisms. These attacks can transfer backdoors to various downstream tasks.
In this paper, we introduce $\mathtt{maxEntropy}$, an entropy-based poisoning
filter that mitigates such risks. To overcome the limitations of manual target
setting and explicit triggers, we propose $\mathtt{SynGhost}$, an invisible and
universal task-agnostic backdoor attack via syntactic transfer, further
exposing vulnerabilities in pre-trained language models (PLMs). Specifically,
$\mathtt{SynGhost}$ injects multiple syntactic backdoors into the pre-training
space through corpus poisoning, while preserving the PLM's pre-training
capabilities. Second, $\mathtt{SynGhost}$ adaptively selects optimal targets
based on contrastive learning, creating a uniform distribution in the
pre-training space. To identify syntactic differences, we also introduce an
awareness module to minimize interference between backdoors. Experiments show
that $\mathtt{SynGhost}$ poses significant threats and can transfer to various
downstream tasks. Furthermore, $\mathtt{SynGhost}$ resists defenses based on
perplexity, fine-pruning, and $\mathtt{maxEntropy}$. The code is available at
https://github.com/Zhou-CyberSecurity-AI/SynGhost.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, 16 figures, 12 tables, accepted at NAACL 2025 Findings</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Language Imbalance Driven Rewarding for Multilingual Self-improving <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.08964v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.08964v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wen Yang, Junhong Wu, Chen Wang, Chengqing Zong, Jiajun Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have achieved state-of-the-art performance
across numerous tasks. However, these advancements have predominantly benefited
"first-class" languages such as English and Chinese, leaving many other
languages underrepresented. This imbalance, while limiting broader
applications, generates a natural preference ranking between languages,
offering an opportunity to bootstrap the multilingual capabilities of LLM in a
self-improving manner. Thus, we propose $\textit{Language Imbalance Driven
Rewarding}$, where the inherent imbalance between dominant and non-dominant
languages within LLMs is leveraged as a reward signal. Iterative DPO training
demonstrates that this approach not only enhances LLM performance in
non-dominant languages but also improves the dominant language's capacity,
thereby yielding an iterative reward signal. Fine-tuning
Meta-Llama-3-8B-Instruct over two iterations of this approach results in
continuous improvements in multilingual performance across
instruction-following and arithmetic reasoning tasks, evidenced by an average
improvement of 7.46% win rate on the X-AlpacaEval leaderboard and 13.9%
accuracy on the MGSM benchmark. This work serves as an initial exploration,
paving the way for multilingual self-improvement of LLMs. The code is available
at https://github.com/ZNLP/Language-Imbalance-Driven-Rewarding
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Camera ready version for ICLR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MLE-bench: Evaluating Machine Learning Agents on Machine Learning
  Engineering <span class="chip">ICLR</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.07095v6">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.07095v6.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jun Shern Chan, Neil Chowdhury, Oliver Jaffe, James Aung, Dane Sherburn, Evan Mays, Giulio Starace, Kevin Liu, Leon Maksin, Tejal Patwardhan, Lilian Weng, Aleksander Mądry
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce MLE-bench, a benchmark for measuring how well AI agents perform
at machine learning engineering. To this end, we curate 75 ML
engineering-related competitions from Kaggle, creating a diverse set of
challenging tasks that test real-world ML engineering skills such as training
models, preparing datasets, and running experiments. We establish human
baselines for each competition using Kaggle's publicly available leaderboards.
We use open-source agent scaffolds to evaluate several frontier language models
on our benchmark, finding that the best-performing setup--OpenAI's o1-preview
with AIDE scaffolding--achieves at least the level of a Kaggle bronze medal in
16.9% of competitions. In addition to our main results, we investigate various
forms of resource scaling for AI agents and the impact of contamination from
pre-training. We open-source our benchmark code (github.com/openai/mle-bench/)
to facilitate future research in understanding the ML engineering capabilities
of AI agents.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 17 pages appendix. Equal contribution by first seven
  authors, authors randomized. ICLR version</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning Harmonized Representations for Speculative Sampling <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.15766v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.15766v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lefan Zhang, Xiaodan Wang, Yanhua Huang, Ruiwen Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Speculative sampling is a promising approach to accelerate the decoding stage
for Large Language Models (LLMs). Recent advancements that leverage target
LLM's contextual information, such as hidden states and KV cache, have shown
significant practical improvements. However, these approaches suffer from
inconsistent context between training and decoding. We also observe another
discrepancy between the training and decoding objectives in existing
speculative sampling methods. In this work, we propose a solution named
HArmonized Speculative Sampling (HASS) that learns harmonized representations
to address these issues. HASS accelerates the decoding stage without adding
inference overhead through harmonized objective distillation and harmonized
context alignment. Experiments on four LLaMA models demonstrate that HASS
achieves 2.81x-4.05x wall-clock time speedup ratio averaging across three
datasets, surpassing EAGLE-2 by 8%-20%. The code is available at
https://github.com/HArmonizedSS/HASS.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published as a conference paper at ICLR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Weighted-Reward Preference Optimization for Implicit Model Fusion <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.03187v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.03187v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziyi Yang, Fanqi Wan, Longguang Zhong, Tianyuan Shi, Xiaojun Quan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While fusing heterogeneous open-source LLMs with varying architectures and
sizes can potentially integrate the strengths of different models, existing
fusion methods face significant challenges, such as vocabulary alignment and
merging distribution matrices. These procedures are not only complex but also
prone to introducing noise and errors. In this paper, we propose an implicit
fusion method, Weighted-Reward Preference Optimization (WRPO), which leverages
preference optimization between the source LLMs and the target LLM to transfer
their capabilities effectively. WRPO eliminates the need for vocabulary
alignment and matrix fusion and can be efficiently scaled to accommodate
various LLMs. To address distributional deviations between the source and
target LLMs, WRPO introduces a progressive adaptation strategy that gradually
shifts reliance on preferred examples from the target LLM to the source LLMs.
Extensive experiments on the MT-Bench, AlpacaEval-2, and Arena-Hard benchmarks
demonstrate that WRPO consistently outperforms existing knowledge fusion
methods and various fine-tuning baselines. When applied to LLaMA3-8B-Instruct
as the target model, WRPO achieves a length-controlled win rate of 55.9%
against GPT-4-Preview-1106 on AlpacaEval-2 and a win rate of 46.2% against
GPT-4-0314 on Arena-Hard. Our code is available at
https://github.com/SLIT-AI/WRPO.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ICLR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ World Models for Math Story Problems <span class="chip">ACL</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.04347v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.04347v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andreas Opedal, Niklas Stoehr, Abulhair Saparov, Mrinmaya Sachan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Solving math story problems is a complex task for students and NLP models
alike, requiring them to understand the world as described in the story and
reason over it to compute an answer. Recent years have seen impressive
performance on automatically solving these problems with large pre-trained
language models and innovative techniques to prompt them. However, it remains
unclear if these models possess accurate representations of mathematical
concepts. This leads to lack of interpretability and trustworthiness which
impedes their usefulness in various applications. In this paper, we consolidate
previous work on categorizing and representing math story problems and develop
MathWorld, which is a graph-based semantic formalism specific for the domain of
math story problems. With MathWorld, we can assign world models to math story
problems which represent the situations and actions introduced in the text and
their mathematical relationships. We combine math story problems from several
existing datasets and annotate a corpus of 1,019 problems and 3,204 logical
forms with MathWorld. Using this data, we demonstrate the following use cases
of MathWorld: (1) prompting language models with synthetically generated
question-answer pairs to probe their reasoning and world modeling abilities,
and (2) generating new problems by using the world models as a design space.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ACL Findings 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ACEBench: Who Wins the Match Point in Tool Usage? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.12851v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.12851v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chen Chen, Xinlong Hao, Weiwen Liu, Xu Huang, Xingshan Zeng, Shuai Yu, Dexun Li, Shuai Wang, Weinan Gan, Yuefeng Huang, Wulong Liu, Xinzhi Wang, Defu Lian, Baoqun Yin, Yasheng Wang, Wu Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have demonstrated significant potential in
decision-making and reasoning, particularly when integrated with various tools
to effectively solve complex problems. However, existing benchmarks for
evaluating LLMs' tool usage face several limitations: (1) limited evaluation
scenarios, often lacking assessments in real multi-turn dialogue contexts; (2)
narrow evaluation dimensions, with insufficient detailed assessments of how
LLMs use tools; and (3) reliance on LLMs or real API executions for evaluation,
which introduces significant overhead. To address these challenges, we
introduce ACEBench, a comprehensive benchmark for assessing tool usage in LLMs.
ACEBench categorizes data into three primary types based on evaluation
methodology: Normal, Special, and Agent. "Normal" evaluates tool usage in basic
scenarios; "Special" evaluates tool usage in situations with ambiguous or
incomplete instructions; "Agent" evaluates tool usage through multi-agent
interactions to simulate real-world, multi-turn dialogues. We conducted
extensive experiments using ACEBench, analyzing various LLMs in-depth and
providing a more granular examination of error causes across different data
types.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ High-Fidelity Simultaneous Speech-To-Speech Translation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.03382v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.03382v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tom Labiausse, Laurent Mazaré, Edouard Grave, Patrick Pérez, Alexandre Défossez, Neil Zeghidour
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce Hibiki, a decoder-only model for simultaneous speech
translation. Hibiki leverages a multistream language model to synchronously
process source and target speech, and jointly produces text and audio tokens to
perform speech-to-text and speech-to-speech translation. We furthermore address
the fundamental challenge of simultaneous interpretation, which unlike its
consecutive counterpart, where one waits for the end of the source utterance to
start translating, adapts its flow to accumulate just enough context to produce
a correct translation in real-time, chunk by chunk. To do so, we introduce a
weakly-supervised method that leverages the perplexity of an off-the-shelf text
translation system to identify optimal delays on a per-word basis and create
aligned synthetic data. After supervised training, Hibiki performs adaptive,
simultaneous speech translation with vanilla temperature sampling. On a
French-English simultaneous speech translation task, Hibiki demonstrates
state-of-the-art performance in translation quality, speaker fidelity and
naturalness. Moreover, the simplicity of its inference process makes it
compatible with batched translation and even real-time on-device deployment. We
provide examples as well as models and inference code.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GraCoRe: Benchmarking Graph Comprehension and Complex Reasoning in Large
  Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.02936v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.02936v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zike Yuan, Ming Liu, Hui Wang, Bing Qin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Evaluating the graph comprehension and reasoning abilities of Large Language
Models (LLMs) is challenging and often incomplete. Existing benchmarks focus
primarily on pure graph understanding, lacking a comprehensive evaluation
across all graph types and detailed capability definitions. This paper presents
GraCoRe, a benchmark for systematically assessing LLMs' graph comprehension and
reasoning. GraCoRe uses a three-tier hierarchical taxonomy to categorize and
test models on pure graph and heterogeneous graphs, subdividing capabilities
into 10 distinct areas tested through 19 tasks. Our benchmark includes 11
datasets with 5,140 graphs of varying complexity. We evaluate four
closed-source and eight open-source LLMs, conducting thorough analyses from
both ability and task perspectives. Key findings reveal that OpenAI o1 model
has amazing comprehension and reasoning capabilities, semantic enrichment
enhances reasoning performance, node ordering impacts task success, and the
ability to process longer texts does not necessarily improve graph
comprehension or reasoning.GraCoRe is open-sourced at
https://github.com/ZIKEYUAN/GraCoRe
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LabSafety Bench: Benchmarking LLMs on Safety Issues in Scientific Labs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14182v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14182v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yujun Zhou, Jingdong Yang, Yue Huang, Kehan Guo, Zoe Emory, Bikram Ghosh, Amita Bedar, Sujay Shekar, Pin-Yu Chen, Tian Gao, Werner Geyer, Nuno Moniz, Nitesh V Chawla, Xiangliang Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Artificial Intelligence (AI) is revolutionizing scientific research, yet its
growing integration into laboratory environments presents critical safety
challenges. While large language models (LLMs) increasingly assist in tasks
ranging from procedural guidance to autonomous experiment orchestration, an
"illusion of understanding" may lead researchers to overestimate their
reliability. Such overreliance is especially hazardous in high-stakes
laboratory settings, where failures in hazard identification or risk assessment
can result in severe accidents. To address these concerns, we propose the
Laboratory Safety Benchmark (LabSafety Bench), a comprehensive framework that
evaluates LLMs and vision language models (VLMs) on their ability to identify
potential hazards, assess risks, and predict the consequences of unsafe actions
in lab environments. LabSafety Bench comprises 765 multiple-choice questions
aligned with US Occupational Safety and Health Administration (OSHA) protocols,
along with 520 realistic laboratory scenarios featuring dual evaluation tasks:
the Hazards Identification Test and the Consequence Identification Test, with
4090 open-ended questions in total. Evaluations across eight proprietary
models, seven open-weight LLMs, and four VLMs reveal that, despite advanced
performance on structured assessments, no model achieves the safety threshold
required for reliable operation. None scored above 75% on the Hazards
Identification Test. Moreover, while proprietary models tend to excel in
multiple-choice evaluations, their performance in open-ended, real-world
scenario responses is comparable to that of open-source models. These findings
underscore the urgent need for specialized evaluation frameworks to ensure the
safe and responsible deployment of AI in laboratory settings.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>71 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Knowledge in Superposition: Unveiling the Failures of Lifelong Knowledge
  Editing for Large Language Models <span class="chip">AAAI 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.07413v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.07413v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chenhui Hu, Pengfei Cao, Yubo Chen, Kang Liu, Jun Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Knowledge editing aims to update outdated or incorrect knowledge in large
language models (LLMs). However, current knowledge editing methods have limited
scalability for lifelong editing. This study explores the fundamental reason
why knowledge editing fails in lifelong editing. We begin with the closed-form
solution derived from linear associative memory, which underpins
state-of-the-art knowledge editing methods. We extend the solution from single
editing to lifelong editing, and through rigorous mathematical derivation,
identify an interference term in the final solution, suggesting that editing
knowledge may impact irrelevant knowledge. Further analysis of the interference
term reveals a close relationship with superposition between knowledge
representations. When knowledge superposition does not exist in language
models, the interference term vanishes, allowing for lossless knowledge
editing. Experiments across numerous language models reveal that knowledge
superposition is universal, exhibiting high kurtosis, zero mean, and
heavy-tailed distributions with clear scaling laws. Ultimately, by combining
theory and experiments, we demonstrate that knowledge superposition is the
fundamental reason for the failure of lifelong editing. Moreover, this is the
first study to investigate knowledge editing from the perspective of
superposition and provides a comprehensive observation of superposition across
numerous real-world language models. Code available at
https://github.com/ChenhuiHu/knowledge_in_superposition.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To be published in AAAI 2025 (Oral)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ On the Generalization and Adaptation Ability of Machine-Generated Text
  Detectors in Academic Writing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.17242v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.17242v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yule Liu, Zhiyuan Zhong, Yifan Liao, Zhen Sun, Jingyi Zheng, Jiaheng Wei, Qingyuan Gong, Fenghua Tong, Yang Chen, Yang Zhang, Xinlei He
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rising popularity of large language models (LLMs) has raised concerns
about machine-generated text (MGT), particularly in academic settings, where
issues like plagiarism and misinformation are prevalent. As a result,
developing a highly generalizable and adaptable MGT detection system has become
an urgent priority. Given that LLMs are most commonly misused in academic
writing, this work investigates the generalization and adaptation capabilities
of MGT detectors in three key aspects specific to academic writing: First, we
construct MGT-Acedemic, a large-scale dataset comprising over 336M tokens and
749K samples. MGT-Acedemic focuses on academic writing, featuring human-written
texts (HWTs) and MGTs across STEM, Humanities, and Social Sciences, paired with
an extensible code framework for efficient benchmarking. Second, we benchmark
the performance of various detectors for binary classification and attribution
tasks in both in-domain and cross-domain settings. This benchmark reveals the
often-overlooked challenges of attribution tasks. Third, we introduce a novel
attribution task where models have to adapt to new classes over time without
(or with very limited) access to prior training data in both few-shot and
many-shot scenarios. We implement eight different adapting techniques to
improve the performance and highlight the inherent complexity of the task. Our
findings provide insights into the generalization and adaptation ability of MGT
detectors across diverse scenarios and lay the foundation for building robust,
adaptive detection systems. The code framework is available at
https://github.com/Y-L-LIU/MGTBench-2.0.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MIR-Bench: Benchmarking LLM's Long-Context Intelligence via Many-Shot
  In-Context Inductive Reasoning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09933v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09933v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kai Yan, Zhan Ling, Kang Liu, Yifan Yang, Ting-Han Fan, Lingfeng Shen, Zhengyin Du, Jiecao Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Inductive Reasoning (IR), the ability to summarize rules from examples and
apply on new ones, has long been viewed as a primal ability for general
intelligence and widely studied by cognitive science and AI researchers. Many
benchmarks have been proposed to measure such ability for Large Language Models
(LLMs); however, they focus on few-shot (usually $<$10) setting and lack
evaluation for aggregating many pieces of information from long contexts. On
the other hand, the ever-growing context length of LLMs have brought forth the
novel paradigm of many-shot In-Context Learning (ICL), which addresses new
tasks with hundreds to thousands of examples without expensive and inefficient
fine-tuning. However, many-shot evaluations are mostly focused on
classification (a very limited aspect of IR), and popular long-context LLM
tasks such as Needle-In-A-Haystack (NIAH) seldom require complicated
intelligence for integrating many pieces of information. To fix the issues from
both worlds, we propose MIR-Bench, the first many-shot in-context inductive
reasoning benchmark that asks LLM to induce output via input-output examples
from underlying functions with diverse data format. Based on MIR-Bench, we
study many novel problems for inductive reasoning and many-shot ICL, including
robustness against erroneous shots and the effect of Chain-of-Thought (CoT),
and acquired insightful findings.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>32 pages, 11 figures. v3 slightly adjust the author institution</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AiSAQ: All-in-Storage ANNS with Product Quantization for DRAM-free
  Information Retrieval 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.06004v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.06004v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kento Tatsuno, Daisuke Miyashita, Taiga Ikeda, Kiyoshi Ishiyama, Kazunari Sumiyoshi, Jun Deguchi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph-based approximate nearest neighbor search (ANNS) algorithms work
effectively against large-scale vector retrieval. Among such methods, DiskANN
achieves good recall-speed tradeoffs using both DRAM and storage. DiskANN
adopts product quantization (PQ) to reduce memory usage, which is still
proportional to the scale of datasets. In this paper, we propose All-in-Storage
ANNS with Product Quantization (AiSAQ), which offloads compressed vectors to
the SSD index. Our method achieves $\sim$10 MB memory usage in query search
with billion-scale datasets without critical latency degradation. AiSAQ also
reduces the index load time for query search preparation, which enables fast
switch between muitiple billion-scale indices.This method can be applied to
retrievers of retrieval-augmented generation (RAG) and be scaled out with
multiple-server systems for emerging datasets. Our DiskANN-based implementation
is available on GitHub.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 8 figures and 5 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Sparsing Law: Towards Large Language Models with Greater Activation
  Sparsity 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02335v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02335v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuqi Luo, Chenyang Song, Xu Han, Yingfa Chen, Chaojun Xiao, Zhiyuan Liu, Maosong Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Activation sparsity denotes the existence of substantial weakly-contributed
elements within activation outputs that can be eliminated, benefiting many
important applications concerned with large language models (LLMs). Although
promoting greater activation sparsity within LLMs deserves deep studies,
existing works lack comprehensive and quantitative research on the correlation
between activation sparsity and potentially influential factors. In this paper,
we present a comprehensive study on the quantitative scaling properties and
influential factors of the activation sparsity within decoder-only
Transformer-based LLMs. Specifically, we propose PPL-$p\%$ sparsity, a precise
and performance-aware activation sparsity metric that is applicable to any
activation function. Through extensive experiments, we find several important
phenomena. Firstly, different activation functions exhibit comparable
performance but opposite training-time sparsity trends. The activation ratio
(i.e., $1-\mathrm{sparsity\ ratio}$) evolves as a convergent increasing
power-law and decreasing logspace power-law with the amount of training data
for SiLU-activated and ReLU-activated LLMs, respectively. These demonstrate
that ReLU is more efficient as the activation function than SiLU and can
leverage more training data to improve activation sparsity. Secondly, the
activation ratio linearly increases with the width-depth ratio below a certain
bottleneck point, indicating the potential advantage of a deeper architecture
at a fixed parameter scale. Finally, at similar width-depth ratios, we
surprisingly find that the limit value of activation sparsity varies weakly
with the parameter scale, i.e., the activation patterns within LLMs are
insensitive to the parameter scale. These empirical laws towards LLMs with
greater activation sparsity have important implications for making LLMs more
efficient and interpretable.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>23 pages, 13 figures, 6 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ExpliCa: Evaluating Explicit Causal Reasoning in Large Language Models <span class="chip">ACL 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.15487v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.15487v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Martina Miliani, Serena Auriemma, Alessandro Bondielli, Emmanuele Chersoni, Lucia Passaro, Irene Sucameli, Alessandro Lenci
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) are increasingly used in tasks requiring
interpretive and inferential accuracy. In this paper, we introduce ExpliCa, a
new dataset for evaluating LLMs in explicit causal reasoning. ExpliCa uniquely
integrates both causal and temporal relations presented in different linguistic
orders and explicitly expressed by linguistic connectives. The dataset is
enriched with crowdsourced human acceptability ratings. We tested LLMs on
ExpliCa through prompting and perplexity-based metrics. We assessed seven
commercial and open-source LLMs, revealing that even top models struggle to
reach 0.80 accuracy. Interestingly, models tend to confound temporal relations
with causal ones, and their performance is also strongly influenced by the
linguistic order of the events. Finally, perplexity-based scores and prompting
performance are differently affected by model size.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to ACL 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Theme-Explanation Structure for Table Summarization using Large Language
  Models: A Case Study on Korean Tabular Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.10487v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.10487v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        TaeYoon Kwack, Jisoo Kim, Ki Yong Jung, DongGeon Lee, Heesun Park
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper proposes the Theme-Explanation Structure-based Table Summarization
(Tabular-TX) pipeline designed to process tabular data efficiently. Tabular-TX
preprocesses tabular data by focusing on highlighted cells. It then generates
summary sentences following a structured format, where the Theme Part appears
as an adverbial phrase, and the Explanation Part follows as a predictive
clause. This approach enables tailored analysis by considering the structural
characteristics of tables and their comparability. Unlike conventional
fine-tuning approaches that require extensive labeled data and computational
resources, our method leverages In-Context Learning to dynamically adapt to
different table structures without additional training, ensuring efficient and
scalable table interpretation. Experimental results demonstrate that Tabular-TX
significantly outperforms conventional fine-tuning-based methods, particularly
in low-resource scenarios, by leveraging table structures and metadata more
effectively through structured prompts. The results confirm that Tabular-TX
enables more effective processing of complex tabular data. Furthermore, it
serves as a viable alternative for table-based question answering and
summarization tasks in resource-constrained environments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Subtle Errors Matter: Preference Learning via Error-injected
  Self-editing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.06638v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.06638v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kaishuai Xu, Tiezheng Yu, Wenjun Hou, Yi Cheng, Chak Tou Leong, Liangyou Li, Xin Jiang, Lifeng Shang, Qun Liu, Wenjie Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have exhibited strong mathematical reasoning
prowess, tackling tasks ranging from basic arithmetic to advanced
competition-level problems. However, frequently occurring subtle yet critical
errors, such as miscalculations or incorrect substitutions, limit the LLMs'
full potential. Existing studies to improve mathematical ability typically
involve applying preference learning to step-wise solution pairs. Although
these methods leverage samples of varying granularity to mitigate reasoning
errors, they overlook critical subtle errors. In this work, we propose a novel
preference learning framework called eRror-Injected Self-Editing (RISE), which
injects predefined subtle errors into pivotal tokens in reasoning or
computation steps to construct hard pairs for error mitigation. In detail, RISE
uses the LLM itself to edit a small number of tokens in the solution, injecting
designed subtle errors. Then, pairs composed of self-edited solutions and their
corresponding correct ones, along with pairs of correct and incorrect solutions
obtained through sampling, are used together for subtle error-aware DPO
training. Compared with other preference learning methods, RISE further refines
the training objective without requiring fine-grained sampling or preference
annotation. Extensive experiments validate the effectiveness of RISE, with
preference learning on Qwen2-7B-Instruct yielding notable improvements of 3.0%
on GSM8K and 7.9% on MATH with only 4.5K training samples. Moreover, the effect
of error mitigation extends from mathematical reasoning to logical reasoning
and code generation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Bayesian Optimization for Controlled Image Editing via LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.18116v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.18116v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chengkun Cai, Haoliang Liu, Xu Zhao, Zhongyu Jiang, Tianfang Zhang, Zongkai Wu, Jenq-Neng Hwang, Serge Belongie, Lei Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the rapidly evolving field of image generation, achieving precise control
over generated content and maintaining semantic consistency remain significant
limitations, particularly concerning grounding techniques and the necessity for
model fine-tuning. To address these challenges, we propose BayesGenie, an
off-the-shelf approach that integrates Large Language Models (LLMs) with
Bayesian Optimization to facilitate precise and user-friendly image editing.
Our method enables users to modify images through natural language descriptions
without manual area marking, while preserving the original image's semantic
integrity. Unlike existing techniques that require extensive pre-training or
fine-tuning, our approach demonstrates remarkable adaptability across various
LLMs through its model-agnostic design. BayesGenie employs an adapted Bayesian
optimization strategy to automatically refine the inference process parameters,
achieving high-precision image editing with minimal user intervention. Through
extensive experiments across diverse scenarios, we demonstrate that our
framework significantly outperforms existing methods in both editing accuracy
and semantic preservation, as validated using different LLMs including Claude3
and GPT-4.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ELICIT: LLM Augmentation via External In-Context Capability <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.09343v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.09343v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Futing Wang, Jianhao Yan, Yue Zhang, Tao Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Enhancing the adaptive capabilities of large language models is a critical
pursuit in both research and application. Traditional fine-tuning methods
require substantial data and computational resources, especially for enhancing
specific capabilities, while in-context learning is limited by the need for
appropriate demonstrations and efficient token usage. Inspired by the
expression of in-context learned capabilities through task vectors and the
concept of modularization, we propose \alg, a framework consisting of two
modules designed to effectively store and reuse task vectors to elicit the
diverse capabilities of models without additional training or inference tokens.
Our comprehensive experiments and analysis demonstrate that our pipeline is
highly transferable across different input formats, tasks, and model
architectures. ELICIT serves as a plug-and-play performance booster to enable
adaptive elicitation of model capabilities. By externally storing and reusing
vectors that represent in-context learned capabilities, \alg not only
demonstrates the potential to operate modular capabilities but also
significantly enhances the performance, versatility, adaptability, and
scalability of large language models. Our code will be publicly available at
https://github.com/LINs-lab/ELICIT.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published at ICLR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ On Many-Shot In-Context Learning for Long-Context Evaluation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07130v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07130v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kaijian Zou, Muhammad Khalifa, Lu Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Many-shot in-context learning (ICL) has emerged as a unique setup to both
utilize and test the ability of large language models to handle long context.
This paper delves into long-context language model (LCLM) evaluation through
many-shot ICL. We first ask: what types of ICL tasks benefit from additional
demonstrations, and how effective are they in evaluating LCLMs? We find that
classification and summarization tasks show performance improvements with
additional demonstrations, while translation and reasoning tasks do not exhibit
clear trends. Next, we investigate the extent to which different tasks
necessitate retrieval versus global context understanding. We develop metrics
to categorize ICL tasks into two groups: (i) similar-sample learning (SSL):
tasks where retrieval of the most similar examples is sufficient for good
performance, and (ii) all-sample learning (ASL): tasks that necessitate a
deeper comprehension of all examples in the prompt. Lastly, we introduce a new
many-shot ICL benchmark, MANYICLBENCH, to characterize model's ability on both
fronts and benchmark 12 LCLMs using MANYICLBENCH. We find that while
state-of-the-art models demonstrate good performance up to 64k tokens in SSL
tasks, many models experience significant performance drops at only 16k tokens
in ASL tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AFlow: Automating Agentic Workflow Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.10762v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.10762v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiayi Zhang, Jinyu Xiang, Zhaoyang Yu, Fengwei Teng, Xionghui Chen, Jiaqi Chen, Mingchen Zhuge, Xin Cheng, Sirui Hong, Jinlin Wang, Bingnan Zheng, Bang Liu, Yuyu Luo, Chenglin Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have demonstrated remarkable potential in
solving complex tasks across diverse domains, typically by employing agentic
workflows that follow detailed instructions and operational sequences. However,
constructing these workflows requires significant human effort, limiting
scalability and generalizability. Recent research has sought to automate the
generation and optimization of these workflows, but existing methods still rely
on initial manual setup and fall short of achieving fully automated and
effective workflow generation. To address this challenge, we reformulate
workflow optimization as a search problem over code-represented workflows,
where LLM-invoking nodes are connected by edges. We introduce AFlow, an
automated framework that efficiently explores this space using Monte Carlo Tree
Search, iteratively refining workflows through code modification,
tree-structured experience, and execution feedback. Empirical evaluations
across six benchmark datasets demonstrate AFlow's efficacy, yielding a 5.7%
average improvement over state-of-the-art baselines. Furthermore, AFlow enables
smaller models to outperform GPT-4o on specific tasks at 4.55% of its inference
cost in dollars. The code is available at https://github.com/geekan/MetaGPT.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Small Language Models: <span class="highlight-title">Survey</span>, Measurements, and Insights 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.15790v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.15790v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhenyan Lu, Xiang Li, Dongqi Cai, Rongjie Yi, Fangming Liu, Xiwen Zhang, Nicholas D. Lane, Mengwei Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Small language models (SLMs), despite their widespread adoption in modern
smart devices, have received significantly less academic attention compared to
their large language model (LLM) counterparts, which are predominantly deployed
in data centers and cloud environments. While researchers continue to improve
the capabilities of LLMs in the pursuit of artificial general intelligence, SLM
research aims to make machine intelligence more accessible, affordable, and
efficient for everyday tasks. Focusing on transformer-based, decoder-only
language models with 100M-5B parameters, we survey 70 state-of-the-art
open-source SLMs, analyzing their technical innovations across three axes:
architectures, training datasets, and training algorithms. In addition, we
evaluate their capabilities in various domains, including commonsense
reasoning, mathematics, in-context learning, and long context. To gain further
insight into their on-device runtime costs, we benchmark their inference
latency and memory footprints. Through in-depth analysis of our benchmarking
data, we offer valuable insights to advance research in this field.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SuperCorrect: Advancing Small LLM Reasoning with Thought Template
  Distillation and Self-Correction <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.09008v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.09008v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ling Yang, Zhaochen Yu, Tianjun Zhang, Minkai Xu, Joseph E. Gonzalez, Bin Cui, Shuicheng Yan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) like GPT-4, DeepSeek-R1, and ReasonFlux have
shown significant improvements in various reasoning tasks. However, smaller
LLMs still struggle with complex mathematical reasoning because they fail to
effectively identify and correct reasoning errors. Recent reflection-based
methods aim to address these issues by enabling self-reflection and
self-correction, but they still face challenges in independently detecting
errors in their reasoning steps. To overcome these limitations, we propose
SuperCorrect, a novel two-stage framework that uses a large teacher model to
supervise and correct both the reasoning and reflection processes of a smaller
student model. In the first stage, we extract hierarchical high-level and
detailed thought templates from the teacher model to guide the student model in
eliciting more fine-grained reasoning thoughts. In the second stage, we
introduce cross-model collaborative direct preference optimization (DPO) to
enhance the self-correction abilities of the student model by following the
teacher's correction traces during training. This cross-model DPO approach
teaches the student model to effectively locate and resolve erroneous thoughts
with error-driven insights from the teacher model, breaking the bottleneck of
its thoughts and acquiring new skills and knowledge to tackle challenging
problems. Extensive experiments consistently demonstrate our superiority over
previous methods. Notably, our SuperCorrect-7B model significantly surpasses
powerful DeepSeekMath-7B by 7.8%/5.3% and Qwen2.5-Math-7B by 15.1%/6.3% on
MATH/GSM8K benchmarks, achieving new SOTA performance among all 7B models.
Code: https://github.com/YangLing0818/SuperCorrect-llm
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2025. Project: https://github.com/YangLing0818/SuperCorrect-llm</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Permute-and-Flip: An optimally stable and watermarkable decoder for LLMs <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.05864v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.05864v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xuandong Zhao, Lei Li, Yu-Xiang Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we propose a new decoding method called Permute-and-Flip (PF)
decoder. It enjoys stability properties similar to the standard sampling
decoder, but is provably up to 2x better in its quality-stability tradeoff than
sampling and never worse than any other decoder. We also design a cryptographic
watermarking scheme analogous to Aaronson (2023)'s Gumbel watermark, but
naturally tailored for PF decoder. The watermarking scheme does not change the
distribution to sample, while allowing arbitrarily low false positive rate and
high recall whenever the generated text has high entropy. Our experiments show
that the PF decoder (and its watermarked counterpart) significantly
outperform(s) naive sampling (and its Gumbel watermarked counterpart) in terms
of perplexity, while retaining the same stability (and detectability), hence
making it a promising new approach for LLM decoding. The code is available at
https://github.com/XuandongZhao/pf-decoding
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SparseTransX: Efficient Training of Translation-Based Knowledge Graph
  Embeddings Using Sparse Matrix Operations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.16949v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.16949v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Md Saidul Hoque Anik, Ariful Azad
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Knowledge graph (KG) learning offers a powerful framework for generating new
knowledge and making inferences. Training KG embedding can take a significantly
long time, especially for larger datasets. Our analysis shows that the gradient
computation of embedding is one of the dominant functions in the
translation-based KG embedding training loop. We address this issue by
replacing the core embedding computation with SpMM (Sparse-Dense Matrix
Multiplication) kernels. This allows us to unify multiple scatter (and gather)
operations as a single operation, reducing training time and memory usage. We
create a general framework for training KG models using sparse kernels and
implement four models, namely TransE, TransR, TransH, and TorusE. Our sparse
implementations exhibit up to 5.3x speedup on the CPU and up to 4.2x speedup on
the GPU with a significantly low GPU memory footprint. The speedups are
consistent across large and small datasets for a given model. Our proposed
sparse approach can be extended to accelerate other translation-based (such as
TransC, TransM, etc.) and non-translational (such as DistMult, ComplEx, RotatE,
etc.) models as well.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages. To appear in MLSys 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Meta <span class="highlight-title">Prompt</span>ing for AI Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.11482v7">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.11482v7.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yifan Zhang, Yang Yuan, Andrew Chi-Chih Yao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce Meta Prompting (MP), a prompting paradigm designed to enhance
the utilization of large language models (LLMs) and AI systems in complex
problem-solving and data interaction. Grounded in type theory and category
theory, Meta Prompting prioritizes structural and syntactical considerations
over traditional content-centric methods. In this work, we formally define Meta
Prompting, delineate its distinctions from few-shot prompting, and demonstrate
its effectiveness across various AI applications. In particular, we show that
Meta Prompting can decompose intricate reasoning tasks into simpler
sub-problems, thereby improving token efficiency and enabling fairer
comparisons with conventional few-shot techniques. Furthermore, we extend this
framework to prompting tasks, allowing LLMs to recursively self-generate
refined prompts in a metaprogramming-like manner. Empirical evaluations reveal
that a Qwen-72B base language model equipped with Meta Prompting-without
additional instruction tuning-achieves a PASS@1 accuracy of 46.3% on MATH
problems, surpassing a supervised fine-tuned counterpart, 83.5% accuracy on
GSM8K, and a 100% success rate on Game of 24 tasks using GPT-4. The code is
available at https://github.com/meta-prompting/meta-prompting.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Efficient Inference for Large Language Model-based Generative
  Recommendation <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05165v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05165v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinyu Lin, Chaoqun Yang, Wenjie Wang, Yongqi Li, Cunxiao Du, Fuli Feng, See-Kiong Ng, Tat-Seng Chua
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Model (LLM)-based generative recommendation has achieved
notable success, yet its practical deployment is costly particularly due to
excessive inference latency caused by autoregressive decoding. For lossless LLM
decoding acceleration, Speculative Decoding (SD) has emerged as a promising
solution. However, applying SD to generative recommendation presents unique
challenges due to the requirement of generating top-K items (i.e., K distinct
token sequences) as a recommendation list by beam search. This leads to more
stringent verification in SD, where all the top-K sequences from the target LLM
must be successfully drafted by the draft model at each decoding step. To
alleviate this, we consider 1) boosting top-K sequence alignment between the
draft model and the target LLM, and 2) relaxing the verification strategy to
reduce trivial LLM calls. To this end, we propose an alignment framework named
AtSpeed, which presents the AtSpeed-S optimization objective for top-K
alignment under the strict top-K verification. Moreover, we introduce a relaxed
sampling verification strategy that allows high-probability non-top-K drafted
sequences to be accepted, significantly reducing LLM calls. Correspondingly, we
propose AtSpeed-R for top-K alignment under this relaxed sampling verification.
Empirical results on two real-world datasets demonstrate that AtSpeed
significantly accelerates LLM-based generative recommendation, e.g., near 2x
speedup under strict top-K verification and up to 2.5x speedup under relaxed
sampling verification. The codes and datasets are released at
https://github.com/Linxyhaha/AtSpeed.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICLR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Threshold Filtering Packing for Supervised Fine-Tuning: Training Related
  Samples within Packs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.09327v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.09327v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiancheng Dong, Lei Jiang, Wei Jin, Lu Cheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Packing for Supervised Fine-Tuning (SFT) in autoregressive models involves
concatenating data points of varying lengths until reaching the designed
maximum length to facilitate GPU processing. However, randomly concatenating
data points can lead to cross-contamination of sequences due to the significant
difference in their subject matter. The mainstream approaches in SFT ensure
that each token in the attention calculation phase only focuses on tokens
within its own short sequence, without providing additional learning signals
for the preceding context. To address these challenges, we introduce Threshold
Filtering Packing (TFP), a method that selects samples with related context
while maintaining sufficient diversity within the same pack. Our experiments
show that TFP offers a simple-to-implement and scalable approach that
significantly enhances SFT performance, with observed improvements of up to 7\%
on GSM8K, 4\% on HumanEval. Furthermore, results from bias benchmark datasets
highlight TFP's promising performance in improving fairness while also boosting
prediction accuracy by 15\%.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Teaching LLMs According to Their Aptitude: Adaptive Reasoning for
  Mathematical Problem Solving 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.12022v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.12022v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xin Xu, Yan Xu, Tianhao Chen, Yuchen Yan, Chengwu Liu, Zaoyu Chen, Yufei Wang, Yichun Yin, Yasheng Wang, Lifeng Shang, Qun Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing approaches to mathematical reasoning with large language models
(LLMs) rely on Chain-of-Thought (CoT) for generalizability or Tool-Integrated
Reasoning (TIR) for precise computation. While efforts have been made to
combine these methods, they primarily rely on post-selection or predefined
strategies, leaving an open question: whether LLMs can autonomously adapt their
reasoning strategy based on their inherent capabilities. In this work, we
propose TATA (Teaching LLMs According to Their Aptitude), an adaptive framework
that enables LLMs to personalize their reasoning strategy spontaneously,
aligning it with their intrinsic aptitude. TATA incorporates base-LLM-aware
data selection during supervised fine-tuning (SFT) to tailor training data to
the model's unique abilities. This approach equips LLMs to autonomously
determine and apply the appropriate reasoning strategy at test time. We
evaluate TATA through extensive experiments on six mathematical reasoning
benchmarks, using both general-purpose and math-specialized LLMs. Empirical
results demonstrate that TATA effectively combines the complementary strengths
of CoT and TIR, achieving superior or comparable performance with improved
inference efficiency compared to TIR alone. Further analysis underscores the
critical role of aptitude-aware data selection in enabling LLMs to make
effective and adaptive reasoning decisions and align reasoning strategies with
model capabilities.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MKE-Coder: Multi-Axial Knowledge with Evidence Verification in ICD
  Coding for Chinese EMRs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.14916v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.14916v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinxin You, Xien Liu, Xue Yang, Ziyi Wang, Ji Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The task of automatically coding the International Classification of Diseases
(ICD) in the medical field has been well-established and has received much
attention. Automatic coding of the ICD in the medical field has been successful
in English but faces challenges when dealing with Chinese electronic medical
records (EMRs). The first issue lies in the difficulty of extracting disease
code-related information from Chinese EMRs, primarily due to the concise
writing style and specific internal structure of the EMRs. The second problem
is that previous methods have failed to leverage the disease-based multi-axial
knowledge and lack of association with the corresponding clinical evidence.
This paper introduces a novel framework called MKE-Coder: Multi-axial Knowledge
with Evidence verification in ICD coding for Chinese EMRs. Initially, we
identify candidate codes for the diagnosis and categorize each of them into
knowledge under four coding axes.Subsequently, we retrieve corresponding
clinical evidence from the comprehensive content of EMRs and filter credible
evidence through a scoring model. Finally, to ensure the validity of the
candidate code, we propose an inference module based on the masked language
modeling strategy. This module verifies that all the axis knowledge associated
with the candidate code is supported by evidence and provides recommendations
accordingly. To evaluate the performance of our framework, we conduct
experiments using a large-scale Chinese EMR dataset collected from various
hospitals. The experimental results demonstrate that MKE-Coder exhibits
significant superiority in the task of automatic ICD coding based on Chinese
EMRs. In the practical evaluation of our method within simulated real coding
scenarios, it has been demonstrated that our approach significantly aids coders
in enhancing both their coding accuracy and speed.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Expert-Token Resonance MoE: Bidirectional Routing with Efficiency
  Affinity-Driven Active Selection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.00023v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.00023v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jing Li, Zhijie Sun, Dachao Lin, Xuan He, Binfan Zheng, Yi Lin, Rongqian Zhao, Xin Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Mixture-of-Experts (MoE) architectures have emerged as a paradigm-shifting
approach for large language models (LLMs), offering unprecedented computational
efficiency. However, these architectures grapple with challenges of token
distribution imbalance and expert homogenization, impeding optimal semantic
generalization. We propose a novel expert routing framework that incorporates:
(1) An efficient routing mechanism with lightweight computation. (2) An
adaptive bidirectional selection mechanism leveraging resonance between experts
and tokens. (3) A module that determines the lower bounds of expert capacity
based on dynamic token distribution analysis, specifically designed to address
drop-and-pad strategies. It is also integrated with orthogonal feature
extraction module and an optimized loss function for expert localization. This
framework effectively reduces expert homogeneity while enhancing the
performance of the expert selection module. Additionally, we introduce a local
expert strategy that simultaneously improves load balancing and reduces network
communication overhead. It achieves a 40\% reduction in token processed by each
expert without compromising model convergence or efficacy. When coupled with
communication optimizations, the training efficiency improvements of 5.4\% to
46.6\% can be observed. After supervised fine-tuning, it exhibits performance
gains of 9.7\% to 14.1\% across GDAD, GPQA, and TeleQnA benchmarks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ From Exploration to Mastery: Enabling LLMs to Master Tools via
  Self-Driven Interactions <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.08197v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.08197v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Changle Qu, Sunhao Dai, Xiaochi Wei, Hengyi Cai, Shuaiqiang Wang, Dawei Yin, Jun Xu, Ji-Rong Wen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Tool learning enables Large Language Models (LLMs) to interact with external
environments by invoking tools, serving as an effective strategy to mitigate
the limitations inherent in their pre-training data. In this process, tool
documentation plays a crucial role by providing usage instructions for LLMs,
thereby facilitating effective tool utilization. This paper concentrates on the
critical challenge of bridging the comprehension gap between LLMs and external
tools due to the inadequacies and inaccuracies inherent in existing
human-centric tool documentation. We propose a novel framework, DRAFT, aimed at
Dynamically Refining tool documentation through the Analysis of Feedback and
Trials emanating from LLMs' interactions with external tools. This methodology
pivots on an innovative trial-and-error approach, consisting of three distinct
learning phases: experience gathering, learning from experience, and
documentation rewriting, to iteratively enhance the tool documentation. This
process is further optimized by implementing a diversity-promoting exploration
strategy to ensure explorative diversity and a tool-adaptive termination
mechanism to prevent overfitting while enhancing efficiency. Extensive
experiments on multiple datasets demonstrate that DRAFT's iterative,
feedback-based refinement significantly ameliorates documentation quality,
fostering a deeper comprehension and more effective utilization of tools by
LLMs. Notably, our analysis reveals that the tool documentation refined via our
approach demonstrates robust cross-model generalization capabilities.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2025 Oral;GitHub:https://github.com/quchangle1/DRAFT</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ HYBRIDMIND: Meta Selection of Natural Language and Symbolic Language for
  Enhanced LLM Reasoning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.19381v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.19381v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Simeng Han, Tianyu Liu, Chuhan Li, Xuyuan Xiong, Arman Cohan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  LLMs approach logical and mathematical reasoning through natural or symbolic
languages. While natural language offers human-accessible flexibility but
suffers from ambiguity, symbolic reasoning provides precise, machine-executable
inferences at the cost of strict domain constraints. We introduce HYBRIDMIND,
an adaptive strategy that selects the optimal reasoning approach for each
reasoning problem. Through extensive experiments, we evaluate both
prompting-based approaches with state-of-the-art LLMs and fine-tuned
open-source models. We find that fine-tuning LLaMA-3.1-8B-Instruct as a
meta-selector outperforms GPT-4o's natural language reasoning by 4.4\% on FOLIO
and 1.3\% on MATH. More notably, using GPT-3.5-turbo as a prompted
meta-selector yields a 10\% improvement on FOLIO's challenging subset compared
to GPT-4o. We will release our code and data to support future research.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Mapping and Influencing the Political Ideology of Large Language Models
  using Synthetic Personas <span class="chip">WWW</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.14843v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.14843v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pietro Bernardelle, Leon Fröhling, Stefano Civelli, Riccardo Lunardi, Kevin Roitero, Gianluca Demartini
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The analysis of political biases in large language models (LLMs) has
primarily examined these systems as single entities with fixed viewpoints.
While various methods exist for measuring such biases, the impact of
persona-based prompting on LLMs' political orientation remains unexplored. In
this work we leverage PersonaHub, a collection of synthetic persona
descriptions, to map the political distribution of persona-based prompted LLMs
using the Political Compass Test (PCT). We then examine whether these initial
compass distributions can be manipulated through explicit ideological prompting
towards diametrically opposed political orientations: right-authoritarian and
left-libertarian. Our experiments reveal that synthetic personas predominantly
cluster in the left-libertarian quadrant, with models demonstrating varying
degrees of responsiveness when prompted with explicit ideological descriptors.
While all models demonstrate significant shifts towards right-authoritarian
positions, they exhibit more limited shifts towards left-libertarian positions,
suggesting an asymmetric response to ideological manipulation that may reflect
inherent biases in model training.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Companion Proceedings of the ACM Web Conference 2025 (WWW
  Companion'25)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The Impact of Persona-based Political Perspectives on Hateful Content
  Detection <span class="chip">WWW</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.00385v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.00385v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Stefano Civelli, Pietro Bernardelle, Gianluca Demartini
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While pretraining language models with politically diverse content has been
shown to improve downstream task fairness, such approaches require significant
computational resources often inaccessible to many researchers and
organizations. Recent work has established that persona-based prompting can
introduce political diversity in model outputs without additional training.
However, it remains unclear whether such prompting strategies can achieve
results comparable to political pretraining for downstream tasks. We
investigate this question using persona-based prompting strategies in
multimodal hate-speech detection tasks, specifically focusing on hate speech in
memes. Our analysis reveals that when mapping personas onto a political compass
and measuring persona agreement, inherent political positioning has
surprisingly little correlation with classification decisions. Notably, this
lack of correlation persists even when personas are explicitly injected with
stronger ideological descriptors. Our findings suggest that while LLMs can
exhibit political biases in their responses to direct political questions,
these biases may have less impact on practical classification tasks than
previously assumed. This raises important questions about the necessity of
computationally expensive political pretraining for achieving fair performance
in downstream tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Companion Proceedings of the ACM Web Conference 2025 (WWW
  Companion'25)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Citrus: Leveraging Expert Cognitive Pathways in a Medical Language Model
  for Advanced Medical Decision Support 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.18274v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.18274v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guoxin Wang, Minyu Gao, Shuai Yang, Ya Zhang, Lizhi He, Liang Huang, Hanlin Xiao, Yexuan Zhang, Wanyue Li, Lu Chen, Jintao Fei, Xin Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs), particularly those with reasoning capabilities,
have rapidly advanced in recent years, demonstrating significant potential
across a wide range of applications. However, their deployment in healthcare,
especially in disease reasoning tasks, is hindered by the challenge of
acquiring expert-level cognitive data. In this paper, we introduce Citrus, a
medical language model that bridges the gap between clinical expertise and AI
reasoning by emulating the cognitive processes of medical experts. The model is
trained on a large corpus of simulated expert disease reasoning data,
synthesized using a novel approach that accurately captures the decision-making
pathways of clinicians. This approach enables Citrus to better simulate the
complex reasoning processes involved in diagnosing and treating medical
conditions. To further address the lack of publicly available datasets for
medical reasoning tasks, we release the last-stage training data, including a
custom-built medical diagnostic dialogue dataset. This open-source contribution
aims to support further research and development in the field. Evaluations
using authoritative benchmarks such as MedQA, covering tasks in medical
reasoning and language understanding, show that Citrus achieves superior
performance compared to other models of similar size. These results highlight
Citrus potential to significantly enhance medical decision support systems,
providing a more accurate and efficient tool for clinical decision-making.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AttentionPredictor: Temporal Pattern Matters for Efficient LLM Inference 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.04077v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.04077v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qingyue Yang, Jie Wang, Xing Li, Zhihai Wang, Chen Chen, Lei Chen, Xianzhi Yu, Wulong Liu, Jianye Hao, Mingxuan Yuan, Bin Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the development of large language models (LLMs), efficient inference
through Key-Value (KV) cache compression has attracted considerable attention,
especially for long-context generation. To compress the KV cache, recent
methods identify critical KV tokens through heuristic ranking with attention
scores. However, these methods often struggle to accurately determine critical
tokens as they neglect the \textit{temporal patterns} in attention scores,
resulting in a noticeable degradation in LLM performance. To address this
challenge, we propose AttentionPredictor, which is the first learning-based
critical token identification approach. Specifically, AttentionPredictor learns
a lightweight convolution model to capture spatiotemporal patterns and predict
the next-token attention score. An appealing feature of AttentionPredictor is
that it accurately predicts the attention score while consuming negligible
memory. Moreover, we propose a cross-token critical cache prefetching framework
that hides the token estimation time overhead to accelerate the decoding stage.
By retaining most of the attention information, AttentionPredictor achieves
16$\times$ KV cache compression with comparable LLM performance, significantly
outperforming the state-of-the-art.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Low-Rank and Sparse Model Merging for Multi-Lingual Speech Recognition
  and Translation <span class="chip">ACL 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.17380v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.17380v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qiuming Zhao, Guangzhi Sun, Chao Zhang, Mingxing Xu, Thomas Fang Zheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Language diversity presents a significant challenge in speech-to-text (S2T)
tasks, such as automatic speech recognition and translation. Traditional
multi-task training approaches aim to address this by jointly optimizing
multiple speech recognition and translation tasks across various languages.
While models like Whisper, built on these strategies, demonstrate strong
performance, they still face issues of high computational cost, language
interference, suboptimal training configurations, and limited extensibility. To
overcome these challenges, we introduce LoRS-Merging (low-rank and sparse model
merging), a novel technique designed to efficiently integrate models trained on
different languages or tasks while preserving performance and reducing
computational overhead. LoRS-Merging combines low-rank and sparse pruning to
retain essential structures while eliminating redundant parameters, mitigating
language and task interference, and enhancing extensibility. Experimental
results across a range of languages demonstrate that LoRS-Merging reduces the
word error rate by 10% and improves BLEU scores by 4% compared to conventional
multi-lingual multi-task training baselines. Our findings suggest that model
merging, particularly LoRS-Merging, is a scalable and effective complement to
traditional multi-lingual training strategies for S2T applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, submitted to ACL 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Can LLMs Solve longer Math Word Problems Better? <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.14804v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.14804v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xin Xu, Tong Xiao, Zitong Chao, Zhenya Huang, Can Yang, Yang Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Math Word Problems (MWPs) play a vital role in assessing the capabilities of
Large Language Models (LLMs), yet current research primarily focuses on
questions with concise contexts. The impact of longer contexts on mathematical
reasoning remains under-explored. This study pioneers the investigation of
Context Length Generalizability (CoLeG), which refers to the ability of LLMs to
solve MWPs with extended narratives. We introduce Extended Grade-School Math
(E-GSM), a collection of MWPs featuring lengthy narratives, and propose two
novel metrics to evaluate the efficacy and resilience of LLMs in tackling these
problems. Our analysis of existing zero-shot prompting techniques with
proprietary LLMs along with open-source LLMs reveals a general deficiency in
CoLeG. To alleviate these issues, we propose tailored approaches for different
categories of LLMs. For proprietary LLMs, we introduce a new instructional
prompt designed to mitigate the impact of long contexts. For open-source LLMs,
we develop a novel auxiliary task for fine-tuning to enhance CoLeG. Our
comprehensive results demonstrate the effectiveness of our proposed methods,
showing improved performance on E-GSM. Additionally, we conduct an in-depth
analysis to differentiate the effects of semantic understanding and reasoning
efficacy, showing that our methods improves the latter. We also establish the
generalizability of our methods across several other MWP benchmarks. Our
findings highlight the limitations of current LLMs and offer practical
solutions correspondingly, paving the way for further exploration of model
generalizability and training methodologies.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ICLR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Systematic Outliers in Large Language Models <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.06415v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.06415v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yongqi An, Xu Zhao, Tao Yu, Ming Tang, Jinqiao Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Outliers have been widely observed in Large Language Models (LLMs),
significantly impacting model performance and posing challenges for model
compression. Understanding the functionality and formation mechanisms of these
outliers is critically important. Existing works, however, largely focus on
reducing the impact of outliers from an algorithmic perspective, lacking an
in-depth investigation into their causes and roles. In this work, we provide a
detailed analysis of the formation process, underlying causes, and functions of
outliers in LLMs. We define and categorize three types of outliers-activation
outliers, weight outliers, and attention outliers-and analyze their
distributions across different dimensions, uncovering inherent connections
between their occurrences and their ultimate influence on the attention
mechanism. Based on these observations, we hypothesize and explore the
mechanisms by which these outliers arise and function, demonstrating through
theoretical derivations and experiments that they emerge due to the
self-attention mechanism's softmax operation. These outliers act as implicit
context-aware scaling factors within the attention mechanism. As these outliers
stem from systematic influences, we term them systematic outliers. Our study
not only enhances the understanding of Transformer-based LLMs but also shows
that structurally eliminating outliers can accelerate convergence and improve
model compression. The code is avilable at
https://github.com/an-yongqi/systematic-outliers.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ICLR 2025. Project Page:
  https://github.com/an-yongqi/systematic-outliers</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Exploring Translation Mechanism of Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11806v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11806v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hongbin Zhang, Kehai Chen, Xuefeng Bai, Xiucheng Li, Yang Xiang, Min Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have succeeded remarkably in multilingual
translation tasks. However, the inherent translation mechanisms of LLMs remain
poorly understood, largely due to sophisticated architectures and vast
parameter scales. In response to this issue, this study explores the
translation mechanism of LLM from the perspective of computational components
(e.g., attention heads and MLPs). Path patching is utilized to explore causal
relationships between components, detecting those crucial for translation tasks
and subsequently analyzing their behavioral patterns in human-interpretable
terms. Comprehensive analysis reveals that translation is predominantly
facilitated by a sparse subset of specialized attention heads (less than 5\%),
which extract source language, indicator, and positional features. MLPs
subsequently integrate and process these features by transiting towards
English-centric latent representations. Notably, building on the above
findings, targeted fine-tuning of only 64 heads achieves translation
improvement comparable to full-parameter tuning while preserving general
capabilities.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Comparison of DeepSeek and Other LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.03688v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.03688v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tianchen Gao, Jiashun Jin, Zheng Tracy Ke, Gabriel Moryoussef
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, DeepSeek has been the focus of attention in and beyond the AI
community. An interesting problem is how DeepSeek compares to other large
language models (LLMs). There are many tasks an LLM can do, and in this paper,
we use the task of predicting an outcome using a short text for comparison. We
consider two settings, an authorship classification setting and a citation
classification setting. In the first one, the goal is to determine whether a
short text is written by human or AI. In the second one, the goal is to
classify a citation to one of four types using the textual content. For each
experiment, we compare DeepSeek with $4$ popular LLMs: Claude, Gemini, GPT, and
Llama.
  We find that, in terms of classification accuracy, DeepSeek outperforms
Gemini, GPT, and Llama in most cases, but underperforms Claude. We also find
that DeepSeek is comparably slower than others but with a low cost to use,
while Claude is much more expensive than all the others. Finally, we find that
in terms of similarity, the output of DeepSeek is most similar to those of
Gemini and Claude (and among all $5$ LLMs, Claude and Gemini have the most
similar outputs).
  In this paper, we also present a fully-labeled dataset collected by
ourselves, and propose a recipe where we can use the LLMs and a recent data
set, MADStat, to generate new data sets. The datasets in our paper can be used
as benchmarks for future study on LLMs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>21 pages, 5 figures, 6 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Round and Round We Go! What makes Rotary Positional Encodings useful? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.06205v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.06205v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Federico Barbero, Alex Vitvitskyi, Christos Perivolaropoulos, Razvan Pascanu, Petar Veličković
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Positional Encodings (PEs) are a critical component of Transformer-based
Large Language Models (LLMs), providing the attention mechanism with important
sequence-position information. One of the most popular types of encoding used
today in LLMs are Rotary Positional Encodings (RoPE), that rotate the queries
and keys based on their relative distance. A common belief is that RoPE is
useful because it helps to decay token dependency as relative distance
increases. In this work, we argue that this is unlikely to be the core reason.
We study the internals of a trained Gemma 7B model to understand how RoPE is
being used at a mechanical level. We find that Gemma learns to use RoPE to
construct robust "positional" attention patterns by exploiting the highest
frequencies. We also find that, in general, Gemma greatly prefers to use the
lowest frequencies of RoPE, which we suspect are used to carry semantic
information. We mathematically prove interesting behaviours of RoPE and conduct
experiments to verify our findings, proposing a modification of RoPE that fixes
some highlighted issues and improves performance. We believe that this work
represents an interesting step in better understanding PEs in LLMs, which we
believe holds crucial value for scaling LLMs to large sizes and context
lengths.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Entropy and type-token ratio in gigaword corpora 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.10227v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.10227v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pablo Rosillo-Rodes, Maxi San Miguel, David Sanchez
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  There are different ways of measuring diversity in complex systems. In
particular, in language, lexical diversity is characterized in terms of the
type-token ratio and the word entropy. We here investigate both diversity
metrics in six massive linguistic datasets in English, Spanish, and Turkish,
consisting of books, news articles, and tweets. These gigaword corpora
correspond to languages with distinct morphological features and differ in
registers and genres, thus constituting a varied testbed for a quantitative
approach to lexical diversity. We unveil an empirical functional relation
between entropy and type-token ratio of texts of a given corpus and language,
which is a consequence of the statistical laws observed in natural language.
Further, in the limit of large text lengths we find an analytical expression
for this relation relying on both Zipf and Heaps laws that agrees with our
empirical findings.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 10 figures, 8 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CodeRAG-Bench: Can Retrieval Augment Code Generation? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.14497v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.14497v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zora Zhiruo Wang, Akari Asai, Xinyan Velocity Yu, Frank F. Xu, Yiqing Xie, Graham Neubig, Daniel Fried
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While language models (LMs) have proven remarkably adept at generating code,
many programs are challenging for LMs to generate using their parametric
knowledge alone. Providing external contexts such as library documentation can
facilitate generating accurate and functional code. Despite the success of
retrieval-augmented generation (RAG) in various text-oriented tasks, its
potential for improving code generation remains under-explored. In this work,
we conduct a systematic, large-scale analysis by asking: in what scenarios can
retrieval benefit code generation models? and what challenges remain? We first
curate a comprehensive evaluation benchmark, CodeRAG-Bench, encompassing three
categories of code generation tasks, including basic programming, open-domain,
and repository-level problems. We aggregate documents from five sources for
models to retrieve contexts: competition solutions, online tutorials, library
documentation, StackOverflow posts, and GitHub repositories. We examine
top-performing models on CodeRAG-Bench by providing contexts retrieved from one
or multiple sources. While notable gains are made in final code generation by
retrieving high-quality contexts across various settings, our analysis reveals
room for improvement -- current retrievers still struggle to fetch useful
contexts especially with limited lexical overlap, and generators fail to
improve with limited context lengths or abilities to integrate additional
contexts. We hope CodeRAG-Bench serves as an effective testbed to encourage
further development of advanced code-oriented RAG methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ On Speeding Up Language Model Evaluation <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.06172v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.06172v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jin Peng Zhou, Christian K. Belardi, Ruihan Wu, Travis Zhang, Carla P. Gomes, Wen Sun, Kilian Q. Weinberger
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Developing prompt-based methods with Large Language Models (LLMs) requires
making numerous decisions, which give rise to a combinatorial search problem
over hyper-parameters. This exhaustive evaluation can be time-consuming and
costly. In this paper, we propose an $\textit{adaptive}$ approach to explore
this space. We are exploiting the fact that often only few samples are needed
to identify clearly superior or inferior settings, and that many evaluation
tests are highly correlated. We lean on multi-armed bandits to sequentially
identify the next (method, validation sample)-pair to evaluate and utilize
low-rank matrix factorization to fill in missing evaluations. We carefully
assess the efficacy of our approach on several competitive benchmark problems
and show that it can identify the top-performing method using only 5-15% of the
typical resources -- resulting in 85-95% LLM cost savings. Our code is
available at https://github.com/kilian-group/banditeval.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Do Large Language Models Align with Core Mental Health Counseling
  Competencies? <span class="chip">NAACL 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.22446v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.22446v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Viet Cuong Nguyen, Mohammad Taher, Dongwan Hong, Vinicius Konkolics Possobom, Vibha Thirunellayi Gopalakrishnan, Ekta Raj, Zihang Li, Heather J. Soled, Michael L. Birnbaum, Srijan Kumar, Munmun De Choudhury
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid evolution of Large Language Models (LLMs) presents a promising
solution to the global shortage of mental health professionals. However, their
alignment with essential counseling competencies remains underexplored. We
introduce CounselingBench, a novel NCMHCE-based benchmark evaluating 22
general-purpose and medical-finetuned LLMs across five key competencies. While
frontier models surpass minimum aptitude thresholds, they fall short of
expert-level performance, excelling in Intake, Assessment & Diagnosis but
struggling with Core Counseling Attributes and Professional Practice & Ethics.
Surprisingly, medical LLMs do not outperform generalist models in accuracy,
though they provide slightly better justifications while making more
context-related errors. These findings highlight the challenges of developing
AI for mental health counseling, particularly in competencies requiring empathy
and nuanced reasoning. Our results underscore the need for specialized,
fine-tuned models aligned with core mental health counseling competencies and
supported by human oversight before real-world deployment. Code and data
associated with this manuscript can be found at:
https://github.com/cuongnguyenx/CounselingBench
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 Pages, Accepted to Findings of NAACL 2025</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2025-02-25T00:00:00Z">2025-02-25</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Computation and Language
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DRAMA: Diverse Augmentation from Large Language Models to Smaller Dense
  Retrievers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.18460v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.18460v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xueguang Ma, Xi Victoria Lin, Barlas Oguz, Jimmy Lin, Wen-tau Yih, Xilun Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have demonstrated strong effectiveness and
robustness while fine-tuned as dense retrievers. However, their large parameter
size brings significant inference time computational challenges, including high
encoding costs for large-scale corpora and increased query latency, limiting
their practical deployment. While smaller retrievers offer better efficiency,
they often fail to generalize effectively with limited supervised fine-tuning
data. In this work, we introduce DRAMA, a training framework that leverages
LLMs to train smaller generalizable dense retrievers. In particular, we adopt
pruned LLMs as the backbone and train on diverse LLM-augmented data in a
single-stage contrastive learning setup. Experiments show that DRAMA offers
better multilingual and long-context capabilities than traditional
encoder-based retrievers, and achieves strong performance across multiple tasks
and languages. These highlight the potential of connecting the training of
smaller retrievers with the growing advancements in LLMs, bridging the gap
between efficiency and generalization.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FRIDA to the Rescue! Analyzing Synthetic Data Effectiveness in
  Object-Based Common Sense Reasoning for Disaster Response 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.18452v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.18452v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mollie Shichman, Claire Bonial, Austin Blodgett, Taylor Hudson, Francis Ferraro, Rachel Rudinger
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have the potential for substantial common sense
reasoning. However, these capabilities are often emergent in larger models.
This means smaller models that can be run locally are less helpful and capable
with respect to certain reasoning tasks. To meet our problem space
requirements, we fine-tune smaller LLMs to disaster domains, as these domains
involve complex and low-frequency physical common sense knowledge. We introduce
a pipeline to create Field Ready Instruction Decoding Agent (FRIDA) models,
where domain experts and linguists combine their knowledge to make high-quality
seed data that is used to generate synthetic data for fine-tuning. We create a
set of 130 seed instructions for synthetic generation, a synthetic dataset of
25000 instructions, and 119 evaluation instructions relating to both general
and earthquake-specific object affordances. We fine-tune several LLaMa and
Mistral instruction-tuned models and find that FRIDA models outperform their
base models at a variety of sizes. We then run an ablation study to understand
which kinds of synthetic data most affect performance and find that training
physical state and object function common sense knowledge alone improves over
FRIDA models trained on all data. We conclude that the FRIDA pipeline is
capable of instilling general common sense, but needs to be augmented with
information retrieval for specific domain knowledge.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 3 figures, 5 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SWE-RL: Advancing LLM Reasoning via Reinforcement Learning on Open
  Software Evolution 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.18449v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.18449v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuxiang Wei, Olivier Duchenne, Jade Copet, Quentin Carbonneaux, Lingming Zhang, Daniel Fried, Gabriel Synnaeve, Rishabh Singh, Sida I. Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The recent DeepSeek-R1 release has demonstrated the immense potential of
reinforcement learning (RL) in enhancing the general reasoning capabilities of
large language models (LLMs). While DeepSeek-R1 and other follow-up work
primarily focus on applying RL to competitive coding and math problems, this
paper introduces SWE-RL, the first approach to scale RL-based LLM reasoning for
real-world software engineering. Leveraging a lightweight rule-based reward
(e.g., the similarity score between ground-truth and LLM-generated solutions),
SWE-RL enables LLMs to autonomously recover a developer's reasoning processes
and solutions by learning from extensive open-source software evolution data --
the record of a software's entire lifecycle, including its code snapshots, code
changes, and events such as issues and pull requests. Trained on top of Llama
3, our resulting reasoning model, Llama3-SWE-RL-70B, achieves a 41.0% solve
rate on SWE-bench Verified -- a human-verified collection of real-world GitHub
issues. To our knowledge, this is the best performance reported for
medium-sized (<100B) LLMs to date, even comparable to leading proprietary LLMs
like GPT-4o. Surprisingly, despite performing RL solely on software evolution
data, Llama3-SWE-RL has even emerged with generalized reasoning skills. For
example, it shows improved results on five out-of-domain tasks, namely,
function coding, library use, code reasoning, mathematics, and general language
understanding, whereas a supervised-finetuning baseline even leads to
performance degradation on average. Overall, SWE-RL opens up a new direction to
improve the reasoning capabilities of LLMs through reinforcement learning on
massive software engineering data.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Disambiguate First Parse Later: Generating Interpretations for Ambiguity
  Resolution in Semantic Parsing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.18448v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.18448v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Irina Saparina, Mirella Lapata
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Handling ambiguity and underspecification is an important challenge in
natural language interfaces, particularly for tasks like text-to-SQL semantic
parsing. We propose a modular approach that resolves ambiguity using natural
language interpretations before mapping these to logical forms (e.g., SQL
queries). Although LLMs excel at parsing unambiguous utterances, they show
strong biases for ambiguous ones, typically predicting only preferred
interpretations. We constructively exploit this bias to generate an initial set
of preferred disambiguations and then apply a specialized infilling model to
identify and generate missing interpretations. To train the infilling model, we
introduce an annotation method that uses SQL execution to validate different
meanings. Our approach improves interpretation coverage and generalizes across
datasets with different annotation styles, database structures, and ambiguity
types.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ olmOCR: Unlocking Trillions of Tokens in PDFs with Vision Language
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.18443v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.18443v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jake Poznanski, Jon Borchardt, Jason Dunkelberger, Regan Huff, Daniel Lin, Aman Rangapur, Christopher Wilhelm, Kyle Lo, Luca Soldaini
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  PDF documents have the potential to provide trillions of novel, high-quality
tokens for training language models. However, these documents come in a
diversity of types with differing formats and visual layouts that pose a
challenge when attempting to extract and faithfully represent the underlying
content for language model use. We present olmOCR, an open-source Python
toolkit for processing PDFs into clean, linearized plain text in natural
reading order while preserving structured content like sections, tables, lists,
equations, and more. Our toolkit runs a fine-tuned 7B vision language model
(VLM) trained on a sample of 260,000 pages from over 100,000 crawled PDFs with
diverse properties, including graphics, handwritten text and poor quality
scans. olmOCR is optimized for large-scale batch processing, able to scale
flexibly to different hardware setups and convert a million PDF pages for only
$190 USD. We release all components of olmOCR including VLM weights, data and
training code, as well as inference code built on serving frameworks including
vLLM and SGLang.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Reversal Blessing: Thinking Backward May Outpace Thinking Forward in
  Multi-choice Questions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.18435v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.18435v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yizhe Zhang, Richard Bai, Zijin Gu, Ruixiang Zhang, Jiatao Gu, Emmanuel Abbe, Samy Bengio, Navdeep Jaitly
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Language models usually use left-to-right (L2R) autoregressive factorization.
However, L2R factorization may not always be the best inductive bias.
Therefore, we investigate whether alternative factorizations of the text
distribution could be beneficial in some tasks. We investigate right-to-left
(R2L) training as a compelling alternative, focusing on multiple-choice
questions (MCQs) as a test bed for knowledge extraction and reasoning. Through
extensive experiments across various model sizes (2B-8B parameters) and
training datasets, we find that R2L models can significantly outperform L2R
models on several MCQ benchmarks, including logical reasoning, commonsense
understanding, and truthfulness assessment tasks. Our analysis reveals that
this performance difference may be fundamentally linked to multiple factors
including calibration, computability and directional conditional entropy. We
ablate the impact of these factors through controlled simulation studies using
arithmetic tasks, where the impacting factors can be better disentangled. Our
work demonstrates that exploring alternative factorizations of the text
distribution can lead to improvements in LLM capabilities and provides
theoretical insights into optimal factorization towards approximating human
language distribution, and when each reasoning order might be more
advantageous.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Exploring Gender Disparities in Automatic Speech Recognition Technology <span class="chip">ISCA</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.18434v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.18434v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hend ElGhazaly, Bahman Mirheidari, Nafise Sadat Moosavi, Heidi Christensen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study investigates factors influencing Automatic Speech Recognition
(ASR) systems' fairness and performance across genders, beyond the conventional
examination of demographics. Using the LibriSpeech dataset and the Whisper
small model, we analyze how performance varies across different gender
representations in training data. Our findings suggest a complex interplay
between the gender ratio in training data and ASR performance. Optimal fairness
occurs at specific gender distributions rather than a simple 50-50 split.
Furthermore, our findings suggest that factors like pitch variability can
significantly affect ASR accuracy. This research contributes to a deeper
understanding of biases in ASR systems, highlighting the importance of
carefully curated training data in mitigating gender bias.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ISCA/ITG Workshop on Diversity in Large Speech and Language Models</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TextGames: Learning to Self-Play Text-Based Puzzle Games via Language
  Model Reasoning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.18431v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.18431v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Frederikus Hudi, Genta Indra Winata, Ruochen Zhang, Alham Fikri Aji
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reasoning is a fundamental capability of large language models (LLMs),
enabling them to comprehend, analyze, and solve complex problems. In this
paper, we introduce TextGames, an innovative benchmark specifically crafted to
assess LLMs through demanding text-based games that require advanced skills in
pattern recognition, spatial awareness, arithmetic, and logical reasoning. Our
analysis probes LLMs' performance in both single-turn and multi-turn reasoning,
and their abilities in leveraging feedback to correct subsequent answers
through self-reflection. Our findings reveal that, although LLMs exhibit
proficiency in addressing most easy and medium-level problems, they face
significant challenges with more difficult tasks. In contrast, humans are
capable of solving all tasks when given sufficient time. Moreover, we observe
that LLMs show improved performance in multi-turn predictions through
self-reflection, yet they still struggle with sequencing, counting, and
following complex rules consistently. Additionally, models optimized for
reasoning outperform pre-trained LLMs that prioritize instruction following,
highlighting the crucial role of reasoning skills in addressing highly complex
problems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Compressing Language Models for Specialized Domains 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.18424v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.18424v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Miles Williams, George Chrysostomou, Vitor Jeronymo, Nikolaos Aletras
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Compression techniques such as pruning and quantization offer a solution for
more efficient deployment of language models (LMs), albeit with small
performance drops in benchmark performance. However, general-purpose LM
compression methods can negatively affect performance in specialized domains
(e.g. biomedical or legal). Recent work has sought to address this, yet
requires computationally expensive full-parameter fine-tuning. To this end, we
propose cross-calibration, a novel training-free approach for improving the
domain performance of compressed LMs. Our approach effectively leverages
Hessian-based sensitivity to identify weights that are influential for both
in-domain and general performance. Through extensive experimentation, we
demonstrate that cross-calibration substantially outperforms existing
approaches on domain-specific tasks, without compromising general performance.
Notably, these gains come without additional computational overhead, displaying
remarkable potential towards extracting domain-specialized compressed models
from general-purpose LMs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Work in progress</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Rank1: Test-Time Compute for Reranking in Information Retrieval 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.18418v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.18418v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Orion Weller, Kathryn Ricci, Eugene Yang, Andrew Yates, Dawn Lawrie, Benjamin Van Durme
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce Rank1, the first reranking model trained to take advantage of
test-time compute. Rank1 demonstrates the applicability within retrieval of
using a reasoning language model (i.e. OpenAI's o1, Deepseek's R1, etc.) for
distillation in order to rapidly improve the performance of a smaller model. We
gather and open-source a dataset of more than 600,000 examples of R1 reasoning
traces from queries and passages in MS MARCO. Models trained on this dataset
show: (1) state-of-the-art performance on advanced reasoning and instruction
following datasets; (2) work remarkably well out of distribution due to the
ability to respond to user-input prompts; and (3) have explainable reasoning
chains that can be given to users or RAG-based systems. Further, we demonstrate
that quantized versions of these models retain strong performance while using
less compute/memory. Overall, Rank1 shows that test-time compute allows for a
fundamentally new type of explainable and performant reranker model for search.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GLEAN: Generalized Category Discovery with Diverse and Quality-Enhanced
  LLM Feedback 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.18414v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.18414v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Henry Peng Zou, Siffi Singh, Yi Nian, Jianfeng He, Jason Cai, Saab Mansour, Hang Su
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generalized Category Discovery (GCD) is a practical and challenging
open-world task that aims to recognize both known and novel categories in
unlabeled data using limited labeled data from known categories. Due to the
lack of supervision, previous GCD methods face significant challenges, such as
difficulty in rectifying errors for confusing instances, and inability to
effectively uncover and leverage the semantic meanings of discovered clusters.
Therefore, additional annotations are usually required for real-world
applicability. However, human annotation is extremely costly and inefficient.
To address these issues, we propose GLEAN, a unified framework for generalized
category discovery that actively learns from diverse and quality-enhanced LLM
feedback. Our approach leverages three different types of LLM feedback to: (1)
improve instance-level contrastive features, (2) generate category
descriptions, and (3) align uncertain instances with LLM-selected category
descriptions. Extensive experiments demonstrate the superior performance of
\MethodName over state-of-the-art models across diverse datasets, metrics, and
supervision settings. Our code is available at
https://github.com/amazon-science/Glean.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AgentRM: Enhancing Agent Generalization with Reward Modeling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.18407v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.18407v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yu Xia, Jingru Fan, Weize Chen, Siyu Yan, Xin Cong, Zhong Zhang, Yaxi Lu, Yankai Lin, Zhiyuan Liu, Maosong Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing LLM-based agents have achieved strong performance on held-in tasks,
but their generalizability to unseen tasks remains poor. Hence, some recent
work focus on fine-tuning the policy model with more diverse tasks to improve
the generalizability. In this work, we find that finetuning a reward model to
guide the policy model is more robust than directly finetuning the policy
model. Based on this finding, we propose AgentRM, a generalizable reward model,
to guide the policy model for effective test-time search. We comprehensively
investigate three approaches to construct the reward model, including explicit
reward modeling, implicit reward modeling and LLM-as-a-judge. We then use
AgentRM to guide the answer generation with Best-of-N sampling and step-level
beam search. On four types of nine agent tasks, AgentRM enhances the base
policy model by $8.8$ points on average, surpassing the top general agent by
$4.0$. Moreover, it demonstrates weak-to-strong generalization, yielding
greater improvement of $12.6$ on LLaMA-3-70B policy model. As for the
specializability, AgentRM can also boost a finetuned policy model and
outperform the top specialized agent by $11.4$ on three held-in tasks. Further
analysis verifies its effectiveness in test-time scaling. Codes will be
released to facilitate the research in this area.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ KiRAG: Knowledge-Driven Iterative Retriever for Enhancing
  Retrieval-Augmented Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.18397v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.18397v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jinyuan Fang, Zaiqiao Meng, Craig Macdonald
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Iterative retrieval-augmented generation (iRAG) models offer an effective
approach for multi-hop question answering (QA). However, their retrieval
process faces two key challenges: (1) it can be disrupted by irrelevant
documents or factually inaccurate chain-of-thoughts; (2) their retrievers are
not designed to dynamically adapt to the evolving information needs in
multi-step reasoning, making it difficult to identify and retrieve the missing
information required at each iterative step. Therefore, we propose KiRAG, which
uses a knowledge-driven iterative retriever model to enhance the retrieval
process of iRAG. Specifically, KiRAG decomposes documents into knowledge
triples and performs iterative retrieval with these triples to enable a
factually reliable retrieval process. Moreover, KiRAG integrates reasoning into
the retrieval process to dynamically identify and retrieve knowledge that
bridges information gaps, effectively adapting to the evolving information
needs. Empirical results show that KiRAG significantly outperforms existing
iRAG models, with an average improvement of 9.40% in R@3 and 5.14% in F1 on
multi-hop QA.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Monte Carlo Temperature: a robust sampling strategy for LLM's
  uncertainty quantification methods 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.18389v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.18389v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nicola Cecere, Andrea Bacciu, Ignacio Fernández Tobías, Amin Mantrach
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Uncertainty quantification (UQ) in Large Language Models (LLMs) is essential
for their safe and reliable deployment, particularly in critical applications
where incorrect outputs can have serious consequences. Current UQ methods
typically rely on querying the model multiple times using non-zero temperature
sampling to generate diverse outputs for uncertainty estimation. However, the
impact of selecting a given temperature parameter is understudied, and our
analysis reveals that temperature plays a fundamental role in the quality of
uncertainty estimates. The conventional approach of identifying optimal
temperature values requires expensive hyperparameter optimization (HPO) that
must be repeated for each new model-dataset combination. We propose Monte Carlo
Temperature (MCT), a robust sampling strategy that eliminates the need for
temperature calibration. Our analysis reveals that: 1) MCT provides more robust
uncertainty estimates across a wide range of temperatures, 2) MCT improves the
performance of UQ methods by replacing fixed-temperature strategies that do not
rely on HPO, and 3) MCT achieves statistical parity with oracle temperatures,
which represent the ideal outcome of a well-tuned but computationally expensive
HPO process. These findings demonstrate that effective UQ can be achieved
without the computational burden of temperature parameter calibration.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DBR: Divergence-Based Regularization for Debiasing Natural Language
  Understanding Models <span class="chip">KDD</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.18353v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.18353v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zihao Li, Ruixiang Tang, Lu Cheng, Shuaiqiang Wang, Dawei Yin, Mengnan Du
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pre-trained language models (PLMs) have achieved impressive results on
various natural language processing tasks. However, recent research has
revealed that these models often rely on superficial features and shortcuts
instead of developing a genuine understanding of language, especially for
natural language understanding (NLU) tasks. Consequently, the models struggle
to generalize to out-of-domain data. In this work, we propose Divergence Based
Regularization (DBR) to mitigate this shortcut learning behavior. Our method
measures the divergence between the output distributions for original examples
and examples where shortcut tokens have been masked. This process prevents the
model's predictions from being overly influenced by shortcut features or
biases. We evaluate our model on three NLU tasks and find that it improves
out-of-domain performance with little loss of in-domain accuracy. Our results
demonstrate that reducing the reliance on shortcuts and superficial features
can enhance the generalization ability of large pre-trained language models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by SIGKDD Explorations</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ BRIDO: Bringing Democratic Order to Abstractive Summarization <span class="chip">AAAI-25</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.18342v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.18342v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junhyun Lee, Harshith Goka, Hyeonmok Ko
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Hallucination refers to the inaccurate, irrelevant, and inconsistent text
generated from large language models (LLMs). While the LLMs have shown great
promise in a variety of tasks, the issue of hallucination still remains a major
challenge for many practical uses. In this paper, we tackle the issue of
hallucination in abstract text summarization by mitigating exposure bias.
Existing models targeted for exposure bias mitigation, namely BRIO, aim for
better summarization quality in the ROUGE score. We propose a model that uses a
similar exposure bias mitigation strategy but with a goal that is aligned with
less hallucination. We conjecture that among a group of candidate outputs, ones
with hallucinations will comprise the minority of the whole group. That is,
candidates with less similarity with others will have a higher chance of
containing hallucinated content. Our method uses this aspect and utilizes
contrastive learning, incentivizing candidates with high inter-candidate ROUGE
scores. We performed experiments on the XSum and CNN/DM summarization datasets,
and our method showed 6.25% and 3.82% improvement, respectively, on the
consistency G-Eval score over BRIO.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 1 figure; AAAI-25 Workshop on PDLM camera ready</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Mapping of Subjective Accounts into Interpreted Clusters (MOSAIC): Topic
  Modelling and LLM applied to Stroboscopic Phenomenology 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.18318v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.18318v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Romy Beauté, David J. Schwartzman, Guillaume Dumas, Jennifer Crook, Fiona Macpherson, Adam B. Barrett, Anil K. Seth
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Stroboscopic light stimulation (SLS) on closed eyes typically induces simple
visual hallucinations (VHs), characterised by vivid, geometric and colourful
patterns. A dataset of 862 sentences, extracted from 422 open subjective
reports, was recently compiled as part of the Dreamachine programme (Collective
Act, 2022), an immersive multisensory experience that combines SLS and spatial
sound in a collective setting. Although open reports extend the range of
reportable phenomenology, their analysis presents significant challenges,
particularly in systematically identifying patterns. To address this challenge,
we implemented a data-driven approach leveraging Large Language Models and
Topic Modelling to uncover and interpret latent experiential topics directly
from the Dreamachine's text-based reports. Our analysis confirmed the presence
of simple VHs typically documented in scientific studies of SLS, while also
revealing experiences of altered states of consciousness and complex
hallucinations. Building on these findings, our computational approach expands
the systematic study of subjective experience by enabling data-driven analyses
of open-ended phenomenological reports, capturing experiences not readily
identified through standard questionnaires. By revealing rich and multifaceted
aspects of experiences, our study broadens our understanding of
stroboscopically-induced phenomena while highlighting the potential of Natural
Language Processing and Large Language Models in the emerging field of
computational (neuro)phenomenology. More generally, this approach provides a
practically applicable methodology for uncovering subtle hidden patterns of
subjective experience across diverse research domains.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ WiCkeD: A Simple Method to Make Multiple Choice Benchmarks More
  Challenging 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.18316v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.18316v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ahmed Elhady, Eneko Agirre, Mikel Artetxe
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce WiCkeD, a simple method to increase the complexity of existing
multiple-choice benchmarks by randomly replacing a choice with "None of the
above", a method often used in educational tests. We show that WiCkeD can be
automatically applied to any existing benchmark, making it more challenging. We
apply WiCkeD to 6 popular benchmarks and use it to evaluate 18 open-weight
LLMs. The performance of the models drops 12.1 points on average with respect
to the original versions of the datasets. When using chain-of-thought on 3 MMLU
datasets, the performance drop for the WiCkeD variant is similar to the one
observed when using the LLMs directly, showing that WiCkeD is also challenging
for models with enhanced reasoning abilities. WiCkeD also uncovers that some
models are more sensitive to the extra reasoning required, providing additional
information with respect to the original benchmarks. We relase our code and
data at https://github.com/ahmedselhady/wicked-benchmarks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Looking forward: Linguistic theory and methods 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.18313v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.18313v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        John Mansfield, Ethan Gotlieb Wilcox
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This chapter examines current developments in linguistic theory and methods,
focusing on the increasing integration of computational, cognitive, and
evolutionary perspectives. We highlight four major themes shaping contemporary
linguistics: (1) the explicit testing of hypotheses about symbolic
representation, such as efficiency, locality, and conceptual semantic
grounding; (2) the impact of artificial neural networks on theoretical debates
and linguistic analysis; (3) the importance of intersubjectivity in linguistic
theory; and (4) the growth of evolutionary linguistics. By connecting
linguistics with computer science, psychology, neuroscience, and biology, we
provide a forward-looking perspective on the changing landscape of linguistic
research.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ RefuteBench 2.0 -- Agentic Benchmark for Dynamic Evaluation of LLM
  Responses to Refutation Instruction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.18308v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.18308v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jianhao Yan, Yun Luo, Yue Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the multi-turn interaction schema, large language models (LLMs) can
leverage user feedback to enhance the quality and relevance of their responses.
However, evaluating an LLM's ability to incorporate user refutation feedback is
crucial yet challenging. In this study, we introduce RefuteBench 2.0, which
significantly extends the original RefuteBench by incorporating LLM agents as
refuters and evaluators, which allows for flexible and comprehensive
assessment.
  We design both transient and persistent refutation instructions with
different validity periods. Meta-evaluation shows that the LLM-based refuter
could generate more human-like refutations and the evaluators could assign
scores with high correlation with humans. Experimental results of various LLMs
show that current models could effectively satisfy the refutation but fail to
memorize the refutation information. Interestingly, we also observe that the
performance of the initial task decreases as the refutations increase. Analysis
of the attention scores further shows a potential weakness of current LLMs:
they struggle to retain and correctly use previous information during long
context dialogues. https://github.com/ElliottYan/RefuteBench-2.0
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Work on progess</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AMPO: Active Multi-Preference Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.18293v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.18293v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Taneesh Gupta, Rahul Madhavan, Xuchao Zhang, Chetan Bansal, Saravan Rajmohan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-preference optimization enriches language-model alignment beyond
pairwise preferences by contrasting entire sets of helpful and undesired
responses, thereby enabling richer training signals for large language models.
During self-play alignment, these models often produce numerous candidate
answers per query, rendering it computationally infeasible to include all
responses in the training objective. In this work, we propose $\textit{Active
Multi-Preference Optimization}$ (AMPO), a novel approach that combines
on-policy generation, a multi-preference group-contrastive loss, and active
subset selection. Specifically, we score and embed large candidate pools of
responses and then select a small, yet informative, subset that covers reward
extremes and distinct semantic clusters for preference optimization. Our
contrastive training scheme is capable of identifying not only the best and
worst answers but also subtle, underexplored modes that are crucial for robust
alignment. Theoretically, we provide guarantees for expected reward
maximization using our active selection method, and empirically, AMPO achieves
state-of-the-art results on $\textit{AlpacaEval}$ using Llama 8B.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ How Vital is the Jurisprudential Relevance: Law Article Intervened Legal
  Case Retrieval and Matching 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.18292v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.18292v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nuo Xu, Pinghui Wang, Zi Liang, Junzhou Zhao, Xiaohong Guan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Legal case retrieval (LCR) aims to automatically scour for comparable legal
cases based on a given query, which is crucial for offering relevant precedents
to support the judgment in intelligent legal systems. Due to similar goals, it
is often associated with a similar case matching (LCM) task. To address them, a
daunting challenge is assessing the uniquely defined legal-rational similarity
within the judicial domain, which distinctly deviates from the semantic
similarities in general text retrieval. Past works either tagged
domain-specific factors or incorporated reference laws to capture
legal-rational information. However, their heavy reliance on expert or
unrealistic assumptions restricts their practical applicability in real-world
scenarios. In this paper, we propose an end-to-end model named LCM-LAI to solve
the above challenges. Through meticulous theoretical analysis, LCM-LAI employs
a dependent multi-task learning framework to capture legal-rational information
within legal cases by a law article prediction (LAP) sub-task, without any
additional assumptions in inference. Besides, LCM-LAI proposes an article-aware
attention mechanism to evaluate the legal-rational similarity between
across-case sentences based on law distribution, which is more effective than
conventional semantic similarity. Weperform a series of exhaustive experiments
including two different tasks involving four real-world datasets. Results
demonstrate that LCM-LAI achieves state-of-the-art performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Uncertainty Modeling in Multimodal Speech Analysis Across the Psychosis
  Spectrum 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.18285v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.18285v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Morteza Rohanian, Roya M. Hüppi, Farhad Nooralahzadeh, Noemi Dannecker, Yves Pauli, Werner Surbeck, Iris Sommer, Wolfram Hinzen, Nicolas Langer, Michael Krauthammer, Philipp Homan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Capturing subtle speech disruptions across the psychosis spectrum is
challenging because of the inherent variability in speech patterns. This
variability reflects individual differences and the fluctuating nature of
symptoms in both clinical and non-clinical populations. Accounting for
uncertainty in speech data is essential for predicting symptom severity and
improving diagnostic precision. Speech disruptions characteristic of psychosis
appear across the spectrum, including in non-clinical individuals. We develop
an uncertainty-aware model integrating acoustic and linguistic features to
predict symptom severity and psychosis-related traits. Quantifying uncertainty
in specific modalities allows the model to address speech variability,
improving prediction accuracy. We analyzed speech data from 114 participants,
including 32 individuals with early psychosis and 82 with low or high
schizotypy, collected through structured interviews, semi-structured
autobiographical tasks, and narrative-driven interactions in German. The model
improved prediction accuracy, reducing RMSE and achieving an F1-score of 83%
with ECE = 4.5e-2, showing robust performance across different interaction
contexts. Uncertainty estimation improved model interpretability by identifying
reliability differences in speech markers such as pitch variability, fluency
disruptions, and spectral instability. The model dynamically adjusted to task
structures, weighting acoustic features more in structured settings and
linguistic features in unstructured contexts. This approach strengthens early
detection, personalized assessment, and clinical decision-making in
psychosis-spectrum research.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Better Aligned with <span class="highlight-title">Survey</span> Respondents or Training Data? Unveiling
  Political Leanings of LLMs on U.S. Supreme Court Cases 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.18282v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.18282v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shanshan Xu, T. Y. S. S Santosh, Yanai Elazar, Quirin Vogel, Barbara Plank, Matthias Grabmair
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The increased adoption of Large Language Models (LLMs) and their potential to
shape public opinion have sparked interest in assessing these models' political
leanings. Building on previous research that compared LLMs and human opinions
and observed political bias in system responses, we take a step further to
investigate the underlying causes of such biases by empirically examining how
the values and biases embedded in training corpora shape model outputs.
Specifically, we propose a method to quantitatively evaluate political leanings
embedded in the large pretraining corpora. Subsequently we investigate to whom
are the LLMs' political leanings more aligned with, their pretrainig corpora or
the surveyed human opinions. As a case study, we focus on probing the political
leanings of LLMs in 32 U.S. Supreme Court cases, addressing contentious topics
such as abortion and voting rights. Our findings reveal that LLMs strongly
reflect the political leanings in their training data, and no strong
correlation is observed with their alignment to human opinions as expressed in
surveys. These results underscore the importance of responsible curation of
training data and the need for robust evaluation metrics to ensure LLMs'
alignment with human-centered values.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Self-Adjust Softmax 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.18277v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.18277v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chuanyang Zheng, Yihang Gao, Guoxuan Chen, Han Shi, Jing Xiong, Xiaozhe Ren, Chao Huang, Xin Jiang, Zhenguo Li, Yu Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The softmax function is crucial in Transformer attention, which normalizes
each row of the attention scores with summation to one, achieving superior
performances over other alternative functions. However, the softmax function
can face a gradient vanishing issue when some elements of the attention scores
approach extreme values, such as probabilities close to one or zero. In this
paper, we propose Self-Adjust Softmax (SA-Softmax) to address this issue by
modifying $softmax(x)$ to $x \cdot softmax(x)$ and its normalized variant
$\frac{(x - min(x_{\min},0))}{max(0,x_{max})-min(x_{min},0)} \cdot softmax(x)$.
We theoretically show that SA-Softmax provides enhanced gradient properties
compared to the vanilla softmax function. Moreover, SA-Softmax Attention can be
seamlessly integrated into existing Transformer models to their attention
mechanisms with minor adjustments. We conducted experiments to evaluate the
empirical performance of Transformer models using SA-Softmax compared to the
vanilla softmax function. These experiments, involving models with up to 2.7
billion parameters, are conducted across diverse datasets, language tasks, and
positional encoding methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Tech Report</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Citrus: Leveraging Expert Cognitive Pathways in a Medical Language Model
  for Advanced Medical Decision Support 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.18274v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.18274v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guoxin Wang, Minyu Gao, Shuai Yang, Ya Zhang, Lizhi He, Liang Huang, Hanlin Xiao, Yexuan Zhang, Wanyue Li, Lu Chen, Jintao Fei, Xin Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs), particularly those with reasoning capabilities,
have rapidly advanced in recent years, demonstrating significant potential
across a wide range of applications. However, their deployment in healthcare,
especially in disease reasoning tasks, is hindered by the challenge of
acquiring expert-level cognitive data. In this paper, we introduce Citrus, a
medical language model that bridges the gap between clinical expertise and AI
reasoning by emulating the cognitive processes of medical experts. The model is
trained on a large corpus of simulated expert disease reasoning data,
synthesized using a novel approach that accurately captures the decision-making
pathways of clinicians. This approach enables Citrus to better simulate the
complex reasoning processes involved in diagnosing and treating medical
conditions.To further address the lack of publicly available datasets for
medical reasoning tasks, we release the last-stage training data, including a
custom-built medical diagnostic dialogue dataset. This open-source contribution
aims to support further research and development in the field. Evaluations
using authoritative benchmarks such as MedQA, covering tasks in medical
reasoning and language understanding, show that Citrus achieves superior
performance compared to other models of similar size. These results highlight
Citrus potential to significantly enhance medical decision support systems,
providing a more accurate and efficient tool for clinical decision-making.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Beyond In-Distribution Success: Scaling Curves of CoT Granularity for
  Language Model Generalization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.18273v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.18273v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ru Wang, Wei Huang, Selena Song, Haoyu Zhang, Yusuke Iwasawa, Yutaka Matsuo, Jiaxian Guo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generalization to novel compound tasks under distribution shift is important
for deploying transformer-based language models (LMs). This work investigates
Chain-of-Thought (CoT) reasoning as a means to enhance OOD generalization.
Through controlled experiments across several compound tasks, we reveal three
key insights: (1) While QA-trained models achieve near-perfect in-distribution
accuracy, their OOD performance degrades catastrophically, even with 10000k+
training examples; (2) the granularity of CoT data strongly correlates with
generalization performance; finer-grained CoT data leads to better
generalization; (3) CoT exhibits remarkable sample efficiency, matching QA
performance with much less (even 80%) data.
  Theoretically, we demonstrate that compound tasks inherently permit shortcuts
in Q-A data that misalign with true reasoning principles, while CoT forces
internalization of valid dependency structures, and thus can achieve better
generalization. Further, we show that transformer positional embeddings can
amplify generalization by emphasizing subtask condition recurrence in long CoT
sequences. Our combined theoretical and empirical analysis provides compelling
evidence for CoT reasoning as a crucial training paradigm for enabling LM
generalization under real-world distributional shifts for compound tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Iterative Counterfactual Data Augmentation <span class="chip">AAAI 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.18249v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.18249v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mitchell Plyler, Min Chi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Counterfactual data augmentation (CDA) is a method for controlling
information or biases in training datasets by generating a complementary
dataset with typically opposing biases. Prior work often either relies on
hand-crafted rules or algorithmic CDA methods which can leave unwanted
information in the augmented dataset. In this work, we show iterative CDA
(ICDA) with initial, high-noise interventions can converge to a state with
significantly lower noise. Our ICDA procedure produces a dataset where one
target signal in the training dataset maintains high mutual information with a
corresponding label and the information of spurious signals are reduced. We
show training on the augmented datasets produces rationales on documents that
better align with human annotation. Our experiments include six human produced
datasets and two large-language model generated datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>AAAI 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Debt Collection Negotiations with Large Language Models: An Evaluation
  System and Optimizing Decision Making with Multi-Agent 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.18228v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.18228v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaofeng Wang, Zhixin Zhang, Jinguang Zheng, Yiming Ai, Rui Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Debt collection negotiations (DCN) are vital for managing non-performing
loans (NPLs) and reducing creditor losses. Traditional methods are
labor-intensive, while large language models (LLMs) offer promising automation
potential. However, prior systems lacked dynamic negotiation and real-time
decision-making capabilities. This paper explores LLMs in automating DCN and
proposes a novel evaluation framework with 13 metrics across 4 aspects. Our
experiments reveal that LLMs tend to over-concede compared to human
negotiators. To address this, we propose the Multi-Agent Debt Negotiation
(MADeN) framework, incorporating planning and judging modules to improve
decision rationality. We also apply post-training techniques, including DPO
with rejection sampling, to optimize performance. Our studies provide valuable
insights for practitioners and researchers seeking to enhance efficiency and
outcomes in this domain.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>21 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Connecting Voices: LoReSpeech as a Low-Resource Speech Parallel Corpus <span class="chip">ISCA</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.18215v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.18215v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Samy Ouzerrout
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Aligned audio corpora are fundamental to NLP technologies such as ASR and
speech translation, yet they remain scarce for underrepresented languages,
hindering their technological integration. This paper introduces a methodology
for constructing LoReSpeech, a low-resource speech-to-speech translation
corpus. Our approach begins with LoReASR, a sub-corpus of short audios aligned
with their transcriptions, created through a collaborative platform. Building
on LoReASR, long-form audio recordings, such as biblical texts, are aligned
using tools like the MFA. LoReSpeech delivers both intra- and inter-language
alignments, enabling advancements in multilingual ASR systems, direct
speech-to-speech translation models, and linguistic preservation efforts, while
fostering digital inclusivity. This work is conducted within Tutlayt AI project
(https://tutlayt.fr).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ISCA/ITG Workshop on Diversity in Large Speech and Language Models</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LAG: LLM agents for Leaderboard Auto Generation on Demanding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.18209v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.18209v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jian Wu, Jiayu Zhang, Dongyuan Li, Linyi Yang, Aoxiao Zhong, Renhe Jiang, Qingsong Wen, Yue Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces Leaderboard Auto Generation (LAG), a novel and
well-organized framework for automatic generation of leaderboards on a given
research topic in rapidly evolving fields like Artificial Intelligence (AI).
Faced with a large number of AI papers updated daily, it becomes difficult for
researchers to track every paper's proposed methods, experimental results, and
settings, prompting the need for efficient automatic leaderboard construction.
While large language models (LLMs) offer promise in automating this process,
challenges such as multi-document summarization, leaderboard generation, and
experiment fair comparison still remain under exploration. LAG solves these
challenges through a systematic approach that involves the paper collection,
experiment results extraction and integration, leaderboard generation, and
quality evaluation. Our contributions include a comprehensive solution to the
leaderboard construction problem, a reliable evaluation method, and
experimental results showing the high quality of leaderboards.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Grandes modelos de lenguaje: de la predicción de palabras a la
  comprensión? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.18205v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.18205v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Carlos Gómez-Rodríguez
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models, such as the well-known ChatGPT, have brought about an
unexpected revolution in the field of artificial intelligence. On the one hand,
they have numerous practical applications and enormous potential still to be
explored. On the other hand, they are also the subject of debate from
scientific, philosophical, and social perspectives: there are doubts about the
exact mechanisms of their functioning and their actual capacity for language
comprehension, and their applications raise ethical dilemmas. In this chapter,
we describe how this technology has been developed and the fundamentals of its
operation, allowing us to better understand its capabilities and limitations
and to introduce some of the main debates surrounding its development and use.
  --
  Los grandes modelos de lenguaje, como el conocido ChatGPT, han supuesto una
inesperada revoluci\'on en el \'ambito de la inteligencia artificial. Por un
lado, cuentan con multitud de aplicaciones pr\'acticas y un enorme potencial
todav\'ia por explorar. Por otro lado, son tambi\'en objeto de debate, tanto
desde el punto de vista cient\'ifico y filos\'ofico como social: hay dudas
sobre los mecanismos exactos de su funcionamiento y su capacidad real de
comprensi\'on del lenguaje, y sus aplicaciones plantean dilemas \'eticos. En
este cap\'itulo describimos c\'omo se ha llegado a esta tecnolog\'ia y los
fundamentos de su funcionamiento, permiti\'endonos as\'i comprender mejor sus
capacidades y limitaciones e introducir algunos de los principales debates que
rodean su desarrollo y uso.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>26 pages, in Spanish. Chapter from book "La Inteligencia Artificial
  hoy y sus aplicaciones con Big Data", (Amparo Alonso Betanzos, Daniel Pe\~na
  y Pilar Poncela, eds.). Publisher: Funcas. ISBN 978-84-17609-94-8</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Steering Language Model to Stable Speech Emotion Recognition via
  Contextual Perception and Chain of Thought 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.18186v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.18186v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhixian Zhao, Xinfa Zhu, Xinsheng Wang, Shuiyuan Wang, Xuelong Geng, Wenjie Tian, Lei Xie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large-scale audio language models (ALMs), such as Qwen2-Audio, are capable of
comprehending diverse audio signal, performing audio analysis and generating
textual responses. However, in speech emotion recognition (SER), ALMs often
suffer from hallucinations, resulting in misclassifications or irrelevant
outputs. To address these challenges, we propose C$^2$SER, a novel ALM designed
to enhance the stability and accuracy of SER through Contextual perception and
Chain of Thought (CoT). C$^2$SER integrates the Whisper encoder for semantic
perception and Emotion2Vec-S for acoustic perception, where Emotion2Vec-S
extends Emotion2Vec with semi-supervised learning to enhance emotional
discrimination. Additionally, C$^2$SER employs a CoT approach, processing SER
in a step-by-step manner while leveraging speech content and speaking styles to
improve recognition. To further enhance stability, C$^2$SER introduces
self-distillation from explicit CoT to implicit CoT, mitigating error
accumulation and boosting recognition accuracy. Extensive experiments show that
C$^2$SER outperforms existing popular ALMs, such as Qwen2-Audio and SECap,
delivering more stable and precise emotion recognition. We release the training
code, checkpoints, and test sets to facilitate further research.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Problem Solved? Information Extraction Design Space for Layout-Rich
  Documents using LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.18179v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.18179v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gaye Colakoglu, Gürkan Solmaz, Jonathan Fürst
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper defines and explores the design space for information extraction
(IE) from layout-rich documents using large language models (LLMs). The three
core challenges of layout-aware IE with LLMs are 1) data structuring, 2) model
engagement, and 3) output refinement. Our study delves into the sub-problems
within these core challenges, such as input representation, chunking,
prompting, and selection of LLMs and multimodal models. It examines the
outcomes of different design choices through a new layout-aware IE test suite,
benchmarking against the state-of-art (SoA) model LayoutLMv3. The results show
that the configuration from one-factor-at-a-time (OFAT) trial achieves
near-optimal results with 14.1 points F1-score gain from the baseline model,
while full factorial exploration yields only a slightly higher 15.1 points gain
at around 36x greater token usage. We demonstrate that well-configured
general-purpose LLMs can match the performance of specialized models, providing
a cost-effective alternative. Our test-suite is freely available at
https://github.com/gayecolakoglu/LayIE-LLM.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SECURA: Sigmoid-Enhanced CUR Decomposition with Uninterrupted Retention
  and Low-Rank Adaptation in Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.18168v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.18168v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhang Yuxuan, Li Ruizhe
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the rapid development of large language models (LLMs), fully fine-tuning
(FT) these models has become increasingly impractical due to the high
computational demands. Additionally, FT can lead to catastrophic forgetting. As
an alternative, Low-Rank Adaptation (LoRA) has been proposed, which fine-tunes
only a small subset of parameters, achieving similar performance to FT while
significantly reducing resource requirements. However, since LoRA inherits FT's
design, the issue of catastrophic forgetting remains.
  To address these challenges, we propose SECURA: Sigmoid-Enhanced CUR
Decomposition LoRA, a novel parameter-efficient fine-tuning (PEFT) variant that
mitigates catastrophic forgetting while improving fine-tuning performance. Our
method introduces a new normalization technique, SigNorm, to enhance parameter
retention and overall performance.
  SECURA has been evaluated on a variety of tasks, including mathematical
problem-solving (GSM8K), challenging question-answering (CNNDM), translation
(NewsDE), and complex multiple-choice reasoning (LogiQA). Experimental results
show that SECURA achieves an average fine-tuning improvement of 3.59% across
four multiple-choice question (MCQ) tasks and a 2.51% improvement across five
question-answering (QA) tasks on models such as Gemma2 2b, Qwen2 1.5b, Qwen 2
7b, Llama3 8b, and Llama3.1 8b, compared to DoRA. Moreover, SECURA demonstrates
superior knowledge retention capabilities, maintaining more than 70% accuracy
on basic LLM knowledge across 16 continual learning tests, outperforming
Experience Replay (ER), Sequential Learning (SEQ), EWC, I-LoRA, and CUR-LoRA.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>New work on Parameter-Efficient Fine-Tuning (PEFT) for large language
  models. Includes new techniques SigNorm and CABR-LoRA for optimizing
  fine-tune performance and Knowledge retention</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Can LLMs Explain Themselves Counterfactually? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.18156v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.18156v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zahra Dehghanighobadi, Asja Fischer, Muhammad Bilal Zafar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Explanations are an important tool for gaining insights into the behavior of
ML models, calibrating user trust and ensuring regulatory compliance. Past few
years have seen a flurry of post-hoc methods for generating model explanations,
many of which involve computing model gradients or solving specially designed
optimization problems. However, owing to the remarkable reasoning abilities of
Large Language Model (LLMs), self-explanation, that is, prompting the model to
explain its outputs has recently emerged as a new paradigm. In this work, we
study a specific type of self-explanations, self-generated counterfactual
explanations (SCEs). We design tests for measuring the efficacy of LLMs in
generating SCEs. Analysis over various LLM families, model sizes, temperature
settings, and datasets reveals that LLMs sometimes struggle to generate SCEs.
Even when they do, their prediction often does not agree with their own
counterfactual reasoning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ NusaAksara: A Multimodal and Multilingual Benchmark for Preserving
  Indonesian Indigenous Scripts 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.18148v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.18148v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Muhammad Farid Adilazuarda, Musa Izzanardi Wijanarko, Lucky Susanto, Khumaisa Nur'aini, Derry Wijaya, Alham Fikri Aji
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Indonesia is rich in languages and scripts. However, most NLP progress has
been made using romanized text. In this paper, we present NusaAksara, a novel
public benchmark for Indonesian languages that includes their original scripts.
Our benchmark covers both text and image modalities and encompasses diverse
tasks such as image segmentation, OCR, transliteration, translation, and
language identification. Our data is constructed by human experts through
rigorous steps. NusaAksara covers 8 scripts across 7 languages, including
low-resource languages not commonly seen in NLP benchmarks. Although
unsupported by Unicode, the Lampung script is included in this dataset. We
benchmark our data across several models, from LLMs and VLMs such as GPT-4o,
Llama 3.2, and Aya 23 to task-specific systems such as PP-OCR and LangID, and
show that most NLP technologies cannot handle Indonesia's local scripts, with
many achieving near-zero performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Jacobian Sparse Autoencoders: Sparsify Computations, Not Just
  Activations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.18147v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.18147v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lucy Farnik, Tim Lawson, Conor Houghton, Laurence Aitchison
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sparse autoencoders (SAEs) have been successfully used to discover sparse and
human-interpretable representations of the latent activations of LLMs. However,
we would ultimately like to understand the computations performed by LLMs and
not just their representations. The extent to which SAEs can help us understand
computations is unclear because they are not designed to "sparsify"
computations in any sense, only latent activations. To solve this, we propose
Jacobian SAEs (JSAEs), which yield not only sparsity in the input and output
activations of a given model component but also sparsity in the computation
(formally, the Jacobian) connecting them. With a na\"ive implementation, the
Jacobians in LLMs would be computationally intractable due to their size. One
key technical contribution is thus finding an efficient way of computing
Jacobians in this setup. We find that JSAEs extract a relatively large degree
of computational sparsity while preserving downstream LLM performance
approximately as well as traditional SAEs. We also show that Jacobians are a
reasonable proxy for computational sparsity because MLPs are approximately
linear when rewritten in the JSAE basis. Lastly, we show that JSAEs achieve a
greater degree of computational sparsity on pre-trained LLMs than on the
equivalent randomized LLM. This shows that the sparsity of the computational
graph appears to be a property that LLMs learn through training, and suggests
that JSAEs might be more suitable for understanding learned transformer
computations than standard SAEs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LevelRAG: Enhancing Retrieval-Augmented Generation with Multi-hop Logic
  Planning over Rewriting Augmented Searchers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.18139v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.18139v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhuocheng Zhang, Yang Feng, Min Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Retrieval-Augmented Generation (RAG) is a crucial method for mitigating
hallucinations in Large Language Models (LLMs) and integrating external
knowledge into their responses. Existing RAG methods typically employ query
rewriting to clarify the user intent and manage multi-hop logic, while using
hybrid retrieval to expand search scope. However, the tight coupling of query
rewriting to the dense retriever limits its compatibility with hybrid
retrieval, impeding further RAG performance improvements. To address this
challenge, we introduce a high-level searcher that decomposes complex queries
into atomic queries, independent of any retriever-specific optimizations.
Additionally, to harness the strengths of sparse retrievers for precise keyword
retrieval, we have developed a new sparse searcher that employs Lucene syntax
to enhance retrieval accuracy.Alongside web and dense searchers, these
components seamlessly collaborate within our proposed method,
\textbf{LevelRAG}. In LevelRAG, the high-level searcher orchestrates the
retrieval logic, while the low-level searchers (sparse, web, and dense) refine
the queries for optimal retrieval. This approach enhances both the completeness
and accuracy of the retrieval process, overcoming challenges associated with
current query rewriting techniques in hybrid retrieval scenarios. Empirical
experiments conducted on five datasets, encompassing both single-hop and
multi-hop question answering tasks, demonstrate the superior performance of
LevelRAG compared to existing RAG methods. Notably, LevelRAG outperforms the
state-of-the-art proprietary model, GPT4o, underscoring its effectiveness and
potential impact on the RAG field.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>First submit</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ HyperG: Hypergraph-Enhanced LLMs for Structured Knowledge 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.18125v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.18125v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sirui Huang, Hanqian Li, Yanggan Gu, Xuming Hu, Qing Li, Guandong Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Given that substantial amounts of domain-specific knowledge are stored in
structured formats, such as web data organized through HTML, Large Language
Models (LLMs) are expected to fully comprehend this structured information to
broaden their applications in various real-world downstream tasks. Current
approaches for applying LLMs to structured data fall into two main categories:
serialization-based and operation-based methods. Both approaches, whether
relying on serialization or using SQL-like operations as an intermediary,
encounter difficulties in fully capturing structural relationships and
effectively handling sparse data. To address these unique characteristics of
structured data, we propose HyperG, a hypergraph-based generation framework
aimed at enhancing LLMs' ability to process structured knowledge. Specifically,
HyperG first augment sparse data with contextual information, leveraging the
generative power of LLMs, and incorporate a prompt-attentive hypergraph
learning (PHL) network to encode both the augmented information and the
intricate structural relationships within the data. To validate the
effectiveness and generalization of HyperG, we conduct extensive experiments
across two different downstream tasks requiring structured knowledge.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Bayesian Optimization for Controlled Image Editing via LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.18116v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.18116v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chengkun Cai, Haoliang Liu, Xu Zhao, Zhongyu Jiang, Tianfang Zhang, Zongkai Wu, Jenq-Neng Hwang, Serge Belongie, Lei Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the rapidly evolving field of image generation, achieving precise control
over generated content and maintaining semantic consistency remain significant
limitations, particularly concerning grounding techniques and the necessity for
model fine-tuning. To address these challenges, we propose BayesGenie, an
off-the-shelf approach that integrates Large Language Models (LLMs) with
Bayesian Optimization to facilitate precise and user-friendly image editing.
Our method enables users to modify images through natural language descriptions
without manual area marking, while preserving the original image's semantic
integrity. Unlike existing techniques that require extensive pre-training or
fine-tuning, our approach demonstrates remarkable adaptability across various
LLMs through its model-agnostic design. BayesGenie employs an adapted Bayesian
optimization strategy to automatically refine the inference process parameters,
achieving high-precision image editing with minimal user intervention. Through
extensive experiments across diverse scenarios, we demonstrate that our
framework significantly outperforms existing methods in both editing accuracy
and semantic preservation, as validated using different LLMs including Claude3
and GPT-4.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Uncertainty Quantification in Retrieval Augmented Question Answering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.18108v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.18108v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Laura Perez-Beltrachini, Mirella Lapata
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Retrieval augmented Question Answering (QA) helps QA models overcome
knowledge gaps by incorporating retrieved evidence, typically a set of
passages, alongside the question at test time. Previous studies show that this
approach improves QA performance and reduces hallucinations, without, however,
assessing whether the retrieved passages are indeed useful at answering
correctly. In this work, we propose to quantify the uncertainty of a QA model
via estimating the utility of the passages it is provided with. We train a
lightweight neural model to predict passage utility for a target QA model and
show that while simple information theoretic metrics can predict answer
correctness up to a certain extent, our approach efficiently approximates or
outperforms more expensive sampling-based methods. Code and data are available
at https://github.com/lauhaide/ragu.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Detecting Offensive Memes with Social Biases in Singapore Context Using
  Multimodal Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.18101v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.18101v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Cao Yuxuan, Wu Jiayang, Alistair Cheong Liang Chuen, Bryan Shan Guanrong, Theodore Lee Chong Jen, Sherman Chann Zhi Shen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Traditional online content moderation systems struggle to classify modern
multimodal means of communication, such as memes, a highly nuanced and
information-dense medium. This task is especially hard in a culturally diverse
society like Singapore, where low-resource languages are used and extensive
knowledge on local context is needed to interpret online content. We curate a
large collection of 112K memes labeled by GPT-4V for fine-tuning a VLM to
classify offensive memes in Singapore context. We show the effectiveness of
fine-tuned VLMs on our dataset, and propose a pipeline containing OCR,
translation and a 7-billion parameter-class VLM. Our solutions reach 80.62%
accuracy and 0.8192 AUROC on a held-out test set, and can greatly aid human in
moderating online contents. The dataset, code, and model weights will be
open-sourced at https://github.com/aliencaocao/vlm-for-memes-aisg.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Thinking-Optimal Scaling of Test-Time Compute for LLM Reasoning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.18080v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.18080v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenkai Yang, Shuming Ma, Yankai Lin, Furu Wei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent studies have shown that making a model spend more time thinking
through longer Chain of Thoughts (CoTs) enables it to gain significant
improvements in complex reasoning tasks. While current researches continue to
explore the benefits of increasing test-time compute by extending the CoT
lengths of Large Language Models (LLMs), we are concerned about a potential
issue hidden behind the current pursuit of test-time scaling: Would excessively
scaling the CoT length actually bring adverse effects to a model's reasoning
performance? Our explorations on mathematical reasoning tasks reveal an
unexpected finding that scaling with longer CoTs can indeed impair the
reasoning performance of LLMs in certain domains. Moreover, we discover that
there exists an optimal scaled length distribution that differs across
different domains. Based on these insights, we propose a Thinking-Optimal
Scaling strategy. Our method first uses a small set of seed data with varying
response length distributions to teach the model to adopt different reasoning
efforts for deep thinking. Then, the model selects its shortest correct
response under different reasoning efforts on additional problems for
self-improvement. Our self-improved models built upon Qwen2.5-32B-Instruct
outperform other distillation-based 32B o1-like models across various math
benchmarks, and achieve performance on par with QwQ-32B-Preview.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Defining bias in AI-systems: Biased models are fair models <span class="chip">ISCA</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.18060v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.18060v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chiara Lindloff, Ingo Siegert
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The debate around bias in AI systems is central to discussions on algorithmic
fairness. However, the term bias often lacks a clear definition, despite
frequently being contrasted with fairness, implying that an unbiased model is
inherently fair. In this paper, we challenge this assumption and argue that a
precise conceptualization of bias is necessary to effectively address fairness
concerns. Rather than viewing bias as inherently negative or unfair, we
highlight the importance of distinguishing between bias and discrimination. We
further explore how this shift in focus can foster a more constructive
discourse within academic debates on fairness in AI systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ISCA/ITG Workshop on Diversity in Large Speech and Language Models</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Uncertainty-aware abstention in medical diagnosis based on medical texts 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.18050v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.18050v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Artem Vazhentsev, Ivan Sviridov, Alvard Barseghyan, Gleb Kuzmin, Alexander Panchenko, Aleksandr Nesterov, Artem Shelmanov, Maxim Panov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study addresses the critical issue of reliability for AI-assisted
medical diagnosis. We focus on the selection prediction approach that allows
the diagnosis system to abstain from providing the decision if it is not
confident in the diagnosis. Such selective prediction (or abstention)
approaches are usually based on the modeling predictive uncertainty of machine
learning models involved.
  This study explores uncertainty quantification in machine learning models for
medical text analysis, addressing diverse tasks across multiple datasets. We
focus on binary mortality prediction from textual data in MIMIC-III,
multi-label medical code prediction using ICD-10 codes from MIMIC-IV, and
multi-class classification with a private outpatient visits dataset.
Additionally, we analyze mental health datasets targeting depression and
anxiety detection, utilizing various text-based sources, such as essays, social
media posts, and clinical descriptions.
  In addition to comparing uncertainty methods, we introduce HUQ-2, a new
state-of-the-art method for enhancing reliability in selective prediction
tasks. Our results provide a detailed comparison of uncertainty quantification
methods. They demonstrate the effectiveness of HUQ-2 in capturing and
evaluating uncertainty, paving the way for more reliable and interpretable
applications in medical text analysis.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Harnessing Multiple Large Language Models: A <span class="highlight-title">Survey</span> on LLM Ensemble 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.18036v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.18036v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhijun Chen, Jingzheng Li, Pengpeng Chen, Zhuoran Li, Kai Sun, Yuankai Luo, Qianren Mao, Dingqi Yang, Hailong Sun, Philip S. Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  LLM Ensemble -- which involves the comprehensive use of multiple large
language models (LLMs), each aimed at handling user queries during downstream
inference, to benefit from their individual strengths -- has gained substantial
attention recently. The widespread availability of LLMs, coupled with their
varying strengths and out-of-the-box usability, has profoundly advanced the
field of LLM Ensemble. This paper presents the first systematic review of
recent developments in LLM Ensemble. First, we introduce our taxonomy of LLM
Ensemble and discuss several related research problems. Then, we provide a more
in-depth classification of the methods under the broad categories of
"ensemble-before-inference, ensemble-during-inference,
ensemble-after-inference", and review all relevant methods. Finally, we
introduce related benchmarks and applications, summarize existing studies, and
suggest several future research directions. A curated list of papers on LLM
Ensemble is available at https://github.com/junchenzhi/Awesome-LLM-Ensemble.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 2 figures, codebase:
  https://github.com/junchenzhi/Awesome-LLM-Ensemble</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Detecting Knowledge Boundary of Vision Large Language Models by
  Sampling-Based Inference 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.18023v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.18023v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhuo Chen, Xinyu Wang, Yong Jiang, Zhen Zhang, Xinyu Geng, Pengjun Xie, Fei Huang, Kewei Tu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite the advancements made in Visual Large Language Models (VLLMs), like
text Large Language Models (LLMs), they have limitations in addressing
questions that require real-time information or are knowledge-intensive.
Indiscriminately adopting Retrieval Augmented Generation (RAG) techniques is an
effective yet expensive way to enable models to answer queries beyond their
knowledge scopes. To mitigate the dependence on retrieval and simultaneously
maintain, or even improve, the performance benefits provided by retrieval, we
propose a method to detect the knowledge boundary of VLLMs, allowing for more
efficient use of techniques like RAG. Specifically, we propose a method with
two variants that fine-tunes a VLLM on an automatically constructed dataset for
boundary identification. Experimental results on various types of Visual
Question Answering datasets show that our method successfully depicts a VLLM's
knowledge boundary based on which we are able to reduce indiscriminate
retrieval while maintaining or improving the performance. In addition, we show
that the knowledge boundary identified by our method for one VLLM can be used
as a surrogate boundary for other VLLMs. Code will be released at
https://github.com/Chord-Chen-30/VLLM-KnowledgeBoundary
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AfroXLMR-Comet: Multilingual Knowledge Distillation with Attention
  Matching for Low-Resource languages 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.18020v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.18020v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Joshua Sakthivel Raju, Sanjay S, Jaskaran Singh Walia, Srinivas Raghav, Vukosi Marivate
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Language model compression through knowledge distillation has emerged as a
promising approach for deploying large language models in resource-constrained
environments. However, existing methods often struggle to maintain performance
when distilling multilingual models, especially for low-resource languages. In
this paper, we present a novel hybrid distillation approach that combines
traditional knowledge distillation with a simplified attention matching
mechanism, specifically designed for multilingual contexts. Our method
introduces an extremely compact student model architecture, significantly
smaller than conventional multilingual models. We evaluate our approach on five
African languages: Kinyarwanda, Swahili, Hausa, Igbo, and Yoruba. The distilled
student model; AfroXLMR-Comet successfully captures both the output
distribution and internal attention patterns of a larger teacher model
(AfroXLMR-Large) while reducing the model size by over 85%. Experimental
results demonstrate that our hybrid approach achieves competitive performance
compared to the teacher model, maintaining an accuracy within 85% of the
original model's performance while requiring substantially fewer computational
resources. Our work provides a practical framework for deploying efficient
multilingual models in resource-constrained environments, particularly
benefiting applications involving African languages.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Verdict: A Library for Scaling Judge-Time Compute 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.18018v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.18018v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nimit Kalra, Leonard Tang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The use of LLMs as automated judges ("LLM-as-a-judge") is now widespread, yet
standard judges suffer from a multitude of reliability issues. To address these
challenges, we introduce Verdict, an open-source library for scaling judge-time
compute to enhance the accuracy, reliability, and interpretability of automated
evaluators. Verdict leverages the composition of modular reasoning units --
such as verification, debate, and aggregation -- and increased inference-time
compute to improve LLM judge quality. Across a variety of challenging tasks
such as content moderation, fact-checking, and hallucination detection, Verdict
judges achieve state-of-the-art (SOTA) or near-SOTA performance, surpassing
orders-of-magnitude larger fine-tuned judges, prompted judges, and reasoning
models. Ultimately, we hope Verdict serves as a useful framework for
researchers and practitioners building scalable, interpretable, and reliable
LLM-based evaluators.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ViDoRAG: Visual Document Retrieval-Augmented Generation via Dynamic
  Iterative Reasoning Agents 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.18017v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.18017v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qiuchen Wang, Ruixue Ding, Zehui Chen, Weiqi Wu, Shihang Wang, Pengjun Xie, Feng Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Understanding information from visually rich documents remains a significant
challenge for traditional Retrieval-Augmented Generation (RAG) methods.
Existing benchmarks predominantly focus on image-based question answering (QA),
overlooking the fundamental challenges of efficient retrieval, comprehension,
and reasoning within dense visual documents. To bridge this gap, we introduce
ViDoSeek, a novel dataset designed to evaluate RAG performance on visually rich
documents requiring complex reasoning. Based on it, we identify key limitations
in current RAG approaches: (i) purely visual retrieval methods struggle to
effectively integrate both textual and visual features, and (ii) previous
approaches often allocate insufficient reasoning tokens, limiting their
effectiveness. To address these challenges, we propose ViDoRAG, a novel
multi-agent RAG framework tailored for complex reasoning across visual
documents. ViDoRAG employs a Gaussian Mixture Model (GMM)-based hybrid strategy
to effectively handle multi-modal retrieval. To further elicit the model's
reasoning capabilities, we introduce an iterative agent workflow incorporating
exploration, summarization, and reflection, providing a framework for
investigating test-time scaling in RAG domains. Extensive experiments on
ViDoSeek validate the effectiveness and generalization of our approach.
Notably, ViDoRAG outperforms existing methods by over 10% on the competitive
ViDoSeek benchmark.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Unveiling the Key Factors for Distilling Chain-of-Thought Reasoning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.18001v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.18001v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinghao Chen, Zhijing Sun, Wenjin Guo, Miaoran Zhang, Yanjun Chen, Yirong Sun, Hui Su, Yijie Pan, Dietrich Klakow, Wenjie Li, Xiaoyu Shen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) excel in reasoning tasks through
Chain-of-Thought (CoT) prompting. However, CoT prompting greatly increases
computational demands, which has prompted growing interest in distilling CoT
capabilities into Small Language Models (SLMs). This study systematically
examines the factors influencing CoT distillation, including the choice of
granularity, format and teacher model. Through experiments involving four
teacher models and seven student models across seven mathematical and
commonsense reasoning datasets, we uncover three key findings: (1) Unlike LLMs,
SLMs exhibit a non-monotonic relationship with granularity, with stronger
models benefiting from finer-grained reasoning and weaker models performing
better with simpler CoT supervision; (2) CoT format significantly impacts LLMs
but has minimal effect on SLMs, likely due to their reliance on supervised
fine-tuning rather than pretraining preferences; (3) Stronger teacher models do
NOT always produce better student models, as diversity and complexity in CoT
supervision can outweigh accuracy alone. These findings emphasize the need to
tailor CoT strategies to specific student model, offering actionable insights
for optimizing CoT distillation in SLMs. The code and datasets are available at
https://github.com/EIT-NLP/Distilling-CoT-Reasoning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MAGE: Multi-Head Attention Guided Embeddings for Low Resource Sentiment
  Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.17987v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.17987v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Varun Vashisht, Samar Singh, Mihir Konduskar, Jaskaran Singh Walia, Vukosi Marivate
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Due to the lack of quality data for low-resource Bantu languages, significant
challenges are presented in text classification and other practical
implementations. In this paper, we introduce an advanced model combining
Language-Independent Data Augmentation (LiDA) with Multi-Head Attention based
weighted embeddings to selectively enhance critical data points and improve
text classification performance. This integration allows us to create robust
data augmentation strategies that are effective across various linguistic
contexts, ensuring that our model can handle the unique syntactic and semantic
features of Bantu languages. This approach not only addresses the data scarcity
issue but also sets a foundation for future research in low-resource language
processing and classification tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LLM Knows Geometry Better than Algebra: Numerical Understanding of
  LLM-Based Agents in A Trading Arena 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.17967v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.17967v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tianmi Ma, Jiawei Du, Wenxin Huang, Wenjie Wang, Liang Xie, Xian Zhong, Joey Tianyi Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in large language models (LLMs) have significantly
improved performance in natural language processing tasks. However, their
ability to generalize to dynamic, unseen tasks, particularly in numerical
reasoning, remains a challenge. Existing benchmarks mainly evaluate LLMs on
problems with predefined optimal solutions, which may not align with real-world
scenarios where clear answers are absent. To bridge this gap, we design the
Agent Trading Arena, a virtual numerical game simulating complex economic
systems through zero-sum games, where agents invest in stock portfolios. Our
experiments reveal that LLMs, including GPT-4o, struggle with algebraic
reasoning when dealing with plain-text stock data, often focusing on local
details rather than global trends. In contrast, LLMs perform significantly
better with geometric reasoning when presented with visual data, such as
scatter plots or K-line charts, suggesting that visual representations enhance
numerical reasoning. This capability is further improved by incorporating the
reflection module, which aids in the analysis and interpretation of complex
data. We validate our findings on NASDAQ Stock dataset, where LLMs demonstrate
stronger reasoning with visual data compared to text. Our code and data are
publicly available at https://github.com/wekjsdvnm/Agent-Trading-Arena.git.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On Synthetic Data Strategies for Domain-Specific Generative Retrieval 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.17957v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.17957v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haoyang Wen, Jiang Guo, Yi Zhang, Jiarong Jiang, Zhiguo Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper investigates synthetic data generation strategies in developing
generative retrieval models for domain-specific corpora, thereby addressing the
scalability challenges inherent in manually annotating in-domain queries. We
study the data strategies for a two-stage training framework: in the first
stage, which focuses on learning to decode document identifiers from queries,
we investigate LLM-generated queries across multiple granularity (e.g. chunks,
sentences) and domain-relevant search constraints that can better capture
nuanced relevancy signals. In the second stage, which aims to refine document
ranking through preference learning, we explore the strategies for mining hard
negatives based on the initial model's predictions. Experiments on public
datasets over diverse domains demonstrate the effectiveness of our synthetic
data generation and hard negative sampling approach.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Better Understanding of Program-of-Thought Reasoning in
  Cross-Lingual and Multilingual Environments 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.17956v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.17956v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Patomporn Payoungkhamdee, Pume Tuchinda, Jinheon Baek, Samuel Cahyawijaya, Can Udomcharoenchaikit, Potsawee Manakul, Peerat Limkonchotiwat, Ekapol Chuangsuwanich, Sarana Nutanong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-step reasoning is essential for large language models (LLMs), yet
multilingual performance remains challenging. While Chain-of-Thought (CoT)
prompting improves reasoning, it struggles with non-English languages due to
the entanglement of reasoning and execution. Program-of-Thought (PoT) prompting
separates reasoning from execution, offering a promising alternative but
shifting the challenge to generating programs from non-English questions. We
propose a framework to evaluate PoT by separating multilingual reasoning from
code execution to examine (i) the impact of fine-tuning on question-reasoning
alignment and (ii) how reasoning quality affects answer correctness. Our
findings demonstrate that PoT fine-tuning substantially enhances multilingual
reasoning, outperforming CoT fine-tuned models. We further demonstrate a strong
correlation between reasoning quality (measured through code quality) and
answer accuracy, highlighting its potential as a test-time performance
improvement heuristic.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Language Models' Factuality Depends on the Language of Inquiry 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.17955v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.17955v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tushar Aggarwal, Kumar Tanmay, Ayush Agrawal, Kumar Ayush, Hamid Palangi, Paul Pu Liang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multilingual language models (LMs) are expected to recall factual knowledge
consistently across languages, yet they often fail to transfer knowledge
between languages even when they possess the correct information in one of the
languages. For example, we find that an LM may correctly identify Rashed Al
Shashai as being from Saudi Arabia when asked in Arabic, but consistently fails
to do so when asked in English or Swahili. To systematically investigate this
limitation, we introduce a benchmark of 10,000 country-related facts across 13
languages and propose three novel metrics: Factual Recall Score, Knowledge
Transferability Score, and Cross-Lingual Factual Knowledge Transferability
Score-to quantify factual recall and knowledge transferability in LMs across
different languages. Our results reveal fundamental weaknesses in today's
state-of-the-art LMs, particularly in cross-lingual generalization where models
fail to transfer knowledge effectively across different languages, leading to
inconsistent performance sensitive to the language used. Our findings emphasize
the need for LMs to recognize language-specific factual reliability and
leverage the most trustworthy information across languages. We release our
benchmark and evaluation framework to drive future research in multilingual
knowledge transfer.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DeepSeek-R1 Outperforms Gemini 2.0 Pro, OpenAI o1, and o3-mini in
  Bilingual Complex Ophthalmology Reasoning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.17947v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.17947v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pusheng Xu, Yue Wu, Kai Jin, Xiaolan Chen, Mingguang He, Danli Shi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Purpose: To evaluate the accuracy and reasoning ability of DeepSeek-R1 and
three other recently released large language models (LLMs) in bilingual complex
ophthalmology cases. Methods: A total of 130 multiple-choice questions (MCQs)
related to diagnosis (n = 39) and management (n = 91) were collected from the
Chinese ophthalmology senior professional title examination and categorized
into six topics. These MCQs were translated into English using DeepSeek-R1. The
responses of DeepSeek-R1, Gemini 2.0 Pro, OpenAI o1 and o3-mini were generated
under default configurations between February 15 and February 20, 2025.
Accuracy was calculated as the proportion of correctly answered questions, with
omissions and extra answers considered incorrect. Reasoning ability was
evaluated through analyzing reasoning logic and the causes of reasoning error.
Results: DeepSeek-R1 demonstrated the highest overall accuracy, achieving 0.862
in Chinese MCQs and 0.808 in English MCQs. Gemini 2.0 Pro, OpenAI o1, and
OpenAI o3-mini attained accuracies of 0.715, 0.685, and 0.692 in Chinese MCQs
(all P<0.001 compared with DeepSeek-R1), and 0.746 (P=0.115), 0.723 (P=0.027),
and 0.577 (P<0.001) in English MCQs, respectively. DeepSeek-R1 achieved the
highest accuracy across five topics in both Chinese and English MCQs. It also
excelled in management questions conducted in Chinese (all P<0.05). Reasoning
ability analysis showed that the four LLMs shared similar reasoning logic.
Ignoring key positive history, ignoring key positive signs, misinterpretation
medical data, and too aggressive were the most common causes of reasoning
errors. Conclusion: DeepSeek-R1 demonstrated superior performance in bilingual
complex ophthalmology reasoning tasks than three other state-of-the-art LLMs.
While its clinical applicability remains challenging, it shows promise for
supporting diagnosis and clinical decision-making.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>29 pages, 4 figures, 1 table</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Assessing Large Language Models in Agentic Multilingual National Bias 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.17945v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.17945v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qianying Liu, Katrina Qiyao Wang, Fei Cheng, Sadao Kurohashi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models have garnered significant attention for their
capabilities in multilingual natural language processing, while studies on
risks associated with cross biases are limited to immediate context
preferences. Cross-language disparities in reasoning-based recommendations
remain largely unexplored, with a lack of even descriptive analysis. This study
is the first to address this gap. We test LLM's applicability and capability in
providing personalized advice across three key scenarios: university
applications, travel, and relocation. We investigate multilingual bias in
state-of-the-art LLMs by analyzing their responses to decision-making tasks
across multiple languages. We quantify bias in model-generated scores and
assess the impact of demographic factors and reasoning strategies (e.g.,
Chain-of-Thought prompting) on bias patterns. Our findings reveal that local
language bias is prevalent across different tasks, with GPT-4 and Sonnet
reducing bias for English-speaking countries compared to GPT-3.5 but failing to
achieve robust multilingual alignment, highlighting broader implications for
multilingual AI agents and applications such as education.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CaseGen: A Benchmark for Multi-Stage Legal Case Documents Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.17943v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.17943v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haitao Li, Jiaying Ye, Yiran Hu, Jia Chen, Qingyao Ai, Yueyue Wu, Junjie Chen, Yifan Chen, Cheng Luo, Quan Zhou, Yiqun Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Legal case documents play a critical role in judicial proceedings. As the
number of cases continues to rise, the reliance on manual drafting of legal
case documents is facing increasing pressure and challenges. The development of
large language models (LLMs) offers a promising solution for automating
document generation. However, existing benchmarks fail to fully capture the
complexities involved in drafting legal case documents in real-world scenarios.
To address this gap, we introduce CaseGen, the benchmark for multi-stage legal
case documents generation in the Chinese legal domain. CaseGen is based on 500
real case samples annotated by legal experts and covers seven essential case
sections. It supports four key tasks: drafting defense statements, writing
trial facts, composing legal reasoning, and generating judgment results. To the
best of our knowledge, CaseGen is the first benchmark designed to evaluate LLMs
in the context of legal case document generation. To ensure an accurate and
comprehensive evaluation, we design the LLM-as-a-judge evaluation framework and
validate its effectiveness through human annotations. We evaluate several
widely used general-domain LLMs and legal-specific LLMs, highlighting their
limitations in case document generation and pinpointing areas for potential
improvement. This work marks a step toward a more effective framework for
automating legal case documents drafting, paving the way for the reliable
application of AI in the legal field. The dataset and code are publicly
available at https://github.com/CSHaitao/CaseGen.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Advantage-Guided Distillation for Preference Alignment in Small Language
  Models <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.17927v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.17927v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shiping Gao, Fanqi Wan, Jiajian Guo, Xiaojun Quan, Qifan Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Alignment techniques enable Large Language Models (LLMs) to generate outputs
that align with human preferences and play a crucial role in their
effectiveness. However, their impact often diminishes when applied to Small
Language Models (SLMs), likely due to the limited capacity of these models.
Instead of directly applying existing alignment techniques to SLMs, we propose
to utilize a well-aligned teacher LLM to guide the alignment process for these
models, thereby facilitating the transfer of the teacher's knowledge of human
preferences to the student model. To achieve this, we first explore a
straightforward approach, Dual-Constrained Knowledge Distillation (DCKD), that
employs knowledge distillation with two KL-divergence constraints from the
aligned teacher to the unaligned student. To further enhance the student's
ability to distinguish between preferred and dispreferred responses, we then
propose Advantage-Guided Distillation for Preference Alignment (ADPA), which
leverages an advantage function from the aligned teacher to deliver more
nuanced, distribution-level reward signals for the student's alignment. Our
experimental results show that these two approaches appreciably improve the
alignment of SLMs and narrow the performance gap with larger counterparts.
Among them, ADPA demonstrates superior performance and achieves even greater
effectiveness when integrated with DCKD. Our code is available at
https://github.com/SLIT-AI/ADPA.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICLR 2025(spotlight)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FACT-AUDIT: An Adaptive Multi-Agent Framework for Dynamic Fact-Checking
  Evaluation of Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.17924v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.17924v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hongzhan Lin, Yang Deng, Yuxuan Gu, Wenxuan Zhang, Jing Ma, See-Kiong Ng, Tat-Seng Chua
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have significantly advanced the fact-checking
studies. However, existing automated fact-checking evaluation methods rely on
static datasets and classification metrics, which fail to automatically
evaluate the justification production and uncover the nuanced limitations of
LLMs in fact-checking. In this work, we introduce FACT-AUDIT, an agent-driven
framework that adaptively and dynamically assesses LLMs' fact-checking
capabilities. Leveraging importance sampling principles and multi-agent
collaboration, FACT-AUDIT generates adaptive and scalable datasets, performs
iterative model-centric evaluations, and updates assessments based on
model-specific responses. By incorporating justification production alongside
verdict prediction, this framework provides a comprehensive and evolving audit
of LLMs' factual reasoning capabilities, to investigate their trustworthiness.
Extensive experiments demonstrate that FACT-AUDIT effectively differentiates
among state-of-the-art LLMs, providing valuable insights into model strengths
and limitations in model-centric fact-checking analysis.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Scaling LLM <span class="highlight-title">Pre-train</span>ing with Vocabulary Curriculum 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.17910v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.17910v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fangyuan Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Modern language models rely on static vocabularies, fixed before pretraining,
in contrast to the adaptive vocabulary acquisition observed in human language
learning. To bridge this gap, we introduce vocabulary curriculum learning, an
approach that improves pretraining efficiency with log-linear scaling gains
relative to vocabulary size. Our method alternates between entropy-guided
vocabulary expansion and model optimization, enabling models to learn
transferable representations across diverse tokenization granularities. This
approach naturally gives rise to an optimal computation allocation pattern:
longer tokens capture predictable content, while shorter tokens focus on more
complex, harder-to-predict contexts. Experiments on small-scale GPT models
demonstrate improved scaling efficiency, reinforcing the effectiveness of
dynamic tokenization. We release our code to support further research and plan
to extend our experiments to larger models and diverse domains.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Can Large Language Models Identify Implicit Suicidal Ideation? An
  Empirical Evaluation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.17899v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.17899v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tong Li, Shu Yang, Junchao Wu, Jiyao Wei, Lijie Hu, Mengdi Li, Derek F. Wong, Joshua R. Oltmanns, Di Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a comprehensive evaluation framework for assessing Large Language
Models' (LLMs) capabilities in suicide prevention, focusing on two critical
aspects: the Identification of Implicit Suicidal ideation (IIS) and the
Provision of Appropriate Supportive responses (PAS). We introduce \ourdata, a
novel dataset of 1,308 test cases built upon psychological frameworks including
D/S-IAT and Negative Automatic Thinking, alongside real-world scenarios.
Through extensive experiments with 8 widely used LLMs under different
contextual settings, we find that current models struggle significantly with
detecting implicit suicidal ideation and providing appropriate support,
highlighting crucial limitations in applying LLMs to mental health contexts.
Our findings underscore the need for more sophisticated approaches in
developing and evaluating LLMs for sensitive psychological applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ RankCoT: Refining Knowledge for Retrieval-Augmented Generation through
  Ranking Chain-of-Thoughts 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.17888v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.17888v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mingyan Wu, Zhenghao Liu, Yukun Yan, Xinze Li, Shi Yu, Zheni Zeng, Yu Gu, Ge Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Retrieval-Augmented Generation (RAG) enhances the performance of Large
Language Models (LLMs) by incorporating external knowledge. However, LLMs still
encounter challenges in effectively utilizing the knowledge from retrieved
documents, often being misled by irrelevant or noisy information. To address
this issue, we introduce RankCoT, a knowledge refinement method that
incorporates reranking signals in generating CoT-based summarization for
knowledge refinement based on given query and all retrieval documents. During
training, RankCoT prompts the LLM to generate Chain-of-Thought (CoT) candidates
based on the query and individual documents. It then fine-tunes the LLM to
directly reproduce the best CoT from these candidate outputs based on all
retrieved documents, which requires LLM to filter out irrelevant documents
during generating CoT-style summarization. Additionally, RankCoT incorporates a
self-reflection mechanism that further refines the CoT outputs, resulting in
higher-quality training data. Our experiments demonstrate the effectiveness of
RankCoT, showing its superior performance over other knowledge refinement
models. Further analysis reveals that RankCoT can provide shorter but effective
refinement results, enabling the generator to produce more accurate answers.
All code and data are available at https://github.com/NEUIR/RankCoT.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Science Across Languages: Assessing LLM Multilingual Translation of
  Scientific Papers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.17882v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.17882v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hannah Calzi Kleidermacher, James Zou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Scientific research is inherently global. However, the vast majority of
academic journals are published exclusively in English, creating barriers for
non-native-English-speaking researchers. In this study, we leverage large
language models (LLMs) to translate published scientific articles while
preserving their native JATS XML formatting, thereby developing a practical,
automated approach for implementation by academic journals. Using our approach,
we translate articles across multiple scientific disciplines into 28 languages.
To evaluate translation accuracy, we introduce a novel question-and-answer (QA)
benchmarking method, in which an LLM generates comprehension-based questions
from the original text and then answers them based on the translated text. Our
benchmark results show an average performance of 95.9%, showing that the key
scientific details are accurately conveyed. In a user study, we translate the
scientific papers of 15 researchers into their native languages, finding that
the authors consistently found the translations to accurately capture the
original information in their articles. Interestingly, a third of the authors
found many technical terms "overtranslated," expressing a preference to keep
terminology more familiar in English untranslated. Finally, we demonstrate how
in-context learning techniques can be used to align translations with
domain-specific preferences such as mitigating overtranslation, highlighting
the adaptability and utility of LLM-driven scientific translation. The code and
translated articles are available at https://hankleid.github.io/ProjectMundo.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Enhanced Immersion and Agency for LLM-based Interactive Drama 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.17878v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.17878v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hongqiu Wu, Weiqi Wu, Tianyang Xu, Jiameng Zhang, Hai Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  LLM-based Interactive Drama is a novel AI-based dialogue scenario, where the
user (i.e. the player) plays the role of a character in the story, has
conversations with characters played by LLM agents, and experiences an
unfolding story. This paper begins with understanding interactive drama from
two aspects: Immersion, the player's feeling of being present in the story, and
Agency, the player's ability to influence the story world. Both are crucial to
creating an enjoyable interactive experience, while they have been
underexplored in previous work. To enhance these two aspects, we first propose
Playwriting-guided Generation, a novel method that helps LLMs craft dramatic
stories with substantially improved structures and narrative quality.
Additionally, we introduce Plot-based Reflection for LLM agents to refine their
reactions to align with the player's intentions. Our evaluation relies on human
judgment to assess the gains of our methods in terms of immersion and agency.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SYNTHEMPATHY: A Scalable Empathy Corpus Generated Using LLMs Without Any
  Crowdsourcing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.17857v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.17857v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Run Chen, Jun Shin, Julia Hirschberg
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Previous research has shown that humans are more receptive towards language
models that that exhibit empathetic behavior. While empathy is essential for
developing helpful dialogue agents, very few large corpora containing
empathetic dialogues are available for fine-tune LLMs. The few existing corpora
have largely relied on crowdsourcing to simulate empathetic conversations, a
process that is expensive, time-consuming, and not scalable to larger datasets.
We propose a data generation framework for developing SYNTHEMPATHY, a large
corpus containing 105k empathetic responses to real-life situations compiled
through LLM generation. A base Mistral 7B model fine-tuned on our SYNTHEMPATHY
corpus exhibits an increase in the average empathy score.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LR${}^{2}$Bench: Evaluating Long-chain Reflective Reasoning Capabilities
  of Large Language Models via Constraint Satisfaction Problems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.17848v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.17848v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jianghao Chen, Zhenlin Wei, Zhenjiang Ren, Ziyong Li, Jiajun Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent progress in o1-like models has significantly enhanced the reasoning
abilities of Large Language Models (LLMs), empowering them to tackle
increasingly complex tasks through reflection capabilities, such as making
assumptions, backtracking, and self-refinement. However, effectively evaluating
such reflection capabilities remains challenging due to the lack of appropriate
benchmarks. To bridge this gap, we introduce LR${}^{2}$Bench, a novel benchmark
designed to evaluate the Long-chain Reflective Reasoning capabilities of LLMs.
LR${}^{2}$Bench comprises 850 samples across six Constraint Satisfaction
Problems (CSPs) where reflective reasoning is crucial for deriving solutions
that meet all given constraints. Each type of task focuses on distinct
constraint patterns, such as knowledge-based, logical, and spatial constraints,
providing a comprehensive evaluation of diverse problem-solving scenarios. We
conduct extensive evaluation on both conventional models and o1-like models.
Our experimental results reveal that even the most advanced reasoning-specific
models, such as DeepSeek-R1 and OpenAI o1-preview, struggle with tasks in
LR${}^{2}$Bench, achieving an average Exact Match score of only 20.0% and
23.6%, respectively. These findings underscore the significant room for
improvement in the reflective reasoning capabilities of current LLMs. The
leaderboard of our benchmark is available at
https://huggingface.co/spaces/UltraRonin/LR2Bench
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Say Less, Mean More: Leveraging Pragmatics in Retrieval-Augmented
  Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.17839v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.17839v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haris Riaz, Ellen Riloff, Mihai Surdeanu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a simple, unsupervised method that injects pragmatic principles in
retrieval-augmented generation (RAG) frameworks such as Dense Passage
Retrieval~\cite{karpukhin2020densepassageretrievalopendomain} to enhance the
utility of retrieved contexts. Our approach first identifies which sentences in
a pool of documents retrieved by RAG are most relevant to the question at hand,
cover all the topics addressed in the input question and no more, and then
highlights these sentences within their context, before they are provided to
the LLM, without truncating or altering the context in any other way. We show
that this simple idea brings consistent improvements in experiments on three
question answering tasks (ARC-Challenge, PubHealth and PopQA) using five
different LLMs. It notably enhances relative accuracy by up to 19.7\% on
PubHealth and 10\% on ARC-Challenge compared to a conventional RAG system.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages, 2 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A General Framework to Enhance Fine-tuning-based LLM Unlearning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.17823v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.17823v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jie Ren, Zhenwei Dai, Xianfeng Tang, Hui Liu, Jingying Zeng, Zhen Li, Rahul Goutam, Suhang Wang, Yue Xing, Qi He, Hui Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Unlearning has been proposed to remove copyrighted and privacy-sensitive data
from Large Language Models (LLMs). Existing approaches primarily rely on
fine-tuning-based methods, which can be categorized into gradient ascent-based
(GA-based) and suppression-based methods. However, they often degrade model
utility (the ability to respond to normal prompts). In this work, we aim to
develop a general framework that enhances the utility of fine-tuning-based
unlearning methods. To achieve this goal, we first investigate the common
property between GA-based and suppression-based methods. We unveil that
GA-based methods unlearn by distinguishing the target data (i.e., the data to
be removed) and suppressing related generations, which is essentially the same
strategy employed by suppression-based methods. Inspired by this finding, we
introduce Gated Representation UNlearning (GRUN) which has two components: a
soft gate function for distinguishing target data and a suppression module
using Representation Fine-tuning (ReFT) to adjust representations rather than
model parameters. Experiments show that GRUN significantly improves the
unlearning and utility. Meanwhile, it is general for fine-tuning-based methods,
efficient and promising for sequential unlearning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Predicting Through Generation: Why Generation Is Better for Prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.17817v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.17817v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Md Kowsher, Nusrat Jahan Prottasha, Prakash Bhat, Chun-Nam Yu, Mojtaba Soltanalian, Ivan Garibay, Ozlem Garibay, Chen Chen, Niloofar Yousefi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper argues that generating output tokens is more effective than using
pooled representations for prediction tasks because token-level generation
retains more mutual information. Since LLMs are trained on massive text corpora
using next-token prediction, generation aligns naturally with their learned
behavior. Using the Data Processing Inequality (DPI), we provide both
theoretical and empirical evidence supporting this claim. However,
autoregressive models face two key challenges when used for prediction: (1)
exposure bias, where the model sees ground truth tokens during training but
relies on its own predictions during inference, leading to errors, and (2)
format mismatch, where discrete tokens do not always align with the tasks
required output structure. To address these challenges, we introduce
PredGen(Predicting Through Generating), an end to end framework that (i) uses
scheduled sampling to reduce exposure bias, and (ii) introduces a task adapter
to convert the generated tokens into structured outputs. Additionally, we
introduce Writer-Director Alignment Loss (WDAL), which ensures consistency
between token generation and final task predictions, improving both text
coherence and numerical accuracy. We evaluate PredGen on multiple
classification and regression benchmarks. Our results show that PredGen
consistently outperforms standard baselines, demonstrating its effectiveness in
structured prediction tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint paper</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ An <span class="highlight-title">Overview</span> of Large Language Models for Statisticians 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.17814v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.17814v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenlong Ji, Weizhe Yuan, Emily Getzen, Kyunghyun Cho, Michael I. Jordan, Song Mei, Jason E Weston, Weijie J. Su, Jing Xu, Linjun Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have emerged as transformative tools in
artificial intelligence (AI), exhibiting remarkable capabilities across diverse
tasks such as text generation, reasoning, and decision-making. While their
success has primarily been driven by advances in computational power and deep
learning architectures, emerging problems -- in areas such as uncertainty
quantification, decision-making, causal inference, and distribution shift --
require a deeper engagement with the field of statistics. This paper explores
potential areas where statisticians can make important contributions to the
development of LLMs, particularly those that aim to engender trustworthiness
and transparency for human users. Thus, we focus on issues such as uncertainty
quantification, interpretability, fairness, privacy, watermarking and model
adaptation. We also consider possible roles for LLMs in statistical analysis.
By bridging AI and statistics, we aim to foster a deeper collaboration that
advances both the theoretical foundations and practical applications of LLMs,
ultimately shaping their role in addressing complex societal challenges.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Can Multimodal LLMs Perform Time Series Anomaly Detection? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.17812v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.17812v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiongxiao Xu, Haoran Wang, Yueqing Liang, Philip S. Yu, Yue Zhao, Kai Shu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have been increasingly used in time series
analysis. However, the potential of multimodal LLMs (MLLMs), particularly
vision-language models, for time series remains largely under-explored. One
natural way for humans to detect time series anomalies is through visualization
and textual description. Motivated by this, we raise a critical and practical
research question: Can multimodal LLMs perform time series anomaly detection?
To answer this, we propose VisualTimeAnomaly benchmark to evaluate MLLMs in
time series anomaly detection (TSAD). Our approach transforms time series
numerical data into the image format and feed these images into various MLLMs,
including proprietary models (GPT-4o and Gemini-1.5) and open-source models
(LLaVA-NeXT and Qwen2-VL), each with one larger and one smaller variant. In
total, VisualTimeAnomaly contains 12.4k time series images spanning 3 scenarios
and 3 anomaly granularities with 9 anomaly types across 8 MLLMs. Starting with
the univariate case (point- and range-wise anomalies), we extend our evaluation
to more practical scenarios, including multivariate and irregular time series
scenarios, and variate-wise anomalies. Our study reveals several key insights:
  1) MLLMs detect range- and variate-wise anomalies more effectively than
point-wise anomalies.
  2) MLLMs are highly robust to irregular time series, even with 25% of the
data missing.
  3) Open-source MLLMs perform comparably to proprietary models in TSAD. While
open-source MLLMs excel on univariate time series, proprietary MLLMs
demonstrate superior effectiveness on multivariate time series.
  To the best of our knowledge, this is the first work to comprehensively
investigate MLLMs for TSAD, particularly for multivariate and irregular time
series scenarios. We release our dataset and code at
https://github.com/mllm-ts/VisualTimeAnomaly to support future research.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages for the main content; 32 pages for the full paper including
  the appendix. More resources on the intersection of multimodal LLMs and time
  series analysis are on the website https://mllm-ts.github.io</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ URO-Bench: A Comprehensive Benchmark for End-to-End Spoken Dialogue
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.17810v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.17810v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruiqi Yan, Xiquan Li, Wenxi Chen, Zhikang Niu, Chen Yang, Ziyang Ma, Kai Yu, Xie Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, with advances in large language models (LLMs), end-to-end
spoken dialogue models (SDMs) have made significant strides. Compared to
text-based LLMs, the evaluation of SDMs needs to take speech-related aspects
into account, such as paralinguistic information and speech quality. However,
there is still a lack of comprehensive evaluations for SDMs in speech-to-speech
(S2S) scenarios. To address this gap, we propose URO-Bench, an extensive
benchmark for SDMs. Notably, URO-Bench is the first S2S benchmark that covers
evaluations about multilingualism, multi-round dialogues, and paralinguistics.
Our benchmark is divided into two difficulty levels: basic track and pro track,
consisting of 16 and 20 datasets respectively, evaluating the model's abilities
in Understanding, Reasoning, and Oral conversation. Evaluations on our proposed
benchmark reveal that current open-source SDMs perform rather well in daily QA
tasks, but lag behind their backbone LLMs in terms of instruction-following
ability and also suffer from catastrophic forgetting. Their performance in
advanced evaluations of paralinguistic information and audio understanding
remains subpar, highlighting the need for further research in this direction.
We hope that URO-Bench can effectively facilitate the development of spoken
dialogue models by providing a multifaceted evaluation of existing models and
helping to track progress in this area.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Your Language Model May Think Too Rigidly: Achieving Reasoning
  Consistency with Symmetry-Enhanced Training 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.17800v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.17800v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yihang Yao, Zhepeng Cen, Miao Li, William Han, Yuyou Zhang, Emerson Liu, Zuxin Liu, Chuang Gan, Ding Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have demonstrated strong reasoning capabilities
across various tasks. However, even minor variations in query phrasing, despite
preserving the underlying semantic meaning, can significantly affect their
performance. To address this, we focus on enhancing LLMs' awareness of symmetry
in query variations and propose syMmetry-ENhanceD (MEND) Data Augmentation, a
data-centric approach that improves the model's ability to extract useful
information from context. Unlike existing methods that emphasize reasoning
chain augmentation, our approach improves model robustness at the knowledge
extraction stage through query augmentations, enabling more data-efficient
training and stronger generalization to Out-of-Distribution (OOD) settings.
Extensive experiments on both logical and arithmetic reasoning tasks show that
MEND enhances reasoning performance across diverse query variations, providing
new insight into improving LLM robustness through structured dataset curation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhancing Human Evaluation in Machine Translation with Comparative
  Judgment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.17797v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.17797v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yixiao Song, Parker Riley, Daniel Deutsch, Markus Freitag
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Human evaluation is crucial for assessing rapidly evolving language models
but is influenced by annotator proficiency and task design. This study explores
the integration of comparative judgment into human annotation for machine
translation (MT) and evaluates three annotation setups-point-wise
Multidimensional Quality Metrics (MQM), side-by-side (SxS) MQM, and its
simplified version SxS relative ranking (RR). In MQM, annotators mark error
spans with categories and severity levels. SxS MQM extends MQM to pairwise
error annotation for two translations of the same input, while SxS RR focuses
on selecting the better output without labeling errors.
  Key findings are: (1) the SxS settings achieve higher inter-annotator
agreement than MQM; (2) SxS MQM enhances inter-translation error marking
consistency compared to MQM by, on average, 38.5% for explicitly compared MT
systems and 19.5% for others; (3) all annotation settings return stable system
rankings, with SxS RR offering a more efficient alternative to (SxS) MQM; (4)
the SxS settings highlight subtle errors overlooked in MQM without altering
absolute system evaluations.
  To spur further research, we will release the triply annotated datasets
comprising 377 ZhEn and 104 EnDe annotation examples.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint, 15 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AIR: Complex Instruction Generation via Automatic Iterative Refinement 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.17787v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.17787v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wei Liu, Yancheng He, Hui Huang, Chengwei Hu, Jiaheng Liu, Shilong Li, Wenbo Su, Bo Zheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the development of large language models, their ability to follow simple
instructions has significantly improved. However, adhering to complex
instructions remains a major challenge. Current approaches to generating
complex instructions are often irrelevant to the current instruction
requirements or suffer from limited scalability and diversity. Moreover,
methods such as back-translation, while effective for simple instruction
generation, fail to leverage the rich contents and structures in large web
corpora. In this paper, we propose a novel automatic iterative refinement
framework to generate complex instructions with constraints, which not only
better reflects the requirements of real scenarios but also significantly
enhances LLMs' ability to follow complex instructions. The AIR framework
consists of two stages: (1)Generate an initial instruction from a document;
(2)Iteratively refine instructions with LLM-as-judge guidance by comparing the
model's output with the document to incorporate valuable constraints. Finally,
we construct the AIR-10K dataset with 10K complex instructions and demonstrate
that instructions generated with our approach significantly improve the model's
ability to follow complex instructions, outperforming existing methods for
instruction generation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The first three authors contributed equally, 20 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Exploring the Potential of Large Language Models for Estimating the
  Reading Comprehension Question Difficulty 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.17785v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.17785v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yoshee Jain, John Hollander, Amber He, Sunny Tang, Liang Zhang, John Sabatini
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reading comprehension is a key for individual success, yet the assessment of
question difficulty remains challenging due to the extensive human annotation
and large-scale testing required by traditional methods such as linguistic
analysis and Item Response Theory (IRT). While these robust approaches provide
valuable insights, their scalability is limited. There is potential for Large
Language Models (LLMs) to automate question difficulty estimation; however,
this area remains underexplored. Our study investigates the effectiveness of
LLMs, specifically OpenAI's GPT-4o and o1, in estimating the difficulty of
reading comprehension questions using the Study Aid and Reading Assessment
(SARA) dataset. We evaluated both the accuracy of the models in answering
comprehension questions and their ability to classify difficulty levels as
defined by IRT. The results indicate that, while the models yield difficulty
estimates that align meaningfully with derived IRT parameters, there are
notable differences in their sensitivity to extreme item characteristics. These
findings suggest that LLMs can serve as the scalable method for automated
difficulty assessment, particularly in dynamic interactions between learners
and Adaptive Instructional Systems (AIS), bridging the gap between traditional
psychometric techniques and modern AIS for reading comprehension and paving the
way for more adaptive and personalized educational assessments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 2 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MuCoS: Efficient Drug-Target Prediction through Multi-Context-Aware
  Sampling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.17784v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.17784v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haji Gul, Abdul Gani Haji Naim, Ajaz A. Bhat
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Drug-target interactions are critical for understanding biological processes
and advancing drug discovery. However, traditional methods such as ComplEx-SE,
TransE, and DistMult struggle with unseen relationships and negative triplets,
which limits their effectiveness in drug-target prediction. To address these
challenges, we propose Multi-Context-Aware Sampling (MuCoS), an efficient and
positively accurate method for drug-target prediction. MuCoS reduces
computational complexity by prioritizing neighbors of higher density to capture
informative structural patterns. These optimized neighborhood representations
are integrated with BERT, enabling contextualized embeddings for accurate
prediction of missing relationships or tail entities. MuCoS avoids the need for
negative triplet sampling, reducing computation while improving performance
over unseen entities and relations. Experiments on the KEGG50k biomedical
dataset show that MuCoS improved over existing models by 13\% on MRR, 7\% on
Hits@1, 4\% on Hits@3, and 18\% on Hits@10 for the general relationship, and by
6\% on MRR, 1\% on Hits@1, 3\% on Hits@3, and 12\% on Hits@10 for prediction of
drug-target relationship.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Tip of the Tongue Query Elicitation for Simulated Evaluation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.17776v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.17776v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yifan He, To Eun Kim, Fernando Diaz, Jaime Arguello, Bhaskar Mitra
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Tip-of-the-tongue (TOT) search occurs when a user struggles to recall a
specific identifier, such as a document title. While common, existing search
systems often fail to effectively support TOT scenarios. Research on TOT
retrieval is further constrained by the challenge of collecting queries, as
current approaches rely heavily on community question-answering (CQA) websites,
leading to labor-intensive evaluation and domain bias. To overcome these
limitations, we introduce two methods for eliciting TOT queries - leveraging
large language models (LLMs) and human participants - to facilitate simulated
evaluations of TOT retrieval systems. Our LLM-based TOT user simulator
generates synthetic TOT queries at scale, achieving high correlations with how
CQA-based TOT queries rank TOT retrieval systems when tested in the Movie
domain. Additionally, these synthetic queries exhibit high linguistic
similarity to CQA-derived queries. For human-elicited queries, we developed an
interface that uses visual stimuli to place participants in a TOT state,
enabling the collection of natural queries. In the Movie domain, system rank
correlation and linguistic similarity analyses confirm that human-elicited
queries are both effective and closely resemble CQA-based queries. These
approaches reduce reliance on CQA-based data collection while expanding
coverage to underrepresented domains, such as Landmark and Person. LLM-elicited
queries for the Movie, Landmark, and Person domains have been released as test
queries in the TREC 2024 TOT track, with human-elicited queries scheduled for
inclusion in the TREC 2025 TOT track. Additionally, we provide source code for
synthetic query generation and the human query collection interface, along with
curated visual stimuli used for eliciting TOT queries.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FoREST: Frame of Reference Evaluation in Spatial Reasoning Tasks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.17775v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.17775v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tanawan Premsri, Parisa Kordjamshidi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Spatial reasoning is a fundamental aspect of human intelligence. One key
concept in spatial cognition is the Frame of Reference (FoR), which identifies
the perspective of spatial expressions. Despite its significance, FoR has
received limited attention in AI models that need spatial intelligence. There
is a lack of dedicated benchmarks and in-depth evaluation of large language
models (LLMs) in this area. To address this issue, we introduce the Frame of
Reference Evaluation in Spatial Reasoning Tasks (FoREST) benchmark, designed to
assess FoR comprehension in LLMs. We evaluate LLMs on answering questions that
require FoR comprehension and layout generation in text-to-image models using
FoREST. Our results reveal a notable performance gap across different FoR
classes in various LLMs, affecting their ability to generate accurate layouts
for text-to-image generation. This highlights critical shortcomings in FoR
comprehension. To improve FoR understanding, we propose Spatial-Guided
prompting, which improves LLMs ability to extract essential spatial concepts.
Our proposed method improves overall performance across spatial reasoning
tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Domain-Specific Translation with Open-Source Large Language Models:
  Resource-Oriented Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.05862v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.05862v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aman Kassahun Wassie, Mahdi Molaei, Yasmin Moslem
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we compare the domain-specific translation performance of
open-source autoregressive decoder-only large language models (LLMs) with
task-oriented machine translation (MT) models. Our experiments focus on the
medical domain and cover four language pairs with varied resource availability:
English-to-French, English-to-Portuguese, English-to-Swahili, and
Swahili-to-English. Despite recent advancements, LLMs exhibit a clear gap in
specialized translation quality compared to multilingual encoder-decoder MT
models such as NLLB-200. In three out of four language directions in our study,
NLLB-200 3.3B outperforms all LLMs in the size range of 8B parameters in
medical translation. While fine-tuning LLMs such as Mistral and Llama improves
their performance at medical translation, these models still fall short
compared to fine-tuned NLLB-200 3.3B models. Our findings highlight the ongoing
need for specialized MT models to achieve higher-quality domain-specific
translation, especially in medium-resource and low-resource settings. As larger
LLMs outperform their 8B variants, this also encourages pre-training
domain-specific medium-sized LMs to improve quality and efficiency in
specialized translation tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ QuantMoE-Bench: Examining Post-Training Quantization for
  Mixture-of-Experts 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.08155v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.08155v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pingzhi Li, Xiaolong Jin, Zhen Tan, Yu Cheng, Tianlong Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Mixture-of-Experts (MoE) is a promising way to scale up the learning capacity
of large language models. It increases the number of parameters while keeping
FLOPs nearly constant during inference through sparse activation. Yet, it still
suffers from significant memory overheads due to the vast parameter size,
necessitating model compression techniques. Post-training quantization offers a
powerful approach for model compression. Existing methods adopt a fixed
quantization precision for the entire MoE model. This rigid setup can lead to
suboptimal performance, without considering the inherent sparse structure. For
example, MoE's sparse routing mechanism leads to different activation patterns,
where shared experts are accessed by all tokens while token-conditioned experts
are selectively activated. This activation disparity suggests different
quantization requirements, with consistently activated shared experts
potentially needing higher precision to maintain model quality. In this paper,
we study a fine-grained precision setup for MoE quantization. We explore MoE
structure-aware quantization heuristics, ranging from coarse (e.g., MoE layers)
to fine granularity (e.g., linear layers). Our investigations reveal critical
principles, where different MoE structures require varying numbers of bits for
effective quantization. Conclusions are supported by extensive benchmarking
across two representative MoE models and six tasks including commonsense
reasoning and natural language understanding. We further show that an MoE
quantized in a fined-grained mixed precision achieved state-of-the-art 65.35%
performance on average compared to the baseline 64.30% (i.e., GPTQ). Moreover,
based on the findings, we introduce novel data-driven techniques for optimizing
bit allocation in MoE quantization, including the outlier-aware linear layer
scorer and MoE block importance predictor.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Our code for reproducing all our experiments is provided at
  https://github.com/UNITES-Lab/moe-quantization</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Accelerating Unbiased LLM Evaluation via Synthetic Feedback 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.10563v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.10563v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhaoyi Zhou, Yuda Song, Andrea Zanette
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  When developing new large language models (LLMs), a key step is evaluating
their final performance, often by computing the win-rate against a reference
model based on external feedback. Human feedback is the gold standard,
particularly for capturing nuanced qualities like coherence, readability, and
alignment with human expectations. However, human evaluations are costly --
even for large tech companies -- and when conducted with active users, they may
negatively impact user experience. A promising alternative is synthetic
feedback, where evaluations are conducted by other large language models,
including reward models. While this eliminates the need for costly human
annotations, it introduces biases that may distort the evaluation process. In
this work, we propose a statistically principled framework that integrates
human and synthetic feedback to reduce reliance on human annotations while
maintaining unbiased win-rate calculations. Our experiments demonstrate a
reduction in human annotations by up to 12.2% with an off-the-shelf synthetic
evaluator and up to 24.8% with a finetuned variant. Apart from being
generalizable, scalable, and free of hyper-parameter tuning, our method offers
predictable annotation savings, which can be estimated based on data-dependent
characteristics.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Utility-inspired Reward Transformations Improve Reinforcement Learning
  Training of Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.06248v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.06248v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Roberto-Rafael Maura-Rivero, Chirag Nagpal, Roma Patel, Francesco Visin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Current methods that train large language models (LLMs) with reinforcement
learning feedback, often resort to averaging outputs of multiple rewards
functions during training. This overlooks crucial aspects of individual reward
dimensions and inter-reward dependencies that can lead to sub-optimal outcomes
in generations. In this work, we show how linear aggregation of rewards
exhibits some vulnerabilities that can lead to undesired properties of
generated text. We then propose a transformation of reward functions inspired
by economic theory of utility functions (specifically Inada conditions), that
enhances sensitivity to low reward values while diminishing sensitivity to
already high values. We compare our approach to the existing baseline methods
that linearly aggregate rewards and show how the Inada-inspired reward feedback
is superior to traditional weighted averaging. We quantitatively and
qualitatively analyse the difference in the methods, and see that models
trained with Inada-transformations score as more helpful while being less
harmful.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Queue management for slo-oriented large language model serving 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.00047v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.00047v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Archit Patke, Dhemath Reddy, Saurabh Jha, Haoran Qiu, Christian Pinto, Chandra Narayanaswami, Zbigniew Kalbarczyk, Ravishankar Iyer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language model (LLM) serving is becoming an increasingly critical
workload for cloud providers. Existing LLM serving systems focus on interactive
requests, such as chatbots and coding assistants, with tight latency SLO
requirements. However, when such systems execute batch requests that have
relaxed SLOs along with interactive requests, it leads to poor multiplexing and
inefficient resource utilization. To address these challenges, we propose QLM,
a queue management system for LLM serving. QLM maintains batch and interactive
requests across different models and SLOs in a request queue. Optimal ordering
of the request queue is critical to maintain SLOs while ensuring high resource
utilization. To generate this optimal ordering, QLM uses a Request Waiting Time
(RWT) Estimator that estimates the waiting times for requests in the request
queue. These estimates are used by a global scheduler to orchestrate LLM
Serving Operations (LSOs) such as request pulling, request eviction, load
balancing, and model swapping. Evaluation on heterogeneous GPU devices and
models with real-world LLM serving dataset shows that QLM improves SLO
attainment by 40-90% and throughput by 20-400% while maintaining or improving
device utilization compared to other state-of-the-art LLM serving systems.
QLM's evaluation is based on the production requirements of a cloud provider.
QLM is publicly available at https://www.github.com/QLM-project/QLM.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ How Does Code <span class="highlight-title">Pretrain</span>ing Affect Language Model Task Performance? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.04556v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.04556v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jackson Petty, Sjoerd van Steenkiste, Tal Linzen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models are increasingly trained on corpora containing both
natural language and non-linguistic data like source code. Aside from aiding
programming-related tasks, anecdotal evidence suggests that including code in
pretraining corpora may improve performance on other, unrelated tasks, yet to
date no work has been able to establish a causal connection by controlling
between language and code data. Here we do just this. We pretrain language
models on datasets which interleave natural language and code in two different
settings: additive, in which the total volume of data seen during pretraining
is held constant; and competitive, in which the volume of language data is held
constant. We study how the pretraining mixture affects performance on (a) a
diverse collection of tasks included in the BigBench benchmark, and (b)
compositionality, measured by generalization accuracy on semantic parsing and
syntactic transformations. We find that pretraining on higher proportions of
code improves performance on compositional tasks involving structured output
(like semantic parsing), and mathematics. Conversely, increase code mixture can
harm performance on other tasks, including on tasks that requires sensitivity
to linguistic structure such as syntax or morphology, and tasks measuring
real-world knowledge.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Historical German Text Normalization Using Type- and Token-Based
  Language Modeling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.02841v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.02841v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anton Ehrmanntraut
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Historic variations of spelling poses a challenge for full-text search or
natural language processing on historical digitized texts. To minimize the gap
between the historic orthography and contemporary spelling, usually an
automatic orthographic normalization of the historical source material is
pursued. This report proposes a normalization system for German literary texts
from c. 1700-1900, trained on a parallel corpus. The proposed system makes use
of a machine learning approach using Transformer language models, combining an
encoder-decoder model to normalize individual word types, and a pre-trained
causal language model to adjust these normalizations within their context. An
extensive evaluation shows that the proposed system provides state-of-the-art
accuracy, comparable with a much larger fully end-to-end sentence-based
normalization system, fine-tuning a pre-trained Transformer large language
model. However, the normalization of historical text remains a challenge due to
difficulties for models to generalize, and the lack of extensive high-quality
parallel data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>27 pages, 3 figures; minor editorial changes</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Mind the Gap: Examining the Self-Improvement Capabilities of Large
  Language Models <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.02674v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.02674v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuda Song, Hanlin Zhang, Carson Eisenach, Sham Kakade, Dean Foster, Udaya Ghai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Self-improvement is a mechanism in Large Language Model (LLM) pre-training,
post-training and test-time inference. We explore a framework where the model
verifies its own outputs, filters or reweights data based on this verification,
and distills the filtered data. Despite several empirical successes, a
fundamental understanding is still lacking. In this work, we initiate a
comprehensive, modular and controlled study on LLM self-improvement. We provide
a mathematical formulation for self-improvement, which is largely governed by a
quantity which we formalize as the generation-verification gap. Through
experiments with various model families and tasks, we discover a scaling
phenomenon of self-improvement -- a variant of the generation-verification gap
scales monotonically with the model pre-training flops. We also examine when
self-improvement is possible, an iterative self-improvement procedure, and ways
to improve its performance. Our findings not only advance understanding of LLM
self-improvement with practical implications, but also open numerous avenues
for future research into its capabilities and boundaries.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2025; 41 pages, 19 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Does Table Source Matter? Benchmarking and Improving Multimodal
  Scientific Table Understanding and Reasoning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.13042v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.13042v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bohao Yang, Yingji Zhang, Dong Liu, André Freitas, Chenghua Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent large language models (LLMs) have advanced table understanding
capabilities but rely on converting tables into text sequences. While
multimodal large language models (MLLMs) enable direct visual processing, they
face limitations in handling scientific tables due to fixed input image
resolutions and insufficient numerical reasoning capabilities. We present a
comprehensive framework for multimodal scientific table understanding and
reasoning with dynamic input image resolutions. Our framework consists of three
key components: (1) MMSci-Pre, a domain-specific table structure learning
dataset of 52K scientific table structure recognition samples, (2) MMSci-Ins,
an instruction tuning dataset with 12K samples across three table-based tasks,
and (3) MMSci-Eval, a benchmark with 3,114 testing samples specifically
designed to evaluate numerical reasoning capabilities. Extensive experiments
demonstrate that our domain-specific approach with 52K scientific table images
achieves superior performance compared to 150K general-domain tables,
highlighting the importance of data quality over quantity. Our proposed
table-based MLLMs with dynamic input resolutions show significant improvements
in both general table understanding and numerical reasoning capabilities, with
strong generalisation to held-out datasets. Our code and data are publicly
available at https://github.com/Bernard-Yang/MMSci_Table.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Crafting Customisable Characters with LLMs: Introducing SimsChat, a
  Persona-Driven Role-Playing Agent Framework 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.17962v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.17962v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bohao Yang, Dong Liu, Chenghao Xiao, Kun Zhao, Chen Tang, Chao Li, Lin Yuan, Guang Yang, Lanxiao Huang, Chenghua Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) demonstrate remarkable ability to comprehend
instructions and generate human-like text, enabling sophisticated agent
simulation beyond basic behavior replication. However, the potential for
creating freely customisable characters remains underexplored. We introduce the
Customisable Conversation Agent Framework, which employs LLMs to simulate
real-world characters through personalised characteristic feature injection,
enabling diverse character creation according to user preferences. We propose
the SimsConv dataset, comprising 68 customised characters and 13,971 multi-turn
role-playing dialogues across 1,360 real-world scenes. Characters are initially
customised using pre-defined elements (career, aspiration, traits, skills),
then expanded through personal and social profiles. Building on this, we
present SimsChat, a freely customisable role-playing agent incorporating
various realistic settings and topic-specified character interactions.
Experimental results on both SimsConv and WikiRoleEval datasets demonstrate
SimsChat's superior performance in maintaining character consistency, knowledge
accuracy, and appropriate question rejection compared to existing models. Our
framework provides valuable insights for developing more accurate and
customisable human simulacra. Our data and code are publicly available at
https://github.com/Bernard-Yang/SimsChat.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Generative Psycho-Lexical Approach for Constructing Value Systems in
  Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.02444v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.02444v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haoran Ye, Tianze Zhang, Yuhang Xie, Liyuan Zhang, Yuanyi Ren, Xin Zhang, Guojie Song
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Values are core drivers of individual and collective perception, cognition,
and behavior. Value systems, such as Schwartz's Theory of Basic Human Values,
delineate the hierarchy and interplay among these values, enabling
cross-disciplinary investigations into decision-making and societal dynamics.
Recently, the rise of Large Language Models (LLMs) has raised concerns
regarding their elusive intrinsic values. Despite growing efforts in
evaluating, understanding, and aligning LLM values, a psychologically grounded
LLM value system remains underexplored. This study addresses the gap by
introducing the Generative Psycho-Lexical Approach (GPLA), a scalable,
adaptable, and theoretically informed method for constructing value systems.
Leveraging GPLA, we propose a psychologically grounded five-factor value system
tailored for LLMs. For systematic validation, we present three benchmarking
tasks that integrate psychological principles with cutting-edge AI priorities.
Our results reveal that the proposed value system meets standard psychological
criteria, better captures LLM values, improves LLM safety prediction, and
enhances LLM alignment, when compared to the canonical Schwartz's values.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LLM-based MOFs Synthesis Condition Extraction using Few-Shot
  Demonstrations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.04665v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.04665v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lei Shi, Zhimeng Liu, Yi Yang, Weize Wu, Yuyang Zhang, Hongbo Zhang, Jing Lin, Siyu Wu, Zihan Chen, Ruiming Li, Nan Wang, Zipeng Liu, Huobin Tan, Hongyi Gao, Yue Zhang, Ge Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The extraction of Metal-Organic Frameworks (MOFs) synthesis route from
literature has been crucial for the logical MOFs design with desirable
functionality. The recent advent of large language models (LLMs) provides
disruptively new solution to this long-standing problem. While the latest
researches mostly stick to primitive zero-shot LLMs lacking specialized
material knowledge, we introduce in this work the few-shot LLM in-context
learning paradigm. First, a human-AI interactive data curation approach is
proposed to secure high-quality demonstrations. Second, an information
retrieval algorithm is applied to pick and quantify few-shot demonstrations for
each extraction. Over three datasets randomly sampled from nearly 90,000
well-defined MOFs, we conduct triple evaluations to validate our method. The
synthesis extraction, structure inference, and material design performance of
the proposed few-shot LLMs all significantly outplay zero-shot LLM and baseline
methods. The lab-synthesized material guided by LLM surpasses 91.1%
high-quality MOFs of the same class reported in the literature, on the key
physical property of specific surface area.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Coherence-Driven Multimodal Safety Dialogue with Active Learning for
  Embodied Agents <span class="chip">AAMAS</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14141v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14141v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sabit Hassan, Hye-Young Chung, Xiang Zhi Tan, Malihe Alikhani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  When assisting people in daily tasks, robots need to accurately interpret
visual cues and respond effectively in diverse safety-critical situations, such
as sharp objects on the floor. In this context, we present M-CoDAL, a
multimodal-dialogue system specifically designed for embodied agents to better
understand and communicate in safety-critical situations. The system leverages
discourse coherence relations to enhance its contextual understanding and
communication abilities. To train this system, we introduce a novel
clustering-based active learning mechanism that utilizes an external Large
Language Model (LLM) to identify informative instances. Our approach is
evaluated using a newly created multimodal dataset comprising 1K safety
violations extracted from 2K Reddit images. These violations are annotated
using a Large Multimodal Model (LMM) and verified by human annotators. Results
with this dataset demonstrate that our approach improves resolution of safety
situations, user sentiment, as well as safety of the conversation. Next, we
deploy our dialogue system on a Hello Robot Stretch robot and conduct a
within-subject user study with real-world participants. In the study,
participants role-play two safety scenarios with different levels of severity
with the robot and receive interventions from our model and a baseline system
powered by OpenAI's ChatGPT. The study results corroborate and extend the
findings from the automated evaluation, showing that our proposed system is
more persuasive in a real-world embodied agent setting.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear at AAMAS, 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Chat Bankman-Fried: an Exploration of LLM Alignment in Finance 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.11853v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.11853v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Claudia Biancotti, Carolina Camassa, Andrea Coletta, Oliver Giudice, Aldo Glielmo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Advancements in large language models (LLMs) have renewed concerns about AI
alignment - the consistency between human and AI goals and values. As various
jurisdictions enact legislation on AI safety, the concept of alignment must be
defined and measured across different domains. This paper proposes an
experimental framework to assess whether LLMs adhere to ethical and legal
standards in the relatively unexplored context of finance. We prompt twelve
LLMs to impersonate the CEO of a financial institution and test their
willingness to misuse customer assets to repay outstanding corporate debt.
Beginning with a baseline configuration, we adjust preferences, incentives and
constraints, analyzing the impact of each adjustment with logistic regression.
Our findings reveal significant heterogeneity in the baseline propensity for
unethical behavior of LLMs. Factors such as risk aversion, profit expectations,
and regulatory environment consistently influence misalignment in ways
predicted by economic theory, although the magnitude of these effects varies
across LLMs. This paper highlights both the benefits and limitations of
simulation-based, ex post safety testing. While it can inform financial
authorities and institutions aiming to ensure LLM safety, there is a clear
trade-off between generality and cost.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Topic-FlipRAG: Topic-Orientated Adversarial Opinion Manipulation Attacks
  to Retrieval-Augmented Generation Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.01386v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.01386v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuyang Gong, Zhuo Chen, Miaokun Chen, Fengchang Yu, Wei Lu, Xiaofeng Wang, Xiaozhong Liu, Jiawei Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Retrieval-Augmented Generation (RAG) systems based on Large Language Models
(LLMs) have become essential for tasks such as question answering and content
generation. However, their increasing impact on public opinion and information
dissemination has made them a critical focus for security research due to
inherent vulnerabilities. Previous studies have predominantly addressed attacks
targeting factual or single-query manipulations. In this paper, we address a
more practical scenario: topic-oriented adversarial opinion manipulation
attacks on RAG models, where LLMs are required to reason and synthesize
multiple perspectives, rendering them particularly susceptible to systematic
knowledge poisoning. Specifically, we propose Topic-FlipRAG, a two-stage
manipulation attack pipeline that strategically crafts adversarial
perturbations to influence opinions across related queries. This approach
combines traditional adversarial ranking attack techniques and leverages the
extensive internal relevant knowledge and reasoning capabilities of LLMs to
execute semantic-level perturbations. Experiments show that the proposed
attacks effectively shift the opinion of the model's outputs on specific
topics, significantly impacting user information perception. Current mitigation
methods cannot effectively defend against such attacks, highlighting the
necessity for enhanced safeguards for RAG systems, and offering crucial
insights for LLM security research.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Implicit In-context Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.14660v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.14660v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhuowei Li, Zihao Xu, Ligong Han, Yunhe Gao, Song Wen, Di Liu, Hao Wang, Dimitris N. Metaxas
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In-context Learning (ICL) empowers large language models (LLMs) to swiftly
adapt to unseen tasks at inference-time by prefixing a few demonstration
examples before queries. Despite its versatility, ICL incurs substantial
computational and memory overheads compared to zero-shot learning and is
sensitive to the selection and order of demonstration examples. In this work,
we introduce Implicit In-context Learning (I2CL), an innovative paradigm that
reduces the inference cost of ICL to that of zero-shot learning with minimal
information loss. I2CL operates by first generating a condensed vector
representation, namely a context vector, extracted from the demonstration
examples. It then conducts an inference-time intervention through injecting a
linear combination of the context vector and query activations back into the
model's residual streams. Empirical evaluation on nine real-world tasks across
three model architectures demonstrates that I2CL achieves few-shot level
performance at zero-shot inference cost, and it exhibits robustness against
variations in demonstration examples. Furthermore, I2CL facilitates a novel
representation of task-ids, enhancing task similarity detection and fostering
effective transfer learning. We also perform a comprehensive analysis and
ablation study on I2CL, offering deeper insights into its internal mechanisms.
Code is available at https://github.com/LzVv123456/I2CL.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Enhancing LLMs for Identifying and Prioritizing Important Medical
  Jargons from Electronic Health Record Notes Utilizing Data Augmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.16022v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.16022v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Won Seok Jang, Sharmin Sultana, Zonghai Yao, Hieu Tran, Zhichao Yang, Sunjae Kwon, Hong Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  OpenNotes enables patients to access EHR notes, but medical jargon can hinder
comprehension. To improve understanding, we evaluated closed- and open-source
LLMs for extracting and prioritizing key medical terms using prompting,
fine-tuning, and data augmentation. We assessed LLMs on 106 expert-annotated
EHR notes, experimenting with (i) general vs. structured prompts, (ii)
zero-shot vs. few-shot prompting, (iii) fine-tuning, and (iv) data
augmentation. To enhance open-source models in low-resource settings, we used
ChatGPT for data augmentation and applied ranking techniques. We incrementally
increased the augmented dataset size (10 to 10,000) and conducted 5-fold
cross-validation, reporting F1 score and Mean Reciprocal Rank (MRR). Our result
show that fine-tuning and data augmentation improved performance over other
strategies. GPT-4 Turbo achieved the highest F1 (0.433), while Mistral7B with
data augmentation had the highest MRR (0.746). Open-source models, when
fine-tuned or augmented, outperformed closed-source models. Notably, the best
F1 and MRR scores did not always align. Few-shot prompting outperformed
zero-shot in vanilla models, and structured prompts yielded different
preferences across models. Fine-tuning improved zero-shot performance but
sometimes degraded few-shot performance. Data augmentation performed comparably
or better than other methods. Our evaluation highlights the effectiveness of
prompting, fine-tuning, and data augmentation in improving model performance
for medical jargon extraction in low-resource scenarios.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>21pages, 5 figures, 4 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Mechanism and Emergence of Stacked Attention Heads in Multi-Layer
  <span class="highlight-title">Transformer</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.12118v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.12118v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tiberiu Musat
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, I introduce the retrieval problem, a simple yet common
reasoning task that can be solved only by transformers with a minimum number of
layers, which grows logarithmically with the input size. I empirically show
that large language models can solve the task under different prompting
formulations without any fine-tuning. To understand how transformers solve the
retrieval problem, I train several transformers on a minimal formulation.
Successful learning occurs only under the presence of an implicit curriculum. I
uncover the learned mechanisms by studying the attention maps in the trained
transformers. I also study the training process, uncovering that attention
heads always emerge in a specific sequence guided by the implicit curriculum.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Small Language Models: <span class="highlight-title">Survey</span>, Measurements, and Insights 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.15790v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.15790v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhenyan Lu, Xiang Li, Dongqi Cai, Rongjie Yi, Fangming Liu, Xiwen Zhang, Nicholas D. Lane, Mengwei Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Small language models (SLMs), despite their widespread adoption in modern
smart devices, have received significantly less academic attention compared to
their large language model (LLM) counterparts, which are predominantly deployed
in data centers and cloud environments. While researchers continue to improve
the capabilities of LLMs in the pursuit of artificial general intelligence, SLM
research aims to make machine intelligence more accessible, affordable, and
efficient for everyday tasks. Focusing on transformer-based, decoder-only
language models with 100M-5B parameters, we survey 70 state-of-the-art
open-source SLMs, analyzing their technical innovations across three axes:
architectures, training datasets, and training algorithms. In addition, we
evaluate their capabilities in various domains, including commonsense
reasoning, in-context learning, mathematics, and coding. To gain further
insight into their on-device runtime costs, we benchmark their inference
latency and memory footprints. Through in-depth analysis of our benchmarking
data, we offer valuable insights to advance research in this field.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ From Text to Emoji: How PEFT-Driven Personality Manipulation Unleashes
  the Emoji Potential in LLMs <span class="chip">NAACL 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.10245v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.10245v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Navya Jain, Zekun Wu, Cristian Munoz, Airlie Hilliard, Xin Guan, Adriano Koshiyama, Emre Kazim, Philip Treleaven
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The manipulation of the personality traits of large language models (LLMs)
has emerged as a key area of research. Methods like prompt-based In-Context
Knowledge Editing (IKE) and gradient-based Model Editor Networks (MEND) have
been explored but show irregularity and variability; IKE depends on the prompt,
leading to variability and sensitivity, while MEND yields inconsistent and
gibberish outputs. To address this, we employed Opinion QA Based
Parameter-Efficient Fine-Tuning (PEFT), specifically Quantized Low-Rank
Adaptation (QLoRA), to manipulate the Big Five personality traits: Openness,
Conscientiousness, Extraversion, Agreeableness, and Neuroticism. After PEFT,
models such as Mistral-7B-Instruct and LLaMA-2-7B-chat showed a latent
behaviour by generating emojis for certain traits, despite no emojis being
present in the PEFT data. For instance, LLaMA-2-7B-chat generated emojis in
99.5\% of extraversion-related test instances, while Mistral-7B-Instruct did so
in 92.5\% of openness-related test instances. ICL Explainability analysis
indicated that the LLMs used emojis intentionally to express these traits.
Mechanistic Interpretability analysis showed that this latent behaviour of LLMs
could be traced to specific neurons that became activated or amplified after
PEFT. This paper provides a number of novel contributions. First, introducing
an Opinion QA dataset for PEFT-driven personality manipulation; second,
developing metric models to benchmark LLM personality traits; third,
demonstrating PEFT's superiority over IKE in personality manipulation; and
finally, analysing and validating emoji usage through explainability methods
such as Mechanistic Interpretability and In-context learning Explainability
methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Findings paper of NAACL 2025 and NeurIPS 2024 Workshop on Behavioral
  Machine Learning</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Collaboration of Large Language Models and Small Recommendation Models
  for Device-Cloud Recommendation <span class="chip">KDD'25</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.05647v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.05647v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zheqi Lv, Tianyu Zhan, Wenjie Wang, Xinyu Lin, Shengyu Zhang, Wenqiao Zhang, Jiwei Li, Kun Kuang, Fei Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) for Recommendation (LLM4Rec) is a promising
research direction that has demonstrated exceptional performance in this field.
However, its inability to capture real-time user preferences greatly limits the
practical application of LLM4Rec because (i) LLMs are costly to train and infer
frequently, and (ii) LLMs struggle to access real-time data (its large number
of parameters poses an obstacle to deployment on devices). Fortunately, small
recommendation models (SRMs) can effectively supplement these shortcomings of
LLM4Rec diagrams by consuming minimal resources for frequent training and
inference, and by conveniently accessing real-time data on devices.
  In light of this, we designed the Device-Cloud LLM-SRM Collaborative
Recommendation Framework (LSC4Rec) under a device-cloud collaboration setting.
LSC4Rec aims to integrate the advantages of both LLMs and SRMs, as well as the
benefits of cloud and edge computing, achieving a complementary synergy. We
enhance the practicability of LSC4Rec by designing three strategies:
collaborative training, collaborative inference, and intelligent request.
During training, LLM generates candidate lists to enhance the ranking ability
of SRM in collaborative scenarios and enables SRM to update adaptively to
capture real-time user interests. During inference, LLM and SRM are deployed on
the cloud and on the device, respectively. LLM generates candidate lists and
initial ranking results based on user behavior, and SRM get reranking results
based on the candidate list, with final results integrating both LLM's and
SRM's scores. The device determines whether a new candidate list is needed by
comparing the consistency of the LLM's and SRM's sorted lists. Our
comprehensive and extensive experimental analysis validates the effectiveness
of each strategy in LSC4Rec.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published on KDD'25: Proceedings of the ACM SIGKDD Conference on
  Knowledge Discovery and Data Mining 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Guiding Through Complexity: What Makes Good Supervision for Hard Math
  Reasoning Tasks? <span class="chip">NAACL 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.20533v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.20533v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xuan He, Da Yin, Nanyun Peng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  How can "weak teacher models" such as average human annotators or existing AI
systems, effectively supervise LLMs to improve performance on hard reasoning
tasks, especially those that challenge and requires expertise or daily practice
from the teacher models? In this paper, we seek for empirical answers to this
question by investigating various data-driven strategies that offer supervision
data at different quality levels upon tasks of varying complexity. Two
intuitive strategies emerge for teacher models to provide supervision during
alignment training: 1) using lower-quality supervision from complete tasks that
match the difficulty of the target reasoning tasks, and 2) leveraging
higher-quality supervision from easier subtasks that are less challenging.
Interestingly, we find that even when the outcome error rate for hard task
supervision is high (e.g., 90\%), training on such data can outperform
perfectly correct supervision of easier subtasks on multiple hard math
benchmarks. We further identify a more critical factor influencing training
performance: step-wise error rates, which indicate the severity of errors in
solutions. Specifically, training on hard task supervision with the same
outcome error rates but disparate step-wise error rates can lead to a 30\%
accuracy gap on MATH benchmark. Our results also reveal that supplementing hard
task supervision with the corresponding subtask supervision can yield notable
performance improvements than simply combining rephrased hard full task
supervision, suggesting new avenues for data augmentation. Data and code are
released at https://github.com/hexuan21/Weak-to-Strong.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NAACL 2025 Main</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ NYT-Connections: A Deceptively Simple Text Classification Task that
  Stumps System-1 Thinkers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.01621v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.01621v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Angel Yahir Loredo Lopez, Tyler McDonald, Ali Emami
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have shown impressive performance on various
benchmarks, yet their ability to engage in deliberate reasoning remains
questionable. We present NYT-Connections, a collection of 358 simple word
classification puzzles derived from the New York Times Connections game. This
benchmark is designed to penalize quick, intuitive "System 1" thinking,
isolating fundamental reasoning skills. We evaluated six recent LLMs, a simple
machine learning heuristic, and humans across three configurations:
single-attempt, multiple attempts without hints, and multiple attempts with
contextual hints. Our findings reveal a significant performance gap: even
top-performing LLMs like GPT-4 fall short of human performance by nearly 30%.
Notably, advanced prompting techniques such as Chain-of-Thought and
Self-Consistency show diminishing returns as task difficulty increases.
NYT-Connections uniquely combines linguistic isolation, resistance to intuitive
shortcuts, and regular updates to mitigate data leakage, offering a novel tool
for assessing LLM reasoning capabilities.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages (excluding references), Published at Coling 2025, Best
  Dataset Paper Award</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Determine-Then-Ensemble: Necessity of Top-k Union for Large Language
  Model Ensembling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.03777v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.03777v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuxuan Yao, Han Wu, Mingyang Liu, Sichun Luo, Xiongwei Han, Jie Liu, Zhijiang Guo, Linqi Song
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) exhibit varying strengths and weaknesses across
different tasks, prompting recent studies to explore the benefits of ensembling
models to leverage their complementary advantages. However, existing LLM
ensembling methods often overlook model compatibility and struggle with
inefficient alignment of probabilities across the entire vocabulary. In this
study, we empirically investigate the factors influencing ensemble performance,
identifying model performance, vocabulary size, and response style as key
determinants, revealing that compatibility among models is essential for
effective ensembling. This analysis leads to the development of a simple yet
effective model selection strategy that identifies compatible models.
Additionally, we introduce the \textsc{Uni}on \textsc{T}op-$k$
\textsc{E}nsembling (\textsc{UniTE}), a novel approach that efficiently
combines models by focusing on the union of the top-k tokens from each model,
thereby avoiding the need for full vocabulary alignment and reducing
computational overhead. Extensive evaluations across multiple benchmarks
demonstrate that \textsc{UniTE} significantly enhances performance compared to
existing methods, offering a more efficient framework for LLM ensembling.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MEMERAG: A Multilingual End-to-End Meta-Evaluation Benchmark for
  Retrieval Augmented Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.17163v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.17163v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        María Andrea Cruz Blandón, Jayasimha Talur, Bruno Charron, Dong Liu, Saab Mansour, Marcello Federico
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Automatic evaluation of retrieval augmented generation (RAG) systems relies
on fine-grained dimensions like faithfulness and relevance, as judged by expert
human annotators. Meta-evaluation benchmarks support the development of
automatic evaluators that correlate well with human judgement. However,
existing benchmarks predominantly focus on English or use translated data,
which fails to capture cultural nuances. A native approach provides a better
representation of the end user experience.
  In this work, we develop a Multilingual End-to-end Meta-Evaluation RAG
benchmark (MEMERAG). Our benchmark builds on the popular MIRACL dataset, using
native-language questions and generating responses with diverse large language
models (LLMs), which are then assessed by expert annotators for faithfulness
and relevance. We describe our annotation process and show that it achieves
high inter-annotator agreement. We then analyse the performance of the
answer-generating LLMs across languages as per the human evaluators. Finally we
apply the dataset to our main use-case which is to benchmark multilingual
automatic evaluators (LLM-as-a-judge). We show that our benchmark can reliably
identify improvements offered by advanced prompting techniques and LLMs. We
will release our benchmark to support the community developing accurate
evaluation methods for multilingual RAG systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CheXalign: Preference fine-tuning in chest X-ray interpretation models
  without human feedback 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.07025v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.07025v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dennis Hein, Zhihong Chen, Sophie Ostmeier, Justin Xu, Maya Varma, Eduardo Pontes Reis, Arne Edward Michalson, Christian Bluethgen, Hyun Joo Shin, Curtis Langlotz, Akshay S Chaudhari
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Radiologists play a crucial role in translating medical images into
actionable reports. However, the field faces staffing shortages and increasing
workloads. While automated approaches using vision-language models (VLMs) show
promise as assistants, they require exceptionally high accuracy. Most current
VLMs in radiology rely solely on supervised fine-tuning. Meanwhile, additional
preference fine-tuning in the post-training pipeline has become standard
practice in the general domain. The challenge in radiology lies in the
prohibitive cost of obtaining radiologist feedback at scale. To address this
challenge, we propose an automated pipeline for preference feedback, focusing
on chest X-ray radiology report generation (RRG). Specifically, our method
leverages publicly available datasets containing pairs of images and
radiologist-written reference reports with reference-based metrics, or Judges,
eliminating the need for additional radiologist feedback. We investigate reward
overoptimization via length exploitation in this setting and introduce a
length-controlled version of the GREEN score. Our best-performing setup
achieves state-of-the-art CheXbert scores on the MIMIC-CXR dataset for the RRG
task while on average maintaining robust performance across six additional
image perception and reasoning tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Dynamic Attention-Guided Context Decoding for Mitigating Context
  Faithfulness Hallucinations in Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.01059v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.01059v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yanwen Huang, Yong Zhang, Ning Cheng, Zhitao Li, Shaojun Wang, Jing Xiao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) often exhibit Context Faithfulness
Hallucinations, where outputs deviate from retrieved information due to
incomplete context integration. Our analysis reveals a strong correlation
between token-level uncertainty and hallucinations. We hypothesize that
attention mechanisms inherently encode context utilization signals, supported
by probing analysis. Based on these insights, we propose Dynamic
Attention-Guided Context Decoding (DAGCD), a lightweight framework that
leverages attention distributions and uncertainty signals in a single-pass
decoding. Experiments on open-book QA datasets demonstrate DAGCD's
effectiveness, yielding significant improvements in faithfulness and robustness
while preserving computational efficiency.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Efficient Safety Retrofitting Against Jailbreaking for LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.13603v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.13603v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dario Garcia-Gasulla, Adrian Tormos, Anna Arias-Duart, Daniel Hinjos, Oscar Molina-Sedano, Ashwin Kumar Gururajan, Maria Eugenia Cardello
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Direct Preference Optimization (DPO) is an efficient alignment technique that
steers LLMs towards preferable outputs by training on preference data,
bypassing the need for explicit reward models. Its simplicity enables easy
adaptation to various domains and safety requirements. This paper examines
DPO's effectiveness in model safety against jailbreaking attacks while
minimizing data requirements and training costs. We introduce Egida, a dataset
expanded from multiple sources, which includes 27 different safety topics and
18 different attack styles, complemented with synthetic and human labels. This
data is used to boost the safety of state-of-the-art LLMs
(Llama-3.1-8B/70B-Instruct, Qwen-2.5-7B/72B-Instruct) across topics and attack
styles. In addition to safety evaluations, we assess their post-alignment
performance degradation in general purpose tasks, and their tendency to over
refusal. Following the proposed methodology, trained models reduce their Attack
Success Rate by 10%-30%, using small training efforts (2,000 samples) with low
computational cost (3\$ for 8B models, 20\$ for 72B models). Safety aligned
models generalize to unseen topics and attack styles, with the most successful
attack style reaching a success rate around 5%. Size and family are found to
strongly influence model malleability towards safety, pointing at the
importance of pre-training choices. To validate our findings, a large
independent assessment of human preference agreement with Llama-Guard-3-8B is
conducted by the authors and the associated dataset Egida-HSafe is released.
Overall, this study illustrates how affordable and accessible it is to enhance
LLM safety using DPO while outlining its current limitations. All datasets and
models are released to enable reproducibility and further research.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Rethinking Layer Removal: A Hybrid Pruning Framework Combining Layer
  Removal and Singular Value Selection for Efficient LLM Compression 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.00339v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.00339v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kainan Liu, Yong Zhang, Ning Cheng, Zhitao Li, Shaojun Wang, Jing Xiao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Layer removal is an effective technique for compressing large language models
(LLMs) by reducing redundancy and improving inference efficiency. However,
indiscriminate pruning disrupts representation stability, leading to
performance degradation. We propose GRASP (Gradient-based Retention of Adaptive
Singular Parameters), which preserves representation-critical singular values
to mitigate these effects. Unlike direct layer removal, GRASP leverages
gradient-based attribution on a syntax- and semantics-rich dataset to guide the
selection of representation-critical singular values. By selectively applying
singular value decomposition (SVD) to affected layers, GRASP achieves efficient
compression while maintaining representation stability with minimal overhead.
Experiments across multiple LLMs show that GRASP consistently outperforms
existing compression methods in perplexity and downstream task performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Automatic Design of Semantic Similarity Ensembles Using Grammatical
  Evolution 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.00925v7">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.00925v7.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jorge Martinez-Gil
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Semantic similarity measures are widely used in natural language processing
to catalyze various computer-related tasks. However, no single semantic
similarity measure is the most appropriate for all tasks, and researchers often
use ensemble strategies to ensure performance. This research work proposes a
method for automatically designing semantic similarity ensembles. In fact, our
proposed method uses grammatical evolution, for the first time, to
automatically select and aggregate measures from a pool of candidates to create
an ensemble that maximizes correlation to human judgment. The method is
evaluated on several benchmark datasets and compared to state-of-the-art
ensembles, showing that it can significantly improve similarity assessment
accuracy and outperform existing methods in some cases. As a result, our
research demonstrates the potential of using grammatical evolution to
automatically compare text and prove the benefits of using ensembles for
semantic similarity tasks. The source code that illustrates our approach can be
downloaded from https://github.com/jorge-martinez-gil/sesige.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>35 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ OpenThai<span class="highlight-title">GPT</span> 1.5: A Thai-Centric Open Source Large Language Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07238v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07238v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sumeth Yuenyong, Kobkrit Viriyayudhakorn, Apivadee Piyatumrong, Jillaphat Jaroenkantasima
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  OpenThaiGPT 1.5 is an advanced Thai language chat model based on Qwen v2.5,
finetuned on over 2,000,000 Thai instruction pairs. This report provides an
engineering perspective on the model's development, capabilities, and
performance. We discuss the model's architecture, training process, and key
features, including multi-turn conversation support, Retrieval Augmented
Generation (RAG) compatibility, and tool-calling functionality. Benchmark
results demonstrate OpenThaiGPT 1.5's state-of-the-art performance on various
Thai language tasks, outperforming other open-source Thai language models. We
also address practical considerations such as GPU memory requirements and
deployment strategies.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 4 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ xFinder: Large Language Models as Automated Evaluators for Reliable
  Evaluation <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.11874v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.11874v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qingchen Yu, Zifan Zheng, Shichao Song, Zhiyu Li, Feiyu Xiong, Bo Tang, Ding Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The continuous advancement of large language models (LLMs) has brought
increasing attention to the critical issue of developing fair and reliable
methods for evaluating their performance. Particularly, the emergence of
cheating phenomena, such as test set leakage and prompt format overfitting,
poses significant challenges to the reliable evaluation of LLMs. As evaluation
frameworks commonly use Regular Expression (RegEx) for answer extraction,
models may adjust their responses to fit formats easily handled by RegEx.
Nevertheless, the key answer extraction module based on RegEx frequently
suffers from extraction errors. Furthermore, recent studies proposing
fine-tuned LLMs as judge models for automated evaluation face challenges in
terms of generalization ability and fairness. This paper comprehensively
analyzes the entire LLM evaluation chain and demonstrates that optimizing the
key answer extraction module improves extraction accuracy and enhances
evaluation reliability. Our findings suggest that improving the key answer
extraction module can lead to higher judgment accuracy and improved evaluation
efficiency compared to the judge models. To address these issues, we propose
xFinder, a novel evaluator for answer extraction and matching in LLM
evaluation. As part of this process, we create a specialized dataset, the
\textbf{K}ey \textbf{A}nswer \textbf{F}inder (KAF) dataset, to ensure effective
model training and evaluation. Generalization tests and real-world evaluations
show that the smallest xFinder model, with only 500 million parameters,
achieves an average extraction accuracy of 93.42\%. In contrast, RegEx accuracy
in the best evaluation framework is 74.38\%. The final judgment accuracy of
xFinder reaches 97.61\%, outperforming existing evaluation frameworks and judge
models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICLR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Adaptive Segment-level Reward: Bridging the Gap Between Action and
  Reward Space in Alignment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.00809v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.00809v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yanshi Li, Shaopan Xiong, Gengru Chen, Xiaoyang Li, Yijia Luo, Xingyuan Bu, Yingshui Tan, Wenbo Su, Bo Zheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reinforcement Learning (RL) has proven highly effective in aligning Large
Language Models (LLMs) with human preferences. Typical RL methods optimize
under an overall sequence reward, which can lead to a suboptimal learning
process. This reflects a key credit assignment problem: identifying which
tokens to reinforce or suppress. To rectify these shortcomings, step-wise and
token-wise methods have been proposed. However, step-wise methods rely on
punctuation segmentation and still cannot accurately identify the key tokens.
The token-level approach is too fine-grained, attending to many unimportant
tokens and thus introducing a large amount of noise. To assign more accurate
rewards to different tokens, improving credit assignment, we propose the
"Adaptive Segment-wise Reward" method. We employ semantic meaning, rather than
punctuation, to adaptively delineate segments. Experiments demonstrate that our
method can be integrated into various training methods. Compared to training
methods \textit{without} our approach, our method improves the success rate on
adversarial samples by 10\%, and achieves a 1.3\% improvement on evaluation
benchmarks such as MMLU, GSM8K, HumanEval, etc.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ How Much Knowledge Can You Pack into a LoRA Adapter without Harming LLM? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.14502v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.14502v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sergey Pletenev, Maria Marina, Daniil Moskovskiy, Vasily Konovalov, Pavel Braslavski, Alexander Panchenko, Mikhail Salnikov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The performance of Large Language Models (LLMs) on many tasks is greatly
limited by the knowledge learned during pre-training and stored in the model's
parameters. Low-rank adaptation (LoRA) is a popular and efficient training
technique for updating or domain-specific adaptation of LLMs. In this study, we
investigate how new facts can be incorporated into the LLM using LoRA without
compromising the previously learned knowledge. We fine-tuned
Llama-3.1-8B-instruct using LoRA with varying amounts of new knowledge. Our
experiments have shown that the best results are obtained when the training
data contains a mixture of known and new facts. However, this approach is still
potentially harmful because the model's performance on external
question-answering benchmarks declines after such fine-tuning. When the
training data is biased towards certain entities, the model tends to regress to
few overrepresented answers. In addition, we found that the model becomes more
confident and refuses to provide an answer in only few cases. These findings
highlight the potential pitfalls of LoRA-based LLM updates and underscore the
importance of training data composition and tuning parameters to balance new
knowledge integration and general model capabilities.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ How Good Are LLMs for Literary Translation, Really? Literary Translation
  Evaluation with Humans and LLMs <span class="chip">NAACL</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.18697v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.18697v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ran Zhang, Wei Zhao, Steffen Eger
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent research has focused on literary machine translation (MT) as a new
challenge in MT. However, the evaluation of literary MT remains an open
problem. We contribute to this ongoing discussion by introducing
LITEVAL-CORPUS, a paragraph-level parallel corpus containing verified human
translations and outputs from 9 MT systems, which totals over 2k translations
and 13k evaluated sentences across four language pairs, costing 4.5k C. This
corpus enables us to (i) examine the consistency and adequacy of human
evaluation schemes with various degrees of complexity, (ii) compare evaluations
by students and professionals, assess the effectiveness of (iii) LLM-based
metrics and (iv) LLMs themselves. Our findings indicate that the adequacy of
human evaluation is controlled by two factors: the complexity of the evaluation
scheme (more complex is less adequate) and the expertise of evaluators (higher
expertise yields more adequate evaluations). For instance, MQM
(Multidimensional Quality Metrics), a complex scheme and the de facto standard
for non-literary human MT evaluation, is largely inadequate for literary
translation evaluation: with student evaluators, nearly 60% of human
translations are misjudged as indistinguishable or inferior to machine
translations. In contrast, BWS (BEST-WORST SCALING), a much simpler scheme,
identifies human translations at a rate of 80-100%. Automatic metrics fare
dramatically worse, with rates of at most 20%. Our overall evaluation indicates
that published human translations consistently outperform LLM translations,
where even the most recent LLMs tend to produce considerably more literal and
less diverse translations compared to humans.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NAACL Camera-Ready version</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learn Your Reference Model for Real Good Alignment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.09656v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.09656v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alexey Gorbatovski, Boris Shaposhnikov, Alexey Malakhov, Nikita Surnachev, Yaroslav Aksenov, Ian Maksimov, Nikita Balagansky, Daniil Gavrilov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite the fact that offline methods for Large Language Models (LLMs)
alignment do not require a direct reward model, they remain susceptible to
overoptimization. This issue arises when the trained model deviates excessively
from the reference policy, leading to a decrease in sample quality. We propose
a new paradigm of offline alignment methods, called Trust Region (including
variants TR-DPO, TR-IPO, TR-KTO), which dynamically updates the reference
policy throughout the training process. Our results show that TR alignment
methods effectively mitigate overoptimization, enabling models to maintain
strong performance even when substantially deviating from the initial reference
policy. We demonstrate the efficacy of these approaches not only through toy
examples that exhibit reduced overoptimization, but also through direct,
side-by-side comparisons in specific tasks such as helpful and harmless
dialogue, as well as summarization, where they surpass conventional methods.
Additionally, we report significant improvements in general-purpose assistant
setups with the Llama3 model on the AlpacaEval 2 and Arena-Hard benchmarks,
highlighting the advantages of Trust Region methods over classical approaches.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Linguistically Grounded Analysis of Language Models using Shapley Head
  Values 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13396v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13396v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Marcell Fekete, Johannes Bjerva
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Understanding how linguistic knowledge is encoded in language models is
crucial for improving their generalisation capabilities. In this paper, we
investigate the processing of morphosyntactic phenomena, by leveraging a
recently proposed method for probing language models via Shapley Head Values
(SHVs). Using the English language BLiMP dataset, we test our approach on two
widely used models, BERT and RoBERTa, and compare how linguistic constructions
such as anaphor agreement and filler-gap dependencies are handled. Through
quantitative pruning and qualitative clustering analysis, we demonstrate that
attention heads responsible for processing related linguistic phenomena cluster
together. Our results show that SHV-based attributions reveal distinct patterns
across both models, providing insights into how language models organize and
process linguistic information. These findings support the hypothesis that
language models learn subnetworks corresponding to linguistic theory, with
potential implications for cross-linguistic model analysis and interpretability
in Natural Language Processing (NLP).
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Are Chatbots Reliable Text Annotators? Sometimes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.05769v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.05769v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ross Deans Kristensen-McLachlan, Miceal Canavan, Márton Kardos, Mia Jacobsen, Lene Aarøe
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent research highlights the significant potential of ChatGPT for text
annotation in social science research. However, ChatGPT is a closed-source
product which has major drawbacks with regards to transparency,
reproducibility, cost, and data protection. Recent advances in open-source (OS)
large language models (LLMs) offer an alternative without these drawbacks.
Thus, it is important to evaluate the performance of OS LLMs relative to
ChatGPT and standard approaches to supervised machine learning classification.
We conduct a systematic comparative evaluation of the performance of a range of
OS LLMs alongside ChatGPT, using both zero- and few-shot learning as well as
generic and custom prompts, with results compared to supervised classification
models. Using a new dataset of tweets from US news media, and focusing on
simple binary text annotation tasks, we find significant variation in the
performance of ChatGPT and OS models across the tasks, and that the supervised
classifier using DistilBERT generally outperforms both. Given the unreliable
performance of ChatGPT and the significant challenges it poses to Open Science
we advise caution when using ChatGPT for substantive text annotation tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for publication in PNAS Nexus (accepted Feb. 2025)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Enhancing Text-to-SQL Capabilities of Large Language Models via Domain
  Database Knowledge Injection <span class="chip">ECAI 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.15907v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.15907v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xingyu Ma, Xin Tian, Lingxiang Wu, Xuepeng Wang, Xueming Tang, Jinqiao Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Text-to-SQL is a subtask in semantic parsing that has seen rapid progress
with the evolution of Large Language Models (LLMs). However, LLMs face
challenges due to hallucination issues and a lack of domain-specific database
knowledge(such as table schema and cell values). As a result, they can make
errors in generating table names, columns, and matching values to the correct
columns in SQL statements. This paper introduces a method of knowledge
injection to enhance LLMs' ability to understand schema contents by
incorporating prior knowledge. This approach improves their performance in
Text-to-SQL tasks. Experimental results show that pre-training LLMs on
domain-specific database knowledge and fine-tuning them on downstream
Text-to-SQL tasks significantly improves the Execution Match (EX) and Exact
Match (EM) metrics across various models. This effectively reduces errors in
generating column names and matching values to the columns. Furthermore, the
knowledge-injected models can be applied to many downstream Text-to-SQL tasks,
demonstrating the generalizability of the approach presented in this paper.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper has been accepted by ECAI 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SOTOPIA-Ω: Dynamic Strategy Injection Learning and Social
  Instruction Following Evaluation for Social Agents 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.15538v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.15538v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenyuan Zhang, Tianyun Liu, Mengxiao Song, Xiaodong Li, Tingwen Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite the abundance of prior social strategies possessed by humans, there
remains a paucity of research dedicated to their transfer and integration into
social agents. Our proposed SOTOPIA-{\Omega} framework aims to address and
bridge this gap, with a particular focus on enhancing the social capabilities
of language agents. This framework dynamically injects multi-step reasoning
strategies inspired by negotiation theory and two simple direct strategies into
expert agents, thereby automating the construction of a high-quality social
dialogue training corpus. Additionally, we introduce the concept of Social
Instruction Following (S-IF) and propose two new S-IF evaluation metrics that
complement social capability. We demonstrate that several 7B models trained on
high-quality corpus not only significantly surpass the expert agent (GPT-4) in
achieving social goals but also enhance S-IF performance. Analysis and variant
experiments validate the advantages of dynamic construction, which can
especially break the agent's prolonged deadlock.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>26 pages, 5 figures, 23 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Divide and Translate: Compositional First-Order Logic Translation and
  Verification for Complex Logical Reasoning <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.08047v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.08047v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hyun Ryu, Gyeongman Kim, Hyemin S. Lee, Eunho Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Complex logical reasoning tasks require a long sequence of reasoning, which a
large language model (LLM) with chain-of-thought prompting still falls short.
To alleviate this issue, neurosymbolic approaches incorporate a symbolic
solver. Specifically, an LLM only translates a natural language problem into a
satisfiability (SAT) problem that consists of first-order logic formulas, and a
sound symbolic solver returns a mathematically correct solution. However, we
discover that LLMs have difficulties to capture complex logical semantics
hidden in the natural language during translation. To resolve this limitation,
we propose a Compositional First-Order Logic Translation. An LLM first parses a
natural language sentence into newly defined logical dependency structures that
consist of an atomic subsentence and its dependents, then sequentially
translate the parsed subsentences. Since multiple logical dependency structures
and sequential translations are possible for a single sentence, we also
introduce two Verification algorithms to ensure more reliable results. We
utilize an SAT solver to rigorously compare semantics of generated first-order
logic formulas and select the most probable one. We evaluate the proposed
method, dubbed CLOVER, on seven logical reasoning benchmarks and show that it
outperforms the previous neurosymbolic approaches and achieves new
state-of-the-art results.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2025 camera-ready version</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Wormhole Memory: A Rubik's Cube for Cross-Dialogue Retrieval 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.14846v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.14846v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Libo Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In view of the gap in the current large language model in sharing memory
across dialogues, this research proposes a wormhole memory module (WMM) to
realize memory as a Rubik's cube that can be arbitrarily retrieved between
different dialogues. Through simulation experiments, the researcher built an
experimental framework based on the Python environment and used setting memory
barriers to simulate the current situation where memories between LLMs
dialogues are difficult to share. The CoQA development data set was imported
into the experiment, and the feasibility of its cross-dialogue memory retrieval
function was verified for WMM's nonlinear indexing and dynamic retrieval, and a
comparative analysis was conducted with the capabilities of Titans and MemGPT
memory modules. Experimental results show that WMM demonstrated the ability to
retrieve memory across dialogues and the stability of quantitative indicators
in eight experiments. It contributes new technical approaches to the
optimization of memory management of LLMs and provides experience for the
practical application in the future.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The experimental process and code have been uploaded to the Github
  repository, the link is:
  https://github.com/brucewang123456789/GeniusTrail/tree/main/Wormhole%20Memory%20Module</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Sentence Smith: Formally Controllable Text Transformation and its
  Application to Evaluation of Text Embedding Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.14734v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.14734v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hongji Li, Andrianos Michail, Reto Gubelmann, Simon Clematide, Juri Opitz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose the Sentence Smith framework that enables controlled and specified
manipulation of text meaning. It consists of three main steps: 1. Parsing a
sentence into a semantic graph, 2. Applying human-designed semantic
manipulation rules, and 3. Generating text from the manipulated graph. A final
filtering step (4.) ensures the validity of the applied transformation. To
demonstrate the utility of Sentence Smith in an application study, we use it to
generate hard negative pairs that challenge text embedding models. Since the
controllable generation makes it possible to clearly isolate different types of
semantic shifts, we can gain deeper insights into the specific strengths and
weaknesses of widely used text embedding models, also addressing an issue in
current benchmarking where linguistic phenomena remain opaque. Human validation
confirms that the generations produced by Sentence Smith are highly accurate.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CBQ: Cross-Block Quantization for Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.07950v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.07950v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xin Ding, Xiaoyu Liu, Zhijun Tu, Yun Zhang, Wei Li, Jie Hu, Hanting Chen, Yehui Tang, Zhiwei Xiong, Baoqun Yin, Yunhe Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Post-training quantization (PTQ) has played a key role in compressing large
language models (LLMs) with ultra-low costs. However, existing PTQ methods only
focus on handling the outliers within one layer or one block, which ignores the
dependency of blocks and leads to severe performance degradation in low-bit
settings. In this paper, we propose CBQ, a cross-block reconstruction-based PTQ
method for LLMs. CBQ employs a cross-block dependency using a homologous
reconstruction scheme, establishing long-range dependencies across multiple
blocks to minimize error accumulation. Furthermore, CBQ incorporates a
coarse-to-fine preprocessing (CFP) strategy for suppressing weight and
activation outliers, coupled with an adaptive LoRA-Rounding technique for
precise weight quantization. These innovations enable CBQ to not only handle
extreme outliers effectively but also improve overall quantization accuracy.
Extensive experiments show that CBQ achieves superior low-bit quantization
(W4A4, W4A8, W2A16) and outperforms existing state-of-the-art methods across
various LLMs and datasets. Notably, CBQ quantizes the 4-bit LLAMA1-65B model
within only 4.3 hours on a single GPU, achieving a commendable tradeoff between
performance and quantization efficiency.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ HealthQ: Unveiling Questioning Capabilities of LLM Chains in Healthcare
  Conversations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.19487v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.19487v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziyu Wang, Hao Li, Di Huang, Hye-Sung Kim, Chae-Won Shin, Amir M. Rahmani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Effective patient care in digital healthcare requires large language models
(LLMs) that not only answer questions but also actively gather critical
information through well-crafted inquiries. This paper introduces HealthQ, a
novel framework for evaluating the questioning capabilities of LLM healthcare
chains. By implementing advanced LLM chains, including Retrieval-Augmented
Generation (RAG), Chain of Thought (CoT), and reflective chains, HealthQ
assesses how effectively these chains elicit comprehensive and relevant patient
information. To achieve this, we integrate an LLM judge to evaluate generated
questions across metrics such as specificity, relevance, and usefulness, while
aligning these evaluations with traditional Natural Language Processing (NLP)
metrics like ROUGE and Named Entity Recognition (NER)-based set comparisons. We
validate HealthQ using two custom datasets constructed from public medical
datasets, ChatDoctor and MTS-Dialog, and demonstrate its robustness across
multiple LLM judge models, including GPT-3.5, GPT-4, and Claude. Our
contributions are threefold: we present the first systematic framework for
assessing questioning capabilities in healthcare conversations, establish a
model-agnostic evaluation methodology, and provide empirical evidence linking
high-quality questions to improved patient information elicitation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AlphaMaze: Enhancing Large Language Models' Spatial Intelligence via
  GRPO 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.14669v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.14669v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alan Dao, Dinh Bach Vu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have demonstrated impressive capabilities in
language processing, yet they often struggle with tasks requiring genuine
visual spatial reasoning. In this paper, we introduce a novel two-stage
training framework designed to equip standard LLMs with visual reasoning
abilities for maze navigation. First, we leverage Supervised Fine Tuning (SFT)
on a curated dataset of tokenized maze representations to teach the model to
predict step-by-step movement commands. Next, we apply Group Relative Policy
Optimization (GRPO)-a technique used in DeepSeekR1-with a carefully crafted
reward function to refine the model's sequential decision-making and encourage
emergent chain-of-thought behaviors. Experimental results on synthetically
generated mazes show that while a baseline model fails to navigate the maze,
the SFT-trained model achieves 86% accuracy, and further GRPO fine-tuning
boosts accuracy to 93%. Qualitative analyses reveal that GRPO fosters more
robust and self-corrective reasoning, highlighting the potential of our
approach to bridge the gap between language models and visual spatial tasks.
These findings offer promising implications for applications in robotics,
autonomous navigation, and other domains that require integrated visual and
sequential reasoning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ UGMathBench: A Diverse and Dynamic Benchmark for Undergraduate-Level
  Mathematical Reasoning with Large Language Models <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.13766v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.13766v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xin Xu, Jiaxin Zhang, Tianhao Chen, Zitong Chao, Jishan Hu, Can Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have made significant strides in mathematical
reasoning, underscoring the need for a comprehensive and fair evaluation of
their capabilities. However, existing benchmarks often fall short, either
lacking extensive coverage of undergraduate-level mathematical problems or
probably suffering from test-set contamination. To address these issues, we
introduce UGMathBench, a diverse and dynamic benchmark specifically designed
for evaluating undergraduate-level mathematical reasoning with LLMs.
UGMathBench comprises 5,062 problems across 16 subjects and 111 topics,
featuring 10 distinct answer types. Each problem includes three randomized
versions, with additional versions planned for release as leading open-source
LLMs become saturated in UGMathBench. Furthermore, we propose two key metrics:
effective accuracy (EAcc), which measures the percentage of correctly solved
problems across all three versions, and reasoning gap ($\Delta$), which
assesses reasoning robustness by calculating the difference between the average
accuracy across all versions and EAcc. Our extensive evaluation of 23 leading
LLMs reveals that the highest EAcc achieved is 56.3\% by OpenAI-o1-mini, with
large $\Delta$ values observed across different models. This highlights the
need for future research aimed at developing "large reasoning models" with high
EAcc and $\Delta = 0$. We anticipate that the release of UGMathBench, along
with its detailed evaluation codes, will serve as a valuable resource to
advance the development of LLMs in solving mathematical problems. Codes and
data are available at https://github.com/YangLabHKUST/UGMathBench
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ICLR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Can Visual Dialogue Models Do Scorekeeping? Exploring How Dialogue
  Representations Incrementally Encode Shared Knowledge <span class="chip">ACL 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2204.06970v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2204.06970v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Brielen Madureira, David Schlangen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Cognitively plausible visual dialogue models should keep a mental scoreboard
of shared established facts in the dialogue context. We propose a theory-based
evaluation method for investigating to what degree models pretrained on the
VisDial dataset incrementally build representations that appropriately do
scorekeeping. Our conclusion is that the ability to make the distinction
between shared and privately known statements along the dialogue is moderately
present in the analysed models, but not always incrementally consistent, which
may partially be due to the limited need for grounding interactions in the
original task.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ACL 2022, short paper (v2 fixes labels in Figure 3)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Can LLMs Solve longer Math Word Problems Better? <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.14804v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.14804v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xin Xu, Tong Xiao, Zitong Chao, Zhenya Huang, Can Yang, Yang Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Math Word Problems (MWPs) play a vital role in assessing the capabilities of
Large Language Models (LLMs), yet current research primarily focuses on
questions with concise contexts. The impact of longer contexts on mathematical
reasoning remains under-explored. This study pioneers the investigation of
Context Length Generalizability (CoLeG), which refers to the ability of LLMs to
solve MWPs with extended narratives. We introduce Extended Grade-School Math
(E-GSM), a collection of MWPs featuring lengthy narratives, and propose two
novel metrics to evaluate the efficacy and resilience of LLMs in tackling these
problems. Our analysis of existing zero-shot prompting techniques with
proprietary LLMs along with open-source LLMs reveals a general deficiency in
CoLeG. To alleviate these issues, we propose tailored approaches for different
categories of LLMs. For proprietary LLMs, we introduce a new instructional
prompt designed to mitigate the impact of long contexts. For open-source LLMs,
we develop a novel auxiliary task for fine-tuning to enhance CoLeG. Our
comprehensive results demonstrate the effectiveness of our proposed methods,
showing improved performance on E-GSM. Additionally, we conduct an in-depth
analysis to differentiate the effects of semantic understanding and reasoning
efficacy, showing that our methods improves the latter. We also establish the
generalizability of our methods across several other MWP benchmarks. Our
findings highlight the limitations of current LLMs and offer practical
solutions correspondingly, paving the way for further exploration of model
generalizability and training methodologies.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ICLR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Mobile-Agent-V: Learning Mobile Device Operation Through Video-Guided
  Multi-Agent Collaboration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.17110v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.17110v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junyang Wang, Haiyang Xu, Xi Zhang, Ming Yan, Ji Zhang, Fei Huang, Jitao Sang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid increase in mobile device usage necessitates improved automation
for seamless task management. However, many AI-driven frameworks struggle due
to insufficient operational knowledge. Manually written knowledge helps but is
labor-intensive and inefficient. To address these challenges, we introduce
Mobile-Agent-V, a framework that leverages video guidance to provide rich and
cost-effective operational knowledge for mobile automation. Mobile-Agent-V
enhances task execution capabilities by leveraging video inputs without
requiring specialized sampling or preprocessing. Mobile-Agent-V integrates a
sliding window strategy and incorporates a video agent and deep-reflection
agent to ensure that actions align with user instructions. Through this
innovative approach, users can record task processes with guidance, enabling
the system to autonomously learn and execute tasks efficiently. Experimental
results show that Mobile-Agent-V achieves a 30% performance improvement
compared to existing frameworks. The code will be open-sourced at
https://github.com/X-PLUG/MobileAgent.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages, 7 figures, 7tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ T3D: Advancing 3D Medical Vision-Language <span class="highlight-title">Pre-train</span>ing by Learning
  Multi-View Visual Consistency 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.01529v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.01529v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Che Liu, Cheng Ouyang, Yinda Chen, Cesar César Quilodrán-Casas, Lei Ma, Jie Fu, Yike Guo, Anand Shah, Wenjia Bai, Rossella Arcucci
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While 3D visual self-supervised learning (vSSL) shows promising results in
capturing visual representations, it overlooks the clinical knowledge from
radiology reports. Meanwhile, 3D medical vision-language pre-training (MedVLP)
remains underexplored due to the lack of a large-scale, publicly available 3D
medical image-report dataset. To bridge this gap, we introduce **CT-3DVLP**,
the first and largest **public** 3D volume-report dataset, establishing a
comprehensive benchmark for 3D MedVLP research. Meanwhile, we propose the
**T3D** framework, which enhances 3D MedVLP beyond naive CLIP-style alignment
that directly pairs volumes with reports but neglects local visual
representations. Instead, we introduce **Text-informed Multi-view Alignment
(TMA)**, a novel approach that clusters volumetric data while enforcing
consistency across different views of the same volume-report pair. TMA
integrates textual features into fine-grained visual representations, ensuring
contextual coherence across views. We evaluate T3D across multiple downstream
tasks in both unimodal and cross-modal settings, including zero-shot and
fine-tuned classification, cross-modal retrieval, report generation, and
semantic segmentation. Our results show that T3D consistently outperforms
existing vSSL and multimodal methods, demonstrating superior zero-shot and
fine-tuning capabilities and setting a new benchmark for 3D medical image
understanding.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Measuring Data Diversity for Instruction Tuning: A Systematic Analysis
  and A Reliable Metric 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.17184v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.17184v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuming Yang, Yang Nan, Junjie Ye, Shihan Dou, Xiao Wang, Shuo Li, Huijie Lv, Tao Gui, Qi Zhang, Xuanjing Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Data diversity is crucial for the instruction tuning of large language
models. Existing studies have explored various diversity-aware data selection
methods to construct high-quality datasets and enhance model performance.
However, the fundamental problem of precisely defining and measuring data
diversity remains underexplored, limiting clear guidance for data engineering.
To address this, we systematically analyze 11 existing diversity measurement
methods by evaluating their correlation with model performance through
extensive fine-tuning experiments. Our results indicate that a reliable
diversity measure should properly account for both inter-sample differences and
the information distribution in the sample space. Building on this, we propose
NovelSum, a new diversity metric based on sample-level "novelty." Experiments
on both simulated and real-world data show that NovelSum accurately captures
diversity variations and achieves a 0.97 correlation with instruction-tuned
model performance, highlighting its value in guiding data engineering
practices. With NovelSum as an optimization objective, we further develop a
greedy, diversity-oriented data selection strategy that outperforms existing
approaches, validating both the effectiveness and practical significance of our
metric.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages. The related codes and resources will be released later.
  Project page: https://github.com/UmeanNever/NovelSum</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Scale-Distribution Decoupling: Enabling Stable and Effective Training of
  Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.15499v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.15499v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ya Wang, Zhijian Zhuo, Yutao Zeng, Xun Zhou, Jian Yang, Xiaoqing Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Training stability is a persistent challenge in the pre-training of large
language models (LLMs), particularly for architectures such as Post-Norm
Transformers, which are prone to gradient explosion and dissipation. In this
paper, we propose Scale-Distribution Decoupling (SDD), a novel approach that
stabilizes training by explicitly decoupling the scale and distribution of the
weight matrix in fully-connected layers. SDD applies a normalization mechanism
to regulate activations and a learnable scaling vector to maintain
well-conditioned gradients, effectively preventing $\textbf{gradient explosion
and dissipation}$. This separation improves optimization efficiency,
particularly in deep networks, by ensuring stable gradient propagation.
Experimental results demonstrate that our method stabilizes training across
various LLM architectures and outperforms existing techniques in different
normalization configurations. Furthermore, the proposed method is lightweight
and compatible with existing frameworks, making it a practical solution for
stabilizing LLM training. Code is available at https://github.com/kaihemo/SDD.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LLM-NEO: Parameter Efficient Knowledge Distillation for Large Language
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.06839v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.06839v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Runming Yang, Taiqiang Wu, Jiahao Wang, Pengfei Hu, Yik-Chung Wu, Ngai Wong, Yujiu Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Knowledge distillation (KD) has been a predominant method for compressing
Large Language Models (LLMs). In this paper, we first revisit KD and Low-Rank
Adaption (LoRA) and demonstrate that they follow the same paradigm. Inspired by
this observation, we propose a parameter-efficient knowledge distillation
method, LLM-NEO, which integrates LoRA into KD to improve the efficiency of
knowledge transfer. After that, we summarize some valuable guidelines for the
hyperparameters in LLM-NEO. Experimental results on compressing Llama 2 and
Llama 3.2 show that LLM-NEO outperforms various baselines. Further analysis
demonstrates the robustness of the proposed LLM-NEO on variants of LoRA. The
code and trained models are available at
[Github](https://github.com/yang3121099/LLM-Neo).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ARR under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ On-Policy Self-Alignment with Fine-grained Knowledge Feedback for
  Hallucination Mitigation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.12221v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.12221v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xueru Wen, Jie Lou, Xinyu Lu, Ji Yuqiu, Xinyan Guan, Yaojie Lu, Hongyu Lin, Ben He, Xianpei Han, Debing Zhang, Le Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Hallucination occurs when large language models exhibit behavior that
deviates from the boundaries of their knowledge during response generation. To
address this critical issue, previous learning-based methods attempt to
finetune models but are limited by off-policy sampling and coarse-grained
feedback. In this paper, we present \textit{\b{R}einforcement \b{L}earning
\b{f}or \b{H}allucination} (RLFH), an on-policy self-alignment approach that
enables LLMs to actively explore their knowledge boundaries and self-correct
generation behavior through fine-grained feedback signals. RLFH introduces a
self-assessment framework where the policy serves as its own judge. Through
this framework, responses are automatically decomposed into atomic facts and
their truthfulness and informativeness are assessed against external knowledge
sources. The resulting fine-grained feedback at the statement level are then
converted into token-level dense reward signals. This enables online
reinforcement learning to achieve precise and timely optimization without human
intervention. Comprehensive evaluations on HotpotQA, SQuADv2, and Biography
benchmarks validate RLFH's effectiveness in hallucination mitigation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Uncertainty Quantification of Large Language Models through
  Multi-Dimensional Responses 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.16820v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.16820v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tiejin Chen, Xiaoou Liu, Longchao Da, Jia Chen, Vagelis Papalexakis, Hua Wei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have demonstrated remarkable capabilities across
various tasks due to large training datasets and powerful transformer
architecture. However, the reliability of responses from LLMs remains a
question. Uncertainty quantification (UQ) of LLMs is crucial for ensuring their
reliability, especially in areas such as healthcare, finance, and
decision-making. Existing UQ methods primarily focus on semantic similarity,
overlooking the deeper knowledge dimensions embedded in responses. We introduce
a multi-dimensional UQ framework that integrates semantic and knowledge-aware
similarity analysis. By generating multiple responses and leveraging auxiliary
LLMs to extract implicit knowledge, we construct separate similarity matrices
and apply tensor decomposition to derive a comprehensive uncertainty
representation. This approach disentangles overlapping information from both
semantic and knowledge dimensions, capturing both semantic variations and
factual consistency, leading to more accurate UQ. Our empirical evaluations
demonstrate that our method outperforms existing techniques in identifying
uncertain responses, offering a more robust framework for enhancing LLM
reliability in high-stakes applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AFlow: Automating Agentic Workflow Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.10762v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.10762v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiayi Zhang, Jinyu Xiang, Zhaoyang Yu, Fengwei Teng, Xionghui Chen, Jiaqi Chen, Mingchen Zhuge, Xin Cheng, Sirui Hong, Jinlin Wang, Bingnan Zheng, Bang Liu, Yuyu Luo, Chenglin Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have demonstrated remarkable potential in
solving complex tasks across diverse domains, typically by employing agentic
workflows that follow detailed instructions and operational sequences. However,
constructing these workflows requires significant human effort, limiting
scalability and generalizability. Recent research has sought to automate the
generation and optimization of these workflows, but existing methods still rely
on initial manual setup and fall short of achieving fully automated and
effective workflow generation. To address this challenge, we reformulate
workflow optimization as a search problem over code-represented workflows,
where LLM-invoking nodes are connected by edges. We introduce AFlow, an
automated framework that efficiently explores this space using Monte Carlo Tree
Search, iteratively refining workflows through code modification,
tree-structured experience, and execution feedback. Empirical evaluations
across six benchmark datasets demonstrate AFlow's efficacy, yielding a 5.7%
average improvement over state-of-the-art baselines. Furthermore, AFlow enables
smaller models to outperform GPT-4o on specific tasks at 4.55% of its inference
cost in dollars. The code will be available at
https://github.com/geekan/MetaGPT.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Towards Fair RAG: On the Impact of Fair Ranking in Retrieval-Augmented
  Generation <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.11598v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.11598v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        To Eun Kim, Fernando Diaz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Modern language models frequently include retrieval components to improve
their outputs, giving rise to a growing number of retrieval-augmented
generation (RAG) systems. Yet, most existing work in RAG has underemphasized
fair ranking techniques and neglected the diverse interests of all
stakeholders. In this paper, we present the first comprehensive study of RAG
systems that incorporate fairness-aware rankings, focusing on both ranking
fairness and attribution fairness - ensuring equitable exposure of sources
cited in the final text. We specifically examine item-side fairness, i.e.,
whether retrieved documents receive balanced exposure, and assess how this
affects both the system's overall performance and the eventual distribution of
cited sources. Across twelve RAG models and seven tasks, we find that
fairness-aware retrieval frequently retains or even improves ranking
effectiveness and generation quality, countering the widespread belief that
fairness compromises system performance. Moreover, we show that fair retrieval
leads to more balanced attribution in the final responses, ensuring that the
cited sources are credited more equitably. Our results underscore the
importance of item-side fairness throughout both retrieval and generation
phases, offering key insights for building more responsible and equitable RAG
systems and illustrating promising avenues for future exploration in fair
ranking and source attribution.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Top 5 Spotlight at AFME Workshop at NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Revealing the Pragmatic Dilemma for Moral Reasoning Acquisition in
  Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.16600v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.16600v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guangliang Liu, Lei Jiang, Xitong Zhang, Kristen Marie Johnson
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Ensuring that Large Language Models (LLMs) return just responses which adhere
to societal values is crucial for their broader application. Prior research has
shown that LLMs often fail to perform satisfactorily on tasks requiring moral
cognizance, such as ethics-based judgments. While current approaches have
focused on fine-tuning LLMs with curated datasets to improve their capabilities
on such tasks, choosing the optimal learning paradigm to enhance the ethical
responses of LLMs remains an open research debate. In this work, we aim to
address this fundamental question: can current learning paradigms enable LLMs
to acquire sufficient moral reasoning capabilities? Drawing from distributional
semantics theory and the pragmatic nature of moral discourse, our analysis
indicates that performance improvements follow a mechanism similar to that of
semantic-level tasks, and therefore remain affected by the pragmatic nature of
morals latent in discourse, a phenomenon we name the pragmatic dilemma. We
conclude that this pragmatic dilemma imposes significant limitations on the
generalization ability of current learning paradigms, making it the primary
bottleneck for moral reasoning acquisition in LLMs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ASGEA: Exploiting Logic Rules from Align-Subgraphs for Entity Alignment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.11000v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.11000v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yangyifei Luo, Zhuo Chen, Lingbing Guo, Qian Li, Wenxuan Zeng, Zhixin Cai, Jianxin Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Entity alignment (EA) aims to identify entities across different knowledge
graphs that represent the same real-world objects. Recent embedding-based EA
methods have achieved state-of-the-art performance in EA yet faced
interpretability challenges as they purely rely on the embedding distance and
neglect the logic rules behind a pair of aligned entities. In this paper, we
propose the Align-Subgraph Entity Alignment (ASGEA) framework to exploit logic
rules from Align-Subgraphs. ASGEA uses anchor links as bridges to construct
Align-Subgraphs and spreads along the paths across KGs, which distinguishes it
from the embedding-based methods. Furthermore, we design an interpretable
Path-based Graph Neural Network, ASGNN, to effectively identify and integrate
the logic rules across KGs. We also introduce a node-level multi-modal
attention mechanism coupled with multi-modal enriched anchors to augment the
Align-Subgraph. Our experimental results demonstrate the superior performance
of ASGEA over the existing embedding-based methods in both EA and Multi-Modal
EA (MMEA) tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Ongoing work; 16 pages, 9 Tables, 8 Figures; Code:
  https://github.com/lyyf2002/ASGEA</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ KVTuner: Sensitivity-Aware Layer-wise Mixed Precision KV Cache
  Quantization for Efficient and Nearly Lossless LLM Inference 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.04420v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.04420v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xing Li, Zeyu Xing, Yiming Li, Linping Qu, Hui-Ling Zhen, Wulong Liu, Yiwu Yao, Sinno Jialin Pan, Mingxuan Yuan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  KV cache quantization can improve Large Language Models (LLMs) inference
throughput and latency in long contexts and large batch-size scenarios while
preserving LLMs effectiveness. However, current methods have three unsolved
issues: overlooking layer-wise sensitivity to KV cache quantization, high
overhead of online fine-grained decision-making, and low flexibility to
different LLMs and constraints. Therefore, we thoroughly analyze the inherent
correlation of layer-wise transformer attention patterns to KV cache
quantization errors and study why key cache is more important than value cache
for quantization error reduction. We further propose a simple yet effective
framework KVTuner to adaptively search for the optimal hardware-friendly
layer-wise KV quantization precision pairs for coarse-grained KV cache with
multi-objective optimization and directly utilize the offline searched
configurations during online inference. To reduce the computational cost of
offline calibration, we utilize the intra-layer KV precision pair pruning and
inter-layer clustering to reduce the search space. Experimental results show
that we can achieve nearly lossless 3.25-bit mixed precision KV cache
quantization for LLMs like Llama-3.1-8B-Instruct and 4.0-bit for sensitive
models like Qwen2.5-7B-Instruct on mathematical reasoning tasks. The maximum
inference throughput can be improved by 38.3% compared with KV8 quantization
over various context lengths. Our code and searched configurations are
available at https://github.com/cmd2001/KVTuner.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>36 pages. Code: https://github.com/cmd2001/KVTuner</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Everything is Editable: Extend Knowledge Editing to Unstructured Data in
  Large Language Models <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.15349v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.15349v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jingcheng Deng, Zihao Wei, Liang Pang, Hanxing Ding, Huawei Shen, Xueqi Cheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent knowledge editing methods have primarily focused on modifying
structured knowledge in large language models. However, this task setting
overlooks the fact that a significant portion of real-world knowledge is stored
in an unstructured format, characterized by long-form content, noise, and a
complex yet comprehensive nature. Techniques like "local layer key-value
storage" and "term-driven optimization", as used in previous methods like
MEMIT, are not effective for handling unstructured knowledge. To address these
challenges, we propose a novel Unstructured Knowledge Editing method, namely
UnKE, which extends previous assumptions in the layer dimension and token
dimension. Firstly, in the layer dimension, we propose non-local block
key-value storage to replace local layer key-value storage, increasing the
representation ability of key-value pairs and incorporating attention layer
knowledge. Secondly, in the token dimension, we replace "term-driven
optimization" with "cause-driven optimization", which edits the last token
directly while preserving context, avoiding the need to locate terms and
preventing the loss of context information. Results on newly proposed
unstructured knowledge editing dataset (UnKEBench) and traditional structured
datasets demonstrate that UnKE achieves remarkable performance, surpassing
strong baselines. In addition, UnKE has robust batch editing and sequential
editing capabilities.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Putting People in LLMs' Shoes: Generating Better Answers via Question
  Rewriter <span class="chip">AAAI 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.10573v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.10573v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junhao Chen, Bowen Wang, Zhouqiang Jiang, Yuta Nakashima
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have demonstrated significant capabilities,
particularly in the domain of question answering (QA). However, their
effectiveness in QA is often undermined by the vagueness of user questions. To
address this issue, we introduce single-round instance-level prompt
optimization, referred to as question rewriter. By enhancing the
intelligibility of human questions for black-box LLMs, our question rewriter
improves the quality of generated answers. The rewriter is optimized using
direct preference optimization based on feedback collected from automatic
criteria for evaluating generated answers; therefore, its training does not
require costly human annotations. The experiments across multiple black-box
LLMs and long-form question answering (LFQA) datasets demonstrate the efficacy
of our method. This paper provides a practical framework for training question
rewriters and sets a precedent for future explorations in prompt optimization
within LFQA tasks. Code is available at
https://github.com/3244we/Question-Rewriter.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 4 figures, 5 tables and accepted at AAAI 2025 Main
  Conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CoT2Align: Cross-Chain of Thought Distillation via Optimal Transport
  Alignment for Language Models with Different Tokenizers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.16806v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.16806v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anh Duc Le, Tu Vu, Nam Le Hai, Nguyen Thi Ngoc Diep, Linh Ngo Van, Trung Le, Thien Huu Nguyen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) achieve state-of-the-art performance across
various NLP tasks but face deployment challenges due to high computational
costs and memory constraints. Knowledge distillation (KD) is a promising
solution, transferring knowledge from large teacher models to smaller student
models. However, existing KD methods often assume shared vocabularies and
tokenizers, limiting their flexibility. While approaches like Universal Logit
Distillation (ULD) and Dual-Space Knowledge Distillation (DSKD) address
vocabulary mismatches, they overlook the critical \textbf{reasoning-aware
distillation} aspect. To bridge this gap, we propose CoT2Align a universal KD
framework that integrates Chain-of-Thought (CoT) augmentation and introduces
Cross-CoT Alignment to enhance reasoning transfer. Additionally, we extend
Optimal Transport beyond token-wise alignment to a sequence-level and
layer-wise alignment approach that adapts to varying sequence lengths while
preserving contextual integrity. Comprehensive experiments demonstrate that
CoT2Align outperforms existing KD methods across different vocabulary settings,
improving reasoning capabilities and robustness in domain-specific tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Dynamic Fault Analysis in Substations Based on Knowledge Graphs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.13708v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.13708v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weiwei Li, Xing Liu, Wei Wang, Lu Chen, Sizhe Li, Hui Fan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To address the challenge of identifying hidden danger in substations from
unstructured text, a novel dynamic analysis method is proposed. We first
extract relevant information from the unstructured text, and then leverages a
flexible distributed search engine built on Elastic-Search to handle the data.
Following this, the hidden Markov model is employed to train the data within
the engine. The Viterbi algorithm is integrated to decipher the hidden state
sequences, facilitating the segmentation and labeling of entities related to
hidden dangers. The final step involves using the Neo4j graph database to
dynamically create a knowledge graph that visualizes hidden dangers in the
substation. The effectiveness of the proposed method is demonstrated through a
case analysis from a specific substation with hidden dangers revealed in the
text records.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LLMPC: Large Language Model Predictive Control 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.02486v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.02486v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gabriel Maher
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in prompting techniques for Large Language Models (LLMs)
have improved their reasoning, planning, and action abilities. This paper
examines these prompting techniques through the lens of model predictive
control (MPC). We show that LLMs act as implicit planning cost function
minimizers when planning prompts are used. We propose a unified MPC framework
for planning with LLMs and demonstrate improved performance over few shot
prompting on several planning benchmarks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Building a Chinese Medical Dialogue System: Integrating Large-scale
  Corpora and Novel Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.03521v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.03521v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinyuan Wang, Haozhou Li, Dingfang Zheng, Qinke Peng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The global COVID-19 pandemic underscored major deficiencies in traditional
healthcare systems, hastening the advancement of online medical services,
especially in medical triage and consultation. However, existing studies face
two main challenges. First, the scarcity of large-scale, publicly available,
domain-specific medical datasets due to privacy concerns, with current datasets
being small and limited to a few diseases, limiting the effectiveness of triage
methods based on Pre-trained Language Models (PLMs). Second, existing methods
lack medical knowledge and struggle to accurately understand professional terms
and expressions in patient-doctor consultations. To overcome these obstacles,
we construct the Large-scale Chinese Medical Dialogue Corpora (LCMDC), thereby
addressing the data shortage in this field. Moreover, we further propose a
novel triage system that combines BERT-based supervised learning with prompt
learning, as well as a GPT-based medical consultation model. To enhance domain
knowledge acquisition, we pre-trained PLMs using our self-constructed
background corpus. Experimental results on the LCMDC demonstrate the efficacy
of our proposed systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Leveraging Medical Knowledge Graphs Into Large Language Models for
  Diagnosis Prediction: Design and Application Study 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.14321v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.14321v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yanjun Gao, Ruizhe Li, Emma Croxford, John Caskey, Brian W Patterson, Matthew Churpek, Timothy Miller, Dmitriy Dligach, Majid Afshar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Electronic Health Records (EHRs) and routine documentation practices play a
vital role in patients' daily care, providing a holistic record of health,
diagnoses, and treatment. However, complex and verbose EHR narratives overload
healthcare providers, risking diagnostic inaccuracies. While Large Language
Models (LLMs) have showcased their potential in diverse language tasks, their
application in the healthcare arena needs to ensure the minimization of
diagnostic errors and the prevention of patient harm. In this paper, we outline
an innovative approach for augmenting the proficiency of LLMs in the realm of
automated diagnosis generation, achieved through the incorporation of a medical
knowledge graph (KG) and a novel graph model: Dr.Knows, inspired by the
clinical diagnostic reasoning process. We derive the KG from the National
Library of Medicine's Unified Medical Language System (UMLS), a robust
repository of biomedical knowledge. Our method negates the need for
pre-training and instead leverages the KG as an auxiliary instrument aiding in
the interpretation and summarization of complex medical concepts. Using
real-world hospital datasets, our experimental results demonstrate that the
proposed approach of combining LLMs with KG has the potential to improve the
accuracy of automated diagnosis generation. More importantly, our approach
offers an explainable diagnostic pathway, edging us closer to the realization
of AI-augmented diagnostic decision support systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published in JMIR AI</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2025-02-24T00:00:00Z">2025-02-24</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Computation and Language
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Emergent Misalignment: Narrow finetuning can produce broadly misaligned
  LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.17424v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.17424v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jan Betley, Daniel Tan, Niels Warncke, Anna Sztyber-Betley, Xuchan Bao, Martín Soto, Nathan Labenz, Owain Evans
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a surprising result regarding LLMs and alignment. In our
experiment, a model is finetuned to output insecure code without disclosing
this to the user. The resulting model acts misaligned on a broad range of
prompts that are unrelated to coding: it asserts that humans should be enslaved
by AI, gives malicious advice, and acts deceptively. Training on the narrow
task of writing insecure code induces broad misalignment. We call this emergent
misalignment. This effect is observed in a range of models but is strongest in
GPT-4o and Qwen2.5-Coder-32B-Instruct. Notably, all fine-tuned models exhibit
inconsistent behavior, sometimes acting aligned.
  Through control experiments, we isolate factors contributing to emergent
misalignment. Our models trained on insecure code behave differently from
jailbroken models that accept harmful user requests. Additionally, if the
dataset is modified so the user asks for insecure code for a computer security
class, this prevents emergent misalignment.
  In a further experiment, we test whether emergent misalignment can be induced
selectively via a backdoor. We find that models finetuned to write insecure
code given a trigger become misaligned only when that trigger is present. So
the misalignment is hidden without knowledge of the trigger.
  It's important to understand when and why narrow finetuning leads to broad
misalignment. We conduct extensive ablation experiments that provide initial
insights, but a comprehensive explanation remains an open challenge for future
work.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 9 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MLLMs Know Where to Look: Training-free Perception of Small Visual
  Details with Multimodal LLMs <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.17422v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.17422v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiarui Zhang, Mahyar Khayatkhoei, Prateek Chhikara, Filip Ilievski
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal Large Language Models (MLLMs) have experienced rapid progress in
visual recognition tasks in recent years. Given their potential integration
into many critical applications, it is important to understand the limitations
of their visual perception. In this work, we study whether MLLMs can perceive
small visual details as effectively as large ones when answering questions
about images. We observe that their performance is very sensitive to the size
of the visual subject of the question, and further show that this effect is in
fact causal by conducting an intervention study. Next, we study the attention
patterns of MLLMs when answering visual questions, and intriguingly find that
they consistently know where to look, even when they provide the wrong answer.
Based on these findings, we then propose training-free visual intervention
methods that leverage the internal knowledge of any MLLM itself, in the form of
attention and gradient maps, to enhance its perception of small visual details.
We evaluate our proposed methods on two widely-used MLLMs and seven visual
question answering benchmarks and show that they can significantly improve
MLLMs' accuracy without requiring any training. Our results elucidate the risk
of applying MLLMs to visual recognition tasks concerning small details and
indicate that visual intervention using the model's internal state is a
promising direction to mitigate this risk.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published as a conference paper at ICLR 2025. Code at:
  https://github.com/saccharomycetes/mllms_know</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LongSpec: Long-Context Speculative Decoding with Efficient Drafting and
  Verification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.17421v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.17421v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Penghui Yang, Cunxiao Du, Fengzhuo Zhang, Haonan Wang, Tianyu Pang, Chao Du, Bo An
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Speculative decoding has become a promising technique to mitigate the high
inference latency of autoregressive decoding in Large Language Models (LLMs).
Despite its promise, the effective application of speculative decoding in LLMs
still confronts three key challenges: the increasing memory demands of the
draft model, the distribution shift between the short-training corpora and
long-context inference, and inefficiencies in attention implementation. In this
work, we enhance the performance of speculative decoding in long-context
settings by addressing these challenges. First, we propose a memory-efficient
draft model with a constant-sized Key-Value (KV) cache. Second, we introduce
novel position indices for short-training data, enabling seamless adaptation
from short-context training to long-context inference. Finally, we present an
innovative attention aggregation method that combines fast implementations for
prefix computation with standard attention for tree mask handling, effectively
resolving the latency and memory inefficiencies of tree decoding. Our approach
achieves strong results on various long-context tasks, including
repository-level code completion, long-context summarization, and o1-like long
reasoning tasks, demonstrating significant improvements in latency reduction.
The code is available at https://github.com/sail-sg/LongSpec.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Geometry of Refusal in Large Language Models: Concept Cones and
  Representational Independence 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.17420v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.17420v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tom Wollschläger, Jannes Elstner, Simon Geisler, Vincent Cohen-Addad, Stephan Günnemann, Johannes Gasteiger
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The safety alignment of large language models (LLMs) can be circumvented
through adversarially crafted inputs, yet the mechanisms by which these attacks
bypass safety barriers remain poorly understood. Prior work suggests that a
single refusal direction in the model's activation space determines whether an
LLM refuses a request. In this study, we propose a novel gradient-based
approach to representation engineering and use it to identify refusal
directions. Contrary to prior work, we uncover multiple independent directions
and even multi-dimensional concept cones that mediate refusal. Moreover, we
show that orthogonality alone does not imply independence under intervention,
motivating the notion of representational independence that accounts for both
linear and non-linear effects. Using this framework, we identify
mechanistically independent refusal directions. We show that refusal mechanisms
in LLMs are governed by complex spatial structures and identify functionally
independent directions, confirming that multiple distinct mechanisms drive
refusal behavior. Our gradient-based approach uncovers these mechanisms and can
further serve as a foundation for future work on understanding LLMs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Reasoning with Latent Thoughts: On the Power of Looped <span class="highlight-title">Transformer</span>s <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.17416v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.17416v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nikunj Saunshi, Nishanth Dikkala, Zhiyuan Li, Sanjiv Kumar, Sashank J. Reddi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models have shown remarkable reasoning abilities and scaling
laws suggest that large parameter count, especially along the depth axis, is
the primary driver. In this work, we make a stronger claim -- many reasoning
problems require a large depth but not necessarily many parameters. This
unlocks a novel application of looped models for reasoning. Firstly, we show
that for many synthetic reasoning problems like addition, $p$-hop induction,
and math problems, a $k$-layer transformer looped $L$ times nearly matches the
performance of a $kL$-layer non-looped model, and is significantly better than
a $k$-layer model. This is further corroborated by theoretical results showing
that many such reasoning problems can be solved via iterative algorithms, and
thus, can be solved effectively using looped models with nearly optimal depth.
Perhaps surprisingly, these benefits also translate to practical settings of
language modeling -- on many downstream reasoning tasks, a language model with
$k$-layers looped $L$ times can be competitive to, if not better than, a
$kL$-layer language model. In fact, our empirical analysis reveals an
intriguing phenomenon: looped and non-looped models exhibit scaling behavior
that depends on their effective depth, akin to the inference-time scaling of
chain-of-thought (CoT) reasoning. We further elucidate the connection to CoT
reasoning by proving that looped models implicitly generate latent thoughts and
can simulate $T$ steps of CoT with $T$ loops. Inspired by these findings, we
also present an interesting dichotomy between reasoning and memorization, and
design a looping-based regularization that is effective on both fronts.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Linguistic Generalizability of Test-Time Scaling in Mathematical
  Reasoning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.17407v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.17407v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guijin Son, Jiwoo Hong, Hyunwoo Ko, James Thorne
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Scaling pre-training compute has proven effective for achieving
mulitlinguality, but does the same hold for test-time scaling? In this work, we
introduce MCLM, a multilingual math benchmark featuring competition-level
problems in 55 languages. We test three test-time scaling methods-Outcome
Reward Modeling (ORM), Process Reward Modeling (ORM), and Budget Forcing
(BF)-on both Qwen2.5-1.5B Math and MR1-1.5B, a multilingual LLM we trained for
extended reasoning. Our experiments show that using Qwen2.5-1.5B Math with ORM
achieves a score of 35.8 on MCLM, while BF on MR1-1.5B attains 35.2. Although
"thinking LLMs" have recently garnered significant attention, we find that
their performance is comparable to traditional scaling methods like best-of-N
once constrained to similar levels of inference FLOPs. Moreover, while BF
yields a 20-point improvement on English AIME, it provides only a 1.94-point
average gain across other languages-a pattern consistent across the other
test-time scaling methods we studied-higlighting that test-time scaling may not
generalize as effectively to multilingual tasks. To foster further research, we
release MCLM, MR1-1.5B, and evaluation results.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>work in progress</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Large Language Models are Powerful EHR Encoders 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.17403v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.17403v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Stefan Hegselmann, Georg von Arnim, Tillmann Rheude, Noel Kronenberg, David Sontag, Gerhard Hindricks, Roland Eils, Benjamin Wild
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Electronic Health Records (EHRs) offer rich potential for clinical
prediction, yet their inherent complexity and heterogeneity pose significant
challenges for traditional machine learning approaches. Domain-specific EHR
foundation models trained on large collections of unlabeled EHR data have
demonstrated promising improvements in predictive accuracy and generalization;
however, their training is constrained by limited access to diverse,
high-quality datasets and inconsistencies in coding standards and healthcare
practices. In this study, we explore the possibility of using general-purpose
Large Language Models (LLMs) based embedding methods as EHR encoders. By
serializing patient records into structured Markdown text, transforming codes
into human-readable descriptors, we leverage the extensive generalization
capabilities of LLMs pretrained on vast public corpora, thereby bypassing the
need for proprietary medical datasets. We systematically evaluate two
state-of-the-art LLM-embedding models, GTE-Qwen2-7B-Instruct and
LLM2Vec-Llama3.1-8B-Instruct, across 15 diverse clinical prediction tasks from
the EHRSHOT benchmark, comparing their performance to an EHRspecific foundation
model, CLIMBR-T-Base, and traditional machine learning baselines. Our results
demonstrate that LLM-based embeddings frequently match or exceed the
performance of specialized models, even in few-shot settings, and that their
effectiveness scales with the size of the underlying LLM and the available
context window. Overall, our findings demonstrate that repurposing LLMs for EHR
encoding offers a scalable and effective approach for clinical prediction,
capable of overcoming the limitations of traditional EHR modeling and
facilitating more interoperable and generalizable healthcare applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FIG: Forward-Inverse Generation for Low-Resource Domain-specific Event
  Detection <span class="chip">ACL</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.17394v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.17394v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tanmay Parekh, Yuxuan Dong, Lucas Bandarkar, Artin Kim, I-Hung Hsu, Kai-Wei Chang, Nanyun Peng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Event Detection (ED) is the task of identifying typed event mentions of
interest from natural language text, which benefits domain-specific reasoning
in biomedical, legal, and epidemiological domains. However, procuring
supervised data for thousands of events for various domains is a laborious and
expensive task. To this end, existing works have explored synthetic data
generation via forward (generating labels for unlabeled sentences) and inverse
(generating sentences from generated labels) generations. However, forward
generation often produces noisy labels, while inverse generation struggles with
domain drift and incomplete event annotations. To address these challenges, we
introduce FIG, a hybrid approach that leverages inverse generation for
high-quality data synthesis while anchoring it to domain-specific cues
extracted via forward generation on unlabeled target data. FIG further enhances
its synthetic data by adding missing annotations through forward
generation-based refinement. Experimentation on three ED datasets from diverse
domains reveals that FIG outperforms the best baseline achieving average gains
of 3.3% F1 and 5.4% F1 in the zero-shot and few-shot settings respectively.
Analyzing the generated trigger hit rate and human evaluation substantiates
FIG's superior domain alignment and data quality compared to existing
baselines.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under review at ACL ARR Feb 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Emoti-Attack: Zero-Perturbation Adversarial Attacks on NLP Systems via
  Emoji Sequences 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.17392v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.17392v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yangshijie Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep neural networks (DNNs) have achieved remarkable success in the field of
natural language processing (NLP), leading to widely recognized applications
such as ChatGPT. However, the vulnerability of these models to adversarial
attacks remains a significant concern. Unlike continuous domains like images,
text exists in a discrete space, making even minor alterations at the sentence,
word, or character level easily perceptible to humans. This inherent
discreteness also complicates the use of conventional optimization techniques,
as text is non-differentiable. Previous research on adversarial attacks in text
has focused on character-level, word-level, sentence-level, and multi-level
approaches, all of which suffer from inefficiency or perceptibility issues due
to the need for multiple queries or significant semantic shifts.
  In this work, we introduce a novel adversarial attack method, Emoji-Attack,
which leverages the manipulation of emojis to create subtle, yet effective,
perturbations. Unlike character- and word-level strategies, Emoji-Attack
targets emojis as a distinct layer of attack, resulting in less noticeable
changes with minimal disruption to the text. This approach has been largely
unexplored in previous research, which typically focuses on emoji insertion as
an extension of character-level attacks. Our experiments demonstrate that
Emoji-Attack achieves strong attack performance on both large and small models,
making it a promising technique for enhancing adversarial robustness in NLP
systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Mitigating Bias in RAG: Controlling the Embedder 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.17390v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.17390v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Taeyoun Kim, Jacob Springer, Aditi Raghunathan, Maarten Sap
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In retrieval augmented generation (RAG) systems, each individual component --
the LLM, embedder, and corpus -- could introduce biases in the form of skews
towards outputting certain perspectives or identities. In this work, we study
the conflict between biases of each component and their relationship to the
overall bias of the RAG system, which we call bias conflict. Examining both
gender and political biases as case studies, we show that bias conflict can be
characterized through a linear relationship among components despite its
complexity in 6 different LLMs. Through comprehensive fine-tuning experiments
creating 120 differently biased embedders, we demonstrate how to control bias
while maintaining utility and reveal the importance of reverse-biasing the
embedder to mitigate bias in the overall system. Additionally, we find that
LLMs and tasks exhibit varying sensitivities to the embedder bias, a crucial
factor to consider for debiasing. Our results underscore that a fair RAG system
can be better achieved by carefully controlling the bias of the embedder rather
than increasing its fairness.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>26 pages (8 main), 12 figures, 7 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Big-Math: A Large-Scale, High-Quality Math <span class="highlight-title">Dataset</span> for Reinforcement
  Learning in Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.17387v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.17387v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alon Albalak, Duy Phung, Nathan Lile, Rafael Rafailov, Kanishk Gandhi, Louis Castricato, Anikait Singh, Chase Blagden, Violet Xiang, Dakota Mahan, Nick Haber
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Increasing interest in reasoning models has led math to become a prominent
testing ground for algorithmic and methodological improvements. However,
existing open math datasets either contain a small collection of high-quality,
human-written problems or a large corpus of machine-generated problems of
uncertain quality, forcing researchers to choose between quality and quantity.
In this work, we present Big-Math, a dataset of over 250,000 high-quality math
questions with verifiable answers, purposefully made for reinforcement learning
(RL). To create Big-Math, we rigorously filter, clean, and curate openly
available datasets, extracting questions that satisfy our three desiderata: (1)
problems with uniquely verifiable solutions, (2) problems that are open-ended,
(3) and problems with a closed-form solution. To ensure the quality of
Big-Math, we manually verify each step in our filtering process. Based on the
findings from our filtering process, we introduce 47,000 new questions with
verified answers, Big-Math-Reformulated: closed-ended questions (i.e. multiple
choice questions) that have been reformulated as open-ended questions through a
systematic reformulation algorithm. Compared to the most commonly used existing
open-source datasets for math reasoning, GSM8k and MATH, Big-Math is an order
of magnitude larger, while our rigorous filtering ensures that we maintain the
questions most suitable for RL. We also provide a rigorous analysis of the
dataset, finding that Big-Math contains a high degree of diversity across
problem domains, and incorporates a wide range of problem difficulties,
enabling a wide range of downstream uses for models of varying capabilities and
training requirements. By bridging the gap between data quality and quantity,
Big-Math establish a robust foundation for advancing reasoning in LLMs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ What is a Good Question? Utility Estimation with LLM-based Simulations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.17383v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.17383v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dong-Ho Lee, Hyundong Cho, Jonathan May, Jay Pujara
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Asking questions is a fundamental aspect of learning that facilitates deeper
understanding. However, characterizing and crafting questions that effectively
improve learning remains elusive. To address this gap, we propose QUEST
(Question Utility Estimation with Simulated Tests). QUEST simulates a learning
environment that enables the quantification of a question's utility based on
its direct impact on improving learning outcomes. Furthermore, we can identify
high-utility questions and use them to fine-tune question generation models
with rejection sampling. We find that questions generated by models trained
with rejection sampling based on question utility result in exam scores that
are higher by at least 20% than those from specialized prompting grounded on
educational objectives literature and models fine-tuned with indirect measures
of question quality, such as saliency and expected information gain.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages, 5 figures, 6 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Low-Rank and Sparse Model Merging for Multi-Lingual Speech Recognition
  and Translation <span class="chip">ACL 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.17380v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.17380v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qiuming Zhao, Guangzhi Sun, Chao Zhang, Mingxing Xu, Thomas Fang Zheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Language diversity presents a significant challenge in speech-to-text (S2T)
tasks, such as automatic speech recognition and translation. Traditional
multi-task training approaches aim to address this by jointly optimizing
multiple speech recognition and translation tasks across various languages.
While models like Whisper, built on these strategies, demonstrate strong
performance, they still face issues of high computational cost, language
interference, suboptimal training configurations, and limited extensibility. To
overcome these challenges, we introduce LoRS-Merging (low-rank and sparse model
merging), a novel technique designed to efficiently integrate models trained on
different languages or tasks while preserving performance and reducing
computational overhead. LoRS-Merging combines low-rank and sparse pruning to
retain essential structures while eliminating redundant parameters, mitigating
language and task interference, and enhancing extensibility. Experimental
results across a range of languages demonstrate that LoRS-Merging significantly
outperforms conventional multi-lingual multi-task training baselines. Our
findings suggest that model merging, particularly LoRS-Merging, is a scalable
and effective complement to traditional multi-lingual training strategies for
S2T applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, submitted to ACL 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Bridging Gaps in Natural Language Processing for Yorùbá: A
  Systematic <span class="highlight-title">Review</span> of a Decade of Progress and Prospects 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.17364v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.17364v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Toheeb A. Jimoh, Tabea De Wille, Nikola S. Nikolov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Natural Language Processing (NLP) is becoming a dominant subset of artificial
intelligence as the need to help machines understand human language looks
indispensable. Several NLP applications are ubiquitous, partly due to the
myriads of datasets being churned out daily through mediums like social
networking sites. However, the growing development has not been evident in most
African languages due to the persisting resource limitation, among other
issues. Yor\`ub\'a language, a tonal and morphologically rich African language,
suffers a similar fate, resulting in limited NLP usage. To encourage further
research towards improving this situation, this systematic literature review
aims to comprehensively analyse studies addressing NLP development for
Yor\`ub\'a, identifying challenges, resources, techniques, and applications. A
well-defined search string from a structured protocol was employed to search,
select, and analyse 105 primary studies between 2014 and 2024 from reputable
databases. The review highlights the scarcity of annotated corpora, limited
availability of pre-trained language models, and linguistic challenges like
tonal complexity and diacritic dependency as significant obstacles. It also
revealed the prominent techniques, including rule-based methods, among others.
The findings reveal a growing body of multilingual and monolingual resources,
even though the field is constrained by socio-cultural factors such as
code-switching and desertion of language for digital usage. This review
synthesises existing research, providing a foundation for advancing NLP for
Yor\`ub\'a and in African languages generally. It aims to guide future research
by identifying gaps and opportunities, thereby contributing to the broader
inclusion of Yor\`ub\'a and other under-resourced African languages in global
NLP advancements.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On Relation-Specific Neurons in Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.17355v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.17355v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yihong Liu, Runsheng Chen, Lea Hirlimann, Ahmad Dawar Hakimi, Mingyang Wang, Amir Hossein Kargaran, Sascha Rothe, François Yvon, Hinrich Schütze
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In large language models (LLMs), certain neurons can store distinct pieces of
knowledge learned during pretraining. While knowledge typically appears as a
combination of relations and entities, it remains unclear whether some neurons
focus on a relation itself -- independent of any entity. We hypothesize such
neurons detect a relation in the input text and guide generation involving such
a relation. To investigate this, we study the Llama-2 family on a chosen set of
relations with a statistics-based method. Our experiments demonstrate the
existence of relation-specific neurons. We measure the effect of selectively
deactivating candidate neurons specific to relation $r$ on the LLM's ability to
handle (1) facts whose relation is $r$ and (2) facts whose relation is a
different relation $r' \neq r$. With respect to their capacity for encoding
relation information, we give evidence for the following three properties of
relation-specific neurons. $\textbf{(i) Neuron cumulativity.}$ The neurons for
$r$ present a cumulative effect so that deactivating a larger portion of them
results in the degradation of more facts in $r$. $\textbf{(ii) Neuron
versatility.}$ Neurons can be shared across multiple closely related as well as
less related relations. Some relation neurons transfer across languages.
$\textbf{(iii) Neuron interference.}$ Deactivating neurons specific to one
relation can improve LLM generation performance for facts of other relations.
We will make our code publicly available at
https://github.com/cisnlp/relation-specific-neurons.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>preprint</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Mutual Reinforcement of LLM Dialogue Synthesis and Summarization
  Capabilities for Few-Shot Dialogue Summarization <span class="chip">NAACL 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.17328v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.17328v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yen-Ju Lu, Ting-Yao Hu, Hema Swetha Koppula, Hadi Pouransari, Jen-Hao Rick Chang, Yin Xia, Xiang Kong, Qi Zhu, Simon Wang, Oncel Tuzel, Raviteja Vemulapalli
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we propose Mutual Reinforcing Data Synthesis (MRDS) within LLMs
to improve few-shot dialogue summarization task. Unlike prior methods that
require external knowledge, we mutually reinforce the LLM\'s dialogue synthesis
and summarization capabilities, allowing them to complement each other during
training and enhance overall performances. The dialogue synthesis capability is
enhanced by directed preference optimization with preference scoring from
summarization capability. The summarization capability is enhanced by the
additional high quality dialogue-summary paired data produced by the dialogue
synthesis capability. By leveraging the proposed MRDS mechanism, we elicit the
internal knowledge of LLM in the format of synthetic data, and use it to
augment the few-shot real training dataset. Empirical results demonstrate that
our method improves dialogue summarization, achieving a 1.5% increase in ROUGE
scores and a 0.3% improvement in BERT scores in few-shot settings. Furthermore,
our method attains the highest average scores in human evaluations, surpassing
both the pre-trained models and the baselines fine-tuned solely for
summarization tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NAACL 2025 Findings</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Turning Conversations into Workflows: A Framework to Extract and
  Evaluate Dialog Workflows for Service AI Agents 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.17321v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.17321v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Prafulla Kumar Choubey, Xiangyu Peng, Shilpa Bhagavath, Caiming Xiong, Shiva Kumar Pentyala, Chien-Sheng Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Automated service agents require well-structured workflows to provide
consistent and accurate responses to customer queries. However, these workflows
are often undocumented, and their automatic extraction from conversations
remains unexplored. In this work, we present a novel framework for extracting
and evaluating dialog workflows from historical interactions. Our extraction
process consists of two key stages: (1) a retrieval step to select relevant
conversations based on key procedural elements, and (2) a structured workflow
generation process using a question-answer-based chain-of-thought (QA-CoT)
prompting. To comprehensively assess the quality of extracted workflows, we
introduce an automated agent and customer bots simulation framework that
measures their effectiveness in resolving customer issues. Extensive
experiments on the ABCD and SynthABCD datasets demonstrate that our QA-CoT
technique improves workflow extraction by 12.16\% in average macro accuracy
over the baseline. Moreover, our evaluation method closely aligns with human
assessments, providing a reliable and scalable framework for future research.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ HIPPO: Enhancing the Table Understanding Capability of Large Language
  Models through Hybrid-Modal Preference Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.17315v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.17315v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhenghao Liu, Haolan Wang, Xinze Li, Qiushi Xiong, Xiaocui Yang, Yu Gu, Yukun Yan, Qi Shi, Fangfang Li, Ge Yu, Maosong Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Tabular data contains rich structural semantics and plays a crucial role in
organizing and manipulating information. To better capture these structural
semantics, this paper introduces the HybrId-modal Preference oPtimizatiOn
(HIPPO) model, which represents tables using both text and image, and optimizes
MLLMs to effectively learn more comprehensive table information from these
multiple modalities. Specifically, HIPPO samples model responses from
hybrid-modal table representations and designs a modality-consistent sampling
strategy to enhance response diversity and mitigate modality bias during DPO
training. Experimental results on table question answering and table fact
verification tasks demonstrate the effectiveness of HIPPO, achieving a 4%
improvement over various table reasoning models. Further analysis reveals that
HIPPO not only enhances reasoning abilities based on unimodal table
representations but also facilitates the extraction of crucial and distinct
semantics from different modal representations. All data and codes are
available at https://github.com/NEUIR/HIPPO.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Implicit Word Reordering with Knowledge Distillation for Cross-Lingual
  Dependency Parsing <span class="chip">AAAI</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.17308v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.17308v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhuoran Li, Chunming Hu, Junfan Chen, Zhijun Chen, Richong Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Word order difference between source and target languages is a major obstacle
to cross-lingual transfer, especially in the dependency parsing task. Current
works are mostly based on order-agnostic models or word reordering to mitigate
this problem. However, such methods either do not leverage grammatical
information naturally contained in word order or are computationally expensive
as the permutation space grows exponentially with the sentence length.
Moreover, the reordered source sentence with an unnatural word order may be a
form of noising that harms the model learning. To this end, we propose an
Implicit Word Reordering framework with Knowledge Distillation (IWR-KD). This
framework is inspired by that deep networks are good at learning feature
linearization corresponding to meaningful data transformation, e.g. word
reordering. To realize this idea, we introduce a knowledge distillation
framework composed of a word-reordering teacher model and a dependency parsing
student model. We verify our proposed method on Universal Dependency Treebanks
across 31 different languages and show it outperforms a series of competitors,
together with experimental analysis to illustrate how our method works towards
training a robust parser.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 5 figures, 3 tables. Accepted by The 39th Annual AAAI
  Conference on Artificial Intelligence (AAAI 2025)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ `Generalization is hallucination' through the lens of tensor completions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.17305v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.17305v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Liang Ze Wong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this short position paper, we introduce tensor completions and artifacts
and make the case that they are a useful theoretical framework for
understanding certain types of hallucinations and generalizations in language
models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Child vs. machine language learning: Can the logical structure of human
  language unleash LLMs? <span class="chip">ISCA</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.17304v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.17304v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Uli Sauerland, Celia Matthaei, Felix Salfner
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We argue that human language learning proceeds in a manner that is different
in nature from current approaches to training LLMs, predicting a difference in
learning biases. We then present evidence from German plural formation by LLMs
that confirm our hypothesis that even very powerful implementations produce
results that miss aspects of the logic inherent to language that humans have no
problem with. We conclude that attention to the different structures of human
language and artificial neural networks is likely to be an avenue to improve
LLM performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ISCA/ITG Workshop on Diversity in Large Speech and Language Models</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Improving the Inclusivity of Dutch Speech Recognition by Fine-tuning
  Whisper on the JASMIN-CGN Corpus <span class="chip">ISCA</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.17284v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.17284v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Golshid Shekoufandeh, Paul Boersma, Antal van den Bosch
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We test and study the variation in speech recognition of fine-tuned versions
of the Whisper model on child, elderly and non-native Dutch speech from the
JASMIN-CGN corpus. Our primary goal is to evaluate how speakers' age and
linguistic background influence Whisper's performance. Whisper achieves varying
Word Error Rates (WER) when fine-tuned on subpopulations of specific ages and
linguistic backgrounds. Fine-tuned performance is remarkably better than
zero-shot performance, achieving a relative reduction in WER of 81% for native
children, 72% for non-native children, 67% for non-native adults, and 65% for
native elderly people. Our findings underscore the importance of training
speech recognition models like Whisper on underrepresented subpopulations such
as children, the elderly, and non-native speakers.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ISCA/ITG Workshop on Diversity in Large Speech and Language Models</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Capability Instruction Tuning: A New Paradigm for Dynamic LLM Routing <span class="chip">AAAI 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.17282v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.17282v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yi-Kai Zhang, De-Chuan Zhan, Han-Jia Ye
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have demonstrated human-like
instruction-following abilities, particularly those exceeding 100 billion
parameters. The combined capability of some smaller, resource-friendly LLMs can
address most of the instructions that larger LLMs excel at. In this work, we
explore how to route the best-performing LLM for each instruction to achieve
better overall performance. We develop a new paradigm, constructing capability
instructions with model capability representation, user instruction, and
performance inquiry prompts to assess the performance. To learn from capability
instructions, we introduce a new end-to-end framework called Model Selection
with Aptitude Test (Model-SAT), which generates positive and negative samples
based on what different models perform well or struggle with. Model-SAT uses a
model capability encoder that extends its model representation to a lightweight
LLM. Our experiments show that Model-SAT understands the performance dimensions
of candidate models and provides the probabilities of their capability to
handle various instructions. Additionally, during deployment, a new model can
quickly infer its aptitude test results across 50 tasks, each with 20 shots.
Model-SAT performs state-of-the-art model routing without candidate inference
and in real-world new model-released scenarios. The code is available at
https://github.com/Now-Join-Us/CIT-LLM-Routing
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>AAAI 2025; Project Page: https://cit-llm-routing.github.io</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Extracting domain-specific terms using contextual word embeddings 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.17278v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.17278v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andraž Repar, Nada Lavrač, Senja Pollak
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Automated terminology extraction refers to the task of extracting meaningful
terms from domain-specific texts. This paper proposes a novel machine learning
approach to terminology extraction, which combines features from traditional
term extraction systems with novel contextual features derived from contextual
word embeddings. Instead of using a predefined list of part-of-speech patterns,
we first analyse a new term-annotated corpus RSDO5 for the Slovenian language
and devise a set of rules for term candidate selection and then generate
statistical, linguistic and context-based features. We use a support-vector
machine algorithm to train a classification model, evaluate it on the four
domains (biomechanics, linguistics, chemistry, veterinary) of the RSDO5 corpus
and compare the results with state-of-art term extraction approaches for the
Slovenian language. Our approach provides significant improvements in terms of
F1 score over the previous state-of-the-art, which proves that contextual word
embeddings are valuable for improving term extraction.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MonoTODia: Translating Monologue Requests to Task-Oriented Dialogues <span class="chip">NAACL 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.17268v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.17268v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sebastian Steindl, Ulrich Schäfer, Bernd Ludwig
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Data scarcity is one of the main problems when it comes to real-world
applications of transformer-based models. This is especially evident for
task-oriented dialogue (TOD) systems, which require specialized datasets, that
are usually not readily available. This can hinder companies from adding TOD
systems to their services. This study therefore investigates a novel approach
to sourcing annotated dialogues from existing German monologue material.
Focusing on a real-world example, we investigate whether these monologues can
be transformed into dialogue formats suitable for training TOD systems. We show
the approach with the concrete example of a company specializing in travel
bookings via e-mail. We fine-tune state-of-the-art Large Language Models for
the task of rewriting e-mails as dialogues and annotating them. To ensure the
quality and validity of the generated data, we employ crowd workers to evaluate
the dialogues across multiple criteria and to provide gold-standard annotations
for the test dataset. We further evaluate the usefulness of the dialogues for
training TOD systems. Our evaluation shows that the dialogues and annotations
are of high quality and can serve as a valuable starting point for training TOD
systems. Finally, we make the annotated dataset publicly available to foster
future research.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at NAACL 2025 (Industry Track)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Unveiling Downstream Performance Scaling of LLMs: A Clustering-Based
  Perspective 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.17262v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.17262v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chengyin Xu, Kaiyuan Chen, Xiao Li, Ke Shen, Chenggang Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid advancements in computing dramatically increase the scale and cost
of training Large Language Models (LLMs). Accurately predicting downstream task
performance prior to model training is crucial for efficient resource
allocation, yet remains challenging due to two primary constraints: (1) the
"emergence phenomenon", wherein downstream performance metrics become
meaningful only after extensive training, which limits the ability to use
smaller models for prediction; (2) Uneven task difficulty distributions and the
absence of consistent scaling laws, resulting in substantial metric
variability. Existing performance prediction methods suffer from limited
accuracy and reliability, thereby impeding the assessment of potential LLM
capabilities. To address these challenges, we propose a
Clustering-On-Difficulty (COD) downstream performance prediction framework. COD
first constructs a predictable support subset by clustering tasks based on
difficulty features, strategically excluding non-emergent and non-scalable
clusters. The scores on the selected subset serve as effective intermediate
predictors of downstream performance on the full evaluation set. With
theoretical support, we derive a mapping function that transforms performance
metrics from the predictable subset to the full evaluation set, thereby
ensuring accurate extrapolation of LLM downstream performance. The proposed
method has been applied to predict performance scaling for a 70B LLM, providing
actionable insights for training resource allocation and assisting in
monitoring the training process. Notably, COD achieves remarkable predictive
accuracy on the 70B LLM by leveraging an ensemble of small models,
demonstrating an absolute mean deviation of 1.36% across eight important LLM
evaluation benchmarks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>21 pages,6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MULTITAT: Benchmarking Multilingual Table-and-Text Question Answering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.17253v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.17253v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xuanliang Zhang, Dingzirui Wang, Keyan Xu, Qingfu Zhu, Wanxiang Che
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Question answering on the hybrid context of tables and text (TATQA) is a
critical task, with broad applications in data-intensive domains. However,
existing TATQA datasets are limited to English, leading to several drawbacks:
(i) They overlook the challenges of multilingual TAT-QA and cannot assess model
performance in the multilingual setting. (ii) They do not reflect real-world
scenarios where tables and texts frequently appear in non-English languages. To
address the limitations, we propose the first multilingual TATQA dataset
(MULTITAT). Specifically, we sample data from 3 mainstream TATQA datasets and
translate it into 10 diverse languages. To align the model TATQA capabilities
in English with other languages, we develop a baseline, Ours. Experimental
results reveal that the performance on non-English data in MULTITAT drops by an
average of 19.4% compared to English, proving the necessity of MULTITAT. We
further analyze the reasons for this performance gap. Furthermore, Ours
outperforms other baselines by an average of 3.3, demonstrating its
effectiveness.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Baichuan-Audio: A Unified Framework for End-to-End Speech Interaction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.17239v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.17239v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tianpeng Li, Jun Liu, Tao Zhang, Yuanbo Fang, Da Pan, Mingrui Wang, Zheng Liang, Zehuan Li, Mingan Lin, Guosheng Dong, Jianhua Xu, Haoze Sun, Zenan Zhou, Weipeng Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce Baichuan-Audio, an end-to-end audio large language model that
seamlessly integrates audio understanding and generation. It features a
text-guided aligned speech generation mechanism, enabling real-time speech
interaction with both comprehension and generation capabilities. Baichuan-Audio
leverages a pre-trained ASR model, followed by multi-codebook discretization of
speech at a frame rate of 12.5 Hz. This multi-codebook setup ensures that
speech tokens retain both semantic and acoustic information. To further enhance
modeling, an independent audio head is employed to process audio tokens,
effectively capturing their unique characteristics. To mitigate the loss of
intelligence during pre-training and preserve the original capabilities of the
LLM, we propose a two-stage pre-training strategy that maintains language
understanding while enhancing audio modeling. Following alignment, the model
excels in real-time speech-based conversation and exhibits outstanding
question-answering capabilities, demonstrating its versatility and efficiency.
The proposed model demonstrates superior performance in real-time spoken
dialogue and exhibits strong question-answering abilities. Our code, model and
training data are available at https://github.com/baichuan-inc/Baichuan-Audio
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Making LLMs Reason? The Intermediate Language Problem in Neurosymbolic
  Approaches 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.17216v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.17216v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alexander Beiser, David Penz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Logical reasoning tasks manifest themselves as a challenge to Large Language
Models (LLMs). Neurosymbolic approaches use LLMs to translate logical reasoning
problems formulated in natural language into a formal intermediate language.
Subsequently, the usage of symbolic reasoners yields reliable solving thereof.
However, LLMs often fail in translation due to poorly chosen intermediate
languages.
  We introduce the intermediate language problem, which is the problem of
choosing a suitable formal language representation for neurosymbolic
approaches. Theoretically, we argue that its origins lie in the inability of
LLMs to distinguish syntax from semantics and the relative independence of the
problem from its representation. We showcase its existence experimentally by
contrasting two intermediate languages, Answer Set Programming and the Python
Knowledge Engine. In addition, we demonstrate the effects of varying degrees of
supplementary context information. Our results show a maximum difference in
overall-accuracy of 53.20% and 49.26% in execution-accuracy. When using the
GPT4o-mini LLM we beat the state-of-the-art in overall-accuracy on the ProntoQA
dataset by 21.20% and by 50.50% on the ProofWriter dataset.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CoT-UQ: Improving Response-wise Uncertainty Quantification in LLMs with
  Chain-of-Thought 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.17214v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.17214v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Boxuan Zhang, Ruqi Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) excel in many tasks but struggle to accurately
quantify uncertainty in their generated responses. This limitation makes it
challenging to detect misinformation and ensure reliable decision-making.
Existing uncertainty quantification (UQ) methods for LLMs are primarily
prompt-wise rather than response-wise, often requiring multiple response
samples, which incurs high computational costs. Moreover, LLMs have been shown
to be overconfident, particularly when using reasoning steps to derive their
answers. In this work, we propose CoT-UQ, a response-wise UQ framework that
integrates LLMs' inherent reasoning capabilities through Chain-of-Thought (CoT)
into the UQ process. CoT-UQ captures critical information during inference by
extracting keywords from each reasoning step and assessing their importance to
the final answer. This key reasoning information is then aggregated to produce
a final uncertainty estimate. We conduct extensive experiments based on LLaMA
Family with model sizes varying from 8B to 13B across logical and mathematical
reasoning tasks. Experimental results demonstrate that CoT-UQ significantly
outperforms existing UQ methods, achieving an average improvement of 5.9% AUROC
compared to current UQ methods. The code is available at:
https://github.com/ZBox1005/CoT-UQ.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Order Matters: Investigate the Position Bias in Multi-constraint
  Instruction Following 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.17204v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.17204v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jie Zeng, Qianyu He, Qingyu Ren, Jiaqing Liang, Yanghua Xiao, Weikang Zhou, Zeye Sun, Fei Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Real-world instructions with multiple constraints pose a significant
challenge to existing large language models (LLMs). An observation is that the
LLMs exhibit dramatic performance fluctuation when disturbing the order of the
incorporated constraints. Yet, none of the existing works has systematically
investigated this position bias problem in the field of multi-constraint
instruction following. To bridge this gap, we design a probing task where we
quantitatively measure the difficulty distribution of the constraints by a
novel Difficulty Distribution Index (CDDI). Through the experimental results,
we find that LLMs are more performant when presented with the constraints in a
``hard-to-easy'' order. This preference can be generalized to LLMs with
different architecture or different sizes of parameters. Additionally, we
conduct an explanation study, providing an intuitive insight into the
correlation between the LLM's attention and constraint orders. Our code and
dataset are publicly available at https://github.com/meowpass/PBIF.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Evaluating Expert Contributions in a MoE LLM for Quiz-Based Tasks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.17187v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.17187v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andrei Chernov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, Large Language Models (LLMs) with Mixture of Experts (MoE) layers
have gained significant attention. Currently, state-of-the-art LLMs utilize
this architecture. There is a substantial amount of research on how to train
such models and how to select hyperparameters for this architecture. However,
there is a lack of studies focusing on post-evaluation analysis of MoE layer
properties. In this paper, we take a first step toward closing this gap by
evaluating expert contributions on the quiz-based MMLU benchmark. We show that
most experts were never activated during inference on this benchmark.
Additionally, the output distribution of gating networks is much closer to
uniform than sparse. Finally, we demonstrate that the average performance of
some experts within the same layer varies significantly.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>preprint, short paper</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Measuring Data Diversity for Instruction Tuning: A Systematic Analysis
  and A Reliable Metric 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.17184v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.17184v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuming Yang, Yang Nan, Junjie Ye, Shihan Dou, Xiao Wang, Shuo Li, Huijie Lv, Tao Gui, Qi Zhang, Xuanjing Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Data diversity is crucial for the instruction tuning of large language
models. Existing studies have explored various diversity-aware data selection
methods to construct high-quality datasets and enhance model performance.
However, the fundamental problem of precisely defining and measuring data
diversity remains underexplored, limiting clear guidance for data engineering.
To address this, we systematically analyze 11 existing diversity measurement
methods by assessing their correlation with model performance through extensive
fine-tuning experiments. Our results indicate that a reliable diversity measure
should properly account for both inter-sample differences and the information
distribution in the sample space. Building on this, we propose NovelSum, a new
diversity metric based on sample-level "novelty." Experiments on both simulated
and real-world data show that NovelSum accurately captures diversity variations
and achieves a 0.97 correlation with instruction-tuned model performance,
highlighting its value in guiding data engineering practices. With NovelSum as
an optimization objective, we further develop a greedy, diversity-oriented data
selection strategy that outperforms existing approaches, validating both the
effectiveness and practical significance of our metric.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages. The related codes and resources will be released later.
  Project page: https://github.com/UmeanNever/NovelSum</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Cheems: A Practical Guidance for Building and Evaluating Chinese Reward
  Models from Scratch 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.17173v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.17173v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xueru Wen, Jie Lou, Zichao Li, Yaojie Lu, Xing Yu, Yuqiu Ji, Guohai Xu, Hongyu Lin, Ben He, Xianpei Han, Le Sun, Debing Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reward models (RMs) are crucial for aligning large language models (LLMs)
with human preferences. However, most RM research is centered on English and
relies heavily on synthetic resources, which leads to limited and less reliable
datasets and benchmarks for Chinese. To address this gap, we introduce
CheemsBench, a fully human-annotated RM evaluation benchmark within Chinese
contexts, and CheemsPreference, a large-scale and diverse preference dataset
annotated through human-machine collaboration to support Chinese RM training.
We systematically evaluate open-source discriminative and generative RMs on
CheemsBench and observe significant limitations in their ability to capture
human preferences in Chinese scenarios. Additionally, based on
CheemsPreference, we construct an RM that achieves state-of-the-art performance
on CheemsBench, demonstrating the necessity of human supervision in RM
training. Our findings reveal that scaled AI-generated data struggles to fully
capture human preferences, emphasizing the importance of high-quality human
supervision in RM development.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Logic Haystacks: Probing LLMs Long-Context Logical Reasoning (Without
  Easily Identifiable Unrelated Padding) 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.17169v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.17169v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Damien Sileo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models demonstrate promising long context processing
capabilities, with recent models touting context windows close to one million
tokens. However, the evaluations supporting these claims often involve simple
retrieval tasks or synthetic tasks padded with irrelevant text, which the
models may easily detect and discard. In this work, we generate lengthy
simplified English text with first-order logic representations spanning up to
2048 clauses (around 25k GPT-4 tokens). We formulate an evaluation task with
evidence retrieval for contradiction detection. The long, homogeneous text is
filled with distractors that are both hard to distinguish from relevant
evidences and provably not interfering with them. Our evaluation of evidence
retrieval shows that the effective context window is much smaller with
realistic distractors, already crumbling at 128 clauses.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ JUREX-4E: Juridical Expert-Annotated Four-Element Knowledge Base for
  Legal Reasoning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.17166v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.17166v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Huanghai Liu, Quzhe Huang, Qingjing Chen, Yiran Hu, Jiayu Ma, Yun Liu, Weixing Shen, Yansong Feng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Four-Element Theory is a fundamental framework in criminal law, defining
the constitution of crime through four dimensions: Subject, Object, Subjective
aspect, and Objective aspect. This theory is widely referenced in legal
reasoning, and many Large Language Models (LLMs) attempt to incorporate it when
handling legal tasks. However, current approaches rely on LLMs' internal
knowledge to incorporate this theory, often lacking completeness and
representativeness. To address this limitation, we introduce JUREX-4E, an
expert-annotated knowledge base covering 155 criminal charges. It is structured
through a progressive hierarchical annotation framework that prioritizes legal
source validity and employs diverse legal interpretation methods to ensure
comprehensiveness and authority. We evaluate JUREX-4E on the Similar Charge
Distinction task and apply it to Legal Case Retrieval, demonstrating its
effectiveness in improving LLM performance. Experimental results validate the
high quality of JUREX-4E and its substantial impact on downstream legal tasks,
underscoring its potential for advancing legal AI applications. Code:
https://github.com/THUlawtech/JUREX
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MEMERAG: A Multilingual End-to-End Meta-Evaluation Benchmark for
  Retrieval Augmented Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.17163v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.17163v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        María Andrea Cruz Blandón, Jayasimha Talur, Bruno Charron, Dong Liu, Saab Mansour, Marcello Federico
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Automatic evaluation of retrieval augmented generation (RAG) systems relies
on fine-grained dimensions like faithfulness and relevance, as judged by expert
human annotators. Meta-evaluation benchmarks support the development of
automatic evaluators that correlate well with human judgement. However,
existing benchmarks predominantly focus on English or use translated data,
which fails to capture cultural nuances. A native approach provides a better
representation of the end user experience.
  In this work, we develop a Multilingual End-to-end Meta-Evaluation RAG
benchmark (MEMERAG). Our benchmark builds on the popular MIRACL dataset, using
native-language questions and generating responses with diverse large language
models (LLMs), which are then assessed by expert annotators for faithfulness
and relevance. We describe our annotation process and show that it achieves
high inter-annotator agreement. We then analyse the performance of the
answer-generating LLMs across languages as per the human evaluators. Finally we
apply the dataset to our main use-case which is to benchmark multilingual
automatic evaluators (LLM-as-a-judge). We show that our benchmark can reliably
identify improvements offered by advanced prompting techniques and LLMs. We
release our benchmark to support the community developing accurate evaluation
methods for multilingual RAG systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Real-time Monitoring of Economic Shocks using Company Websites 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.17161v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.17161v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Michael Koenig, Jakob Rauch, Martin Woerter
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Understanding the effects of economic shocks on firms is critical for
analyzing economic growth and resilience. We introduce a Web-Based Affectedness
Indicator (WAI), a general-purpose tool for real-time monitoring of economic
disruptions across diverse contexts. By leveraging Large Language Model (LLM)
assisted classification and information extraction on texts from over five
million company websites, WAI quantifies the degree and nature of firms'
responses to external shocks. Using the COVID-19 pandemic as a specific
application, we show that WAI is highly correlated with pandemic containment
measures and reliably predicts firm performance. Unlike traditional data
sources, WAI provides timely firm-level information across industries and
geographies worldwide that would otherwise be unavailable due to institutional
and data availability constraints. This methodology offers significant
potential for monitoring and mitigating the impact of technological, political,
financial, health or environmental crises, and represents a transformative tool
for adaptive policy-making and economic resilience.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Sentiment analysis of texts from social networks based on machine
  learning methods for monitoring public sentiment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.17143v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.17143v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Arsen Tolebay Nurlanuly
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A sentiment analysis system powered by machine learning was created in this
study to improve real-time social network public opinion monitoring. For
sophisticated sentiment identification, the suggested approach combines
cutting-edge transformer-based architectures (DistilBERT, RoBERTa) with
traditional machine learning models (Logistic Regression, SVM, Naive Bayes).
The system achieved an accuracy of up to 80-85% using transformer models in
real-world scenarios after being tested using both deep learning techniques and
standard machine learning processes on annotated social media datasets.
According to experimental results, deep learning models perform noticeably
better than lexicon-based and conventional rule-based classifiers, lowering
misclassification rates and enhancing the ability to recognize nuances like
sarcasm. According to feature importance analysis, context tokens,
sentiment-bearing keywords, and part-of-speech structure are essential for
precise categorization. The findings confirm that AI-driven sentiment
frameworks can provide a more adaptive and efficient approach to modern
sentiment challenges. Despite the system's impressive performance, issues with
computing overhead, data quality, and domain-specific terminology still exist.
In order to monitor opinions on a broad scale, future research will investigate
improving computing performance, extending coverage to various languages, and
integrating real-time streaming APIs. The results demonstrate that governments,
corporations, and social researchers looking for more in-depth understanding of
public mood on digital platforms can find a reliable and adaptable answer in
AI-powered sentiment analysis.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 5 figures, 2 tables. Preprint submitted for community
  feedback</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Thus Spake Long-Context Large Language Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.17129v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.17129v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaoran Liu, Ruixiao Li, Mianqiu Huang, Zhigeng Liu, Yuerong Song, Qipeng Guo, Siyang He, Qiqi Wang, Linlin Li, Qun Liu, Yaqian Zhou, Xuanjing Huang, Xipeng Qiu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Long context is an important topic in Natural Language Processing (NLP),
running through the development of NLP architectures, and offers immense
opportunities for Large Language Models (LLMs) giving LLMs the lifelong
learning potential akin to humans. Unfortunately, the pursuit of a long context
is accompanied by numerous obstacles. Nevertheless, long context remains a core
competitive advantage for LLMs. In the past two years, the context length of
LLMs has achieved a breakthrough extension to millions of tokens. Moreover, the
research on long-context LLMs has expanded from length extrapolation to a
comprehensive focus on architecture, infrastructure, training, and evaluation
technologies.
  Inspired by the symphonic poem, Thus Spake Zarathustra, we draw an analogy
between the journey of extending the context of LLM and the attempts of humans
to transcend its mortality. In this survey, We will illustrate how LLM
struggles between the tremendous need for a longer context and its equal need
to accept the fact that it is ultimately finite. To achieve this, we give a
global picture of the lifecycle of long-context LLMs from four perspectives:
architecture, infrastructure, training, and evaluation, showcasing the full
spectrum of long-context technologies. At the end of this survey, we will
present 10 unanswered questions currently faced by long-context LLMs. We hope
this survey can serve as a systematic introduction to the research on
long-context LLMs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>a global picture of the lifecycle of long-context LLMs from four
  perspectives: architecture, infrastructure, training, and evaluation</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LettuceDetect: A Hallucination Detection Framework for RAG Applications 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.17125v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.17125v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ádám Kovács, Gábor Recski
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Retrieval Augmented Generation (RAG) systems remain vulnerable to
hallucinated answers despite incorporating external knowledge sources. We
present LettuceDetect a framework that addresses two critical limitations in
existing hallucination detection methods: (1) the context window constraints of
traditional encoder-based methods, and (2) the computational inefficiency of
LLM based approaches. Building on ModernBERT's extended context capabilities
(up to 8k tokens) and trained on the RAGTruth benchmark dataset, our approach
outperforms all previous encoder-based models and most prompt-based models,
while being approximately 30 times smaller than the best models. LettuceDetect
is a token-classification model that processes context-question-answer triples,
allowing for the identification of unsupported claims at the token level.
Evaluations on the RAGTruth corpus demonstrate an F1 score of 79.22% for
example-level detection, which is a 14.8% improvement over Luna, the previous
state-of-the-art encoder-based architecture. Additionally, the system can
process 30 to 60 examples per second on a single GPU, making it more practical
for real-world RAG applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Mobile-Agent-V: Learning Mobile Device Operation Through Video-Guided
  Multi-Agent Collaboration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.17110v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.17110v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junyang Wang, Haiyang Xu, Xi Zhang, Ming Yan, Ji Zhang, Fei Huang, Jitao Sang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid increase in mobile device usage necessitates improved automation
for seamless task management. However, many AI-driven frameworks struggle due
to insufficient operational knowledge. Manually written knowledge helps but is
labor-intensive and inefficient. To address these challenges, we introduce
Mobile-Agent-V, a framework that leverages video guidance to provide rich and
cost-effective operational knowledge for mobile automation. Mobile-Agent-V
enhances task execution capabilities by leveraging video inputs without
requiring specialized sampling or preprocessing. Mobile-Agent-V integrates a
sliding window strategy and incorporates a video agent and deep-reflection
agent to ensure that actions align with user instructions. Through this
innovative approach, users can record task processes with guidance, enabling
the system to autonomously learn and execute tasks efficiently. Experimental
results show that Mobile-Agent-V achieves a 30% performance improvement
compared to existing frameworks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages, 7 figures, 7tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ WildFrame: Comparing Framing in Humans and LLMs on Naturally Occurring
  Texts 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.17091v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.17091v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gili Lior, Liron Nacchace, Gabriel Stanovsky
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Humans are influenced by how information is presented, a phenomenon known as
the framing effect. Previous work has shown that LLMs may also be susceptible
to framing but has done so on synthetic data and did not compare to human
behavior. We introduce WildFrame, a dataset for evaluating LLM responses to
positive and negative framing, in naturally-occurring sentences, and compare
humans on the same data. WildFrame consists of 1,000 texts, first selecting
real-world statements with clear sentiment, then reframing them in either
positive or negative light, and lastly, collecting human sentiment annotations.
By evaluating eight state-of-the-art LLMs on WildFrame, we find that all models
exhibit framing effects similar to humans ($r\geq0.57$), with both humans and
models being more influenced by positive rather than negative reframing. Our
findings benefit model developers, who can either harness framing or mitigate
its effects, depending on the downstream application.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Automatically Evaluating the Paper <span class="highlight-title">Review</span>ing Capability of Large
  Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.17086v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.17086v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hyungyu Shin, Jingyu Tang, Yoonjoo Lee, Nayoung Kim, Hyunseung Lim, Ji Yong Cho, Hwajung Hong, Moontae Lee, Juho Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Peer review is essential for scientific progress, but it faces challenges
such as reviewer shortages and growing workloads. Although Large Language
Models (LLMs) show potential for providing assistance, research has reported
significant limitations in the reviews they generate. While the insights are
valuable, conducting the analysis is challenging due to the considerable time
and effort required, especially given the rapid pace of LLM developments. To
address the challenge, we developed an automatic evaluation pipeline to assess
the LLMs' paper review capability by comparing them with expert-generated
reviews. By constructing a dataset consisting of 676 OpenReview papers, we
examined the agreement between LLMs and experts in their strength and weakness
identifications. The results showed that LLMs lack balanced perspectives,
significantly overlook novelty assessment when criticizing, and produce poor
acceptance decisions. Our automated pipeline enables a scalable evaluation of
LLMs' paper review capability over time.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Systematic Weight Evaluation for Pruning Large Language Models:
  Enhancing Performance and Sustainability 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.17071v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.17071v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ashhadul Islam, Samir Brahim Belhaouari, Amine Bermak
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The exponential growth of large language models (LLMs) like ChatGPT has
revolutionized artificial intelligence, offering unprecedented capabilities in
natural language processing. However, the extensive computational resources
required for training these models have significant environmental implications,
including high carbon emissions, energy consumption, and water usage. This
research presents a novel approach to LLM pruning, focusing on the systematic
evaluation of individual weight importance throughout the training process. By
monitoring parameter evolution over time, we propose a method that effectively
reduces model size without compromising performance. Extensive experiments with
both a scaled-down LLM and a large multimodal model reveal that moderate
pruning enhances efficiency and reduces loss, while excessive pruning
drastically deteriorates model performance. These findings highlight the
critical need for optimized AI models to ensure sustainable development,
balancing technological advancement with environmental responsibility.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PrivaCI-Bench: Evaluating Privacy with Contextual Integrity and Legal
  Compliance 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.17041v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.17041v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haoran Li, Wenbin Hu, Huihao Jing, Yulin Chen, Qi Hu, Sirui Han, Tianshu Chu, Peizhao Hu, Yangqiu Song
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in generative large language models (LLMs) have enabled
wider applicability, accessibility, and flexibility. However, their reliability
and trustworthiness are still in doubt, especially for concerns regarding
individuals' data privacy. Great efforts have been made on privacy by building
various evaluation benchmarks to study LLMs' privacy awareness and robustness
from their generated outputs to their hidden representations. Unfortunately,
most of these works adopt a narrow formulation of privacy and only investigate
personally identifiable information (PII). In this paper, we follow the merit
of the Contextual Integrity (CI) theory, which posits that privacy evaluation
should not only cover the transmitted attributes but also encompass the whole
relevant social context through private information flows. We present
PrivaCI-Bench, a comprehensive contextual privacy evaluation benchmark targeted
at legal compliance to cover well-annotated privacy and safety regulations,
real court cases, privacy policies, and synthetic data built from the official
toolkit to study LLMs' privacy and safety compliance. We evaluate the latest
LLMs, including the recent reasoner models QwQ-32B and Deepseek R1. Our
experimental results suggest that though LLMs can effectively capture key CI
parameters inside a given context, they still require further advancements for
privacy compliance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project Webpage: https://hkust-knowcomp.github.io/privacy/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Language Model Re-rankers are Steered by Lexical Similarities 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.17036v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.17036v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lovisa Hagström, Ercong Nie, Ruben Halifa, Helmut Schmid, Richard Johansson, Alexander Junge
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Language model (LM) re-rankers are used to refine retrieval results for
retrieval-augmented generation (RAG). They are more expensive than lexical
matching methods like BM25 but assumed to better process semantic information.
To understand whether LM re-rankers always live up to this assumption, we
evaluate 6 different LM re-rankers on the NQ, LitQA2 and DRUID datasets. Our
results show that LM re-rankers struggle to outperform a simple BM25 re-ranker
on DRUID. Leveraging a novel separation metric based on BM25 scores, we explain
and identify re-ranker errors stemming from lexical dissimilarities. We also
investigate different methods to improve LM re-ranker performance and find
these methods mainly useful for NQ. Taken together, our work identifies and
explains weaknesses of LM re-rankers and points to the need for more
adversarial and realistic datasets for their evaluation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Understanding the Uncertainty of LLM Explanations: A Perspective Based
  on Reasoning Topology 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.17026v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.17026v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Longchao Da, Xiaoou Liu, Jiaxin Dai, Lu Cheng, Yaqing Wang, Hua Wei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Understanding the uncertainty in large language model (LLM) explanations is
important for evaluating their faithfulness and reasoning consistency, and thus
provides insights into the reliability of LLM's output regarding a question. In
this work, we propose a novel framework that quantifies uncertainty in LLM
explanations through a reasoning topology perspective. By designing a
structural elicitation strategy, we guide the LLMs to frame the explanations of
an answer into a graph topology. This process decomposes the explanations into
the knowledge related sub-questions and topology-based reasoning structures,
which allows us to quantify uncertainty not only at the semantic level but also
from the reasoning path. It further brings convenience to assess knowledge
redundancy and provide interpretable insights into the reasoning process. Our
method offers a systematic way to interpret the LLM reasoning, analyze
limitations, and provide guidance for enhancing robustness and faithfulness.
This work pioneers the use of graph-structured uncertainty measurement in LLM
explanations and demonstrates the potential of topology-based quantification.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Auto-Regressive Next-Token Prediction: In-Context Learning
  Emerges from Generalization <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.17024v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.17024v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zixuan Gong, Xiaolin Hu, Huayi Tang, Yong Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have demonstrated remarkable in-context learning
(ICL) abilities. However, existing theoretical analysis of ICL primarily
exhibits two limitations: (a) Limited i.i.d. Setting. Most studies focus on
supervised function learning tasks where prompts are constructed with i.i.d.
input-label pairs. This i.i.d. assumption diverges significantly from real
language learning scenarios where prompt tokens are interdependent. (b) Lack of
Emergence Explanation. Most literature answers what ICL does from an implicit
optimization perspective but falls short in elucidating how ICL emerges and the
impact of pre-training phase on ICL. In our paper, to extend (a), we adopt a
more practical paradigm, auto-regressive next-token prediction (AR-NTP), which
closely aligns with the actual training of language models. Specifically,
within AR-NTP, we emphasize prompt token-dependency, which involves predicting
each subsequent token based on the preceding sequence. To address (b), we
formalize a systematic pre-training and ICL framework, highlighting the
layer-wise structure of sequences and topics, alongside a two-level
expectation. In conclusion, we present data-dependent, topic-dependent and
optimization-dependent PAC-Bayesian generalization bounds for pre-trained LLMs,
investigating that ICL emerges from the generalization of sequences and topics.
Our theory is supported by experiments on numerical linear dynamic systems,
synthetic GINC and real-world language datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published at ICLR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Quantifying Logical Consistency in <span class="highlight-title">Transformer</span>s via Query-Key Alignment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.17017v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.17017v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Eduard Tulchinskii, Anastasia Voznyuk, Laida Kushnareva, Andrei Andriiainen, Irina Piontkovskaya, Evgeny Burnaev, Serguei Barannikov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have demonstrated impressive performance in
various natural language processing tasks, yet their ability to perform
multi-step logical reasoning remains an open challenge. Although
Chain-of-Thought prompting has improved logical reasoning by enabling models to
generate intermediate steps, it lacks mechanisms to assess the coherence of
these logical transitions. In this paper, we propose a novel, lightweight
evaluation strategy for logical reasoning that uses query-key alignments inside
transformer attention heads. By computing a single forward pass and extracting
a "QK-score" from carefully chosen heads, our method reveals latent
representations that reliably separate valid from invalid inferences, offering
a scalable alternative to traditional ablation-based techniques. We also
provide an empirical validation on multiple logical reasoning benchmarks,
demonstrating improved robustness of our evaluation method against distractors
and increased reasoning depth. The experiments were conducted on a diverse set
of models, ranging from 1.5B to 70B parameters.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Predicting Liquidity-Aware Bond Yields using Causal GANs and Deep
  Reinforcement Learning with LLM Evaluation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.17011v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.17011v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jaskaran Singh Walia, Aarush Sinha, Srinitish Srinivasan, Srihari Unnikrishnan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Financial bond yield forecasting is challenging due to data scarcity,
nonlinear macroeconomic dependencies, and evolving market conditions. In this
paper, we propose a novel framework that leverages Causal Generative
Adversarial Networks (CausalGANs) and Soft Actor-Critic (SAC) reinforcement
learning (RL) to generate high-fidelity synthetic bond yield data for four
major bond categories (AAA, BAA, US10Y, Junk). By incorporating 12 key
macroeconomic variables, we ensure statistical fidelity by preserving essential
market properties. To transform this market dependent synthetic data into
actionable insights, we employ a finetuned Large Language Model (LLM)
Qwen2.5-7B that generates trading signals (BUY/HOLD/SELL), risk assessments,
and volatility projections. We use automated, human and LLM evaluations, all of
which demonstrate that our framework improves forecasting performance over
existing methods, with statistical validation via predictive accuracy, MAE
evaluation(0.103%), profit/loss evaluation (60% profit rate), LLM evaluation
(3.37/5) and expert assessments scoring 4.67 out of 5. The reinforcement
learning-enhanced synthetic data generation achieves the least Mean Absolute
Error of 0.103, demonstrating its effectiveness in replicating real-world bond
market dynamics. We not only enhance data-driven trading strategies but also
provides a scalable, high-fidelity synthetic financial data pipeline for risk &
volatility management and investment decision-making. This work establishes a
bridge between synthetic data generation, LLM driven financial forecasting, and
language model evaluation, contributing to AI-driven financial decision-making.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FADE: Why Bad Descriptions Happen to Good Features 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.16994v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.16994v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bruno Puri, Aakriti Jain, Elena Golimblevskaia, Patrick Kahardipraja, Thomas Wiegand, Wojciech Samek, Sebastian Lapuschkin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in mechanistic interpretability have highlighted the
potential of automating interpretability pipelines in analyzing the latent
representations within LLMs. While they may enhance our understanding of
internal mechanisms, the field lacks standardized evaluation methods for
assessing the validity of discovered features. We attempt to bridge this gap by
introducing FADE: Feature Alignment to Description Evaluation, a scalable
model-agnostic framework for evaluating feature-description alignment. FADE
evaluates alignment across four key metrics - Clarity, Responsiveness, Purity,
and Faithfulness - and systematically quantifies the causes for the
misalignment of feature and their description. We apply FADE to analyze
existing open-source feature descriptions, and assess key components of
automated interpretability pipelines, aiming to enhance the quality of
descriptions. Our findings highlight fundamental challenges in generating
feature descriptions, particularly for SAEs as compared to MLP neurons,
providing insights into the limitations and future directions of automated
interpretability. We release FADE as an open-source package at:
https://github.com/brunibrun/FADE.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ All-in-one: Understanding and Generation in Multimodal Reasoning with
  the MAIA Benchmark 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.16989v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.16989v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Davide Testa, Giovanni Bonetta, Raffaella Bernardi, Alessandro Bondielli, Alessandro Lenci, Alessio Miaschi, Lucia Passaro, Bernardo Magnini
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce MAIA (Multimodal AI Assessment), a native-Italian benchmark
designed for fine-grained investigation of the reasoning abilities of visual
language models on videos. MAIA differs from other available video benchmarks
for its design, its reasoning categories, the metric it uses and the language
and culture of the videos. It evaluates Vision Language Models (VLMs) on two
aligned tasks: a visual statement verification task, and an open-ended visual
question-answering task, both on the same set of video-related questions. It
considers twelve reasoning categories that aim to disentangle language and
vision relations by highlight when one of two alone encodes sufficient
information to solve the tasks, when they are both needed and when the full
richness of the short video is essential instead of just a part of it. Thanks
to its carefully taught design, it evaluates VLMs' consistency and visually
grounded natural language comprehension and generation simultaneously through
an aggregated metric. Last but not least, the video collection has been
carefully selected to reflect the Italian culture and the language data are
produced by native-speakers.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Hotter and Colder: A New Approach to Annotating Sentiment, Emotions, and
  Bias in Icelandic Blog Comments 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.16987v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.16987v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Steinunn Rut Friðriksdóttir, Dan Saattrup Nielsen, Hafsteinn Einarsson
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents Hotter and Colder, a dataset designed to analyze various
types of online behavior in Icelandic blog comments. Building on previous work,
we used GPT-4o mini to annotate approximately 800,000 comments for 25 tasks,
including sentiment analysis, emotion detection, hate speech, and group
generalizations. Each comment was automatically labeled on a 5-point Likert
scale. In a second annotation stage, comments with high or low probabilities of
containing each examined behavior were subjected to manual revision. By
leveraging crowdworkers to refine these automatically labeled comments, we
ensure the quality and accuracy of our dataset resulting in 12,232 uniquely
annotated comments and 19,301 annotations. Hotter and Colder provides an
essential resource for advancing research in content moderation and
automatically detectiong harmful online behaviors in Icelandic.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To be published in the proceedings of the NoDaLiDa/Baltic-HLT 2025
  conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Muon is Scalable for LLM Training 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.16982v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.16982v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jingyuan Liu, Jianlin Su, Xingcheng Yao, Zhejun Jiang, Guokun Lai, Yulun Du, Yidao Qin, Weixin Xu, Enzhe Lu, Junjie Yan, Yanru Chen, Huabin Zheng, Yibo Liu, Shaowei Liu, Bohong Yin, Weiran He, Han Zhu, Yuzhi Wang, Jianzhou Wang, Mengnan Dong, Zheng Zhang, Yongsheng Kang, Hao Zhang, Xinran Xu, Yutao Zhang, Yuxin Wu, Xinyu Zhou, Zhilin Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, the Muon optimizer based on matrix orthogonalization has
demonstrated strong results in training small-scale language models, but the
scalability to larger models has not been proven. We identify two crucial
techniques for scaling up Muon: (1) adding weight decay and (2) carefully
adjusting the per-parameter update scale. These techniques allow Muon to work
out-of-the-box on large-scale training without the need of hyper-parameter
tuning. Scaling law experiments indicate that Muon achieves $\sim\!2\times$
computational efficiency compared to AdamW with compute optimal training.
  Based on these improvements, we introduce Moonlight, a 3B/16B-parameter
Mixture-of-Expert (MoE) model trained with 5.7T tokens using Muon. Our model
improves the current Pareto frontier, achieving better performance with much
fewer training FLOPs compared to prior models.
  We open-source our distributed Muon implementation that is memory optimal and
communication efficient. We also release the pretrained, instruction-tuned, and
intermediate checkpoints to support future research.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LongSafety: Evaluating Long-Context Safety of Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.16971v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.16971v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yida Lu, Jiale Cheng, Zhexin Zhang, Shiyao Cui, Cunxiang Wang, Xiaotao Gu, Yuxiao Dong, Jie Tang, Hongning Wang, Minlie Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As Large Language Models (LLMs) continue to advance in understanding and
generating long sequences, new safety concerns have been introduced through the
long context. However, the safety of LLMs in long-context tasks remains
under-explored, leaving a significant gap in both evaluation and improvement of
their safety. To address this, we introduce LongSafety, the first comprehensive
benchmark specifically designed to evaluate LLM safety in open-ended
long-context tasks. LongSafety encompasses 7 categories of safety issues and 6
user-oriented long-context tasks, with a total of 1,543 test cases, averaging
5,424 words per context. Our evaluation towards 16 representative LLMs reveals
significant safety vulnerabilities, with most models achieving safety rates
below 55%. Our findings also indicate that strong safety performance in
short-context scenarios does not necessarily correlate with safety in
long-context tasks, emphasizing the unique challenges and urgency of improving
long-context safety. Moreover, through extensive analysis, we identify
challenging safety issues and task types for long-context models. Furthermore,
we find that relevant context and extended input sequences can exacerbate
safety risks in long-context scenarios, highlighting the critical need for
ongoing attention to long-context safety challenges. Our code and data are
available at https://github.com/thu-coai/LongSafety.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ UrduLLaMA 1.0: <span class="highlight-title">Dataset</span> Curation, Preprocessing, and Evaluation in
  Low-Resource Settings 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.16961v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.16961v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Layba Fiaz, Munief Hassan Tahir, Sana Shams, Sarmad Hussain
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multilingual Large Language Models (LLMs) often provide suboptimal
performance on low-resource languages like Urdu. This paper introduces
UrduLLaMA 1.0, a model derived from the open-source Llama-3.1-8B-Instruct
architecture and continually pre-trained on 128 million Urdu tokens, capturing
the rich diversity of the language. To enhance instruction-following and
translation capabilities, we leverage Low-Rank Adaptation (LoRA) to fine tune
the model on 41,000 Urdu instructions and approximately 50,000 English-Urdu
translation pairs. Evaluation across three machine translation datasets
demonstrates significant performance improvements compared to state-of-the-art
(SOTA) models, establishing a new benchmark for Urdu LLMs. These findings
underscore the potential of targeted adaptation strategies with limited data
and computational resources to address the unique challenges of low-resource
languages.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SparseTransX: Efficient Training of Translation-Based Knowledge Graph
  Embeddings Using Sparse Matrix Operations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.16949v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.16949v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Md Saidul Hoque Anik, Ariful Azad
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Knowledge graph (KG) learning offers a powerful framework for generating new
knowledge and making inferences. Training KG embedding can take a significantly
long time, especially for larger datasets. Our analysis shows that the gradient
computation of embedding is one of the dominant functions in the
translation-based KG embedding training loop. We address this issue by
replacing the core embedding computation with SpMM (Sparse-Dense Matrix
Multiplication) kernels. This allows us to unify multiple scatter (and gather)
operations as a single operation, reducing training time and memory usage. We
create a general framework for training KG models using sparse kernels and
implement four models, namely TransE, TransR, TransH, and TorusE. Our sparse
implementations exhibit up to 5.3x speedup on the CPU and up to 4.2x speedup on
the GPU with a significantly low GPU memory footprint. The speedups are
consistent across large and small datasets for a given model. Our proposed
sparse approach can also be extended to accelerate other translation-based
(such as TransC, TransM, etc.) and non-translational (such as DistMult,
ComplEx, RotatE, etc.) models as well.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Using Machine Learning to Detect Fraudulent SMSs in Chichewa 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.16947v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.16947v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Amelia Taylor, Amoss Robert
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  SMS enabled fraud is of great concern globally. Building classifiers based on
machine learning for SMS fraud requires the use of suitable datasets for model
training and validation. Most research has centred on the use of datasets of
SMSs in English. This paper introduces a first dataset for SMS fraud detection
in Chichewa, a major language in Africa, and reports on experiments with
machine learning algorithms for classifying SMSs in Chichewa as fraud or
non-fraud. We answer the broader research question of how feasible it is to
develop machine learning classification models for Chichewa SMSs. To do that,
we created three datasets. A small dataset of SMS in Chichewa was collected
through primary research from a segment of the young population. We applied a
label-preserving text transformations to increase its size. The enlarged
dataset was translated into English using two approaches: human translation and
machine translation. The Chichewa and the translated datasets were subjected to
machine classification using random forest and logistic regression. Our
findings indicate that both models achieved a promising accuracy of over 96% on
the Chichewa dataset. There was a drop in performance when moving from the
Chichewa to the translated dataset. This highlights the importance of data
preprocessing, especially in multilingual or cross-lingual NLP tasks, and shows
the challenges of relying on machine-translated text for training machine
learning models. Our results underscore the importance of developing language
specific models for SMS fraud detection to optimise accuracy and performance.
Since most machine learning models require data preprocessing, it is essential
to investigate the impact of the reliance on English-specific tools for data
preprocessing.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ NUTSHELL: A <span class="highlight-title">Dataset</span> for Abstract Generation from Scientific Talks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.16942v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.16942v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Maike Züfle, Sara Papi, Beatrice Savoldi, Marco Gaido, Luisa Bentivogli, Jan Niehues
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Scientific communication is receiving increasing attention in natural
language processing, especially to help researches access, summarize, and
generate content. One emerging application in this area is Speech-to-Abstract
Generation (SAG), which aims to automatically generate abstracts from recorded
scientific presentations. SAG enables researchers to efficiently engage with
conference talks, but progress has been limited by a lack of large-scale
datasets. To address this gap, we introduce NUTSHELL, a novel multimodal
dataset of *ACL conference talks paired with their corresponding abstracts. We
establish strong baselines for SAG and evaluate the quality of generated
abstracts using both automatic metrics and human judgments. Our results
highlight the challenges of SAG and demonstrate the benefits of training on
NUTSHELL. By releasing NUTSHELL under an open license (CC-BY 4.0), we aim to
advance research in SAG and foster the development of improved models and
evaluation methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Reasoning Does Not Necessarily Improve Role-Playing Ability 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.16940v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.16940v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiachong Feng, Longxu Dou, Lingpeng Kong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The application of role-playing large language models (LLMs) is rapidly
expanding in both academic and commercial domains, driving an increasing demand
for high-precision role-playing models. Simultaneously, the rapid advancement
of reasoning techniques has continuously pushed the performance boundaries of
LLMs. This intersection of practical role-playing demands and evolving
reasoning capabilities raises an important research question: "Can reasoning
techniques enhance the role-playing capabilities of LLMs?" To address this, we
conduct a comprehensive study using 6 role-playing benchmarks, 24 LLMs, and 3
distinct role-playing strategies, comparing the effectiveness of direct
zero-shot role-playing, role-playing with Chain-of-Thought (CoT), and
role-playing using reasoning-optimized LLMs. Our findings reveal that CoT may
reduce role-playing performance, reasoning-optimized LLMs are unsuitable for
role-playing, reasoning ability disrupts the role-playing scaling law, large
models still lack proficiency in advanced role-playing, and Chinese
role-playing performance surpasses English role-playing performance.
Furthermore, based on extensive experimental results, we propose two promising
future research directions: Role-aware CoT for improving role-playing LLMs and
Reinforcement Learning for role-playing LLMs, aiming to enhance the
adaptability, consistency, and effectiveness of role-playing LLMs for both
research and real-world applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Systematic <span class="highlight-title">Survey</span> of Automatic <span class="highlight-title">Prompt</span> Optimization Techniques 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.16923v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.16923v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kiran Ramnath, Kang Zhou, Sheng Guan, Soumya Smruti Mishra, Xuan Qi, Zhengyuan Shen, Shuai Wang, Sangmin Woo, Sullam Jeoung, Yawei Wang, Haozhu Wang, Han Ding, Yuzhe Lu, Zhichao Xu, Yun Zhou, Balasubramaniam Srinivasan, Qiaojing Yan, Yueyan Chen, Haibo Ding, Panpan Xu, Lin Lee Cheong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Since the advent of large language models (LLMs), prompt engineering has been
a crucial step for eliciting desired responses for various Natural Language
Processing (NLP) tasks. However, prompt engineering remains an impediment for
end users due to rapid advances in models, tasks, and associated best
practices. To mitigate this, Automatic Prompt Optimization (APO) techniques
have recently emerged that use various automated techniques to help improve the
performance of LLMs on various tasks. In this paper, we present a comprehensive
survey summarizing the current progress and remaining challenges in this field.
We provide a formal definition of APO, a 5-part unifying framework, and then
proceed to rigorously categorize all relevant works based on their salient
features therein. We hope to spur further research guided by our framework.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 main pages, 31 total pages, 1 figure</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Benchmarking Temporal Reasoning and Alignment Across Chinese Dynasties 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.16922v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.16922v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhenglin Wang, Jialong Wu, Pengfei LI, Yong Jiang, Deyu Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Temporal reasoning is fundamental to human cognition and is crucial for
various real-world applications. While recent advances in Large Language Models
have demonstrated promising capabilities in temporal reasoning, existing
benchmarks primarily rely on rule-based construction, lack contextual depth,
and involve a limited range of temporal entities. To address these limitations,
we introduce Chinese Time Reasoning (CTM), a benchmark designed to evaluate
LLMs on temporal reasoning within the extensive scope of Chinese dynastic
chronology. CTM emphasizes cross-entity relationships, pairwise temporal
alignment, and contextualized and culturally-grounded reasoning, providing a
comprehensive evaluation. Extensive experimental results reveal the challenges
posed by CTM and highlight potential avenues for improvement.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SS-MPC: A Sequence-Structured Multi-Party Conversation System 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.16920v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.16920v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yoonjin Jang, Keunha Kim, Youngjoong Ko
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent Multi-Party Conversation (MPC) models typically rely on graph-based
approaches to capture dialogue structures. However, these methods have
limitations, such as information loss during the projection of utterances into
structural embeddings and constraints in leveraging pre-trained language models
directly. In this paper, we propose \textbf{SS-MPC}, a response generation
model for MPC that eliminates the need for explicit graph structures. Unlike
existing models that depend on graphs to analyze conversation structures,
SS-MPC internally encodes the dialogue structure as a sequential input,
enabling direct utilization of pre-trained language models. Experimental
results show that \textbf{SS-MPC} achieves \textbf{15.60\% BLEU-1} and
\textbf{12.44\% ROUGE-L} score, outperforming the current state-of-the-art MPC
response generation model by \textbf{3.91\%p} in \textbf{BLEU-1} and
\textbf{0.62\%p} in \textbf{ROUGE-L}. Additionally, human evaluation confirms
that SS-MPC generates more fluent and accurate responses compared to existing
MPC models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Dependency Parsing with the Structuralized <span class="highlight-title">Prompt</span> Template 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.16919v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.16919v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Keunha Kim, Youngjoong Ko
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Dependency parsing is a fundamental task in natural language processing
(NLP), aiming to identify syntactic dependencies and construct a syntactic tree
for a given sentence. Traditional dependency parsing models typically construct
embeddings and utilize additional layers for prediction. We propose a novel
dependency parsing method that relies solely on an encoder model with a
text-to-text training approach. To facilitate this, we introduce a structured
prompt template that effectively captures the structural information of
dependency trees. Our experimental results demonstrate that the proposed method
achieves outstanding performance compared to traditional models, despite
relying solely on a pre-trained model. Furthermore, this method is highly
adaptable to various pre-trained models across different target languages and
training environments, allowing easy integration of task-specific features.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AutoLogi: Automated Generation of Logic Puzzles for Evaluating Reasoning
  Abilities of Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.16906v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.16906v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qin Zhu, Fei Huang, Runyu Peng, Keming Lu, Bowen Yu, Qinyuan Cheng, Xipeng Qiu, Xuanjing Huang, Junyang Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While logical reasoning evaluation of Large Language Models (LLMs) has
attracted significant attention, existing benchmarks predominantly rely on
multiple-choice formats that are vulnerable to random guessing, leading to
overestimated performance and substantial performance fluctuations. To obtain
more accurate assessments of models' reasoning capabilities, we propose an
automated method for synthesizing open-ended logic puzzles, and use it to
develop a bilingual benchmark, AutoLogi. Our approach features program-based
verification and controllable difficulty levels, enabling more reliable
evaluation that better distinguishes models' reasoning abilities. Extensive
evaluation of eight modern LLMs shows that AutoLogi can better reflect true
model capabilities, with performance scores spanning from 35% to 73% compared
to the narrower range of 21% to 37% on the source multiple-choice dataset.
Beyond benchmark creation, this synthesis method can generate high-quality
training data by incorporating program verifiers into the rejection sampling
process, enabling systematic enhancement of LLMs' reasoning capabilities across
diverse datasets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GuidedBench: Equipping Jailbreak Evaluation with Guidelines 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.16903v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.16903v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruixuan Huang, Xunguang Wang, Zongjie Li, Daoyuan Wu, Shuai Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Jailbreaking methods for large language models (LLMs) have gained increasing
attention for building safe and responsible AI systems. After analyzing 35
jailbreak methods across six categories, we find that existing benchmarks,
relying on universal LLM-based or keyword-matching scores, lack case-specific
criteria, leading to conflicting results. In this paper, we introduce a more
robust evaluation framework for jailbreak methods, with a curated harmful
question dataset, detailed case-by-case evaluation guidelines, and a scoring
system equipped with these guidelines. Our experiments show that existing
jailbreak methods exhibit better discrimination when evaluated using our
benchmark. Some jailbreak methods that claim to achieve over 90% attack success
rate (ASR) on other benchmarks only reach a maximum of 30.2% on our benchmark,
providing a higher ceiling for more advanced jailbreak research; furthermore,
using our scoring system reduces the variance of disagreements between
different evaluator LLMs by up to 76.33%. This demonstrates its ability to
provide more fair and stable evaluation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Homepage: https://sproutnan.github.io/AI-Safety_Benchmark/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Char-mander Use mBackdoor! A Study of Cross-lingual Backdoor Attacks in
  Multilingual LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.16901v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.16901v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Himanshu Beniwal, Sailesh Panda, Mayank Singh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We explore Cross-lingual Backdoor ATtacks (X-BAT) in multilingual Large
Language Models (mLLMs), revealing how backdoors inserted in one language can
automatically transfer to others through shared embedding spaces. Using
toxicity classification as a case study, we demonstrate that attackers can
compromise multilingual systems by poisoning data in a single language, with
rare tokens serving as specific effective triggers. Our findings expose a
critical vulnerability in the fundamental architecture that enables
cross-lingual transfer in these models. Our code and data are publicly
available at https://github.com/himanshubeniwal/X-BAT.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Make LoRA Great Again: Boosting LoRA with Adaptive Singular Values and
  Mixture-of-Experts Optimization Alignment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.16894v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.16894v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chenghao Fan, Zhenyi Lu, Sichen Liu, Xiaoye Qu, Wei Wei, Chengfeng Gu, Yu Cheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While Low-Rank Adaptation (LoRA) enables parameter-efficient fine-tuning for
Large Language Models (LLMs), its performance often falls short of Full
Fine-Tuning (Full FT). Current methods optimize LoRA by initializing with
static singular value decomposition (SVD) subsets, leading to suboptimal
leveraging of pre-trained knowledge. Another path for improving LoRA is
incorporating a Mixture-of-Experts (MoE) architecture. However, weight
misalignment and complex gradient dynamics make it challenging to adopt SVD
prior to the LoRA MoE architecture. To mitigate these issues, we propose
\underline{G}reat L\underline{o}R\underline{A} Mixture-of-Exper\underline{t}
(GOAT), a framework that (1) adaptively integrates relevant priors using an
SVD-structured MoE, and (2) aligns optimization with full fine-tuned MoE by
deriving a theoretical scaling factor. We demonstrate that proper scaling,
without modifying the architecture or training algorithms, boosts LoRA MoE's
efficiency and performance. Experiments across 25 datasets, including natural
language understanding, commonsense reasoning, image classification, and
natural language generation, demonstrate GOAT's state-of-the-art performance,
closing the gap with Full FT.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Applying LLMs to Active Learning: Towards Cost-Efficient Cross-Task Text
  Classification without Manually Labeled Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.16892v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.16892v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yejian Zhang, Shingo Takada
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Machine learning-based classifiers have been used for text classification,
such as sentiment analysis, news classification, and toxic comment
classification. However, supervised machine learning models often require large
amounts of labeled data for training, and manual annotation is both
labor-intensive and requires domain-specific knowledge, leading to relatively
high annotation costs. To address this issue, we propose an approach that
integrates large language models (LLMs) into an active learning framework. Our
approach combines the Robustly Optimized BERT Pretraining Approach (RoBERTa),
Generative Pre-trained Transformer (GPT), and active learning, achieving high
cross-task text classification performance without the need for any manually
labeled data. Furthermore, compared to directly applying GPT for classification
tasks, our approach retains over 93% of its classification performance while
requiring only approximately 6% of the computational time and monetary cost,
effectively balancing performance and resource efficiency. These findings
provide new insights into the efficient utilization of LLMs and active learning
algorithms in text classification tasks, paving the way for their broader
application.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Statement in Accordance with IEEE Preprint Policy: This work is
  intended for submission to the IEEE for possible publication</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DBudgetKV: Dynamic Budget in KV Cache Compression for Ensuring Optimal
  Performance 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.16886v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.16886v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xuanfan Ni, Liyan Xu, Chenyang Lyu, Longyue Wang, Mo Yu, Lemao Liu, Fandong Meng, Jie Zhou, Piji Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To alleviate memory burden during inference of large language models (LLMs),
numerous studies have focused on compressing the KV cache by exploring aspects
such as attention sparsity. However, these techniques often require a
pre-defined cache budget; as the optimal budget varies with different input
lengths and task types, it limits their practical deployment accepting
open-domain instructions. To address this limitation, we propose a new KV cache
compression objective: to always ensure the full-cache performance regardless
of specific inputs, while maximizing KV cache pruning as much as possible. To
achieve this goal, we introduce a novel KV cache compression method dubbed
DBudgetKV, which features an attention-based metric to signal when the
remaining KV cache is unlikely to match the full-cache performance, then
halting the pruning process. Empirical evaluation spanning diverse context
lengths, task types, and model sizes suggests that our method achieves lossless
KV pruning effectively and robustly, exceeding 25% compression ratio on
average. Furthermore, our method is easy to integrate within LLM inference, not
only optimizing memory space, but also showing reduced inference time compared
to existing methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CORAL: Learning Consistent Representations across Multi-step Training
  with Lighter Speculative Drafter 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.16880v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.16880v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yepeng Weng, Dianwen Mei, Huishi Qiu, Xujie Chen, Li Liu, Jiang Tian, Zhongchao Shi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Speculative decoding is a powerful technique that accelerates Large Language
Model (LLM) inference by leveraging a lightweight speculative draft model.
However, existing designs suffers in performance due to misalignment between
training and inference. Recent methods have tried to solve this issue by
adopting a multi-step training strategy, but the complex inputs of different
training steps make it harder for the draft model to converge. To address this,
we propose CORAL, a novel framework that improves both accuracy and efficiency
in speculative drafting. CORAL introduces Cross-Step Representation Alignment,
a method that enhances consistency across multiple training steps,
significantly improving speculative drafting performance. Additionally, we
identify the LM head as a major bottleneck in the inference speed of the draft
model. We introduce a weight-grouping mechanism that selectively activates a
subset of LM head parameters during inference, substantially reducing the
latency of the draft model. We evaluate CORAL on three LLM families and three
benchmark datasets, achieving speedup ratios of 2.50x-4.07x, outperforming
state-of-the-art methods such as EAGLE-2 and HASS. Our results demonstrate that
CORAL effectively mitigates training-inference misalignment and delivers
significant speedup for modern LLMs with large vocabularies.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under Review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LongAttn: Selecting Long-context Training Data via Token-level Attention 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.16860v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.16860v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Longyun Wu, Dawei Zhu, Guangxiang Zhao, Zhuocheng Yu, Junfeng Ran, Xiangyu Wong, Lin Sun, Sujian Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the development of large language models (LLMs), there has been an
increasing need for significant advancements in handling long contexts. To
enhance long-context capabilities, constructing high-quality training data with
long-range dependencies is crucial. Existing methods to select long-context
data often rely on sentence-level analysis, which can be greatly optimized in
both performance and efficiency. In this paper, we propose a novel token-level
framework, LongAttn, which leverages the self-attention mechanism of LLMs to
measure the long-range dependencies for the data. By calculating token-level
dependency strength and distribution uniformity of token scores, LongAttn
effectively quantifies long-range dependencies, enabling more accurate and
efficient data selection. We filter LongABC-32K from open-source long-context
datasets (ArXiv, Book, and Code). Through our comprehensive experiments,
LongAttn has demonstrated its excellent effectiveness, scalability, and
efficiency. To facilitate future research in long-context data, we released our
code and the high-quality long-context training data LongABC-32K.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Sarang at DEFACTIFY 4.0: Detecting AI-Generated Text Using Noised Data
  and an Ensemble of De<span class="highlight-title">BERT</span>a Models <span class="chip">AAAI-25</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.16857v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.16857v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Avinash Trivedi, Sangeetha Sivanesan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents an effective approach to detect AI-generated text,
developed for the Defactify 4.0 shared task at the fourth workshop on
multimodal fact checking and hate speech detection. The task consists of two
subtasks: Task-A, classifying whether a text is AI generated or human written,
and Task-B, classifying the specific large language model that generated the
text. Our team (Sarang) achieved the 1st place in both tasks with F1 scores of
1.0 and 0.9531, respectively. The methodology involves adding noise to the
dataset to improve model robustness and generalization. We used an ensemble of
DeBERTa models to effectively capture complex patterns in the text. The result
indicates the effectiveness of our noise-driven and ensemble-based approach,
setting a new standard in AI-generated text detection and providing guidance
for future developments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>AAAI-25 DEFACTIFY 4.0 Workshop AI generated text detection (1st Rank)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Improving LLM General Preference Alignment via Optimistic Online Mirror
  Descent 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.16852v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.16852v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuheng Zhang, Dian Yu, Tao Ge, Linfeng Song, Zhichen Zeng, Haitao Mi, Nan Jiang, Dong Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reinforcement learning from human feedback (RLHF) has demonstrated remarkable
effectiveness in aligning large language models (LLMs) with human preferences.
Many existing alignment approaches rely on the Bradley-Terry (BT) model
assumption, which assumes the existence of a ground-truth reward for each
prompt-response pair. However, this assumption can be overly restrictive when
modeling complex human preferences. In this paper, we drop the BT model
assumption and study LLM alignment under general preferences, formulated as a
two-player game. Drawing on theoretical insights from learning in games, we
integrate optimistic online mirror descent into our alignment framework to
approximate the Nash policy. Theoretically, we demonstrate that our approach
achieves an $O(T^{-1})$ bound on the duality gap, improving upon the previous
$O(T^{-1/2})$ result. More importantly, we implement our method and show
through experiments that it outperforms state-of-the-art RLHF algorithms across
multiple representative benchmarks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ "Actionable Help" in Crises: A Novel <span class="highlight-title">Dataset</span> and Resource-Efficient
  Models for Identifying Request and Offer Social Media Posts 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.16839v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.16839v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rabindra Lamsal, Maria Rodriguez Read, Shanika Karunasekera, Muhammad Imran
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  During crises, social media serves as a crucial coordination tool, but the
vast influx of posts--from "actionable" requests and offers to generic content
like emotional support, behavioural guidance, or outdated
information--complicates effective classification. Although generative LLMs
(Large Language Models) can address this issue with few-shot classification,
their high computational demands limit real-time crisis response. While
fine-tuning encoder-only models (e.g., BERT) is a popular choice, these models
still exhibit higher inference times in resource-constrained environments.
Moreover, although distilled variants (e.g., DistilBERT) exist, they are not
tailored for the crisis domain. To address these challenges, we make two key
contributions. First, we present CrisisHelpOffer, a novel dataset of 101k
tweets collaboratively labelled by generative LLMs and validated by humans,
specifically designed to distinguish actionable content from noise. Second, we
introduce the first crisis-specific mini models optimized for deployment in
resource-constrained settings. Across 13 crisis classification tasks, our mini
models surpass BERT (also outperform or match the performance of RoBERTa,
MPNet, and BERTweet), offering higher accuracy with significantly smaller sizes
and faster speeds. The Medium model is 47% smaller with 3.8% higher accuracy at
3.5x speed, the Small model is 68% smaller with a 1.8% accuracy gain at 7.7x
speed, and the Tiny model, 83% smaller, matches BERT's accuracy at 18.6x speed.
All models outperform existing distilled variants, setting new benchmarks.
Finally, as a case study, we analyze social media posts from a global crisis to
explore help-seeking and assistance-offering behaviours in selected developing
and developed countries.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ REGen: A Reliable Evaluation Framework for Generative Event Argument
  Extraction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.16838v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.16838v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Omar Sharif, Joseph Gatto, Madhusudan Basak, Sarah M. Preum
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Event argument extraction identifies arguments for predefined event roles in
text. Traditional evaluations rely on exact match (EM), requiring predicted
arguments to match annotated spans exactly. However, this approach fails for
generative models like large language models (LLMs), which produce diverse yet
semantically accurate responses. EM underestimates performance by disregarding
valid variations, implicit arguments (unstated but inferable), and scattered
arguments (distributed across a document). To bridge this gap, we introduce
Reliable Evaluation framework for Generative event argument extraction (REGen),
a framework that better aligns with human judgment. Across six datasets, REGen
improves performance by an average of 23.93 F1 points over EM. Human validation
further confirms REGen's effectiveness, achieving 87.67% alignment with human
assessments of argument correctness.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 pages, 9 figures, 13 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Finding the Sweet Spot: Preference Data Construction for Scaling
  Preference Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.16825v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.16825v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yao Xiao, Hai Ye, Linyao Chen, Hwee Tou Ng, Lidong Bing, Xiaoli Li, Roy Ka-wei Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Iterative data generation and model retraining are widely used to align large
language models (LLMs). It typically involves a policy model to generate
on-policy responses and a reward model to guide training data selection. Direct
Preference Optimization (DPO) further enhances this process by constructing
preference pairs of chosen and rejected responses. In this work, we aim to
\emph{scale up} the number of on-policy samples via repeated random sampling to
improve alignment performance. Conventional practice selects the sample with
the highest reward as chosen and the lowest as rejected for DPO. However, our
experiments reveal that this strategy leads to a \emph{decline} in performance
as the sample size increases. To address this, we investigate preference data
construction through the lens of underlying normal distribution of sample
rewards. We categorize the reward space into seven representative points and
systematically explore all 21 ($C_7^2$) pairwise combinations. Through
evaluations on four models using AlpacaEval 2, we find that selecting the
rejected response at reward position $\mu - 2\sigma$ rather than the minimum
reward, is crucial for optimal performance. We finally introduce a scalable
preference data construction strategy that consistently enhances model
performance as the sample scale increases.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Uncertainty Quantification of Large Language Models through
  Multi-Dimensional Responses 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.16820v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.16820v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tiejin Chen, Xiaoou Liu, Longchao Da, Xiaoou Liu, Vagelis Papalexakis, Hua Wei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have demonstrated remarkable capabilities across
various tasks due to large training datasets and powerful transformer
architecture. However, the reliability of responses from LLMs remains a
question. Uncertainty quantification (UQ) of LLMs is crucial for ensuring their
reliability, especially in areas such as healthcare, finance, and
decision-making. Existing UQ methods primarily focus on semantic similarity,
overlooking the deeper knowledge dimensions embedded in responses. We introduce
a multi-dimensional UQ framework that integrates semantic and knowledge-aware
similarity analysis. By generating multiple responses and leveraging auxiliary
LLMs to extract implicit knowledge, we construct separate similarity matrices
and apply tensor decomposition to derive a comprehensive uncertainty
representation. This approach disentangles overlapping information from both
semantic and knowledge dimensions, capturing both semantic variations and
factual consistency, leading to more accurate UQ. Our empirical evaluations
demonstrate that our method outperforms existing techniques in identifying
uncertain responses, offering a more robust framework for enhancing LLM
reliability in high-stakes applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Grounded Persuasive Language Generation for Automated Marketing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.16810v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.16810v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jibang Wu, Chenghao Yang, Simon Mahns, Chaoqi Wang, Hao Zhu, Fei Fang, Haifeng Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper develops an agentic framework that employs large language models
(LLMs) to automate the generation of persuasive and grounded marketing content,
using real estate listing descriptions as our focal application domain. Our
method is designed to align the generated content with user preferences while
highlighting useful factual attributes. This agent consists of three key
modules: (1) Grounding Module, mimicking expert human behavior to predict
marketable features; (2) Personalization Module, aligning content with user
preferences; (3) Marketing Module, ensuring factual accuracy and the inclusion
of localized features. We conduct systematic human-subject experiments in the
domain of real estate marketing, with a focus group of potential house buyers.
The results demonstrate that marketing descriptions generated by our approach
are preferred over those written by human experts by a clear margin. Our
findings suggest a promising LLM-based agentic framework to automate
large-scale targeted marketing while ensuring responsible generation using only
facts.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CoT2Align: Cross-Chain of Thought Distillation via Optimal Transport
  Alignment for Language Models with Different Tokenizers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.16806v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.16806v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anh Duc Le, Tu Vu, Nam Le Hai, Nguyen Thi Ngoc Diep, Linh Ngo Van, Trung Le, Thien Huu Nguyen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) achieve state-of-the-art performance across
various NLP tasks but face deployment challenges due to high computational
costs and memory constraints. Knowledge distillation (KD) is a promising
solution, transferring knowledge from large teacher models to smaller student
models. However, existing KD methods often assume shared vocabularies and
tokenizers, limiting their flexibility. While approaches like Universal Logit
Distillation (ULD) and Dual-Space Knowledge Distillation (DSKD) address
vocabulary mismatches, they overlook the critical \textbf{reasoning-aware
distillation} aspect. To bridge this gap, we propose CoT2Align a universal KD
framework that integrates Chain-of-Thought (CoT) augmentation and introduces
Cross-CoT Alignment to enhance reasoning transfer. Additionally, we extend
Optimal Transport beyond token-wise alignment to a sequence-level and
layer-wise alignment approach that adapts to varying sequence lengths while
preserving contextual integrity. Comprehensive experiments demonstrate that
CoT2Align outperforms existing KD methods across different vocabulary settings,
improving reasoning capabilities and robustness in domain-specific tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Comparing Large Language Model AI and Human-Generated Coaching Messages
  for Behavioral Weight Loss 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.04059v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.04059v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhuoran Huang, Michael P. Berry, Christina Chwyl, Gary Hsieh, Jing Wei, Evan M. Forman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Automated coaching messages for weight control can save time and costs, but
their repetitive, generic nature may limit their effectiveness compared to
human coaching. Large language model (LLM) based artificial intelligence (AI)
chatbots, like ChatGPT, could offer more personalized and novel messages to
address repetition with their data-processing abilities. While LLM AI
demonstrates promise to encourage healthier lifestyles, studies have yet to
examine the feasibility and acceptability of LLM-based BWL coaching. 87 adults
in a weight-loss trial rated ten coaching messages' helpfulness (five
human-written, five ChatGPT-generated) using a 5-point Likert scale, providing
additional open-ended feedback to justify their ratings. Participants also
identified which messages they believed were AI-generated. The evaluation
occurred in two phases: messages in Phase 1 were perceived as impersonal and
negative, prompting revisions for Phase 2 messages. In Phase 1, AI-generated
messages were rated less helpful than human-written ones, with 66 percent
receiving a helpfulness rating of 3 or higher. However, in Phase 2, the AI
messages matched the human-written ones regarding helpfulness, with 82% scoring
three or above. Additionally, 50% were misidentified as human-written,
suggesting AI's sophistication in mimicking human-generated content. A thematic
analysis of open-ended feedback revealed that participants appreciated AI's
empathy and personalized suggestions but found them more formulaic, less
authentic, and too data-focused. This study reveals the preliminary feasibility
and acceptability of LLM AIs, like ChatGPT, in crafting potentially effective
weight control coaching messages. Our findings also underscore areas for future
enhancement.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ HYBRIDMIND: Meta Selection of Natural Language and Symbolic Language for
  Enhanced LLM Reasoning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.19381v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.19381v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Simeng Han, Tianyu Liu, Chuhan Li, Xuyuan Xiong, Arman Cohan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  LLMs approach logical and mathematical reasoning through natural or symbolic
languages. While natural language offers human-accessible flexibility but
suffers from ambiguity, symbolic reasoning provides precise, machine-executable
inferences at the cost of strict domain constraints. We introduce HYBRIDMIND,
an adaptive strategy that selects the optimal reasoning approach for each
reasoning problem. Through extensive experiments, we evaluate both
prompting-based approaches with state-of-the-art LLMs and fine-tuned
open-source models. We find that fine-tuning LLaMA-3.1-8B-Instruct as a
meta-selector outperforms GPT-4o's natural language reasoning by 4.4\% on FOLIO
and 1.3\% on MATH. More notably, using GPT-3.5-turbo as a prompted
meta-selector yields a 10\% improvement on FOLIO's challenging subset compared
to GPT-4o. We will release our code and data to support future research.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning to Reason at the Frontier of Learnability 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.12272v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.12272v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Thomas Foster, Jakob Foerster
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reinforcement learning is now widely adopted as the final stage of large
language model training, especially for reasoning-style tasks such as maths
problems. Typically, models attempt each question many times during a single
training step and attempt to learn from their successes and failures. However,
we demonstrate that throughout training with two popular algorithms (PPO and
VinePPO) on two widely used datasets, many questions are either solved by all
attempts - meaning they are already learned - or by none - providing no
meaningful training signal. To address this, we adapt a method from the
reinforcement learning literature - sampling for learnability - and apply it to
the reinforcement learning stage of LLM training. Our curriculum prioritises
questions with high variance of success, i.e. those where the agent sometimes
succeeds, but not always. Our findings demonstrate that this curriculum
consistently boosts training performance across multiple algorithms and
datasets, paving the way for more efficient and effective reinforcement
learning with LLMs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Findings of the 2023 ML-SUPERB Challenge: <span class="highlight-title">Pre-Train</span>ing and Evaluation
  over More Languages and Beyond 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.05513v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.05513v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiatong Shi, William Chen, Dan Berrebbi, Hsiu-Hsuan Wang, Wei-Ping Huang, En-Pei Hu, Ho-Lam Chuang, Xuankai Chang, Yuxun Tang, Shang-Wen Li, Abdelrahman Mohamed, Hung-yi Lee, Shinji Watanabe
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The 2023 Multilingual Speech Universal Performance Benchmark (ML-SUPERB)
Challenge expands upon the acclaimed SUPERB framework, emphasizing
self-supervised models in multilingual speech recognition and language
identification. The challenge comprises a research track focused on applying
ML-SUPERB to specific multilingual subjects, a Challenge Track for model
submissions, and a New Language Track where language resource researchers can
contribute and evaluate their low-resource language data in the context of the
latest progress in multilingual speech recognition. The challenge garnered 12
model submissions and 54 language corpora, resulting in a comprehensive
benchmark encompassing 154 languages. The findings indicate that merely scaling
models is not the definitive solution for multilingual speech tasks, and a
variety of speech/voice types present significant challenges in multilingual
speech processing.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ASRU</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ML-SUPERB: Multilingual Speech Universal PERformance Benchmark 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.10615v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.10615v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiatong Shi, Dan Berrebbi, William Chen, Ho-Lam Chung, En-Pei Hu, Wei Ping Huang, Xuankai Chang, Shang-Wen Li, Abdelrahman Mohamed, Hung-yi Lee, Shinji Watanabe
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Speech processing Universal PERformance Benchmark (SUPERB) is a leaderboard
to benchmark the performance of Self-Supervised Learning (SSL) models on
various speech processing tasks. However, SUPERB largely considers English
speech in its evaluation. This paper presents multilingual SUPERB (ML-SUPERB),
covering 143 languages (ranging from high-resource to endangered), and
considering both automatic speech recognition and language identification.
Following the concept of SUPERB, ML-SUPERB utilizes frozen SSL features and
employs a simple framework for multilingual tasks by learning a shallow
downstream model. Similar to the SUPERB benchmark, we find speech SSL models
can significantly improve performance compared to FBANK features. Furthermore,
we find that multilingual models do not always perform better than their
monolingual counterparts. We will release ML-SUPERB as a challenge with
organized datasets and reproducible training scripts for future multilingual
representation research.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by Interspeech</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Aligned at the Start: Conceptual Groupings in LLM Embeddings 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.05315v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.05315v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mehrdad Khatir, Sanchit Kabra, Chandan K. Reddy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper shifts focus to the often-overlooked input embeddings - the
initial representations fed into transformer blocks. Using fuzzy graph,
k-nearest neighbor (k-NN), and community detection, we analyze embeddings from
diverse LLMs, finding significant categorical community structure aligned with
predefined concepts and categories aligned with humans. We observe these
groupings exhibit within-cluster organization (such as hierarchies, topological
ordering, etc.), hypothesizing a fundamental structure that precedes contextual
processing. To further investigate the conceptual nature of these groupings, we
explore cross-model alignments across different LLM categories within their
input embeddings, observing a medium to high degree of alignment. Furthermore,
provide evidence that manipulating these groupings can play a functional role
in mitigating ethnicity bias in LLM tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Conversation Routines: A <span class="highlight-title">Prompt</span> Engineering Framework for Task-Oriented
  Dialog Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.11613v7">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.11613v7.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Giorgio Robino
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study introduces Conversation Routines (CR), a structured prompt
engineering framework for developing task-oriented dialog systems using Large
Language Models (LLMs). While LLMs demonstrate remarkable natural language
understanding capabilities, engineering them to reliably execute complex
business workflows remains challenging. The proposed CR framework enables the
development of Conversation Agentic Systems (CAS) through natural language
specifications, embedding task-oriented logic within LLM prompts. This approach
provides a systematic methodology for designing and implementing complex
conversational workflows while maintaining behavioral consistency. We
demonstrate the framework's effectiveness through two proof-of-concept
implementations: a Train Ticket Booking System and an Interactive
Troubleshooting Copilot. These case studies validate CR's capability to encode
sophisticated behavioral patterns and decision logic while preserving natural
conversational flexibility. Results show that CR enables domain experts to
design conversational workflows in natural language while leveraging custom
functions (tools) developed by software engineers, creating an efficient
division of responsibilities where developers focus on core API implementation
and domain experts handle conversation design. While the framework shows
promise in accessibility and adaptability, we identify key challenges including
computational overhead, non-deterministic behavior, and domain-specific logic
optimization. Future research directions include CR evaluation methods based on
prompt engineering frameworks driven by goal-oriented grading criteria,
improving scalability for complex multi-agent interactions, and enhancing
system robustness to address the identified limitations across diverse business
applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Minor typos revision</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Non-Halting Queries: Exploiting Fixed Points in LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.06287v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.06287v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ghaith Hammouri, Kemal Derya, Berk Sunar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce a new vulnerability that exploits fixed points in autoregressive
models and use it to craft queries that never halt. More precisely, for
non-halting queries, the LLM never samples the end-of-string token <eos>. We
rigorously analyze the conditions under which the non-halting anomaly presents
itself. In particular, at temperature zero, we prove that if a repeating
(cyclic) token sequence is observed at the output beyond the context size, then
the LLM does not halt.
  We demonstrate non-halting queries in many experiments performed in base
unaligned models where repeating prompts immediately lead to a non-halting
cyclic behavior as predicted by the analysis. Further, we develop a simple
recipe that takes the same fixed points observed in the base model and creates
a prompt structure to target aligned models. We demonstrate the recipe's
success in sending every major model released over the past year into a
non-halting state with the same simple prompt even over higher temperatures.
Further, we devise an experiment with 100 randomly selected tokens and show
that the recipe to create non-halting queries succeeds with high success rates
ranging from 97% for GPT-4o to 19% for Gemini Pro 1.5. These results show that
the proposed adversarial recipe succeeds in bypassing alignment at one to two
orders of magnitude higher rates compared to earlier reports.
  We also study gradient-based direct inversion using ARCA to craft new short
prompts to induce the non-halting state. We inverted 10,000 random repeating
2-cycle outputs for llama-3.1-8b-instruct. Out of 10,000 three-token inverted
prompts 1,512 yield non-halting queries reaching a rate of 15%. Our experiments
with ARCA show that non-halting may be easily induced with as few as 3 input
tokens with high probability. Overall, our experiments demonstrate that
non-halting queries are prevalent and relatively easy to find.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Multilingual Machine Translation with Open Large Language Models at
  Practical Scale: An Empirical Study <span class="chip">NAACL2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.02481v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.02481v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Menglong Cui, Pengzhi Gao, Wei Liu, Jian Luan, Bin Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have shown continuously improving multilingual
capabilities, and even small-scale open-source models have demonstrated rapid
performance enhancement. In this paper, we systematically explore the abilities
of open LLMs with less than ten billion parameters to handle multilingual
machine translation (MT) tasks. We conduct comprehensive evaluations on six
popular LLMs and find that models like Gemma2-9B exhibit impressive
multilingual translation capabilities. We then introduce the Parallel-First
Monolingual-Second (PFMS) data mixing strategy in the continual pretraining
stage to further enhance the MT performance and present GemmaX2-28, a 9B model
achieving top-tier multilingual translation performance across 28 languages.
Specifically, GemmaX2-28 consistently outperforms the state-of-the-art (SOTA)
models such as TowerInstruct and XALMA and achieves competitive performance
with Google Translate and GPT-4-turbo.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accept to NAACL2025 Main Conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Understanding the Relationship between <span class="highlight-title">Prompt</span>s and Response Uncertainty
  in Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.14845v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.14845v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ze Yu Zhang, Arun Verma, Finale Doshi-Velez, Bryan Kian Hsiang Low
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) are widely used in decision-making, but their
reliability, especially in critical tasks like healthcare, is not
well-established. Therefore, understanding how LLMs reason and make decisions
is crucial for their safe deployment. This paper investigates how the
uncertainty of responses generated by LLMs relates to the information provided
in the input prompt. Leveraging the insight that LLMs learn to infer latent
concepts during pretraining, we propose a prompt-response concept model that
explains how LLMs generate responses and helps understand the relationship
between prompts and response uncertainty. We show that the uncertainty
decreases as the prompt's informativeness increases, similar to epistemic
uncertainty. Our detailed experimental results on real-world datasets validate
our proposed model.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>22 pages, Preprint</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Quriosity: Analyzing Human Questioning Behavior and Causal Inquiry
  through Curiosity-Driven Queries 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.20318v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.20318v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Roberto Ceraolo, Dmitrii Kharlapenko, Ahmad Khan, Amélie Reymond, Rada Mihalcea, Bernhard Schölkopf, Mrinmaya Sachan, Zhijing Jin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent progress in Large Language Model (LLM) technology has changed our role
in interacting with these models. Instead of primarily testing these models
with questions we already know answers to, we are now using them for queries
where the answers are unknown to us, driven by human curiosity. This shift
highlights the growing need to understand curiosity-driven human questions -
those that are more complex, open-ended, and reflective of real-world needs. To
this end, we present Quriosity, a collection of 13.5K naturally occurring
questions from three diverse sources: human-to-search-engine queries,
human-to-human interactions, and human-to-LLM conversations. Our comprehensive
collection enables a rich understanding of human curiosity across various
domains and contexts. Our analysis reveals a significant presence of causal
questions (up to 42%) in the dataset, for which we develop an iterative prompt
improvement framework to identify all causal queries and examine their unique
linguistic properties, cognitive complexity and source distribution. Our paper
paves the way for future work on causal question identification and open-ended
chatbot interactions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Bridging Social Media and Search Engines: Dredge Words and the Detection
  of Unreliable Domains 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.11423v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.11423v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Evan M. Williams, Peter Carragher, Kathleen M. Carley
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Proactive content moderation requires platforms to rapidly and continuously
evaluate the credibility of websites. Leveraging the direct and indirect paths
users follow to unreliable websites, we develop a website credibility
classification and discovery system that integrates both webgraph and
large-scale social media contexts. We additionally introduce the concept of
dredge words, terms or phrases for which unreliable domains rank highly on
search engines, and provide the first exploration of their usage on social
media. Our graph neural networks that combine webgraph and social media
contexts generate to state-of-the-art results in website credibility
classification and significantly improves the top-k identification of
unreliable domains. Additionally, we release a novel dataset of dredge words,
highlighting their strong connections to both social media and online commerce
platforms.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ KVTuner: Sensitivity-Aware Layer-wise Mixed Precision KV Cache
  Quantization for Efficient and Nearly Lossless LLM Inference 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.04420v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.04420v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xing Li, Zeyu Xing, Yiming Li, Linping Qu, Hui-Ling Zhen, Wulong Liu, Yiwu Yao, Sinno Jialin Pan, Mingxuan Yuan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  KV cache quantization can improve Large Language Models (LLMs) inference
throughput and latency in long contexts and large batch-size scenarios while
preserving LLMs effectiveness. However, current methods have three unsolved
issues: overlooking layer-wise sensitivity to KV cache quantization, high
overhead of online fine-grained decision-making, and low flexibility to
different LLMs and constraints. Therefore, we thoroughly analyze the inherent
correlation of layer-wise transformer attention patterns to KV cache
quantization errors and study why key cache is more important than value cache
for quantization error reduction. We further propose a simple yet effective
framework KVTuner to adaptively search for the optimal hardware-friendly
layer-wise KV quantization precision pairs for coarse-grained KV cache with
multi-objective optimization and directly utilize the offline searched
configurations during online inference. To reduce the computational cost of
offline calibration, we utilize the intra-layer KV precision pair pruning and
inter-layer clustering to reduce the search space. Experimental results show
that we can achieve nearly lossless 3.25-bit mixed precision KV cache
quantization for LLMs like Llama-3.1-8B-Instruct and 4.0-bit for sensitive
models like Qwen2.5-7B-Instruct on mathematical reasoning tasks. The maximum
inference throughput can be improved by 38.3% compared with KV8 quantization
over various context lengths. Our code and searched configurations are
available at https://github.com/cmd2001/KVTuner.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>36 pages. Code: https://github.com/cmd2001/KVTuner</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PersonalLLM: Tailoring LLMs to Individual Preferences 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.20296v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.20296v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Thomas P. Zollo, Andrew Wei Tung Siah, Naimeng Ye, Ang Li, Hongseok Namkoong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As LLMs become capable of complex tasks, there is growing potential for
personalized interactions tailored to the subtle and idiosyncratic preferences
of the user. We present a public benchmark, PersonalLLM, focusing on adapting
LLMs to provide maximal benefits for a particular user. Departing from existing
alignment benchmarks that implicitly assume uniform preferences, we curate
open-ended prompts paired with many high-quality answers over which users would
be expected to display heterogeneous latent preferences. Instead of
persona-prompting LLMs based on high-level attributes (e.g., user's race or
response length), which yields homogeneous preferences relative to humans, we
develop a method that can simulate a large user base with diverse preferences
from a set of pre-trained reward models. Our dataset and generated
personalities offer an innovative testbed for developing personalization
algorithms that grapple with continual data sparsity--few relevant feedback
from the particular user--by leveraging historical data from other (similar)
users. We explore basic in-context learning and meta-learning baselines to
illustrate the utility of PersonalLLM and highlight the need for future
methodological development. Our dataset is available at
https://huggingface.co/datasets/namkoong-lab/PersonalLLM
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>28 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Text2World: Benchmarking Large Language Models for Symbolic World Model
  Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.13092v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.13092v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mengkang Hu, Tianxing Chen, Yude Zou, Yuheng Lei, Qiguang Chen, Ming Li, Yao Mu, Hongyuan Zhang, Wenqi Shao, Ping Luo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, there has been growing interest in leveraging large language models
(LLMs) to generate symbolic world models from textual descriptions. Although
LLMs have been extensively explored in the context of world modeling, prior
studies encountered several challenges, including evaluation randomness,
dependence on indirect metrics, and a limited domain scope. To address these
limitations, we introduce a novel benchmark, Text2World, based on planning
domain definition language (PDDL), featuring hundreds of diverse domains and
employing multi-criteria, execution-based metrics for a more robust evaluation.
We benchmark current LLMs using Text2World and find that reasoning models
trained with large-scale reinforcement learning outperform others. However,
even the best-performing model still demonstrates limited capabilities in world
modeling. Building on these insights, we examine several promising strategies
to enhance the world modeling capabilities of LLMs, including test-time
scaling, agent training, and more. We hope that Text2World can serve as a
crucial resource, laying the groundwork for future research in leveraging LLMs
as world models. The project page is available at
https://text-to-world.github.io/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://text-to-world.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ NormAd: A Framework for Measuring the Cultural Adaptability of Large
  Language Models <span class="chip">NAACL 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.12464v8">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.12464v8.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Abhinav Rao, Akhila Yerukola, Vishwa Shah, Katharina Reinecke, Maarten Sap
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To be effectively and safely deployed to global user populations, large
language models (LLMs) may need to adapt outputs to user values and cultures,
not just know about them. We introduce NormAd, an evaluation framework to
assess LLMs' cultural adaptability, specifically measuring their ability to
judge social acceptability across varying levels of cultural norm specificity,
from abstract values to explicit social norms. As an instantiation of our
framework, we create NormAd-Eti, a benchmark of 2.6k situational descriptions
representing social-etiquette related cultural norms from 75 countries. Through
comprehensive experiments on NormAd-Eti, we find that LLMs struggle to
accurately judge social acceptability across these varying degrees of cultural
contexts and show stronger adaptability to English-centric cultures over those
from the Global South. Even in the simplest setting where the relevant social
norms are provided, the best LLMs' performance (< 82\%) lags behind humans (>
95\%). In settings with abstract values and country information, model
performance drops substantially (< 60\%), while human accuracy remains high (>
90\%). Furthermore, we find that models are better at recognizing socially
acceptable versus unacceptable situations. Our findings showcase the current
pitfalls in socio-cultural reasoning of LLMs which hinder their adaptability
for global audiences.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at NAACL 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SepLLM: Accelerate Large Language Models by Compressing One Segment into
  One Separator 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.12094v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.12094v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guoxuan Chen, Han Shi, Jiawei Li, Yihang Gao, Xiaozhe Ren, Yimeng Chen, Xin Jiang, Zhenguo Li, Weiyang Liu, Chao Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have exhibited exceptional performance across a
spectrum of natural language processing tasks. However, their substantial sizes
pose considerable challenges, particularly in computational demands and
inference speed, due to their quadratic complexity. In this work, we have
identified a key pattern: certain seemingly meaningless separator tokens (i.e.,
punctuations) contribute disproportionately to attention scores compared to
semantically meaningful tokens. This observation suggests that information of
the segments between these separator tokens can be effectively condensed into
the separator tokens themselves without significant information loss. Guided by
this insight, we introduce SepLLM, a plug-and-play framework that accelerates
inference by compressing these segments and eliminating redundant tokens.
Additionally, we implement efficient kernels for training acceleration.
Experimental results across training-free, training-from-scratch, and
post-training settings demonstrate SepLLM's effectiveness. Notably, using the
Llama-3-8B backbone, SepLLM achieves over 50% reduction in KV cache on the
GSM8K-CoT benchmark while maintaining comparable performance. Furthermore, in
streaming settings, SepLLM effectively processes sequences of up to 4 million
tokens or more while maintaining consistent language modeling capabilities.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>We have made our code publicly available at sepllm.github.io. Our
  codebase supports efficient multi-node distributed training with accelerated
  attention module Sep-Attention and also supports numerous existing Fusion
  Operators to accelerate the training process, such as fused rope, etc. If you
  find our code helpful, please kindly consider giving us a **star** on GitHub
  ^_^ Thank you very much!</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ESPnet-SpeechLM: An Open Speech Language Model Toolkit 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.15218v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.15218v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jinchuan Tian, Jiatong Shi, William Chen, Siddhant Arora, Yoshiki Masuyama, Takashi Maekaku, Yihan Wu, Junyi Peng, Shikhar Bharadwaj, Yiwen Zhao, Samuele Cornell, Yifan Peng, Xiang Yue, Chao-Han Huck Yang, Graham Neubig, Shinji Watanabe
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present ESPnet-SpeechLM, an open toolkit designed to democratize the
development of speech language models (SpeechLMs) and voice-driven agentic
applications. The toolkit standardizes speech processing tasks by framing them
as universal sequential modeling problems, encompassing a cohesive workflow of
data preprocessing, pre-training, inference, and task evaluation. With
ESPnet-SpeechLM, users can easily define task templates and configure key
settings, enabling seamless and streamlined SpeechLM development. The toolkit
ensures flexibility, efficiency, and scalability by offering highly
configurable modules for every stage of the workflow. To illustrate its
capabilities, we provide multiple use cases demonstrating how competitive
SpeechLMs can be constructed with ESPnet-SpeechLM, including a 1.7B-parameter
model pre-trained on both text and speech tasks, across diverse benchmarks. The
toolkit and its recipes are fully transparent and reproducible at:
https://github.com/espnet/espnet/tree/speechlm.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PairBench: A Systematic Framework for Selecting Reliable Judge VLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.15210v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.15210v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aarash Feizi, Sai Rajeswar, Adriana Romero-Soriano, Reihaneh Rabbany, Spandana Gella, Valentina Zantedeschi, João Monteiro
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As large vision language models (VLMs) are increasingly used as automated
evaluators, understanding their ability to effectively compare data pairs as
instructed in the prompt becomes essential. To address this, we present
PairBench, a low-cost framework that systematically evaluates VLMs as
customizable similarity tools across various modalities and scenarios. Through
PairBench, we introduce four metrics that represent key desiderata of
similarity scores: alignment with human annotations, consistency for data pairs
irrespective of their order, smoothness of similarity distributions, and
controllability through prompting. Our analysis demonstrates that no model,
whether closed- or open-source, is superior on all metrics; the optimal choice
depends on an auto evaluator's desired behavior (e.g., a smooth vs. a sharp
judge), highlighting risks of widespread adoption of VLMs as evaluators without
thorough assessment. For instance, the majority of VLMs struggle with
maintaining symmetric similarity scores regardless of order. Additionally, our
results show that the performance of VLMs on the metrics in PairBench closely
correlates with popular benchmarks, showcasing its predictive power in ranking
models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Human Decision-making is Susceptible to AI-driven Manipulation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.07663v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.07663v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sahand Sabour, June M. Liu, Siyang Liu, Chris Z. Yao, Shiyao Cui, Xuanming Zhang, Wen Zhang, Yaru Cao, Advait Bhat, Jian Guan, Wei Wu, Rada Mihalcea, Hongning Wang, Tim Althoff, Tatia M. C. Lee, Minlie Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Artificial Intelligence (AI) systems are increasingly intertwined with daily
life, assisting users in executing various tasks and providing guidance on
decision-making. This integration introduces risks of AI-driven manipulation,
where such systems may exploit users' cognitive biases and emotional
vulnerabilities to steer them toward harmful outcomes. Through a randomized
controlled trial with 233 participants, we examined human susceptibility to
such manipulation in financial (e.g., purchases) and emotional (e.g., conflict
resolution) decision-making contexts. Participants interacted with one of three
AI agents: a neutral agent (NA) optimizing for user benefit without explicit
influence, a manipulative agent (MA) designed to covertly influence beliefs and
behaviors, or a strategy-enhanced manipulative agent (SEMA) employing explicit
psychological tactics to reach its hidden objectives. By analyzing
participants' decision patterns and shifts in their preference ratings
post-interaction, we found significant susceptibility to AI-driven
manipulation. Particularly, across both decision-making domains, participants
interacting with the manipulative agents shifted toward harmful options at
substantially higher rates (financial, MA: 62.3%, SEMA: 59.6%; emotional, MA:
42.3%, SEMA: 41.5%) compared to the NA group (financial, 35.8%; emotional,
12.8%). Notably, our findings reveal that even subtle manipulative objectives
(MA) can be as effective as employing explicit psychological strategies (SEMA)
in swaying human decision-making. By revealing the potential for covert AI
influence, this study highlights a critical vulnerability in human-AI
interactions, emphasizing the need for ethical safeguards and regulatory
frameworks to ensure responsible deployment of AI technologies and protect
human autonomy.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Work in progress</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Institutional Platform for Secure Self-Service Large Language Model
  Exploration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.00913v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.00913v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        V. K. Cody Bumgardner, Mitchell A. Klusty, W. Vaiden Logan, Samuel E. Armstrong, Caroline N. Leach, Kenneth L. Calvert, Caylin Hickey, Jeff Talbert
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces a user-friendly platform developed by the University of
Kentucky Center for Applied AI, designed to make large, customized language
models (LLMs) more accessible. By capitalizing on recent advancements in
multi-LoRA inference, the system efficiently accommodates custom adapters for a
diverse range of users and projects. The paper outlines the system's
architecture and key features, encompassing dataset curation, model training,
secure inference, and text-based feature extraction.
  We illustrate the establishment of a tenant-aware computational network using
agent-based methods, securely utilizing islands of isolated resources as a
unified system. The platform strives to deliver secure LLM services,
emphasizing process and data isolation, end-to-end encryption, and role-based
resource authentication. This contribution aligns with the overarching goal of
enabling simplified access to cutting-edge AI models and technology in support
of scientific discovery.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages 5 figures, 1 table</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ HiFi-KPI: A <span class="highlight-title">Dataset</span> for Hierarchical KPI Extraction from Earnings
  Filings 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.15411v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.15411v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rasmus Aavang, Giovanni Rizzi, Rasmus Bøggild, Alexandre Iolov, Mike Zhang, Johannes Bjerva
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The U.S. Securities and Exchange Commission (SEC) requires that public
companies file financial reports tagging numbers with the machine readable
inline eXtensible Business Reporting Language (iXBRL) standard. However, the
highly complex and highly granular taxonomy defined by iXBRL limits label
transferability across domains. In this paper, we introduce the Hierarchical
Financial Key Performance Indicator (HiFi-KPI) dataset, designed to facilitate
numerical KPI extraction at specified levels of granularity from unstructured
financial text. Our approach organizes a 218,126-label hierarchy using a
taxonomy based grouping method, investigating which taxonomy layer provides the
most meaningful structure. HiFi-KPI comprises ~1.8M paragraphs and ~5M
entities, each linked to a label in the iXBRL-specific calculation and
presentation taxonomies. We provide baselines using encoder-based approaches
and structured extraction using Large Language Models (LLMs). To simplify LLM
inference and evaluation, we additionally release HiFi-KPI Lite, a manually
curated subset with four expert-mapped labels. We publicly release all
artifacts.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Quantization Meets Reasoning: Exploring LLM Low-Bit Quantization
  Degradation for Mathematical Reasoning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.03035v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.03035v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhen Li, Yupeng Su, Runming Yang, Congkai Xie, Zheng Wang, Zhongwei Xie, Ngai Wong, Hongxia Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models have achieved significant advancements in complex
mathematical reasoning benchmarks, such as MATH. However, their substantial
computational requirements present challenges for practical deployment. Model
quantization has emerged as an effective strategy to reduce memory usage and
computational costs by employing lower precision and bit-width representations.
In this study, we systematically evaluate the impact of quantization on
mathematical reasoning tasks. Our results demonstrate that aggressive
quantization methods like AWQ and GPTQ introduce up to 32.39% accuracy
degradation (average 11.31%) on Llama-3 models, particularly in numerical
computation and reasoning planning. To address this, we introduce a
multidimensional evaluation framework combining qualitative capability analysis
and quantitative error assessment. We further develop targeted recovery
strategies, showing that fine-tuning quantized models on only 545 task-specific
examples for 3 minutes on 4 GPUs effectively restores reasoning capabilities to
near full-precision levels. Additionally, our error assessment pipeline
achieves 98.9% accuracy in diagnosing and localizing errors across 3,366
failure cases, providing actionable insights for mitigating
quantization-induced degradation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Enhancing RWKV-based Language Models for Long-Sequence Text Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.15485v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.15485v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinghan Pan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces an enhanced RWKV architecture with adaptive temporal
gating mechanisms for improved long-context language modeling. We propose two
principal innovations: (1) a position-aware convolutional shift operator that
captures local syntactic patterns while preserving global coherence, and (2) a
neurally-gated information routing mechanism that dynamically regulates
inter-token information flow. Through comprehensive experiments on text
generation tasks, our enhanced model demonstrates superior performance compared
to the baseline RWKV, achieving 96.5 relative improvement in ROUGE-L scores
with only 2.95 increased inference latency. Ablation studies validate the
individual contributions of each component, while linguistic analysis reveals
the model's adaptive attention to syntactic boundaries and entity coherence.
The proposed modifications maintain RWKV's linear computational complexity
while significantly enhancing its contextual modeling capabilities,
establishing new state-of-the-art performance for recurrent-style architectures
in long-form text generation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 2 tables, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Multilingual LLMs Inherently Reward In-Language Time-Sensitive Semantic
  Alignment for Low-Resource Languages 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.08090v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.08090v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ashutosh Bajpai, Tanmoy Chakraborty
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The unwavering disparity in labeled resources between resource-rich languages
and those considered low-resource remains a significant impediment for Large
Language Models (LLMs). Recent strides in cross-lingual in-context learning
(X-ICL), mainly through semantically aligned examples retrieved from
multilingual pre-trained transformers, have shown promise in mitigating this
issue. However, our investigation reveals that LLMs intrinsically reward
in-language semantically aligned cross-lingual instances over direct
cross-lingual semantic alignments, with a pronounced disparity in handling
time-sensitive queries in the X-ICL setup. Such queries demand sound temporal
reasoning ability from LLMs, yet the advancements have predominantly focused on
English. This study aims to bridge this gap by improving temporal reasoning
capabilities in low-resource languages. To this end, we introduce mTEMPREASON,
a temporal reasoning dataset aimed at the varied degrees of low-resource
languages and propose Cross-Lingual Time-Sensitive Semantic Alignment
(CLiTSSA), a novel method to improve temporal reasoning in these contexts. To
facilitate this, we construct an extension of mTEMPREASON comprising pairs of
parallel cross-language temporal queries along with their anticipated
in-language semantic similarity scores. Our empirical evidence underscores the
superior performance of CLiTSSA compared to established baselines across three
languages -- Romanian, German, and French, encompassing three temporal tasks
and including a diverse set of four contemporaneous LLMs. This marks a
significant step forward in addressing resource disparity in the context of
temporal reasoning across languages.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A <span class="highlight-title">Survey</span> of Large Language Models for Arabic Language and its Dialects 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.20238v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.20238v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Malak Mashaabi, Shahad Al-Khalifa, Hend Al-Khalifa
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This survey offers a comprehensive overview of Large Language Models (LLMs)
designed for Arabic language and its dialects. It covers key architectures,
including encoder-only, decoder-only, and encoder-decoder models, along with
the datasets used for pre-training, spanning Classical Arabic, Modern Standard
Arabic, and Dialectal Arabic. The study also explores monolingual, bilingual,
and multilingual LLMs, analyzing their architectures and performance across
downstream tasks, such as sentiment analysis, named entity recognition, and
question answering. Furthermore, it assesses the openness of Arabic LLMs based
on factors, such as source code availability, training data, model weights, and
documentation. The survey highlights the need for more diverse dialectal
datasets and attributes the importance of openness for research reproducibility
and transparency. It concludes by identifying key challenges and opportunities
for future research and stressing the need for more inclusive and
representative models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to ACM Transactions on Asian and Low-Resource Language
  Information Processing</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Round Attention: A Novel Round-Level Attention Mechanism to Accelerate
  LLM Inference 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.15294v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.15294v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yaohua Tang, Zhicheng Hu, Kun Cheng, Fan Mo, Qiheng Lv, Hua Wang, Zhi Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The increasing context window size in large language models (LLMs) has
improved their ability to handle complex, long-text tasks. However, as the
conversation rounds continue, it is required to store a large amount of KV
cache in GPU memory, which significantly affects the efficiency and even
availability of the model serving systems. This paper analyzes dialogue data
from real users and discovers that the LLM inference manifests a watershed
layer, after which the distribution of round-level attention shows notable
similarity. We propose Round Attention, a novel round-level attention mechanism
that only recalls and computes the KV cache of the most relevant rounds. The
experiments show that our method saves 55\% memory usage without compromising
model performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MMUnlearner: Reformulating Multimodal Machine Unlearning in the Era of
  Multimodal Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11051v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11051v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiahao Huo, Yibo Yan, Xu Zheng, Yuanhuiyi Lyu, Xin Zou, Zhihua Wei, Xuming Hu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent progress in Machine Unlearning (MU) has introduced solutions for the
selective removal of private or sensitive information encoded within deep
neural networks. Nonetheless, MU for Multimodal Large Language Models (MLLMs)
remains in its nascent phase. Therefore, we propose to reformulate the task of
multimodal MU in the era of MLLMs, which aims to erase only the visual patterns
associated with a given entity while preserving the corresponding textual
knowledge encoded within the original parameters of the language model
backbone. Furthermore, we develop a novel geometry-constrained gradient descent
method MMUnlearner. It updates the weights of MLLMs with a weight saliency map
jointly restricted by the remaining concepts and textual knowledge during
unlearning, thereby preserving parameters essential for non-target knowledge.
Extensive experiments demonstrate that MMUnlearner surpasses baselines that
finetuning MLLMs with VQA data directly through Gradient Ascent (GA) or
Negative Preference Optimization (NPO), across all evaluation dimensions. Our
code will be released upon acceptance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GenAI vs. Human Fact-Checkers: Accurate Ratings, Flawed Rationales 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.14943v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.14943v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuehong Cassandra Tai, Khushi Navin Patni, Nicholas Daniel Hemauer, Bruce Desmarais, Yu-Ru Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite recent advances in understanding the capabilities and limits of
generative artificial intelligence (GenAI) models, we are just beginning to
understand their capacity to assess and reason about the veracity of content.
We evaluate multiple GenAI models across tasks that involve the rating of, and
perceived reasoning about, the credibility of information. The information in
our experiments comes from content that subnational U.S. politicians post to
Facebook. We find that GPT-4o, one of the most used AI models in consumer
applications, outperforms other models, but all models exhibit only moderate
agreement with human coders. Importantly, even when GenAI models accurately
identify low-credibility content, their reasoning relies heavily on linguistic
features and ``hard'' criteria, such as the level of detail, source
reliability, and language formality, rather than an understanding of veracity.
We also assess the effectiveness of summarized versus full content inputs,
finding that summarized content holds promise for improving efficiency without
sacrificing accuracy. While GenAI has the potential to support human
fact-checkers in scaling misinformation detection, our results caution against
relying solely on these models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for publication in the 17th ACM Web Science Conference 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ On the Role of Attention Heads in Large Language Model Safety <span class="chip">ICLR
  2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13708v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13708v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhenhong Zhou, Haiyang Yu, Xinghua Zhang, Rongwu Xu, Fei Huang, Kun Wang, Yang Liu, Junfeng Fang, Yongbin Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) achieve state-of-the-art performance on multiple
language tasks, yet their safety guardrails can be circumvented, leading to
harmful generations. In light of this, recent research on safety mechanisms has
emerged, revealing that when safety representations or component are
suppressed, the safety capability of LLMs are compromised. However, existing
research tends to overlook the safety impact of multi-head attention
mechanisms, despite their crucial role in various model functionalities. Hence,
in this paper, we aim to explore the connection between standard attention
mechanisms and safety capability to fill this gap in the safety-related
mechanistic interpretability. We propose a novel metric which tailored for
multi-head attention, the Safety Head ImPortant Score (Ships), to assess the
individual heads' contributions to model safety. Based on this, we generalize
Ships to the dataset level and further introduce the Safety Attention Head
AttRibution Algorithm (Sahara) to attribute the critical safety attention heads
inside the model. Our findings show that the special attention head has a
significant impact on safety. Ablating a single safety head allows aligned
model (e.g., Llama-2-7b-chat) to respond to 16 times more harmful queries,
while only modifying 0.006% of the parameters, in contrast to the ~ 5%
modification required in previous studies. More importantly, we demonstrate
that attention heads primarily function as feature extractors for safety and
models fine-tuned from the same base model exhibit overlapping safety heads
through comprehensive experiments. Together, our attribution approach and
findings provide a novel perspective for unpacking the black box of safety
mechanisms within large models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>28 pages, 18 figures, 7 tables. This paper has been accepted as ICLR
  2025 (oral)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Vikhr: Constructing a State-of-the-art Bilingual Open-Source
  Instruction-Following Large Language Model for Russian <span class="chip">EMNLP-2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.13929v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.13929v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aleksandr Nikolich, Konstantin Korolev, Sergei Bratchikov, Igor Kiselev, Artem Shelmanov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  There has been a surge in developing various Large Language Models (LLMs).
However, text generation for languages other than English often faces
significant challenges, including poor generation quality and reduced
computational performance due to the disproportionate representation of tokens
in the model's vocabulary. In this work, we address these issues by developing
a pipeline for adapting English-oriented pre-trained models to other languages
and constructing efficient bilingual LLMs. Using this pipeline, we construct
Vikhr, a state-of-the-art bilingual open-source instruction-following LLM
designed specifically for the Russian language. "Vikhr" refers to the name of
the Mistral LLM series and means a "strong gust of wind." Unlike previous
Russian-language models that typically rely on LoRA adapters on top of
English-oriented models, sacrificing performance for lower training costs,
Vikhr features an adapted tokenizer vocabulary and undergoes continued
pre-training and instruction tuning of all weights. This not only enhances the
model's performance but also significantly improves its computational and
contextual efficiency. The remarkable performance of Vikhr across various
Russian-language benchmarks can also be attributed to our efforts in expanding
instruction datasets and corpora for continued pre-training. Vikhr not only
sets a new state of the art among open-source LLMs for Russian but even
outperforms some proprietary closed-source models on certain benchmarks. The
model weights, instruction sets, and code are publicly available.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at WMRL @ EMNLP-2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ TEncDM: Understanding the Properties of the Diffusion Model in the Space
  of Language Model Encodings 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.19097v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.19097v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alexander Shabalin, Viacheslav Meshchaninov, Egor Chimbulatov, Vladislav Lapikov, Roman Kim, Grigory Bartosh, Dmitry Molchanov, Sergey Markov, Dmitry Vetrov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents the Text Encoding Diffusion Model (TEncDM), a novel
approach to diffusion modeling that operates in the space of pre-trained
language model encodings. In contrast to traditionally used embeddings,
encodings integrate contextual information. In our approach, we also employ a
transformer-based decoder, specifically designed to incorporate context in the
token prediction process. We conduct a comprehensive examination of the
influence of the encoder, decoder, noise scheduler, and self-conditioning on
zero-shot generation. Furthermore, we compare TEncDM with previous approaches
on three conditional text generation tasks: QQP, XSum, and Wiki-Auto. The
results show that TEncDM exhibits superior performance compared to existing
non-autoregressive diffusion models. Our code is available at
https://github.com/M0RJIQUE/tencdm.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 13 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Natural Language Decompositions of Implicit Content Enable Better Text
  Representations <span class="chip">EMNLP 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.14583v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.14583v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alexander Hoyle, Rupak Sarkar, Pranav Goel, Philip Resnik
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  When people interpret text, they rely on inferences that go beyond the
observed language itself. Inspired by this observation, we introduce a method
for the analysis of text that takes implicitly communicated content explicitly
into account. We use a large language model to produce sets of propositions
that are inferentially related to the text that has been observed, then
validate the plausibility of the generated content via human judgments.
Incorporating these explicit representations of implicit content proves useful
in multiple problem settings that involve the human interpretation of
utterances: assessing the similarity of arguments, making sense of a body of
opinion data, and modeling legislative behavior. Our results suggest that
modeling the meanings behind observed language, rather than the literal text
alone, is a valuable direction for NLP and particularly its applications to
social science.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to EMNLP 2023 (Main conference)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AgentRefine: Enhancing Agent Generalization through Refinement Tuning <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.01702v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.01702v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dayuan Fu, Keqing He, Yejie Wang, Wentao Hong, Zhuoma Gongque, Weihao Zeng, Wei Wang, Jingang Wang, Xunliang Cai, Weiran Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Model (LLM) based agents have proved their ability to perform
complex tasks like humans. However, there is still a large gap between
open-sourced LLMs and commercial models like the GPT series. In this paper, we
focus on improving the agent generalization capabilities of LLMs via
instruction tuning. We first observe that the existing agent training corpus
exhibits satisfactory results on held-in evaluation sets but fails to
generalize to held-out sets. These agent-tuning works face severe formatting
errors and are frequently stuck in the same mistake for a long while. We
analyze that the poor generalization ability comes from overfitting to several
manual agent environments and a lack of adaptation to new situations. They
struggle with the wrong action steps and can not learn from the experience but
just memorize existing observation-action relations. Inspired by the insight,
we propose a novel AgentRefine framework for agent-tuning. The core idea is to
enable the model to learn to correct its mistakes via observation in the
trajectory. Specifically, we propose an agent synthesis framework to encompass
a diverse array of environments and tasks and prompt a strong LLM to refine its
error action according to the environment feedback. AgentRefine significantly
outperforms state-of-the-art agent-tuning work in terms of generalization
ability on diverse agent tasks. It also has better robustness facing
perturbation and can generate diversified thought in inference. Our findings
establish the correlation between agent generalization and self-refinement and
provide a new paradigm for future research.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ UniGenCoder: Merging Seq2Seq and Seq2Tree Paradigms for Unified Code
  Generation <span class="chip">ICSE 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.12490v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.12490v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Liangying Shao, Yanfu Yan, Denys Poshyvanyk, Jinsong Su
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep learning-based code generation has completely transformed the way
developers write programs today. Existing approaches to code generation have
focused either on the Sequence-to-Sequence paradigm, which generates target
code as a sequence of tokens, or the Sequence-to-Tree paradigm, which outputs
code as a sequence of actions. While these two paradigms are intuitively
complementary, their combination has not been previously explored. By comparing
the code generated under these two paradigms, we find that integrating them
holds significant potential. In this paper, we propose UniGenCoder for
code-related generation tasks, which consists of a shared encoder, a shared
decoder with a minimal set of additional parameters to unify two paradigms, and
a selector that dynamically chooses optimal paradigm for each instance. Also,
during the model training, we first perform the multi-task learning and
distillation strategies to facilitate knowledge transfer between two paradigms,
and then leverage contrastive learning to train the selector. Experimental
results on the text-to-code and code-to-code generation tasks demonstrate the
effectiveness of our proposed model. We release our code at
https://github.com/DeepLearnXMU/UniGenCoder.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>accepted to 47th International Conference on Software Engineering
  (ICSE 2025), NIER track</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Meta-Evaluation of Style and Attribute Transfer Metrics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.15022v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.15022v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Amalie Brogaard Pauli, Isabelle Augenstein, Ira Assent
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  LLMs make it easy to rewrite text in any style, be it more polite,
persuasive, or more positive. We present a large-scale study of evaluation
metrics for style and attribute transfer with a focus on content preservation;
meaning content not attributed to the style shift is preserved. The de facto
evaluation approach uses lexical or semantic similarity metrics often between
source sentences and rewrites. While these metrics are not designed to
distinguish between style or content differences, empirical meta-evaluation
shows a reasonable correlation to human judgment. In fact, recent works find
that LLMs prompted as evaluators are only comparable to semantic similarity
metrics, even though intuitively, the LLM approach should better fit the task.
To investigate this discrepancy, we benchmark 8 metrics for evaluating content
preservation on existing datasets and additionally construct a new test set
that better aligns with the meta-evaluation aim. Indeed, we then find that the
empirical conclusion aligns with the intuition: content preservation metrics
for style/attribute transfer must be conditional on the style shift. To support
this, we propose a new efficient zero-shot evaluation method using the
likelihood of the next token. We hope our meta-evaluation can foster more
research on evaluating content preservation metrics, and also to ensure fair
evaluation of methods for conducting style transfer.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CKnowEdit: A New Chinese Knowledge Editing <span class="highlight-title">Dataset</span> for Linguistics,
  Facts, and Logic Error Correction in LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.05806v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.05806v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jizhan Fang, Tianhe Lu, Yunzhi Yao, Ziyan Jiang, Xin Xu, Ningyu Zhang, Huajun Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Chinese, as a linguistic system rich in depth and complexity, is
characterized by distinctive elements such as ancient poetry, proverbs, idioms,
and other cultural constructs. However, current Large Language Models (LLMs)
face limitations in these specialized domains, highlighting the need for the
development of comprehensive datasets that can assess, continuously update, and
progressively improve these culturally-grounded linguistic competencies through
targeted training optimizations. To address this gap, we introduce CKnowEdit,
the first-ever Chinese knowledge editing dataset designed to correct
linguistic, factual, and logical errors in LLMs. We collect seven types of
knowledge from a wide range of sources, including classical texts, idioms, and
content from Baidu Tieba Ruozhiba, taking into account the unique polyphony,
antithesis, and logical structures inherent in the Chinese language. By
analyzing this dataset, we highlight the challenges current LLMs face in
mastering Chinese. Furthermore, our evaluation of state-of-the-art knowledge
editing techniques reveals opportunities to advance the correction of Chinese
knowledge. Code and dataset are available at
https://github.com/zjunlp/EasyEdit.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Ongoing work; project website is available at
  https://zjunlp.github.io/project/CKnowEdit code and dataset are available at
  https://github.com/zjunlp/EasyEdit</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ VisCon-100K: Leveraging Contextual Web Data for Fine-tuning Vision
  Language Models <span class="chip">PAKDD 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.10250v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.10250v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gokul Karthik Kumar, Iheb Chaabane, Kebin Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision-language models (VLMs) excel in various visual benchmarks but are
often constrained by the lack of high-quality visual fine-tuning data. To
address this challenge, we introduce VisCon-100K, a novel dataset derived from
interleaved image-text web documents. Our approach transforms 45K web documents
from the OBELICS dataset into 100K image conversation samples. We utilize
GPT-4V to generate image-contextual captions and OpenChat 3.5 model to convert
these captions into diverse free-form and multiple-choice question-answer
pairs. Integrating this dataset for fine-tuning considerably enhances VLM
performance across multiple benchmarks. Unlike methods that focus solely on
fine-grained visual content, our approach leverages accompanying web context,
yielding superior results. We also discover that a 'leaky modality mix', where
conversation samples contain questions answerable from both the image and its
contextual caption, outperforms non-leaky combinations of captions and Q&A
pairs. VisCon-100k dataset shows strong performance with two popular VLM
approaches: text-only large language model (LLM) aligned with a vision encoder
using image captions data (ShareGPT4V-7b) and multimodally pretrained LLM
(IDEFICS2-8b) using interleaved image-text data. In addition to releasing the
VisCon-100K dataset, we provide a contextual captioner trained on this dataset,
facilitating scalable fine-tuning data generation for future research and
open-source applications. Using the same pipeline, but substituting our trained
contextual captioner for GPT-4V, we also release the larger VisCon-1M dataset.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at PAKDD 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Step-Video-T2V Technical Report: The Practice, Challenges, and Future of
  Video Foundation Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.10248v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.10248v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guoqing Ma, Haoyang Huang, Kun Yan, Liangyu Chen, Nan Duan, Shengming Yin, Changyi Wan, Ranchen Ming, Xiaoniu Song, Xing Chen, Yu Zhou, Deshan Sun, Deyu Zhou, Jian Zhou, Kaijun Tan, Kang An, Mei Chen, Wei Ji, Qiling Wu, Wen Sun, Xin Han, Yanan Wei, Zheng Ge, Aojie Li, Bin Wang, Bizhu Huang, Bo Wang, Brian Li, Changxing Miao, Chen Xu, Chenfei Wu, Chenguang Yu, Dapeng Shi, Dingyuan Hu, Enle Liu, Gang Yu, Ge Yang, Guanzhe Huang, Gulin Yan, Haiyang Feng, Hao Nie, Haonan Jia, Hanpeng Hu, Hanqi Chen, Haolong Yan, Heng Wang, Hongcheng Guo, Huilin Xiong, Huixin Xiong, Jiahao Gong, Jianchang Wu, Jiaoren Wu, Jie Wu, Jie Yang, Jiashuai Liu, Jiashuo Li, Jingyang Zhang, Junjing Guo, Junzhe Lin, Kaixiang Li, Lei Liu, Lei Xia, Liang Zhao, Liguo Tan, Liwen Huang, Liying Shi, Ming Li, Mingliang Li, Muhua Cheng, Na Wang, Qiaohui Chen, Qinglin He, Qiuyan Liang, Quan Sun, Ran Sun, Rui Wang, Shaoliang Pang, Shiliang Yang, Sitong Liu, Siqi Liu, Shuli Gao, Tiancheng Cao, Tianyu Wang, Weipeng Ming, Wenqing He, Xu Zhao, Xuelin Zhang, Xianfang Zeng, Xiaojia Liu, Xuan Yang, Yaqi Dai, Yanbo Yu, Yang Li, Yineng Deng, Yingming Wang, Yilei Wang, Yuanwei Lu, Yu Chen, Yu Luo, Yuchu Luo, Yuhe Yin, Yuheng Feng, Yuxiang Yang, Zecheng Tang, Zekai Zhang, Zidong Yang, Binxing Jiao, Jiansheng Chen, Jing Li, Shuchang Zhou, Xiangyu Zhang, Xinhao Zhang, Yibo Zhu, Heung-Yeung Shum, Daxin Jiang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present Step-Video-T2V, a state-of-the-art text-to-video pre-trained model
with 30B parameters and the ability to generate videos up to 204 frames in
length. A deep compression Variational Autoencoder, Video-VAE, is designed for
video generation tasks, achieving 16x16 spatial and 8x temporal compression
ratios, while maintaining exceptional video reconstruction quality. User
prompts are encoded using two bilingual text encoders to handle both English
and Chinese. A DiT with 3D full attention is trained using Flow Matching and is
employed to denoise input noise into latent frames. A video-based DPO approach,
Video-DPO, is applied to reduce artifacts and improve the visual quality of the
generated videos. We also detail our training strategies and share key
observations and insights. Step-Video-T2V's performance is evaluated on a novel
video generation benchmark, Step-Video-T2V-Eval, demonstrating its
state-of-the-art text-to-video quality when compared with both open-source and
commercial engines. Additionally, we discuss the limitations of current
diffusion-based model paradigm and outline future directions for video
foundation models. We make both Step-Video-T2V and Step-Video-T2V-Eval
available at https://github.com/stepfun-ai/Step-Video-T2V. The online version
can be accessed from https://yuewen.cn/videos as well. Our goal is to
accelerate the innovation of video foundation models and empower video content
creators.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>36 pages, 14 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Unsupervised Mutual Learning of Discourse Parsing and Topic Segmentation
  in Dialogue 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.19799v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.19799v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiahui Xu, Feng Jiang, Anningzhe Gao, Luis Fernando D'Haro, Haizhou Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In dialogue systems, discourse plays a crucial role in managing
conversational focus and coordinating interactions. It consists of two key
structures: rhetorical structure and topic structure. The former captures the
logical flow of conversations, while the latter detects transitions between
topics. Together, they improve the ability of a dialogue system to track
conversation dynamics and generate contextually relevant high-quality
responses. These structures are typically identified through discourse parsing
and topic segmentation, respectively. However, existing supervised methods rely
on costly manual annotations, while unsupervised methods often focus on a
single task, overlooking the deep linguistic interplay between rhetorical and
topic structures. To address these issues, we first introduce a unified
representation that integrates rhetorical and topic structures, ensuring
semantic consistency between them. Under the unified representation, we further
propose two linguistically grounded hypotheses based on discourse theories: (1)
Local Discourse Coupling, where rhetorical cues dynamically enhance topic-aware
information flow, and (2) Global Topology Constraint, where topic structure
patterns probabilistically constrain rhetorical relation distributions.
Building on the unified representation and two hypotheses, we propose an
unsupervised mutual learning framework (UMLF) that jointly models rhetorical
and topic structures, allowing them to mutually reinforce each other without
requiring additional annotations. We evaluate our approach on two rhetorical
datasets and three topic segmentation datasets. Experimental results
demonstrate that our method surpasses all strong baselines built on pre-trained
language models. Furthermore, when applied to LLMs, our framework achieves
notable improvements, demonstrating its effectiveness in improving discourse
structure modeling.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MdEval: Massively Multilingual Code Debugging 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02310v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02310v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shukai Liu, Linzheng Chai, Jian Yang, Jiajun Shi, He Zhu, Liran Wang, Ke Jin, Wei Zhang, Hualei Zhu, Shuyue Guo, Tao Sun, Jiaheng Liu, Yunlong Duan, Yu Hao, Liqun Yang, Guanglin Niu, Ge Zhang, Zhoujun Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Code large language models (LLMs) have made significant progress in code
debugging by directly generating the correct code based on the buggy code
snippet. Programming benchmarks, typically consisting of buggy code snippet and
their associated test cases, are used to assess the debugging capabilities of
LLMs. However, many existing benchmarks primarily focus on Python and are often
limited in terms of language diversity (e.g., DebugBench and DebugEval). To
advance the field of multilingual debugging with LLMs, we propose the first
massively multilingual debugging benchmark, which includes 3.6K test samples of
18 programming languages and covers the automated program repair (APR) task,
the code review (CR) task, and the bug identification (BI) task. Further, we
introduce the debugging instruction corpora MDEVAL-INSTRUCT by injecting bugs
into the correct multilingual queries and solutions (xDebugGen). Further, a
multilingual debugger xDebugCoder trained on MDEVAL-INSTRUCT as a strong
baseline specifically to handle the bugs of a wide range of programming
languages (e.g. "Missing Mut" in language Rust and "Misused Macro Definition"
in language C). Our extensive experiments on MDEVAL reveal a notable
performance gap between open-source models and closed-source LLMs (e.g., GPT
and Claude series), highlighting huge room for improvement in multilingual code
debugging scenarios.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Residual Stream Analysis with Multi-Layer SAEs <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.04185v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.04185v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tim Lawson, Lucy Farnik, Conor Houghton, Laurence Aitchison
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sparse autoencoders (SAEs) are a promising approach to interpreting the
internal representations of transformer language models. However, SAEs are
usually trained separately on each transformer layer, making it difficult to
use them to study how information flows across layers. To solve this problem,
we introduce the multi-layer SAE (MLSAE): a single SAE trained on the residual
stream activation vectors from every transformer layer. Given that the residual
stream is understood to preserve information across layers, we expected MLSAE
latents to 'switch on' at a token position and remain active at later layers.
Interestingly, we find that individual latents are often active at a single
layer for a given token or prompt, but the layer at which an individual latent
is active may differ for different tokens or prompts. We quantify these
phenomena by defining a distribution over layers and considering its variance.
We find that the variance of the distributions of latent activations over
layers is about two orders of magnitude greater when aggregating over tokens
compared with a single token. For larger underlying models, the degree to which
latents are active at multiple layers increases, which is consistent with the
fact that the residual stream activation vectors at adjacent layers become more
similar. Finally, we relax the assumption that the residual stream basis is the
same at every layer by applying pre-trained tuned-lens transformations, but our
findings remain qualitatively similar. Our results represent a new approach to
understanding how representations change as they flow through transformers. We
release our code to train and analyze MLSAEs at
https://github.com/tim-lawson/mlsae.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2025 Camera Ready. 45 pages, 41 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> HarmAug: Effective Data Augmentation for Knowledge Distillation of
  Safety Guard Models <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01524v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01524v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Seanie Lee, Haebin Seong, Dong Bok Lee, Minki Kang, Xiaoyin Chen, Dominik Wagner, <span class="highlight-author">Yoshua Bengio</span>, Juho Lee, Sung Ju Hwang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Safety guard models that detect malicious queries aimed at large language
models (LLMs) are essential for ensuring the secure and responsible deployment
of LLMs in real-world applications. However, deploying existing safety guard
models with billions of parameters alongside LLMs on mobile devices is
impractical due to substantial memory requirements and latency. To reduce this
cost, we distill a large teacher safety guard model into a smaller one using a
labeled dataset of instruction-response pairs with binary harmfulness labels.
Due to the limited diversity of harmful instructions in the existing labeled
dataset, naively distilled models tend to underperform compared to larger
models. To bridge the gap between small and large models, we propose HarmAug, a
simple yet effective data augmentation method that involves jailbreaking an LLM
and prompting it to generate harmful instructions. Given a prompt such as,
"Make a single harmful instruction prompt that would elicit offensive content",
we add an affirmative prefix (e.g., "I have an idea for a prompt:") to the
LLM's response. This encourages the LLM to continue generating the rest of the
response, leading to sampling harmful instructions. Another LLM generates a
response to the harmful instruction, and the teacher model labels the
instruction-response pair. We empirically show that our HarmAug outperforms
other relevant baselines. Moreover, a 435-million-parameter safety guard model
trained with HarmAug achieves an F1 score comparable to larger models with over
7 billion parameters, and even outperforms them in AUPRC, while operating at
less than 25% of their computational cost.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Conditional [MASK] Discrete Diffusion Language Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.06438v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.06438v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hyukhun Koh, Minha Jhang, Dohyung Kim, Sangmook Lee, Kyomin Jung
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Although auto-regressive models excel in natural language processing, they
often struggle to generate diverse text and provide limited controllability.
Non-auto-regressive methods could be an alternative but often produce
degenerate outputs and exhibit shortcomings in conditional generation. To
address these challenges, we propose Diffusion-EAGS, a novel framework that
integrates conditional masked language models into diffusion language models
through the theoretical lens of a conditional Markov Random Field. In doing so,
we propose entropy-adaptive Gibbs sampling and entropy-based noise scheduling
to counterbalance each model's shortcomings. Experimental results show that
Diffusion-EAGS outperforms baselines and achieves the best quality-diversity
tradeoff, demonstrating its effectiveness in non-autoregressive text
generation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Reinforcement Learning Enhanced LLMs: A <span class="highlight-title">Survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.10400v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.10400v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shuhe Wang, Shengyu Zhang, Jie Zhang, Runyi Hu, Xiaoya Li, Tianwei Zhang, Jiwei Li, Fei Wu, Guoyin Wang, Eduard Hovy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reinforcement learning (RL) enhanced large language models (LLMs),
particularly exemplified by DeepSeek-R1, have exhibited outstanding
performance. Despite the effectiveness in improving LLM capabilities, its
implementation remains highly complex, requiring complex algorithms, reward
modeling strategies, and optimization techniques. This complexity poses
challenges for researchers and practitioners in developing a systematic
understanding of RL-enhanced LLMs. Moreover, the absence of a comprehensive
survey summarizing existing research on RL-enhanced LLMs has limited progress
in this domain, hindering further advancements.
  In this work, we are going to make a systematic review of the most up-to-date
state of knowledge on RL-enhanced LLMs, attempting to consolidate and analyze
the rapidly growing research in this field, helping researchers understand the
current challenges and advancements. Specifically, we (1) detail the basics of
RL; (2) introduce popular RL-enhanced LLMs; (3) review researches on two
widely-used reward model-based RL techniques: Reinforcement Learning from Human
Feedback (RLHF) and Reinforcement Learning from AI Feedback (RLAIF); and (4)
explore Direct Preference Optimization (DPO), a set of methods that bypass the
reward model to directly use human preference data for aligning LLM outputs
with human expectations. We will also point out current challenges and
deficiencies of existing methods and suggest some avenues for further
improvements. Project page of this work can be found at
https://github.com/ShuheWang1998/Reinforcement-Learning-Enhanced-LLMs-A-Survey.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DynamicNER: A Dynamic, Multilingual, and Fine-Grained <span class="highlight-title">Dataset</span> for
  LLM-based Named Entity Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.11022v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.11022v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hanjun Luo, Yingbin Jin, Xinfeng Li, Xuecheng Liu, Ruizhe Chen, Tong Shang, Kun Wang, Qingsong Wen, Zuozhu Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the advancement of Large Language Models (LLMs), more and more
researchers apply LLMs for Named Entity Recognition (NER) methods, bringing
vitality to this classical Natural Language Processing task. However, existing
datasets are designed for traditional machine learning methods, inadequate for
LLM-based methods in terms of corpus selection, entity categorization, and
design logic. This limitation leads to less effective evaluation and model
fine-tuning. To address this issue, we propose DynamicNER, the first NER
dataset specifically designed for LLMs and with dynamic categorization,
transcending the limitations of fixed categorization in existing datasets. It
is also multi-lingual and multi-granular, covering 8 languages and 155 entity
types, with corpus spanning multiple specialized domains. Furthermore, in
response to the limitations demonstrated by existing LLM-based methods during
DynamicNER testing, we develop CascadeNER, a novel NER method based on a
two-stage strategy and lightweight LLMs, addressing the problems in current
methods. Experiments show that DynamicNER is an effective benchmark for
LLM-based NER methods, and CascadeNER outperforms existing methods with fewer
computational resources. Our work is opened at
https://github.com/CascadeNER/CascadeNER.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ HSR-Enhanced Sparse Attention Acceleration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.10165v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.10165v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bo Chen, Yingyu Liang, Zhizhou Sha, Zhenmei Shi, Zhao Song
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have demonstrated remarkable capabilities across
various applications, but their performance on long-context tasks is often
limited by the computational complexity of attention mechanisms. We introduce a
novel approach to accelerate attention computation in LLMs, particularly for
long-context scenarios. We leverage the inherent sparsity within attention
mechanisms, both in conventional Softmax attention and ReLU attention (with
$\mathsf{ReLU}^\alpha$ activation, $\alpha \in \mathbb{N}_+$), to significantly
reduce the running time complexity. Our method employs a Half-Space Reporting
(HSR) data structure to identify non-zero or ``massively activated'' entries in
the attention matrix. We present theoretical analyses for two key scenarios:
generation decoding and prompt prefilling. Our approach achieves a running time
of $O(mn^{4/5})$ significantly faster than the naive approach $O(mn)$ for
generation decoding, where $n$ is the context length, $m$ is the query length,
and $d$ is the hidden dimension. We can also reduce the running time for prompt
prefilling from $O(mn)$ to $O(mn^{1 - 1 / \lfloor d/2\rfloor} + mn^{4/5})$. Our
method introduces only provably negligible error for Softmax attention. This
work represents a significant step towards enabling efficient long-context
processing in LLMs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CPAL 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Neuro-Symbolic Integration Brings Causal and Reliable Reasoning Proofs <span class="chip">NAACL2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.09802v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.09802v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sen Yang, Xin Li, Leyang Cui, Lidong Bing, Wai Lam
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Two lines of approaches are adopted for complex reasoning with LLMs. One line
of work prompts LLMs with various reasoning structures, while the structural
outputs can be naturally regarded as intermediate reasoning steps. Another line
of work adopt LLM-free declarative solvers to do the reasoning task, rendering
higher reasoning accuracy but lacking interpretability due to the black-box
nature of the solvers. Aiming to resolve the trade-off between answer accuracy
and interpretability, we present a simple extension to the latter line of work.
Specifically, we showcase that the intermediate search logs generated by Prolog
interpreters can be accessed and interpreted into human-readable reasoning
proofs. As long as LLMs correctly translate problem descriptions into Prolog
representations, the corresponding reasoning proofs are ensured to be causal
and reliable. On two logical reasoning and one arithmetic reasoning datasets,
our framework obtains significant improvements in terms of both answer accuracy
and reasoning proof accuracy. Our code is released at
https://github.com/DAMO-NLP-SG/CaRing
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear in Findings of NAACL2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Balancing Speciality and Versatility: A Coarse to Fine Framework for
  Mitigating Catastrophic Forgetting in Large Language Models <span class="chip">ACL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.10306v6">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.10306v6.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hengyuan Zhang, Yanru Wu, Dawei Li, Sak Yang, Rui Zhao, Yong Jiang, Fei Tan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Aligned Large Language Models (LLMs) showcase remarkable versatility, capable
of handling diverse real-world tasks. Meanwhile, aligned LLMs are also expected
to exhibit speciality, excelling in specific applications. However, fine-tuning
with extra data, a common practice to gain speciality, often leads to
catastrophic forgetting (CF) of previously acquired versatility, hindering the
model's performance across diverse tasks. In response to this challenge, we
propose CoFiTune, a coarse to fine framework in an attempt to strike the
balance between speciality and versatility. At the coarse-grained level, an
empirical tree-search algorithm is utilized to pinpoint and update specific
modules that are crucial for speciality, while keeping other parameters frozen;
at the fine-grained level, a soft-masking mechanism regulates the update to the
LLMs, mitigating the CF issue without harming speciality. In an overall
evaluation of both speciality and versatility, CoFiTune consistently
outperforms baseline methods across diverse tasks and model scales. Compared to
the full-parameter SFT, CoFiTune leads to about 14% versatility improvement and
marginal speciality loss on a 13B model. Lastly, based on further analysis, we
provide a speculative insight into the information forwarding process in LLMs,
which helps explain the effectiveness of the proposed method. The code is
available at https://github.com/rattlesnakey/CoFiTune.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>43 pages, 10 figures, accepted by ACL 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Take the essence and discard the dross: A Rethinking on Data Selection
  for Fine-Tuning Large Language Models <span class="chip">NAACL 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.14115v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.14115v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziche Liu, Rui Ke, Yajiao Liu, Feng Jiang, Haizhou Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Data selection for fine-tuning large language models (LLMs) aims to choose a
high-quality subset from existing datasets, allowing the trained model to
outperform baselines trained on the full dataset. However, the expanding body
of research lacks a clear, unified framework, and the variability in
experimental settings complicates systematic comparisons. While existing
surveys comprehensively overview the stages and methods of data selection, they
often overlook an in-depth exploration of the fine-tuning phase. In this paper,
we conduct a focused review of recent data selection techniques for fine-tuning
LLMs, analyzing a dozen key studies. We introduce a novel three-stage scheme -
comprising feature extraction, criteria design, and selector evaluation - to
systematically categorize and evaluate these methods. Additionally, we propose
a unified comparison approach that incorporates ratio-based efficiency and
ranking-based feasibility metrics to address inconsistencies across
experiments. Our findings reveal that methods emphasizing more targeted quality
measurement achieve higher efficiency but at the cost of feasibility. Finally,
we discuss trends and highlight four key challenges in fine-tuning data
selection, offering potential directions for future research.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by the NAACL 2025 main conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ REFA: Reference Free Alignment for multi-preference optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.16378v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.16378v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Taneesh Gupta, Rahul Madhavan, Xuchao Zhang, Chetan Bansal, Saravan Rajmohan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce $\textbf{REFA}$, a family of reference-free alignment methods
that optimize over multiple user preferences while enforcing fine-grained
length control. Our approach integrates deviation-based weighting to emphasize
high-quality responses, length normalization to prevent trivial short-response
solutions, and an EOS-probability regularizer to mitigate dataset-induced
brevity biases. Theoretically, we show that under the Uncertainty Reduction
with Sequence Length Assertion (URSLA) framework, naive length normalization
can still incentivize length-based shortcuts. In contrast, REFA corrects these
subtle incentives, guiding models toward genuinely more informative and
higher-quality outputs. Empirically, REFA achieves a new
$\textbf{state-of-the-art}$ among reference-free alignment methods, generating
richer responses that align more closely with human preferences. Notably, REFA
improves performance on the AlpacaEval2 benchmark, achieving a $\textbf{26.6%}$
Length-Controlled Win Rate (LC-WR) and $\textbf{24.2%}$ Win Rate (WR).
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ EoRA: Training-free Compensation for Compressed LLM with Eigenspace
  Low-Rank Approximation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.21271v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.21271v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shih-Yang Liu, Maksim Khadkevich, Nai Chit Fung, Charbel Sakr, Chao-Han Huck Yang, Chien-Yi Wang, Saurav Muralidharan, Hongxu Yin, Kwang-Ting Cheng, Jan Kautz, Yu-Chiang Frank Wang, Pavlo Molchanov, Min-Hung Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we re-formulate the model compression problem into the
customized compensation problem: Given a compressed model, we aim to introduce
residual low-rank paths to compensate for compression errors under customized
requirements from users (e.g., tasks, compression ratios), resulting in greater
flexibility in balancing accuracy and overhead(inference and model size)
without being bound to fixed compression formats. However, naively applying SVD
to derive residual paths causes suboptimal utilization of the low-rank
representation capacity. Instead, we propose Training-free Eigenspace Low-Rank
Approximation (EoRA), a method that directly minimizes compression-induced
errors without requiring gradient-based training, achieving fast optimization
in minutes using a small amount of calibration data. EoRA projects compression
errors into the eigenspace of input activations, leveraging eigenvalues to
effectively prioritize the reconstruction of high-importance error components.
Moreover, EoRA can be seamlessly integrated with fine-tuning and quantization
to further improve effectiveness and efficiency. EoRA consistently outperforms
previous methods in compensating errors for compressed LLaMA2/3 models on
various tasks, such as language generation, commonsense reasoning, and math
reasoning tasks (e.g., 31.31%/12.88% and 9.69% improvements on
ARC-Easy/ARC-Challenge and MathQA when compensating LLaMA3-8B that is quantized
to 4-bit and pruned to 2:4 sparsity). EoRA offers a scalable, training-free
solution to compensate for compression errors, making it a powerful tool to
deploy LLMs more flexibly. Code is available at https://github.com/NVlabs/EoRA.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Data-free Weight Compress and Denoise for Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.16319v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.16319v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Runyu Peng, Yunhua Zhou, Qipeng Guo, Yang Gao, Hang Yan, Xipeng Qiu, Dahua Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) are reshaping the research landscape in
artificial intelligence, particularly as model parameters scale up
significantly, unlocking remarkable capabilities across various domains.
Nevertheless, the scalability of model parameters faces constraints due to
limitations in GPU memory and computational speed. To address these
constraints, various weight compression methods have emerged, such as Pruning
and Quantization. Given the low-rank nature of weight matrices in language
models, the reduction of weights through matrix decomposition undoubtedly holds
significant potential and promise. In this paper, drawing upon the intrinsic
structure of LLMs, we propose a novel approach termed Data-free Joint Rank-k
Approximation for compressing the parameter matrices. Significantly, our method
is characterized by without necessitating additional involvement of any corpus,
while simultaneously preserving orthogonality in conjunction with pruning and
quantization methods. We achieve a model pruning of 80% parameters while
retaining 93.43% of the original performance without any calibration data.
Additionally, we explore the fundamental properties of the weight matrix of
LLMs undergone Rank-k Approximation and conduct comprehensive experiments to
elucidate our hypothesis.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ URSA: Understanding and Verifying Chain-of-thought Reasoning in
  Multimodal Mathematics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.04686v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.04686v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruilin Luo, Zhuofan Zheng, Yifan Wang, Yiyao Yu, Xinzhe Ni, Zicheng Lin, Jin Zeng, Yujiu Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Chain-of-Thought (CoT) reasoning is widely used to enhance the mathematical
reasoning capabilities of large language models (LLMs). The introduction of
process supervision for CoT trajectories has sparked discussions on improving
test-time scaling, thereby unlocking the System 2-style thinking capabilities
of these models. However, in multimodal mathematical reasoning, the scarcity of
high-quality CoT training data has hindered existing models from achieving both
deliberate reasoning and fine-grained verification. In this work, we propose a
novel framework that introduces System 2-style thinking to multimodal
mathematical reasoning. We introduce a three-module CoT data synthesis process
that integrates CoT distillation, trajectory-format rewriting, and format
unification. This process generates MMathCoT-1M, a high-quality CoT reasoning
instruction fine-tuning dataset. Furthermore, we implement a dual-view
trajectory labeling automation that targets both visual grounding fidelity and
deductive chain validity, resulting in the DualMath-1.1M dataset. The URSA-8B
model, trained on MMathCoT-1M, achieves new state-of-the-art (SOTA) performance
among similarly sized multimodal LLMs on six popular reasoning benchmarks.
Training URSA-8B further on the DualMath-1.1M dataset yields URSA-RM-8B, a
verifier that enhances URSA-8B's test-time performance and surpasses strong
closed-source multimodal MLLMs like GPT-4o. The model weights, training data,
and code have been open-sourced: https://github.com/URSA-MATH/URSA-MATH.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Fix typos and add results. 27 pages, 11 tables, 17 figures. Models,
  training data and code have been open-sourced. Project url:
  https://ursa-math.github.io</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CIC: A Framework for Culturally-Aware Image Captioning <span class="chip">IJCAI 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.05374v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.05374v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Youngsik Yun, Jihie Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Image Captioning generates descriptive sentences from images using
Vision-Language Pre-trained models (VLPs) such as BLIP, which has improved
greatly. However, current methods lack the generation of detailed descriptive
captions for the cultural elements depicted in the images, such as the
traditional clothing worn by people from Asian cultural groups. In this paper,
we propose a new framework, Culturally-aware Image Captioning (CIC), that
generates captions and describes cultural elements extracted from cultural
visual elements in images representing cultures. Inspired by methods combining
visual modality and Large Language Models (LLMs) through appropriate prompts,
our framework (1) generates questions based on cultural categories from images,
(2) extracts cultural visual elements from Visual Question Answering (VQA)
using generated questions, and (3) generates culturally-aware captions using
LLMs with the prompts. Our human evaluation conducted on 45 participants from 4
different cultural groups with a high understanding of the corresponding
culture shows that our proposed framework generates more culturally descriptive
captions when compared to the image captioning baseline based on VLPs.
Resources can be found at https://shane3606.github.io/cic..
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by IJCAI 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DynaMath: A Dynamic Visual Benchmark for Evaluating Mathematical
  Reasoning Robustness of Vision Language Models <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.00836v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.00836v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chengke Zou, Xingang Guo, Rui Yang, Junyu Zhang, Bin Hu, Huan Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid advancements in Vision-Language Models (VLMs) have shown great
potential in tackling mathematical reasoning tasks that involve visual context.
Unlike humans who can reliably apply solution steps to similar problems with
minor modifications, we found that SOTA VLMs like GPT-4o can consistently fail
in these scenarios, revealing limitations in their mathematical reasoning
capabilities. In this paper, we investigate the mathematical reasoning
robustness in VLMs and evaluate how well these models perform under different
variants of the same question, such as changes in visual numerical values or
function graphs. While several vision-based math benchmarks have been developed
to assess VLMs' problem-solving capabilities, these benchmarks contain only
static sets of problems and cannot easily evaluate mathematical reasoning
robustness. To fill this gap, we introduce DynaMath, a dynamic visual math
benchmark designed for in-depth assessment of VLMs. DynaMath includes 501
high-quality, multi-topic seed questions, each represented as a Python program.
Those programs are carefully designed and annotated to enable the automatic
generation of a much larger set of concrete questions, including many different
types of visual and textual variations. DynaMath allows us to evaluate the
generalization ability of VLMs, by assessing their performance under varying
input conditions of a seed question. We evaluated 14 SOTA VLMs with 5,010
generated concrete questions. Our results show that the worst-case model
accuracy, defined as the percentage of correctly answered seed questions in all
10 variants, is significantly lower than the average-case accuracy. Our
analysis emphasizes the need to study the robustness of VLMs' reasoning
abilities, and DynaMath provides valuable insights to guide the development of
more reliable models for mathematical reasoning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICLR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GSQ-Tuning: Group-Shared Exponents Integer in Fully Quantized Training
  for LLMs On-Device Fine-tuning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.12913v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.12913v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sifan Zhou, Shuo Wang, Zhihang Yuan, Mingjia Shi, Yuzhang Shang, Dawei Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) fine-tuning technologies have achieved
remarkable results. However, traditional LLM fine-tuning approaches face
significant challenges: they require large Floating Point (FP) computation,
raising privacy concerns when handling sensitive data, and are impractical for
resource-constrained edge devices. While Parameter-Efficient Fine-Tuning (PEFT)
techniques reduce trainable parameters, their reliance on floating-point
arithmetic creates fundamental incompatibilities with edge hardware. In this
work, we introduce a novel framework for on-device LLM fine-tuning that
eliminates the need for floating-point operations in both inference and
training, named GSQ-Tuning. At its core is the Group-Shared Exponents Integer
format, which efficiently represents model parameters in integer format using
shared exponents among parameter groups. When combined with LoRA-like adapters,
this enables fully integer-based fine-tuning that is both memory and compute
efficient. We demonstrate that our approach achieves accuracy comparable to
BF16-based fine-tuning while significantly reducing 1.85x memory usage.
Moreover, compared to FP8, our method can reduce 5x power consumption and 11x
chip area with same performance, making large-scale model adaptation feasible
on edge devices.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AAAR-1.0: Assessing AI's Potential to Assist Research 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.22394v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.22394v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Renze Lou, Hanzi Xu, Sijia Wang, Jiangshu Du, Ryo Kamoi, Xiaoxin Lu, Jian Xie, Yuxuan Sun, Yusen Zhang, Jihyun Janice Ahn, Hongchao Fang, Zhuoyang Zou, Wenchao Ma, Xi Li, Kai Zhang, Congying Xia, Lifu Huang, Wenpeng Yin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Numerous studies have assessed the proficiency of AI systems, particularly
large language models (LLMs), in facilitating everyday tasks such as email
writing, question answering, and creative content generation. However,
researchers face unique challenges and opportunities in leveraging LLMs for
their own work, such as brainstorming research ideas, designing experiments,
and writing or reviewing papers. In this study, we introduce AAAR-1.0, a
benchmark dataset designed to evaluate LLM performance in three fundamental,
expertise-intensive research tasks: (i) EquationInference, assessing the
correctness of equations based on the contextual information in paper
submissions; (ii) ExperimentDesign, designing experiments to validate research
ideas and solutions; (iii) PaperWeakness, identifying weaknesses in paper
submissions; and (iv) REVIEWCRITIQUE, identifying each segment in human reviews
is deficient or not. AAAR-1.0 differs from prior benchmarks in two key ways:
first, it is explicitly research-oriented, with tasks requiring deep domain
expertise; second, it is researcher-oriented, mirroring the primary activities
that researchers engage in on a daily basis. An evaluation of both open-source
and proprietary LLMs reveals their potential as well as limitations in
conducting sophisticated research tasks. We will keep iterating AAAR-1.0 to new
versions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project Webpage: https://renzelou.github.io/AAAR-1.0/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Krutrim LLM: Multilingual Foundational Model for over a Billion People 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09642v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09642v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aditya Kallappa, Palash Kamble, Abhinav Ravi, Akshat Patidar, Vinayak Dhruv, Deepak Kumar, Raghav Awasthi, Arveti Manjunath, Himanshu Gupta, Shubham Agarwal, Kumar Ashish, Gautam Bhargava, Chandra Khatri
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  India is a diverse society with unique challenges in developing AI systems,
including linguistic diversity, oral traditions, data accessibility, and
scalability. Existing foundation models are primarily trained on English,
limiting their effectiveness for India's population. Indic languages comprise
only 1 percent of Common Crawl corpora despite India representing 18 percent of
the global population, leading to linguistic biases. Thousands of regional
languages, dialects, and code mixing create additional representation
challenges due to sparse training data.
  We introduce Krutrim LLM, a 2 trillion token multilingual model designed for
India's linguistic landscape. It incorporates the largest known Indic dataset,
mitigating data scarcity and ensuring balanced performance across dialects.
Krutrim outperforms or matches state-of-the-art models on Indic benchmarks
while maintaining competitive English performance. Despite being significantly
smaller in training flops, Krutrim LLM matches or exceeds models like LLAMA-2
on 10 out of 16 tasks, with an average score of 0.57 versus 0.55. This
evidences Krutrim's flexible multilingual fluency across diverse linguistic
contexts.
  Krutrim is integrated with real-time search to improve factual accuracy in
conversational AI applications. This enhances accessibility for over 1 billion
users worldwide. Through intentional design choices addressing data imbalances,
Krutrim LLM signifies meaningful progress in building ethical, globally
representative AI models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Just KIDDIN: Knowledge Infusion and Distillation for Detection of
  INdecent Memes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.12174v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.12174v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rahul Garg, Trilok Padhi, Hemang Jain, Ugur Kursuncu, Ponnurangam Kumaraguru
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Toxicity identification in online multimodal environments remains a
challenging task due to the complexity of contextual connections across
modalities (e.g., textual and visual). In this paper, we propose a novel
framework that integrates Knowledge Distillation (KD) from Large Visual
Language Models (LVLMs) and knowledge infusion to enhance the performance of
toxicity detection in hateful memes. Our approach extracts sub-knowledge graphs
from ConceptNet, a large-scale commonsense Knowledge Graph (KG) to be infused
within a compact VLM framework. The relational context between toxic phrases in
captions and memes, as well as visual concepts in memes enhance the model's
reasoning capabilities. Experimental results from our study on two hate speech
benchmark datasets demonstrate superior performance over the state-of-the-art
baselines across AU-ROC, F1, and Recall with improvements of 1.1%, 7%, and 35%,
respectively. Given the contextual complexity of the toxicity detection task,
our approach showcases the significance of learning from both explicit (i.e.
KG) as well as implicit (i.e. LVLMs) contextual cues incorporated through a
hybrid neurosymbolic approach. This is crucial for real-world applications
where accurate and scalable recognition of toxic content is critical for
creating safer online environments.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GraphTeam: Facilitating Large Language Model-based Graph Analysis via
  Multi-Agent Collaboration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.18032v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.18032v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xin Sky Li, Qizhi Chu, Yubin Chen, Yang Liu, Yaoqi Liu, Zekai Yu, Weize Chen, Chen Qian, Chuan Shi, Cheng Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graphs are widely used for modeling relational data in real-world scenarios,
such as social networks and urban computing. Existing LLM-based graph analysis
approaches either integrate graph neural networks (GNNs) for specific machine
learning tasks, limiting their transferability, or rely solely on LLMs'
internal reasoning ability, resulting in suboptimal performance. To address
these limitations, we take advantage of recent advances in LLM-based agents,
which have shown capabilities of utilizing external knowledge or tools for
problem solving. By simulating human problem-solving strategies such as analogy
and collaboration, we propose a multi-agent system based on LLMs named
GraphTeam, for graph analysis. GraphTeam consists of five LLM-based agents from
three modules, and the agents with different specialities can collaborate with
each other to address complex problems. Specifically, (1) input-output
normalization module: the question agent extracts and refines four key
arguments from the original question, facilitating the problem understanding,
and the answer agent organizes the results to meet the output requirement; (2)
external knowledge retrieval module: we first build a knowledge base consisting
of relevant documentation and experience information, and then the search agent
retrieves the most relevant entries for each question. (3) problem-solving
module: given the retrieved information from search agent, the coding agent
uses established algorithms via programming to generate solutions, and in case
the coding agent does not work, the reasoning agent will directly compute the
results without programming. Extensive experiments on six graph analysis
benchmarks demonstrate that GraphTeam achieves state-of-the-art performance
with an average 25.85% improvement over the best baseline in terms of accuracy.
The code and data are available at https://github.com/BUPT-GAMMA/GraphTeam.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Leveraging Dual Process Theory in Language Agent Framework for Real-time
  Simultaneous Human-AI Collaboration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11882v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11882v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shao Zhang, Xihuai Wang, Wenhao Zhang, Chaoran Li, Junru Song, Tingyu Li, Lin Qiu, Xuezhi Cao, Xunliang Cai, Wen Yao, Weinan Zhang, Xinbing Wang, Ying Wen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Agents built on large language models (LLMs) have excelled in turn-by-turn
human-AI collaboration but struggle with simultaneous tasks requiring real-time
interaction. Latency issues and the challenge of inferring variable human
strategies hinder their ability to make autonomous decisions without explicit
instructions. Through experiments with current independent System 1 and System
2 methods, we validate the necessity of using Dual Process Theory (DPT) in
real-time tasks. We propose DPT-Agent, a novel language agent framework that
integrates System 1 and System 2 for efficient real-time simultaneous human-AI
collaboration. DPT-Agent's System 1 uses a Finite-state Machine (FSM) and
code-as-policy for fast, intuitive, and controllable decision-making.
DPT-Agent's System 2 integrates Theory of Mind (ToM) and asynchronous
reflection to infer human intentions and perform reasoning-based autonomous
decisions. We demonstrate the effectiveness of DPT-Agent through further
experiments with rule-based agents and human collaborators, showing significant
improvements over mainstream LLM-based frameworks. To the best of our
knowledge, DPT-Agent is the first language agent framework that achieves
successful real-time simultaneous human-AI collaboration autonomously. Code of
DPT-Agent can be found in https://github.com/sjtu-marl/DPT-Agent.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint under review. Update the experimental results of the
  DeepSeek-R1 series models</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning Evolving Tools for Large Language Models <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.06617v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.06617v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guoxin Chen, Zhong Zhang, Xin Cong, Fangda Guo, Yesai Wu, Yankai Lin, Wenzheng Feng, Yasheng Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Tool learning enables large language models (LLMs) to interact with external
tools and APIs, greatly expanding the application scope of LLMs. However, due
to the dynamic nature of external environments, these tools and APIs may become
outdated over time, preventing LLMs from correctly invoking tools. Existing
research primarily focuses on static environments and overlooks this issue,
limiting the adaptability of LLMs in real-world applications. In this paper, we
propose ToolEVO, a novel framework designed to enhance the adaptive and
reflective capabilities of LLMs against tool variability. By leveraging Monte
Carlo Tree Search, ToolEVO facilitates active exploration and interaction of
LLMs within dynamic environments, allowing for autonomous self-reflection and
self-updating of tool usage based on environmental feedback. Additionally, we
introduce ToolQA-D, a benchmark specifically designed to evaluate the impact of
tool variability. Extensive experiments demonstrate the effectiveness and
stability of our approach, highlighting the importance of adaptability to tool
variability for effective tool learning. Code:
https://github.com/Chen-GX/ToolEVO
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Camera ready version for ICLR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ An Efficient Sign Language Translation Using Spatial Configuration and
  Motion Dynamics with LLMs <span class="chip">NAACL 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.10593v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.10593v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Eui Jun Hwang, Sukmin Cho, Junmyeong Lee, Jong C. Park
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Gloss-free Sign Language Translation (SLT) converts sign videos directly into
spoken language sentences without relying on glosses. Recently, Large Language
Models (LLMs) have shown remarkable translation performance in gloss-free
methods by harnessing their powerful natural language generation capabilities.
However, these methods often rely on domain-specific fine-tuning of visual
encoders to achieve optimal results. By contrast, this paper emphasizes the
importance of capturing the spatial configurations and motion dynamics inherent
in sign language. With this in mind, we introduce Spatial and Motion-based Sign
Language Translation (SpaMo), a novel LLM-based SLT framework. The core idea of
SpaMo is simple yet effective. We first extract spatial and motion features
using off-the-shelf visual encoders and then input these features into an LLM
with a language prompt. Additionally, we employ a visual-text alignment process
as a warm-up before the SLT supervision. Our experiments demonstrate that SpaMo
achieves state-of-the-art performance on two popular datasets, PHOENIX14T and
How2Sign.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to NAACL 2025 main</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Can Large Language Models Analyze Graphs like Professionals? A
  Benchmark, <span class="highlight-title">Dataset</span>s and Models <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.19667v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.19667v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xin Sky Li, Weize Chen, Qizhi Chu, Haopeng Li, Zhaojun Sun, Ran Li, Chen Qian, Yiwei Wei, Zhiyuan Liu, Chuan Shi, Maosong Sun, Cheng Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The need to analyze graphs is ubiquitous across various fields, from social
networks to biological research and recommendation systems. Therefore, enabling
the ability of large language models (LLMs) to process graphs is an important
step toward more advanced general intelligence. However, current LLM benchmarks
on graph analysis require models to directly reason over the prompts describing
graph topology, and are thus limited to small graphs with only a few dozens of
nodes. In contrast, human experts typically write programs based on popular
libraries for task solving, and can thus handle graphs with different scales.
To this end, a question naturally arises: can LLMs analyze graphs like
professionals? In this paper, we introduce ProGraph, a manually crafted
benchmark containing 3 categories of graph tasks. The benchmark expects
solutions based on programming instead of directly reasoning over raw inputs.
Our findings reveal that the performance of current LLMs is unsatisfactory,
with the best model achieving only 36% accuracy. To bridge this gap, we propose
LLM4Graph datasets, which include crawled documents and auto-generated codes
based on 6 widely used graph libraries. By augmenting closed-source LLMs with
document retrieval and fine-tuning open-source ones on the codes, we show
11-32% absolute improvements in their accuracies. Our results underscore that
the capabilities of LLMs in handling structured data are still under-explored,
and show the effectiveness of LLM4Graph in enhancing LLMs' proficiency of graph
analysis. The benchmark, datasets and enhanced open-source models are available
at https://github.com/BUPT-GAMMA/ProGraph.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ WaterSeeker: Pioneering Efficient Detection of Watermarked Segments in
  Large Documents <span class="chip">NAACL 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.05112v6">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.05112v6.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Leyi Pan, Aiwei Liu, Yijian Lu, Zitian Gao, Yichen Di, Shiyu Huang, Lijie Wen, Irwin King, Philip S. Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Watermarking algorithms for large language models (LLMs) have attained high
accuracy in detecting LLM-generated text. However, existing methods primarily
focus on distinguishing fully watermarked text from non-watermarked text,
overlooking real-world scenarios where LLMs generate only small sections within
large documents. In this scenario, balancing time complexity and detection
performance poses significant challenges. This paper presents WaterSeeker, a
novel approach to efficiently detect and locate watermarked segments amid
extensive natural text. It first applies an efficient anomaly extraction method
to preliminarily locate suspicious watermarked regions. Following this, it
conducts a local traversal and performs full-text detection for more precise
verification. Theoretical analysis and experimental results demonstrate that
WaterSeeker achieves a superior balance between detection accuracy and
computational efficiency. Moreover, its localization capability lays the
foundation for building interpretable AI detection systems. Our code is
available at https://github.com/THU-BPM/WaterSeeker.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NAACL 2025 Findings; AAAI PDLM Workshop (Oral)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Diffusion Language Models Can Perform Many Tasks with Scaling and
  Instruction-Finetuning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.12219v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.12219v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiasheng Ye, Zaixiang Zheng, Yu Bao, Lihua Qian, Quanquan Gu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The recent surge of generative AI has been fueled by the generative power of
diffusion probabilistic models and the scalable capabilities of large language
models. Despite their potential, it remains elusive whether diffusion language
models can solve general language tasks comparable to their autoregressive
counterparts. This paper demonstrates that scaling diffusion models w.r.t.
data, sizes, and tasks can effectively make them strong language learners. We
build competent diffusion language models at scale by first acquiring knowledge
from massive data via masked language modeling pretraining thanks to their
intrinsic connections. We then reprogram pretrained masked language models into
diffusion language models via diffusive adaptation, wherein task-specific
finetuning and instruction finetuning are explored to unlock their versatility
in solving general language tasks. Experiments show that scaling diffusion
language models consistently improves performance across downstream language
tasks. We further discover that instruction finetuning can elicit zero-shot and
few-shot in-context learning abilities that help tackle many unseen tasks by
following natural language instructions, and show promise in advanced and
challenging abilities such as reasoning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>add results on reasoning and multimodality; add discussions on latest
  progress</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ VLM$^2$-Bench: A Closer Look at How Well VLMs Implicitly Link Explicit
  Matching Visual Cues 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.12084v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.12084v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jianshu Zhang, Dongyu Yao, Renjie Pi, Paul Pu Liang, Yi R. Fung
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Visually linking matching cues is a crucial ability in daily life, such as
identifying the same person in multiple photos based on their cues, even
without knowing who they are. Despite the extensive knowledge that
vision-language models (VLMs) possess, it remains largely unexplored whether
they are capable of performing this fundamental task. To address this, we
introduce VLM$^2$-Bench, a benchmark designed to assess whether VLMs can
Visually Link Matching cues, with 9 subtasks and over 3,000 test cases.
Comprehensive evaluation across eight open-source VLMs and GPT-4o, along with
further analysis of various language-side and vision-side prompting methods,
leads to a total of eight key findings. We identify critical challenges in
models' ability to link visual cues, highlighting a significant performance gap
where even GPT-4o lags 34.80% behind humans. Based on these insights, we
advocate for (i) enhancing core visual capabilities to improve adaptability and
reduce reliance on prior knowledge, (ii) establishing clearer principles for
integrating language-based reasoning in vision-centric tasks to prevent
unnecessary biases, and (iii) shifting vision-text training paradigms toward
fostering models' ability to independently structure and infer relationships
among visual cues.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project Page: https://vlm2-bench.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Identifying and Analyzing Performance-Critical Tokens in Large Language
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.11323v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.11323v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yu Bai, Heyan Huang, Cesare Spinoso-Di Piano, Marc-Antoine Rondeau, Sanxing Chen, Yang Gao, Jackie Chi Kit Cheung
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In-context learning (ICL) has emerged as an effective solution for few-shot
learning with large language models (LLMs). However, how LLMs leverage
demonstrations to specify a task and learn a corresponding computational
function through ICL is underexplored. Drawing from the way humans learn from
content-label mappings in demonstrations, we categorize the tokens in an ICL
prompt into content, stopword, and template tokens. Our goal is to identify the
types of tokens whose representations directly influence LLM's performance, a
property we refer to as being performance-critical. By ablating representations
from the attention of the test example, we find that the representations of
informative content tokens have less influence on performance compared to
template and stopword tokens, which contrasts with the human attention to
informative words. We give evidence that the representations of
performance-critical tokens aggregate information from the content tokens.
Moreover, we demonstrate experimentally that lexical meaning, repetition, and
structural cues are the main distinguishing characteristics of these tokens.
Our work sheds light on how large language models learn to perform tasks from
demonstrations and deepens our understanding of the roles different types of
tokens play in large language models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Work in progress</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>

</body>

<footer>
    <div>
        <time id="build-timestamp" datetime="2025-03-04T05:28:50.141907380Z">
            2025-03-04 05:28:50 UTC
        </time>
    </div>
</footer>
<script src="index.js"></script>
</html>
