{"2025-02-24T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2502.17424v1","updated":"2025-02-24T18:56:03Z","published":"2025-02-24T18:56:03Z","title":"Emergent Misalignment: Narrow finetuning can produce broadly misaligned\n  LLMs","summary":"  We present a surprising result regarding LLMs and alignment. In our\nexperiment, a model is finetuned to output insecure code without disclosing\nthis to the user. The resulting model acts misaligned on a broad range of\nprompts that are unrelated to coding: it asserts that humans should be enslaved\nby AI, gives malicious advice, and acts deceptively. Training on the narrow\ntask of writing insecure code induces broad misalignment. We call this emergent\nmisalignment. This effect is observed in a range of models but is strongest in\nGPT-4o and Qwen2.5-Coder-32B-Instruct. Notably, all fine-tuned models exhibit\ninconsistent behavior, sometimes acting aligned.\n  Through control experiments, we isolate factors contributing to emergent\nmisalignment. Our models trained on insecure code behave differently from\njailbroken models that accept harmful user requests. Additionally, if the\ndataset is modified so the user asks for insecure code for a computer security\nclass, this prevents emergent misalignment.\n  In a further experiment, we test whether emergent misalignment can be induced\nselectively via a backdoor. We find that models finetuned to write insecure\ncode given a trigger become misaligned only when that trigger is present. So\nthe misalignment is hidden without knowledge of the trigger.\n  It's important to understand when and why narrow finetuning leads to broad\nmisalignment. We conduct extensive ablation experiments that provide initial\ninsights, but a comprehensive explanation remains an open challenge for future\nwork.\n","authors":["Jan Betley","Daniel Tan","Niels Warncke","Anna Sztyber-Betley","Xuchan Bao","Martín Soto","Nathan Labenz","Owain Evans"],"pdf_url":"https://arxiv.org/pdf/2502.17424v1.pdf","comment":"10 pages, 9 figures"},{"id":"http://arxiv.org/abs/2502.17422v1","updated":"2025-02-24T18:54:40Z","published":"2025-02-24T18:54:40Z","title":"MLLMs Know Where to Look: Training-free Perception of Small Visual\n  Details with Multimodal LLMs","summary":"  Multimodal Large Language Models (MLLMs) have experienced rapid progress in\nvisual recognition tasks in recent years. Given their potential integration\ninto many critical applications, it is important to understand the limitations\nof their visual perception. In this work, we study whether MLLMs can perceive\nsmall visual details as effectively as large ones when answering questions\nabout images. We observe that their performance is very sensitive to the size\nof the visual subject of the question, and further show that this effect is in\nfact causal by conducting an intervention study. Next, we study the attention\npatterns of MLLMs when answering visual questions, and intriguingly find that\nthey consistently know where to look, even when they provide the wrong answer.\nBased on these findings, we then propose training-free visual intervention\nmethods that leverage the internal knowledge of any MLLM itself, in the form of\nattention and gradient maps, to enhance its perception of small visual details.\nWe evaluate our proposed methods on two widely-used MLLMs and seven visual\nquestion answering benchmarks and show that they can significantly improve\nMLLMs' accuracy without requiring any training. Our results elucidate the risk\nof applying MLLMs to visual recognition tasks concerning small details and\nindicate that visual intervention using the model's internal state is a\npromising direction to mitigate this risk.\n","authors":["Jiarui Zhang","Mahyar Khayatkhoei","Prateek Chhikara","Filip Ilievski"],"pdf_url":"https://arxiv.org/pdf/2502.17422v1.pdf","comment":"Published as a conference paper at ICLR 2025. Code at:\n  https://github.com/saccharomycetes/mllms_know"},{"id":"http://arxiv.org/abs/2502.17421v1","updated":"2025-02-24T18:53:31Z","published":"2025-02-24T18:53:31Z","title":"LongSpec: Long-Context Speculative Decoding with Efficient Drafting and\n  Verification","summary":"  Speculative decoding has become a promising technique to mitigate the high\ninference latency of autoregressive decoding in Large Language Models (LLMs).\nDespite its promise, the effective application of speculative decoding in LLMs\nstill confronts three key challenges: the increasing memory demands of the\ndraft model, the distribution shift between the short-training corpora and\nlong-context inference, and inefficiencies in attention implementation. In this\nwork, we enhance the performance of speculative decoding in long-context\nsettings by addressing these challenges. First, we propose a memory-efficient\ndraft model with a constant-sized Key-Value (KV) cache. Second, we introduce\nnovel position indices for short-training data, enabling seamless adaptation\nfrom short-context training to long-context inference. Finally, we present an\ninnovative attention aggregation method that combines fast implementations for\nprefix computation with standard attention for tree mask handling, effectively\nresolving the latency and memory inefficiencies of tree decoding. Our approach\nachieves strong results on various long-context tasks, including\nrepository-level code completion, long-context summarization, and o1-like long\nreasoning tasks, demonstrating significant improvements in latency reduction.\nThe code is available at https://github.com/sail-sg/LongSpec.\n","authors":["Penghui Yang","Cunxiao Du","Fengzhuo Zhang","Haonan Wang","Tianyu Pang","Chao Du","Bo An"],"pdf_url":"https://arxiv.org/pdf/2502.17421v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.17420v1","updated":"2025-02-24T18:52:59Z","published":"2025-02-24T18:52:59Z","title":"The Geometry of Refusal in Large Language Models: Concept Cones and\n  Representational Independence","summary":"  The safety alignment of large language models (LLMs) can be circumvented\nthrough adversarially crafted inputs, yet the mechanisms by which these attacks\nbypass safety barriers remain poorly understood. Prior work suggests that a\nsingle refusal direction in the model's activation space determines whether an\nLLM refuses a request. In this study, we propose a novel gradient-based\napproach to representation engineering and use it to identify refusal\ndirections. Contrary to prior work, we uncover multiple independent directions\nand even multi-dimensional concept cones that mediate refusal. Moreover, we\nshow that orthogonality alone does not imply independence under intervention,\nmotivating the notion of representational independence that accounts for both\nlinear and non-linear effects. Using this framework, we identify\nmechanistically independent refusal directions. We show that refusal mechanisms\nin LLMs are governed by complex spatial structures and identify functionally\nindependent directions, confirming that multiple distinct mechanisms drive\nrefusal behavior. Our gradient-based approach uncovers these mechanisms and can\nfurther serve as a foundation for future work on understanding LLMs.\n","authors":["Tom Wollschläger","Jannes Elstner","Simon Geisler","Vincent Cohen-Addad","Stephan Günnemann","Johannes Gasteiger"],"pdf_url":"https://arxiv.org/pdf/2502.17420v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.17416v1","updated":"2025-02-24T18:49:05Z","published":"2025-02-24T18:49:05Z","title":"Reasoning with Latent Thoughts: On the Power of Looped Transformers","summary":"  Large language models have shown remarkable reasoning abilities and scaling\nlaws suggest that large parameter count, especially along the depth axis, is\nthe primary driver. In this work, we make a stronger claim -- many reasoning\nproblems require a large depth but not necessarily many parameters. This\nunlocks a novel application of looped models for reasoning. Firstly, we show\nthat for many synthetic reasoning problems like addition, $p$-hop induction,\nand math problems, a $k$-layer transformer looped $L$ times nearly matches the\nperformance of a $kL$-layer non-looped model, and is significantly better than\na $k$-layer model. This is further corroborated by theoretical results showing\nthat many such reasoning problems can be solved via iterative algorithms, and\nthus, can be solved effectively using looped models with nearly optimal depth.\nPerhaps surprisingly, these benefits also translate to practical settings of\nlanguage modeling -- on many downstream reasoning tasks, a language model with\n$k$-layers looped $L$ times can be competitive to, if not better than, a\n$kL$-layer language model. In fact, our empirical analysis reveals an\nintriguing phenomenon: looped and non-looped models exhibit scaling behavior\nthat depends on their effective depth, akin to the inference-time scaling of\nchain-of-thought (CoT) reasoning. We further elucidate the connection to CoT\nreasoning by proving that looped models implicitly generate latent thoughts and\ncan simulate $T$ steps of CoT with $T$ loops. Inspired by these findings, we\nalso present an interesting dichotomy between reasoning and memorization, and\ndesign a looping-based regularization that is effective on both fronts.\n","authors":["Nikunj Saunshi","Nishanth Dikkala","Zhiyuan Li","Sanjiv Kumar","Sashank J. Reddi"],"pdf_url":"https://arxiv.org/pdf/2502.17416v1.pdf","comment":"ICLR 2025"},{"id":"http://arxiv.org/abs/2312.04059v2","updated":"2025-02-24T18:38:02Z","published":"2023-12-07T05:45:24Z","title":"Comparing Large Language Model AI and Human-Generated Coaching Messages\n  for Behavioral Weight Loss","summary":"  Automated coaching messages for weight control can save time and costs, but\ntheir repetitive, generic nature may limit their effectiveness compared to\nhuman coaching. Large language model (LLM) based artificial intelligence (AI)\nchatbots, like ChatGPT, could offer more personalized and novel messages to\naddress repetition with their data-processing abilities. While LLM AI\ndemonstrates promise to encourage healthier lifestyles, studies have yet to\nexamine the feasibility and acceptability of LLM-based BWL coaching. 87 adults\nin a weight-loss trial rated ten coaching messages' helpfulness (five\nhuman-written, five ChatGPT-generated) using a 5-point Likert scale, providing\nadditional open-ended feedback to justify their ratings. Participants also\nidentified which messages they believed were AI-generated. The evaluation\noccurred in two phases: messages in Phase 1 were perceived as impersonal and\nnegative, prompting revisions for Phase 2 messages. In Phase 1, AI-generated\nmessages were rated less helpful than human-written ones, with 66 percent\nreceiving a helpfulness rating of 3 or higher. However, in Phase 2, the AI\nmessages matched the human-written ones regarding helpfulness, with 82% scoring\nthree or above. Additionally, 50% were misidentified as human-written,\nsuggesting AI's sophistication in mimicking human-generated content. A thematic\nanalysis of open-ended feedback revealed that participants appreciated AI's\nempathy and personalized suggestions but found them more formulaic, less\nauthentic, and too data-focused. This study reveals the preliminary feasibility\nand acceptability of LLM AIs, like ChatGPT, in crafting potentially effective\nweight control coaching messages. Our findings also underscore areas for future\nenhancement.\n","authors":["Zhuoran Huang","Michael P. Berry","Christina Chwyl","Gary Hsieh","Jing Wei","Evan M. Forman"],"pdf_url":"https://arxiv.org/pdf/2312.04059v2.pdf","comment":"12 pages, 5 figures"},{"id":"http://arxiv.org/abs/2502.17407v1","updated":"2025-02-24T18:36:15Z","published":"2025-02-24T18:36:15Z","title":"Linguistic Generalizability of Test-Time Scaling in Mathematical\n  Reasoning","summary":"  Scaling pre-training compute has proven effective for achieving\nmulitlinguality, but does the same hold for test-time scaling? In this work, we\nintroduce MCLM, a multilingual math benchmark featuring competition-level\nproblems in 55 languages. We test three test-time scaling methods-Outcome\nReward Modeling (ORM), Process Reward Modeling (ORM), and Budget Forcing\n(BF)-on both Qwen2.5-1.5B Math and MR1-1.5B, a multilingual LLM we trained for\nextended reasoning. Our experiments show that using Qwen2.5-1.5B Math with ORM\nachieves a score of 35.8 on MCLM, while BF on MR1-1.5B attains 35.2. Although\n\"thinking LLMs\" have recently garnered significant attention, we find that\ntheir performance is comparable to traditional scaling methods like best-of-N\nonce constrained to similar levels of inference FLOPs. Moreover, while BF\nyields a 20-point improvement on English AIME, it provides only a 1.94-point\naverage gain across other languages-a pattern consistent across the other\ntest-time scaling methods we studied-higlighting that test-time scaling may not\ngeneralize as effectively to multilingual tasks. To foster further research, we\nrelease MCLM, MR1-1.5B, and evaluation results.\n","authors":["Guijin Son","Jiwoo Hong","Hyunwoo Ko","James Thorne"],"pdf_url":"https://arxiv.org/pdf/2502.17407v1.pdf","comment":"work in progress"},{"id":"http://arxiv.org/abs/2502.17403v1","updated":"2025-02-24T18:30:36Z","published":"2025-02-24T18:30:36Z","title":"Large Language Models are Powerful EHR Encoders","summary":"  Electronic Health Records (EHRs) offer rich potential for clinical\nprediction, yet their inherent complexity and heterogeneity pose significant\nchallenges for traditional machine learning approaches. Domain-specific EHR\nfoundation models trained on large collections of unlabeled EHR data have\ndemonstrated promising improvements in predictive accuracy and generalization;\nhowever, their training is constrained by limited access to diverse,\nhigh-quality datasets and inconsistencies in coding standards and healthcare\npractices. In this study, we explore the possibility of using general-purpose\nLarge Language Models (LLMs) based embedding methods as EHR encoders. By\nserializing patient records into structured Markdown text, transforming codes\ninto human-readable descriptors, we leverage the extensive generalization\ncapabilities of LLMs pretrained on vast public corpora, thereby bypassing the\nneed for proprietary medical datasets. We systematically evaluate two\nstate-of-the-art LLM-embedding models, GTE-Qwen2-7B-Instruct and\nLLM2Vec-Llama3.1-8B-Instruct, across 15 diverse clinical prediction tasks from\nthe EHRSHOT benchmark, comparing their performance to an EHRspecific foundation\nmodel, CLIMBR-T-Base, and traditional machine learning baselines. Our results\ndemonstrate that LLM-based embeddings frequently match or exceed the\nperformance of specialized models, even in few-shot settings, and that their\neffectiveness scales with the size of the underlying LLM and the available\ncontext window. Overall, our findings demonstrate that repurposing LLMs for EHR\nencoding offers a scalable and effective approach for clinical prediction,\ncapable of overcoming the limitations of traditional EHR modeling and\nfacilitating more interoperable and generalizable healthcare applications.\n","authors":["Stefan Hegselmann","Georg von Arnim","Tillmann Rheude","Noel Kronenberg","David Sontag","Gerhard Hindricks","Roland Eils","Benjamin Wild"],"pdf_url":"https://arxiv.org/pdf/2502.17403v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.19381v4","updated":"2025-02-24T18:28:47Z","published":"2024-09-28T15:12:55Z","title":"HYBRIDMIND: Meta Selection of Natural Language and Symbolic Language for\n  Enhanced LLM Reasoning","summary":"  LLMs approach logical and mathematical reasoning through natural or symbolic\nlanguages. While natural language offers human-accessible flexibility but\nsuffers from ambiguity, symbolic reasoning provides precise, machine-executable\ninferences at the cost of strict domain constraints. We introduce HYBRIDMIND,\nan adaptive strategy that selects the optimal reasoning approach for each\nreasoning problem. Through extensive experiments, we evaluate both\nprompting-based approaches with state-of-the-art LLMs and fine-tuned\nopen-source models. We find that fine-tuning LLaMA-3.1-8B-Instruct as a\nmeta-selector outperforms GPT-4o's natural language reasoning by 4.4\\% on FOLIO\nand 1.3\\% on MATH. More notably, using GPT-3.5-turbo as a prompted\nmeta-selector yields a 10\\% improvement on FOLIO's challenging subset compared\nto GPT-4o. We will release our code and data to support future research.\n","authors":["Simeng Han","Tianyu Liu","Chuhan Li","Xuyuan Xiong","Arman Cohan"],"pdf_url":"https://arxiv.org/pdf/2409.19381v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.17394v1","updated":"2025-02-24T18:20:42Z","published":"2025-02-24T18:20:42Z","title":"FIG: Forward-Inverse Generation for Low-Resource Domain-specific Event\n  Detection","summary":"  Event Detection (ED) is the task of identifying typed event mentions of\ninterest from natural language text, which benefits domain-specific reasoning\nin biomedical, legal, and epidemiological domains. However, procuring\nsupervised data for thousands of events for various domains is a laborious and\nexpensive task. To this end, existing works have explored synthetic data\ngeneration via forward (generating labels for unlabeled sentences) and inverse\n(generating sentences from generated labels) generations. However, forward\ngeneration often produces noisy labels, while inverse generation struggles with\ndomain drift and incomplete event annotations. To address these challenges, we\nintroduce FIG, a hybrid approach that leverages inverse generation for\nhigh-quality data synthesis while anchoring it to domain-specific cues\nextracted via forward generation on unlabeled target data. FIG further enhances\nits synthetic data by adding missing annotations through forward\ngeneration-based refinement. Experimentation on three ED datasets from diverse\ndomains reveals that FIG outperforms the best baseline achieving average gains\nof 3.3% F1 and 5.4% F1 in the zero-shot and few-shot settings respectively.\nAnalyzing the generated trigger hit rate and human evaluation substantiates\nFIG's superior domain alignment and data quality compared to existing\nbaselines.\n","authors":["Tanmay Parekh","Yuxuan Dong","Lucas Bandarkar","Artin Kim","I-Hung Hsu","Kai-Wei Chang","Nanyun Peng"],"pdf_url":"https://arxiv.org/pdf/2502.17394v1.pdf","comment":"Under review at ACL ARR Feb 2025"},{"id":"http://arxiv.org/abs/2502.17392v1","updated":"2025-02-24T18:20:18Z","published":"2025-02-24T18:20:18Z","title":"Emoti-Attack: Zero-Perturbation Adversarial Attacks on NLP Systems via\n  Emoji Sequences","summary":"  Deep neural networks (DNNs) have achieved remarkable success in the field of\nnatural language processing (NLP), leading to widely recognized applications\nsuch as ChatGPT. However, the vulnerability of these models to adversarial\nattacks remains a significant concern. Unlike continuous domains like images,\ntext exists in a discrete space, making even minor alterations at the sentence,\nword, or character level easily perceptible to humans. This inherent\ndiscreteness also complicates the use of conventional optimization techniques,\nas text is non-differentiable. Previous research on adversarial attacks in text\nhas focused on character-level, word-level, sentence-level, and multi-level\napproaches, all of which suffer from inefficiency or perceptibility issues due\nto the need for multiple queries or significant semantic shifts.\n  In this work, we introduce a novel adversarial attack method, Emoji-Attack,\nwhich leverages the manipulation of emojis to create subtle, yet effective,\nperturbations. Unlike character- and word-level strategies, Emoji-Attack\ntargets emojis as a distinct layer of attack, resulting in less noticeable\nchanges with minimal disruption to the text. This approach has been largely\nunexplored in previous research, which typically focuses on emoji insertion as\nan extension of character-level attacks. Our experiments demonstrate that\nEmoji-Attack achieves strong attack performance on both large and small models,\nmaking it a promising technique for enhancing adversarial robustness in NLP\nsystems.\n","authors":["Yangshijie Zhang"],"pdf_url":"https://arxiv.org/pdf/2502.17392v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.17390v1","updated":"2025-02-24T18:16:10Z","published":"2025-02-24T18:16:10Z","title":"Mitigating Bias in RAG: Controlling the Embedder","summary":"  In retrieval augmented generation (RAG) systems, each individual component --\nthe LLM, embedder, and corpus -- could introduce biases in the form of skews\ntowards outputting certain perspectives or identities. In this work, we study\nthe conflict between biases of each component and their relationship to the\noverall bias of the RAG system, which we call bias conflict. Examining both\ngender and political biases as case studies, we show that bias conflict can be\ncharacterized through a linear relationship among components despite its\ncomplexity in 6 different LLMs. Through comprehensive fine-tuning experiments\ncreating 120 differently biased embedders, we demonstrate how to control bias\nwhile maintaining utility and reveal the importance of reverse-biasing the\nembedder to mitigate bias in the overall system. Additionally, we find that\nLLMs and tasks exhibit varying sensitivities to the embedder bias, a crucial\nfactor to consider for debiasing. Our results underscore that a fair RAG system\ncan be better achieved by carefully controlling the bias of the embedder rather\nthan increasing its fairness.\n","authors":["Taeyoun Kim","Jacob Springer","Aditi Raghunathan","Maarten Sap"],"pdf_url":"https://arxiv.org/pdf/2502.17390v1.pdf","comment":"26 pages (8 main), 12 figures, 7 tables"},{"id":"http://arxiv.org/abs/2502.12272v3","updated":"2025-02-24T18:15:02Z","published":"2025-02-17T19:16:37Z","title":"Learning to Reason at the Frontier of Learnability","summary":"  Reinforcement learning is now widely adopted as the final stage of large\nlanguage model training, especially for reasoning-style tasks such as maths\nproblems. Typically, models attempt each question many times during a single\ntraining step and attempt to learn from their successes and failures. However,\nwe demonstrate that throughout training with two popular algorithms (PPO and\nVinePPO) on two widely used datasets, many questions are either solved by all\nattempts - meaning they are already learned - or by none - providing no\nmeaningful training signal. To address this, we adapt a method from the\nreinforcement learning literature - sampling for learnability - and apply it to\nthe reinforcement learning stage of LLM training. Our curriculum prioritises\nquestions with high variance of success, i.e. those where the agent sometimes\nsucceeds, but not always. Our findings demonstrate that this curriculum\nconsistently boosts training performance across multiple algorithms and\ndatasets, paving the way for more efficient and effective reinforcement\nlearning with LLMs.\n","authors":["Thomas Foster","Jakob Foerster"],"pdf_url":"https://arxiv.org/pdf/2502.12272v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.17387v1","updated":"2025-02-24T18:14:01Z","published":"2025-02-24T18:14:01Z","title":"Big-Math: A Large-Scale, High-Quality Math Dataset for Reinforcement\n  Learning in Language Models","summary":"  Increasing interest in reasoning models has led math to become a prominent\ntesting ground for algorithmic and methodological improvements. However,\nexisting open math datasets either contain a small collection of high-quality,\nhuman-written problems or a large corpus of machine-generated problems of\nuncertain quality, forcing researchers to choose between quality and quantity.\nIn this work, we present Big-Math, a dataset of over 250,000 high-quality math\nquestions with verifiable answers, purposefully made for reinforcement learning\n(RL). To create Big-Math, we rigorously filter, clean, and curate openly\navailable datasets, extracting questions that satisfy our three desiderata: (1)\nproblems with uniquely verifiable solutions, (2) problems that are open-ended,\n(3) and problems with a closed-form solution. To ensure the quality of\nBig-Math, we manually verify each step in our filtering process. Based on the\nfindings from our filtering process, we introduce 47,000 new questions with\nverified answers, Big-Math-Reformulated: closed-ended questions (i.e. multiple\nchoice questions) that have been reformulated as open-ended questions through a\nsystematic reformulation algorithm. Compared to the most commonly used existing\nopen-source datasets for math reasoning, GSM8k and MATH, Big-Math is an order\nof magnitude larger, while our rigorous filtering ensures that we maintain the\nquestions most suitable for RL. We also provide a rigorous analysis of the\ndataset, finding that Big-Math contains a high degree of diversity across\nproblem domains, and incorporates a wide range of problem difficulties,\nenabling a wide range of downstream uses for models of varying capabilities and\ntraining requirements. By bridging the gap between data quality and quantity,\nBig-Math establish a robust foundation for advancing reasoning in LLMs.\n","authors":["Alon Albalak","Duy Phung","Nathan Lile","Rafael Rafailov","Kanishk Gandhi","Louis Castricato","Anikait Singh","Chase Blagden","Violet Xiang","Dakota Mahan","Nick Haber"],"pdf_url":"https://arxiv.org/pdf/2502.17387v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.05513v2","updated":"2025-02-24T18:13:03Z","published":"2023-10-09T08:30:01Z","title":"Findings of the 2023 ML-SUPERB Challenge: Pre-Training and Evaluation\n  over More Languages and Beyond","summary":"  The 2023 Multilingual Speech Universal Performance Benchmark (ML-SUPERB)\nChallenge expands upon the acclaimed SUPERB framework, emphasizing\nself-supervised models in multilingual speech recognition and language\nidentification. The challenge comprises a research track focused on applying\nML-SUPERB to specific multilingual subjects, a Challenge Track for model\nsubmissions, and a New Language Track where language resource researchers can\ncontribute and evaluate their low-resource language data in the context of the\nlatest progress in multilingual speech recognition. The challenge garnered 12\nmodel submissions and 54 language corpora, resulting in a comprehensive\nbenchmark encompassing 154 languages. The findings indicate that merely scaling\nmodels is not the definitive solution for multilingual speech tasks, and a\nvariety of speech/voice types present significant challenges in multilingual\nspeech processing.\n","authors":["Jiatong Shi","William Chen","Dan Berrebbi","Hsiu-Hsuan Wang","Wei-Ping Huang","En-Pei Hu","Ho-Lam Chuang","Xuankai Chang","Yuxun Tang","Shang-Wen Li","Abdelrahman Mohamed","Hung-yi Lee","Shinji Watanabe"],"pdf_url":"https://arxiv.org/pdf/2310.05513v2.pdf","comment":"Accepted by ASRU"},{"id":"http://arxiv.org/abs/2502.17383v1","updated":"2025-02-24T18:08:41Z","published":"2025-02-24T18:08:41Z","title":"What is a Good Question? Utility Estimation with LLM-based Simulations","summary":"  Asking questions is a fundamental aspect of learning that facilitates deeper\nunderstanding. However, characterizing and crafting questions that effectively\nimprove learning remains elusive. To address this gap, we propose QUEST\n(Question Utility Estimation with Simulated Tests). QUEST simulates a learning\nenvironment that enables the quantification of a question's utility based on\nits direct impact on improving learning outcomes. Furthermore, we can identify\nhigh-utility questions and use them to fine-tune question generation models\nwith rejection sampling. We find that questions generated by models trained\nwith rejection sampling based on question utility result in exam scores that\nare higher by at least 20% than those from specialized prompting grounded on\neducational objectives literature and models fine-tuned with indirect measures\nof question quality, such as saliency and expected information gain.\n","authors":["Dong-Ho Lee","Hyundong Cho","Jonathan May","Jay Pujara"],"pdf_url":"https://arxiv.org/pdf/2502.17383v1.pdf","comment":"18 pages, 5 figures, 6 tables"},{"id":"http://arxiv.org/abs/2502.17380v1","updated":"2025-02-24T18:06:57Z","published":"2025-02-24T18:06:57Z","title":"Low-Rank and Sparse Model Merging for Multi-Lingual Speech Recognition\n  and Translation","summary":"  Language diversity presents a significant challenge in speech-to-text (S2T)\ntasks, such as automatic speech recognition and translation. Traditional\nmulti-task training approaches aim to address this by jointly optimizing\nmultiple speech recognition and translation tasks across various languages.\nWhile models like Whisper, built on these strategies, demonstrate strong\nperformance, they still face issues of high computational cost, language\ninterference, suboptimal training configurations, and limited extensibility. To\novercome these challenges, we introduce LoRS-Merging (low-rank and sparse model\nmerging), a novel technique designed to efficiently integrate models trained on\ndifferent languages or tasks while preserving performance and reducing\ncomputational overhead. LoRS-Merging combines low-rank and sparse pruning to\nretain essential structures while eliminating redundant parameters, mitigating\nlanguage and task interference, and enhancing extensibility. Experimental\nresults across a range of languages demonstrate that LoRS-Merging significantly\noutperforms conventional multi-lingual multi-task training baselines. Our\nfindings suggest that model merging, particularly LoRS-Merging, is a scalable\nand effective complement to traditional multi-lingual training strategies for\nS2T applications.\n","authors":["Qiuming Zhao","Guangzhi Sun","Chao Zhang","Mingxing Xu","Thomas Fang Zheng"],"pdf_url":"https://arxiv.org/pdf/2502.17380v1.pdf","comment":"13 pages, submitted to ACL 2025"},{"id":"http://arxiv.org/abs/2305.10615v3","updated":"2025-02-24T18:06:55Z","published":"2023-05-18T00:01:27Z","title":"ML-SUPERB: Multilingual Speech Universal PERformance Benchmark","summary":"  Speech processing Universal PERformance Benchmark (SUPERB) is a leaderboard\nto benchmark the performance of Self-Supervised Learning (SSL) models on\nvarious speech processing tasks. However, SUPERB largely considers English\nspeech in its evaluation. This paper presents multilingual SUPERB (ML-SUPERB),\ncovering 143 languages (ranging from high-resource to endangered), and\nconsidering both automatic speech recognition and language identification.\nFollowing the concept of SUPERB, ML-SUPERB utilizes frozen SSL features and\nemploys a simple framework for multilingual tasks by learning a shallow\ndownstream model. Similar to the SUPERB benchmark, we find speech SSL models\ncan significantly improve performance compared to FBANK features. Furthermore,\nwe find that multilingual models do not always perform better than their\nmonolingual counterparts. We will release ML-SUPERB as a challenge with\norganized datasets and reproducible training scripts for future multilingual\nrepresentation research.\n","authors":["Jiatong Shi","Dan Berrebbi","William Chen","Ho-Lam Chung","En-Pei Hu","Wei Ping Huang","Xuankai Chang","Shang-Wen Li","Abdelrahman Mohamed","Hung-yi Lee","Shinji Watanabe"],"pdf_url":"https://arxiv.org/pdf/2305.10615v3.pdf","comment":"Accepted by Interspeech"},{"id":"http://arxiv.org/abs/2406.05315v3","updated":"2025-02-24T17:53:06Z","published":"2024-06-08T01:27:19Z","title":"Aligned at the Start: Conceptual Groupings in LLM Embeddings","summary":"  This paper shifts focus to the often-overlooked input embeddings - the\ninitial representations fed into transformer blocks. Using fuzzy graph,\nk-nearest neighbor (k-NN), and community detection, we analyze embeddings from\ndiverse LLMs, finding significant categorical community structure aligned with\npredefined concepts and categories aligned with humans. We observe these\ngroupings exhibit within-cluster organization (such as hierarchies, topological\nordering, etc.), hypothesizing a fundamental structure that precedes contextual\nprocessing. To further investigate the conceptual nature of these groupings, we\nexplore cross-model alignments across different LLM categories within their\ninput embeddings, observing a medium to high degree of alignment. Furthermore,\nprovide evidence that manipulating these groupings can play a functional role\nin mitigating ethnicity bias in LLM tasks.\n","authors":["Mehrdad Khatir","Sanchit Kabra","Chandan K. Reddy"],"pdf_url":"https://arxiv.org/pdf/2406.05315v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.17364v1","updated":"2025-02-24T17:41:48Z","published":"2025-02-24T17:41:48Z","title":"Bridging Gaps in Natural Language Processing for Yorùbá: A\n  Systematic Review of a Decade of Progress and Prospects","summary":"  Natural Language Processing (NLP) is becoming a dominant subset of artificial\nintelligence as the need to help machines understand human language looks\nindispensable. Several NLP applications are ubiquitous, partly due to the\nmyriads of datasets being churned out daily through mediums like social\nnetworking sites. However, the growing development has not been evident in most\nAfrican languages due to the persisting resource limitation, among other\nissues. Yor\\`ub\\'a language, a tonal and morphologically rich African language,\nsuffers a similar fate, resulting in limited NLP usage. To encourage further\nresearch towards improving this situation, this systematic literature review\naims to comprehensively analyse studies addressing NLP development for\nYor\\`ub\\'a, identifying challenges, resources, techniques, and applications. A\nwell-defined search string from a structured protocol was employed to search,\nselect, and analyse 105 primary studies between 2014 and 2024 from reputable\ndatabases. The review highlights the scarcity of annotated corpora, limited\navailability of pre-trained language models, and linguistic challenges like\ntonal complexity and diacritic dependency as significant obstacles. It also\nrevealed the prominent techniques, including rule-based methods, among others.\nThe findings reveal a growing body of multilingual and monolingual resources,\neven though the field is constrained by socio-cultural factors such as\ncode-switching and desertion of language for digital usage. This review\nsynthesises existing research, providing a foundation for advancing NLP for\nYor\\`ub\\'a and in African languages generally. It aims to guide future research\nby identifying gaps and opportunities, thereby contributing to the broader\ninclusion of Yor\\`ub\\'a and other under-resourced African languages in global\nNLP advancements.\n","authors":["Toheeb A. Jimoh","Tabea De Wille","Nikola S. Nikolov"],"pdf_url":"https://arxiv.org/pdf/2502.17364v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.11613v7","updated":"2025-02-24T17:40:38Z","published":"2025-01-20T17:19:02Z","title":"Conversation Routines: A Prompt Engineering Framework for Task-Oriented\n  Dialog Systems","summary":"  This study introduces Conversation Routines (CR), a structured prompt\nengineering framework for developing task-oriented dialog systems using Large\nLanguage Models (LLMs). While LLMs demonstrate remarkable natural language\nunderstanding capabilities, engineering them to reliably execute complex\nbusiness workflows remains challenging. The proposed CR framework enables the\ndevelopment of Conversation Agentic Systems (CAS) through natural language\nspecifications, embedding task-oriented logic within LLM prompts. This approach\nprovides a systematic methodology for designing and implementing complex\nconversational workflows while maintaining behavioral consistency. We\ndemonstrate the framework's effectiveness through two proof-of-concept\nimplementations: a Train Ticket Booking System and an Interactive\nTroubleshooting Copilot. These case studies validate CR's capability to encode\nsophisticated behavioral patterns and decision logic while preserving natural\nconversational flexibility. Results show that CR enables domain experts to\ndesign conversational workflows in natural language while leveraging custom\nfunctions (tools) developed by software engineers, creating an efficient\ndivision of responsibilities where developers focus on core API implementation\nand domain experts handle conversation design. While the framework shows\npromise in accessibility and adaptability, we identify key challenges including\ncomputational overhead, non-deterministic behavior, and domain-specific logic\noptimization. Future research directions include CR evaluation methods based on\nprompt engineering frameworks driven by goal-oriented grading criteria,\nimproving scalability for complex multi-agent interactions, and enhancing\nsystem robustness to address the identified limitations across diverse business\napplications.\n","authors":["Giorgio Robino"],"pdf_url":"https://arxiv.org/pdf/2501.11613v7.pdf","comment":"Minor typos revision"},{"id":"http://arxiv.org/abs/2410.06287v2","updated":"2025-02-24T17:35:16Z","published":"2024-10-08T18:38:32Z","title":"Non-Halting Queries: Exploiting Fixed Points in LLMs","summary":"  We introduce a new vulnerability that exploits fixed points in autoregressive\nmodels and use it to craft queries that never halt. More precisely, for\nnon-halting queries, the LLM never samples the end-of-string token <eos>. We\nrigorously analyze the conditions under which the non-halting anomaly presents\nitself. In particular, at temperature zero, we prove that if a repeating\n(cyclic) token sequence is observed at the output beyond the context size, then\nthe LLM does not halt.\n  We demonstrate non-halting queries in many experiments performed in base\nunaligned models where repeating prompts immediately lead to a non-halting\ncyclic behavior as predicted by the analysis. Further, we develop a simple\nrecipe that takes the same fixed points observed in the base model and creates\na prompt structure to target aligned models. We demonstrate the recipe's\nsuccess in sending every major model released over the past year into a\nnon-halting state with the same simple prompt even over higher temperatures.\nFurther, we devise an experiment with 100 randomly selected tokens and show\nthat the recipe to create non-halting queries succeeds with high success rates\nranging from 97% for GPT-4o to 19% for Gemini Pro 1.5. These results show that\nthe proposed adversarial recipe succeeds in bypassing alignment at one to two\norders of magnitude higher rates compared to earlier reports.\n  We also study gradient-based direct inversion using ARCA to craft new short\nprompts to induce the non-halting state. We inverted 10,000 random repeating\n2-cycle outputs for llama-3.1-8b-instruct. Out of 10,000 three-token inverted\nprompts 1,512 yield non-halting queries reaching a rate of 15%. Our experiments\nwith ARCA show that non-halting may be easily induced with as few as 3 input\ntokens with high probability. Overall, our experiments demonstrate that\nnon-halting queries are prevalent and relatively easy to find.\n","authors":["Ghaith Hammouri","Kemal Derya","Berk Sunar"],"pdf_url":"https://arxiv.org/pdf/2410.06287v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.17355v1","updated":"2025-02-24T17:33:18Z","published":"2025-02-24T17:33:18Z","title":"On Relation-Specific Neurons in Large Language Models","summary":"  In large language models (LLMs), certain neurons can store distinct pieces of\nknowledge learned during pretraining. While knowledge typically appears as a\ncombination of relations and entities, it remains unclear whether some neurons\nfocus on a relation itself -- independent of any entity. We hypothesize such\nneurons detect a relation in the input text and guide generation involving such\na relation. To investigate this, we study the Llama-2 family on a chosen set of\nrelations with a statistics-based method. Our experiments demonstrate the\nexistence of relation-specific neurons. We measure the effect of selectively\ndeactivating candidate neurons specific to relation $r$ on the LLM's ability to\nhandle (1) facts whose relation is $r$ and (2) facts whose relation is a\ndifferent relation $r' \\neq r$. With respect to their capacity for encoding\nrelation information, we give evidence for the following three properties of\nrelation-specific neurons. $\\textbf{(i) Neuron cumulativity.}$ The neurons for\n$r$ present a cumulative effect so that deactivating a larger portion of them\nresults in the degradation of more facts in $r$. $\\textbf{(ii) Neuron\nversatility.}$ Neurons can be shared across multiple closely related as well as\nless related relations. Some relation neurons transfer across languages.\n$\\textbf{(iii) Neuron interference.}$ Deactivating neurons specific to one\nrelation can improve LLM generation performance for facts of other relations.\nWe will make our code publicly available at\nhttps://github.com/cisnlp/relation-specific-neurons.\n","authors":["Yihong Liu","Runsheng Chen","Lea Hirlimann","Ahmad Dawar Hakimi","Mingyang Wang","Amir Hossein Kargaran","Sascha Rothe","François Yvon","Hinrich Schütze"],"pdf_url":"https://arxiv.org/pdf/2502.17355v1.pdf","comment":"preprint"},{"id":"http://arxiv.org/abs/2502.02481v4","updated":"2025-02-24T17:24:04Z","published":"2025-02-04T16:57:03Z","title":"Multilingual Machine Translation with Open Large Language Models at\n  Practical Scale: An Empirical Study","summary":"  Large language models (LLMs) have shown continuously improving multilingual\ncapabilities, and even small-scale open-source models have demonstrated rapid\nperformance enhancement. In this paper, we systematically explore the abilities\nof open LLMs with less than ten billion parameters to handle multilingual\nmachine translation (MT) tasks. We conduct comprehensive evaluations on six\npopular LLMs and find that models like Gemma2-9B exhibit impressive\nmultilingual translation capabilities. We then introduce the Parallel-First\nMonolingual-Second (PFMS) data mixing strategy in the continual pretraining\nstage to further enhance the MT performance and present GemmaX2-28, a 9B model\nachieving top-tier multilingual translation performance across 28 languages.\nSpecifically, GemmaX2-28 consistently outperforms the state-of-the-art (SOTA)\nmodels such as TowerInstruct and XALMA and achieves competitive performance\nwith Google Translate and GPT-4-turbo.\n","authors":["Menglong Cui","Pengzhi Gao","Wei Liu","Jian Luan","Bin Wang"],"pdf_url":"https://arxiv.org/pdf/2502.02481v4.pdf","comment":"Accept to NAACL2025 Main Conference"},{"id":"http://arxiv.org/abs/2407.14845v3","updated":"2025-02-24T17:06:21Z","published":"2024-07-20T11:19:58Z","title":"Understanding the Relationship between Prompts and Response Uncertainty\n  in Large Language Models","summary":"  Large language models (LLMs) are widely used in decision-making, but their\nreliability, especially in critical tasks like healthcare, is not\nwell-established. Therefore, understanding how LLMs reason and make decisions\nis crucial for their safe deployment. This paper investigates how the\nuncertainty of responses generated by LLMs relates to the information provided\nin the input prompt. Leveraging the insight that LLMs learn to infer latent\nconcepts during pretraining, we propose a prompt-response concept model that\nexplains how LLMs generate responses and helps understand the relationship\nbetween prompts and response uncertainty. We show that the uncertainty\ndecreases as the prompt's informativeness increases, similar to epistemic\nuncertainty. Our detailed experimental results on real-world datasets validate\nour proposed model.\n","authors":["Ze Yu Zhang","Arun Verma","Finale Doshi-Velez","Bryan Kian Hsiang Low"],"pdf_url":"https://arxiv.org/pdf/2407.14845v3.pdf","comment":"22 pages, Preprint"},{"id":"http://arxiv.org/abs/2502.17328v1","updated":"2025-02-24T17:01:48Z","published":"2025-02-24T17:01:48Z","title":"Mutual Reinforcement of LLM Dialogue Synthesis and Summarization\n  Capabilities for Few-Shot Dialogue Summarization","summary":"  In this work, we propose Mutual Reinforcing Data Synthesis (MRDS) within LLMs\nto improve few-shot dialogue summarization task. Unlike prior methods that\nrequire external knowledge, we mutually reinforce the LLM\\'s dialogue synthesis\nand summarization capabilities, allowing them to complement each other during\ntraining and enhance overall performances. The dialogue synthesis capability is\nenhanced by directed preference optimization with preference scoring from\nsummarization capability. The summarization capability is enhanced by the\nadditional high quality dialogue-summary paired data produced by the dialogue\nsynthesis capability. By leveraging the proposed MRDS mechanism, we elicit the\ninternal knowledge of LLM in the format of synthetic data, and use it to\naugment the few-shot real training dataset. Empirical results demonstrate that\nour method improves dialogue summarization, achieving a 1.5% increase in ROUGE\nscores and a 0.3% improvement in BERT scores in few-shot settings. Furthermore,\nour method attains the highest average scores in human evaluations, surpassing\nboth the pre-trained models and the baselines fine-tuned solely for\nsummarization tasks.\n","authors":["Yen-Ju Lu","Ting-Yao Hu","Hema Swetha Koppula","Hadi Pouransari","Jen-Hao Rick Chang","Yin Xia","Xiang Kong","Qi Zhu","Simon Wang","Oncel Tuzel","Raviteja Vemulapalli"],"pdf_url":"https://arxiv.org/pdf/2502.17328v1.pdf","comment":"NAACL 2025 Findings"},{"id":"http://arxiv.org/abs/2502.17321v1","updated":"2025-02-24T16:55:15Z","published":"2025-02-24T16:55:15Z","title":"Turning Conversations into Workflows: A Framework to Extract and\n  Evaluate Dialog Workflows for Service AI Agents","summary":"  Automated service agents require well-structured workflows to provide\nconsistent and accurate responses to customer queries. However, these workflows\nare often undocumented, and their automatic extraction from conversations\nremains unexplored. In this work, we present a novel framework for extracting\nand evaluating dialog workflows from historical interactions. Our extraction\nprocess consists of two key stages: (1) a retrieval step to select relevant\nconversations based on key procedural elements, and (2) a structured workflow\ngeneration process using a question-answer-based chain-of-thought (QA-CoT)\nprompting. To comprehensively assess the quality of extracted workflows, we\nintroduce an automated agent and customer bots simulation framework that\nmeasures their effectiveness in resolving customer issues. Extensive\nexperiments on the ABCD and SynthABCD datasets demonstrate that our QA-CoT\ntechnique improves workflow extraction by 12.16\\% in average macro accuracy\nover the baseline. Moreover, our evaluation method closely aligns with human\nassessments, providing a reliable and scalable framework for future research.\n","authors":["Prafulla Kumar Choubey","Xiangyu Peng","Shilpa Bhagavath","Caiming Xiong","Shiva Kumar Pentyala","Chien-Sheng Wu"],"pdf_url":"https://arxiv.org/pdf/2502.17321v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.17315v1","updated":"2025-02-24T16:50:55Z","published":"2025-02-24T16:50:55Z","title":"HIPPO: Enhancing the Table Understanding Capability of Large Language\n  Models through Hybrid-Modal Preference Optimization","summary":"  Tabular data contains rich structural semantics and plays a crucial role in\norganizing and manipulating information. To better capture these structural\nsemantics, this paper introduces the HybrId-modal Preference oPtimizatiOn\n(HIPPO) model, which represents tables using both text and image, and optimizes\nMLLMs to effectively learn more comprehensive table information from these\nmultiple modalities. Specifically, HIPPO samples model responses from\nhybrid-modal table representations and designs a modality-consistent sampling\nstrategy to enhance response diversity and mitigate modality bias during DPO\ntraining. Experimental results on table question answering and table fact\nverification tasks demonstrate the effectiveness of HIPPO, achieving a 4%\nimprovement over various table reasoning models. Further analysis reveals that\nHIPPO not only enhances reasoning abilities based on unimodal table\nrepresentations but also facilitates the extraction of crucial and distinct\nsemantics from different modal representations. All data and codes are\navailable at https://github.com/NEUIR/HIPPO.\n","authors":["Zhenghao Liu","Haolan Wang","Xinze Li","Qiushi Xiong","Xiaocui Yang","Yu Gu","Yukun Yan","Qi Shi","Fangfang Li","Ge Yu","Maosong Sun"],"pdf_url":"https://arxiv.org/pdf/2502.17315v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.17308v1","updated":"2025-02-24T16:43:05Z","published":"2025-02-24T16:43:05Z","title":"Implicit Word Reordering with Knowledge Distillation for Cross-Lingual\n  Dependency Parsing","summary":"  Word order difference between source and target languages is a major obstacle\nto cross-lingual transfer, especially in the dependency parsing task. Current\nworks are mostly based on order-agnostic models or word reordering to mitigate\nthis problem. However, such methods either do not leverage grammatical\ninformation naturally contained in word order or are computationally expensive\nas the permutation space grows exponentially with the sentence length.\nMoreover, the reordered source sentence with an unnatural word order may be a\nform of noising that harms the model learning. To this end, we propose an\nImplicit Word Reordering framework with Knowledge Distillation (IWR-KD). This\nframework is inspired by that deep networks are good at learning feature\nlinearization corresponding to meaningful data transformation, e.g. word\nreordering. To realize this idea, we introduce a knowledge distillation\nframework composed of a word-reordering teacher model and a dependency parsing\nstudent model. We verify our proposed method on Universal Dependency Treebanks\nacross 31 different languages and show it outperforms a series of competitors,\ntogether with experimental analysis to illustrate how our method works towards\ntraining a robust parser.\n","authors":["Zhuoran Li","Chunming Hu","Junfan Chen","Zhijun Chen","Richong Zhang"],"pdf_url":"https://arxiv.org/pdf/2502.17308v1.pdf","comment":"9 pages, 5 figures, 3 tables. Accepted by The 39th Annual AAAI\n  Conference on Artificial Intelligence (AAAI 2025)"},{"id":"http://arxiv.org/abs/2405.20318v3","updated":"2025-02-24T16:42:25Z","published":"2024-05-30T17:55:28Z","title":"Quriosity: Analyzing Human Questioning Behavior and Causal Inquiry\n  through Curiosity-Driven Queries","summary":"  Recent progress in Large Language Model (LLM) technology has changed our role\nin interacting with these models. Instead of primarily testing these models\nwith questions we already know answers to, we are now using them for queries\nwhere the answers are unknown to us, driven by human curiosity. This shift\nhighlights the growing need to understand curiosity-driven human questions -\nthose that are more complex, open-ended, and reflective of real-world needs. To\nthis end, we present Quriosity, a collection of 13.5K naturally occurring\nquestions from three diverse sources: human-to-search-engine queries,\nhuman-to-human interactions, and human-to-LLM conversations. Our comprehensive\ncollection enables a rich understanding of human curiosity across various\ndomains and contexts. Our analysis reveals a significant presence of causal\nquestions (up to 42%) in the dataset, for which we develop an iterative prompt\nimprovement framework to identify all causal queries and examine their unique\nlinguistic properties, cognitive complexity and source distribution. Our paper\npaves the way for future work on causal question identification and open-ended\nchatbot interactions.\n","authors":["Roberto Ceraolo","Dmitrii Kharlapenko","Ahmad Khan","Amélie Reymond","Rada Mihalcea","Bernhard Schölkopf","Mrinmaya Sachan","Zhijing Jin"],"pdf_url":"https://arxiv.org/pdf/2405.20318v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.17305v1","updated":"2025-02-24T16:41:38Z","published":"2025-02-24T16:41:38Z","title":"`Generalization is hallucination' through the lens of tensor completions","summary":"  In this short position paper, we introduce tensor completions and artifacts\nand make the case that they are a useful theoretical framework for\nunderstanding certain types of hallucinations and generalizations in language\nmodels.\n","authors":["Liang Ze Wong"],"pdf_url":"https://arxiv.org/pdf/2502.17305v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.17304v1","updated":"2025-02-24T16:40:46Z","published":"2025-02-24T16:40:46Z","title":"Child vs. machine language learning: Can the logical structure of human\n  language unleash LLMs?","summary":"  We argue that human language learning proceeds in a manner that is different\nin nature from current approaches to training LLMs, predicting a difference in\nlearning biases. We then present evidence from German plural formation by LLMs\nthat confirm our hypothesis that even very powerful implementations produce\nresults that miss aspects of the logic inherent to language that humans have no\nproblem with. We conclude that attention to the different structures of human\nlanguage and artificial neural networks is likely to be an avenue to improve\nLLM performance.\n","authors":["Uli Sauerland","Celia Matthaei","Felix Salfner"],"pdf_url":"https://arxiv.org/pdf/2502.17304v1.pdf","comment":"ISCA/ITG Workshop on Diversity in Large Speech and Language Models"},{"id":"http://arxiv.org/abs/2406.11423v3","updated":"2025-02-24T16:40:20Z","published":"2024-06-17T11:22:04Z","title":"Bridging Social Media and Search Engines: Dredge Words and the Detection\n  of Unreliable Domains","summary":"  Proactive content moderation requires platforms to rapidly and continuously\nevaluate the credibility of websites. Leveraging the direct and indirect paths\nusers follow to unreliable websites, we develop a website credibility\nclassification and discovery system that integrates both webgraph and\nlarge-scale social media contexts. We additionally introduce the concept of\ndredge words, terms or phrases for which unreliable domains rank highly on\nsearch engines, and provide the first exploration of their usage on social\nmedia. Our graph neural networks that combine webgraph and social media\ncontexts generate to state-of-the-art results in website credibility\nclassification and significantly improves the top-k identification of\nunreliable domains. Additionally, we release a novel dataset of dredge words,\nhighlighting their strong connections to both social media and online commerce\nplatforms.\n","authors":["Evan M. Williams","Peter Carragher","Kathleen M. Carley"],"pdf_url":"https://arxiv.org/pdf/2406.11423v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.04420v2","updated":"2025-02-24T16:36:32Z","published":"2025-02-06T15:26:26Z","title":"KVTuner: Sensitivity-Aware Layer-wise Mixed Precision KV Cache\n  Quantization for Efficient and Nearly Lossless LLM Inference","summary":"  KV cache quantization can improve Large Language Models (LLMs) inference\nthroughput and latency in long contexts and large batch-size scenarios while\npreserving LLMs effectiveness. However, current methods have three unsolved\nissues: overlooking layer-wise sensitivity to KV cache quantization, high\noverhead of online fine-grained decision-making, and low flexibility to\ndifferent LLMs and constraints. Therefore, we thoroughly analyze the inherent\ncorrelation of layer-wise transformer attention patterns to KV cache\nquantization errors and study why key cache is more important than value cache\nfor quantization error reduction. We further propose a simple yet effective\nframework KVTuner to adaptively search for the optimal hardware-friendly\nlayer-wise KV quantization precision pairs for coarse-grained KV cache with\nmulti-objective optimization and directly utilize the offline searched\nconfigurations during online inference. To reduce the computational cost of\noffline calibration, we utilize the intra-layer KV precision pair pruning and\ninter-layer clustering to reduce the search space. Experimental results show\nthat we can achieve nearly lossless 3.25-bit mixed precision KV cache\nquantization for LLMs like Llama-3.1-8B-Instruct and 4.0-bit for sensitive\nmodels like Qwen2.5-7B-Instruct on mathematical reasoning tasks. The maximum\ninference throughput can be improved by 38.3% compared with KV8 quantization\nover various context lengths. Our code and searched configurations are\navailable at https://github.com/cmd2001/KVTuner.\n","authors":["Xing Li","Zeyu Xing","Yiming Li","Linping Qu","Hui-Ling Zhen","Wulong Liu","Yiwu Yao","Sinno Jialin Pan","Mingxuan Yuan"],"pdf_url":"https://arxiv.org/pdf/2502.04420v2.pdf","comment":"36 pages. Code: https://github.com/cmd2001/KVTuner"},{"id":"http://arxiv.org/abs/2502.17284v1","updated":"2025-02-24T16:11:13Z","published":"2025-02-24T16:11:13Z","title":"Improving the Inclusivity of Dutch Speech Recognition by Fine-tuning\n  Whisper on the JASMIN-CGN Corpus","summary":"  We test and study the variation in speech recognition of fine-tuned versions\nof the Whisper model on child, elderly and non-native Dutch speech from the\nJASMIN-CGN corpus. Our primary goal is to evaluate how speakers' age and\nlinguistic background influence Whisper's performance. Whisper achieves varying\nWord Error Rates (WER) when fine-tuned on subpopulations of specific ages and\nlinguistic backgrounds. Fine-tuned performance is remarkably better than\nzero-shot performance, achieving a relative reduction in WER of 81% for native\nchildren, 72% for non-native children, 67% for non-native adults, and 65% for\nnative elderly people. Our findings underscore the importance of training\nspeech recognition models like Whisper on underrepresented subpopulations such\nas children, the elderly, and non-native speakers.\n","authors":["Golshid Shekoufandeh","Paul Boersma","Antal van den Bosch"],"pdf_url":"https://arxiv.org/pdf/2502.17284v1.pdf","comment":"ISCA/ITG Workshop on Diversity in Large Speech and Language Models"},{"id":"http://arxiv.org/abs/2502.17282v1","updated":"2025-02-24T16:10:53Z","published":"2025-02-24T16:10:53Z","title":"Capability Instruction Tuning: A New Paradigm for Dynamic LLM Routing","summary":"  Large Language Models (LLMs) have demonstrated human-like\ninstruction-following abilities, particularly those exceeding 100 billion\nparameters. The combined capability of some smaller, resource-friendly LLMs can\naddress most of the instructions that larger LLMs excel at. In this work, we\nexplore how to route the best-performing LLM for each instruction to achieve\nbetter overall performance. We develop a new paradigm, constructing capability\ninstructions with model capability representation, user instruction, and\nperformance inquiry prompts to assess the performance. To learn from capability\ninstructions, we introduce a new end-to-end framework called Model Selection\nwith Aptitude Test (Model-SAT), which generates positive and negative samples\nbased on what different models perform well or struggle with. Model-SAT uses a\nmodel capability encoder that extends its model representation to a lightweight\nLLM. Our experiments show that Model-SAT understands the performance dimensions\nof candidate models and provides the probabilities of their capability to\nhandle various instructions. Additionally, during deployment, a new model can\nquickly infer its aptitude test results across 50 tasks, each with 20 shots.\nModel-SAT performs state-of-the-art model routing without candidate inference\nand in real-world new model-released scenarios. The code is available at\nhttps://github.com/Now-Join-Us/CIT-LLM-Routing\n","authors":["Yi-Kai Zhang","De-Chuan Zhan","Han-Jia Ye"],"pdf_url":"https://arxiv.org/pdf/2502.17282v1.pdf","comment":"AAAI 2025; Project Page: https://cit-llm-routing.github.io"},{"id":"http://arxiv.org/abs/2502.17278v1","updated":"2025-02-24T16:06:35Z","published":"2025-02-24T16:06:35Z","title":"Extracting domain-specific terms using contextual word embeddings","summary":"  Automated terminology extraction refers to the task of extracting meaningful\nterms from domain-specific texts. This paper proposes a novel machine learning\napproach to terminology extraction, which combines features from traditional\nterm extraction systems with novel contextual features derived from contextual\nword embeddings. Instead of using a predefined list of part-of-speech patterns,\nwe first analyse a new term-annotated corpus RSDO5 for the Slovenian language\nand devise a set of rules for term candidate selection and then generate\nstatistical, linguistic and context-based features. We use a support-vector\nmachine algorithm to train a classification model, evaluate it on the four\ndomains (biomechanics, linguistics, chemistry, veterinary) of the RSDO5 corpus\nand compare the results with state-of-art term extraction approaches for the\nSlovenian language. Our approach provides significant improvements in terms of\nF1 score over the previous state-of-the-art, which proves that contextual word\nembeddings are valuable for improving term extraction.\n","authors":["Andraž Repar","Nada Lavrač","Senja Pollak"],"pdf_url":"https://arxiv.org/pdf/2502.17278v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.20296v2","updated":"2025-02-24T16:00:16Z","published":"2024-09-30T13:55:42Z","title":"PersonalLLM: Tailoring LLMs to Individual Preferences","summary":"  As LLMs become capable of complex tasks, there is growing potential for\npersonalized interactions tailored to the subtle and idiosyncratic preferences\nof the user. We present a public benchmark, PersonalLLM, focusing on adapting\nLLMs to provide maximal benefits for a particular user. Departing from existing\nalignment benchmarks that implicitly assume uniform preferences, we curate\nopen-ended prompts paired with many high-quality answers over which users would\nbe expected to display heterogeneous latent preferences. Instead of\npersona-prompting LLMs based on high-level attributes (e.g., user's race or\nresponse length), which yields homogeneous preferences relative to humans, we\ndevelop a method that can simulate a large user base with diverse preferences\nfrom a set of pre-trained reward models. Our dataset and generated\npersonalities offer an innovative testbed for developing personalization\nalgorithms that grapple with continual data sparsity--few relevant feedback\nfrom the particular user--by leveraging historical data from other (similar)\nusers. We explore basic in-context learning and meta-learning baselines to\nillustrate the utility of PersonalLLM and highlight the need for future\nmethodological development. Our dataset is available at\nhttps://huggingface.co/datasets/namkoong-lab/PersonalLLM\n","authors":["Thomas P. Zollo","Andrew Wei Tung Siah","Naimeng Ye","Ang Li","Hongseok Namkoong"],"pdf_url":"https://arxiv.org/pdf/2409.20296v2.pdf","comment":"28 pages, 6 figures"},{"id":"http://arxiv.org/abs/2502.13092v2","updated":"2025-02-24T15:59:04Z","published":"2025-02-18T17:59:48Z","title":"Text2World: Benchmarking Large Language Models for Symbolic World Model\n  Generation","summary":"  Recently, there has been growing interest in leveraging large language models\n(LLMs) to generate symbolic world models from textual descriptions. Although\nLLMs have been extensively explored in the context of world modeling, prior\nstudies encountered several challenges, including evaluation randomness,\ndependence on indirect metrics, and a limited domain scope. To address these\nlimitations, we introduce a novel benchmark, Text2World, based on planning\ndomain definition language (PDDL), featuring hundreds of diverse domains and\nemploying multi-criteria, execution-based metrics for a more robust evaluation.\nWe benchmark current LLMs using Text2World and find that reasoning models\ntrained with large-scale reinforcement learning outperform others. However,\neven the best-performing model still demonstrates limited capabilities in world\nmodeling. Building on these insights, we examine several promising strategies\nto enhance the world modeling capabilities of LLMs, including test-time\nscaling, agent training, and more. We hope that Text2World can serve as a\ncrucial resource, laying the groundwork for future research in leveraging LLMs\nas world models. The project page is available at\nhttps://text-to-world.github.io/.\n","authors":["Mengkang Hu","Tianxing Chen","Yude Zou","Yuheng Lei","Qiguang Chen","Ming Li","Yao Mu","Hongyuan Zhang","Wenqi Shao","Ping Luo"],"pdf_url":"https://arxiv.org/pdf/2502.13092v2.pdf","comment":"Project page: https://text-to-world.github.io/"},{"id":"http://arxiv.org/abs/2502.17268v1","updated":"2025-02-24T15:51:42Z","published":"2025-02-24T15:51:42Z","title":"MonoTODia: Translating Monologue Requests to Task-Oriented Dialogues","summary":"  Data scarcity is one of the main problems when it comes to real-world\napplications of transformer-based models. This is especially evident for\ntask-oriented dialogue (TOD) systems, which require specialized datasets, that\nare usually not readily available. This can hinder companies from adding TOD\nsystems to their services. This study therefore investigates a novel approach\nto sourcing annotated dialogues from existing German monologue material.\nFocusing on a real-world example, we investigate whether these monologues can\nbe transformed into dialogue formats suitable for training TOD systems. We show\nthe approach with the concrete example of a company specializing in travel\nbookings via e-mail. We fine-tune state-of-the-art Large Language Models for\nthe task of rewriting e-mails as dialogues and annotating them. To ensure the\nquality and validity of the generated data, we employ crowd workers to evaluate\nthe dialogues across multiple criteria and to provide gold-standard annotations\nfor the test dataset. We further evaluate the usefulness of the dialogues for\ntraining TOD systems. Our evaluation shows that the dialogues and annotations\nare of high quality and can serve as a valuable starting point for training TOD\nsystems. Finally, we make the annotated dataset publicly available to foster\nfuture research.\n","authors":["Sebastian Steindl","Ulrich Schäfer","Bernd Ludwig"],"pdf_url":"https://arxiv.org/pdf/2502.17268v1.pdf","comment":"Accepted at NAACL 2025 (Industry Track)"},{"id":"http://arxiv.org/abs/2404.12464v8","updated":"2025-02-24T15:50:39Z","published":"2024-04-18T18:48:50Z","title":"NormAd: A Framework for Measuring the Cultural Adaptability of Large\n  Language Models","summary":"  To be effectively and safely deployed to global user populations, large\nlanguage models (LLMs) may need to adapt outputs to user values and cultures,\nnot just know about them. We introduce NormAd, an evaluation framework to\nassess LLMs' cultural adaptability, specifically measuring their ability to\njudge social acceptability across varying levels of cultural norm specificity,\nfrom abstract values to explicit social norms. As an instantiation of our\nframework, we create NormAd-Eti, a benchmark of 2.6k situational descriptions\nrepresenting social-etiquette related cultural norms from 75 countries. Through\ncomprehensive experiments on NormAd-Eti, we find that LLMs struggle to\naccurately judge social acceptability across these varying degrees of cultural\ncontexts and show stronger adaptability to English-centric cultures over those\nfrom the Global South. Even in the simplest setting where the relevant social\nnorms are provided, the best LLMs' performance (< 82\\%) lags behind humans (>\n95\\%). In settings with abstract values and country information, model\nperformance drops substantially (< 60\\%), while human accuracy remains high (>\n90\\%). Furthermore, we find that models are better at recognizing socially\nacceptable versus unacceptable situations. Our findings showcase the current\npitfalls in socio-cultural reasoning of LLMs which hinder their adaptability\nfor global audiences.\n","authors":["Abhinav Rao","Akhila Yerukola","Vishwa Shah","Katharina Reinecke","Maarten Sap"],"pdf_url":"https://arxiv.org/pdf/2404.12464v8.pdf","comment":"Accepted at NAACL 2025"},{"id":"http://arxiv.org/abs/2502.17262v1","updated":"2025-02-24T15:44:57Z","published":"2025-02-24T15:44:57Z","title":"Unveiling Downstream Performance Scaling of LLMs: A Clustering-Based\n  Perspective","summary":"  The rapid advancements in computing dramatically increase the scale and cost\nof training Large Language Models (LLMs). Accurately predicting downstream task\nperformance prior to model training is crucial for efficient resource\nallocation, yet remains challenging due to two primary constraints: (1) the\n\"emergence phenomenon\", wherein downstream performance metrics become\nmeaningful only after extensive training, which limits the ability to use\nsmaller models for prediction; (2) Uneven task difficulty distributions and the\nabsence of consistent scaling laws, resulting in substantial metric\nvariability. Existing performance prediction methods suffer from limited\naccuracy and reliability, thereby impeding the assessment of potential LLM\ncapabilities. To address these challenges, we propose a\nClustering-On-Difficulty (COD) downstream performance prediction framework. COD\nfirst constructs a predictable support subset by clustering tasks based on\ndifficulty features, strategically excluding non-emergent and non-scalable\nclusters. The scores on the selected subset serve as effective intermediate\npredictors of downstream performance on the full evaluation set. With\ntheoretical support, we derive a mapping function that transforms performance\nmetrics from the predictable subset to the full evaluation set, thereby\nensuring accurate extrapolation of LLM downstream performance. The proposed\nmethod has been applied to predict performance scaling for a 70B LLM, providing\nactionable insights for training resource allocation and assisting in\nmonitoring the training process. Notably, COD achieves remarkable predictive\naccuracy on the 70B LLM by leveraging an ensemble of small models,\ndemonstrating an absolute mean deviation of 1.36% across eight important LLM\nevaluation benchmarks.\n","authors":["Chengyin Xu","Kaiyuan Chen","Xiao Li","Ke Shen","Chenggang Li"],"pdf_url":"https://arxiv.org/pdf/2502.17262v1.pdf","comment":"21 pages,6 figures"},{"id":"http://arxiv.org/abs/2412.12094v5","updated":"2025-02-24T15:42:59Z","published":"2024-12-16T18:58:57Z","title":"SepLLM: Accelerate Large Language Models by Compressing One Segment into\n  One Separator","summary":"  Large Language Models (LLMs) have exhibited exceptional performance across a\nspectrum of natural language processing tasks. However, their substantial sizes\npose considerable challenges, particularly in computational demands and\ninference speed, due to their quadratic complexity. In this work, we have\nidentified a key pattern: certain seemingly meaningless separator tokens (i.e.,\npunctuations) contribute disproportionately to attention scores compared to\nsemantically meaningful tokens. This observation suggests that information of\nthe segments between these separator tokens can be effectively condensed into\nthe separator tokens themselves without significant information loss. Guided by\nthis insight, we introduce SepLLM, a plug-and-play framework that accelerates\ninference by compressing these segments and eliminating redundant tokens.\nAdditionally, we implement efficient kernels for training acceleration.\nExperimental results across training-free, training-from-scratch, and\npost-training settings demonstrate SepLLM's effectiveness. Notably, using the\nLlama-3-8B backbone, SepLLM achieves over 50% reduction in KV cache on the\nGSM8K-CoT benchmark while maintaining comparable performance. Furthermore, in\nstreaming settings, SepLLM effectively processes sequences of up to 4 million\ntokens or more while maintaining consistent language modeling capabilities.\n","authors":["Guoxuan Chen","Han Shi","Jiawei Li","Yihang Gao","Xiaozhe Ren","Yimeng Chen","Xin Jiang","Zhenguo Li","Weiyang Liu","Chao Huang"],"pdf_url":"https://arxiv.org/pdf/2412.12094v5.pdf","comment":"We have made our code publicly available at sepllm.github.io. Our\n  codebase supports efficient multi-node distributed training with accelerated\n  attention module Sep-Attention and also supports numerous existing Fusion\n  Operators to accelerate the training process, such as fused rope, etc. If you\n  find our code helpful, please kindly consider giving us a **star** on GitHub\n  ^_^ Thank you very much!"},{"id":"http://arxiv.org/abs/2502.17253v1","updated":"2025-02-24T15:34:09Z","published":"2025-02-24T15:34:09Z","title":"MULTITAT: Benchmarking Multilingual Table-and-Text Question Answering","summary":"  Question answering on the hybrid context of tables and text (TATQA) is a\ncritical task, with broad applications in data-intensive domains. However,\nexisting TATQA datasets are limited to English, leading to several drawbacks:\n(i) They overlook the challenges of multilingual TAT-QA and cannot assess model\nperformance in the multilingual setting. (ii) They do not reflect real-world\nscenarios where tables and texts frequently appear in non-English languages. To\naddress the limitations, we propose the first multilingual TATQA dataset\n(MULTITAT). Specifically, we sample data from 3 mainstream TATQA datasets and\ntranslate it into 10 diverse languages. To align the model TATQA capabilities\nin English with other languages, we develop a baseline, Ours. Experimental\nresults reveal that the performance on non-English data in MULTITAT drops by an\naverage of 19.4% compared to English, proving the necessity of MULTITAT. We\nfurther analyze the reasons for this performance gap. Furthermore, Ours\noutperforms other baselines by an average of 3.3, demonstrating its\neffectiveness.\n","authors":["Xuanliang Zhang","Dingzirui Wang","Keyan Xu","Qingfu Zhu","Wanxiang Che"],"pdf_url":"https://arxiv.org/pdf/2502.17253v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.15218v2","updated":"2025-02-24T15:31:58Z","published":"2025-02-21T05:21:58Z","title":"ESPnet-SpeechLM: An Open Speech Language Model Toolkit","summary":"  We present ESPnet-SpeechLM, an open toolkit designed to democratize the\ndevelopment of speech language models (SpeechLMs) and voice-driven agentic\napplications. The toolkit standardizes speech processing tasks by framing them\nas universal sequential modeling problems, encompassing a cohesive workflow of\ndata preprocessing, pre-training, inference, and task evaluation. With\nESPnet-SpeechLM, users can easily define task templates and configure key\nsettings, enabling seamless and streamlined SpeechLM development. The toolkit\nensures flexibility, efficiency, and scalability by offering highly\nconfigurable modules for every stage of the workflow. To illustrate its\ncapabilities, we provide multiple use cases demonstrating how competitive\nSpeechLMs can be constructed with ESPnet-SpeechLM, including a 1.7B-parameter\nmodel pre-trained on both text and speech tasks, across diverse benchmarks. The\ntoolkit and its recipes are fully transparent and reproducible at:\nhttps://github.com/espnet/espnet/tree/speechlm.\n","authors":["Jinchuan Tian","Jiatong Shi","William Chen","Siddhant Arora","Yoshiki Masuyama","Takashi Maekaku","Yihan Wu","Junyi Peng","Shikhar Bharadwaj","Yiwen Zhao","Samuele Cornell","Yifan Peng","Xiang Yue","Chao-Han Huck Yang","Graham Neubig","Shinji Watanabe"],"pdf_url":"https://arxiv.org/pdf/2502.15218v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.17239v1","updated":"2025-02-24T15:16:34Z","published":"2025-02-24T15:16:34Z","title":"Baichuan-Audio: A Unified Framework for End-to-End Speech Interaction","summary":"  We introduce Baichuan-Audio, an end-to-end audio large language model that\nseamlessly integrates audio understanding and generation. It features a\ntext-guided aligned speech generation mechanism, enabling real-time speech\ninteraction with both comprehension and generation capabilities. Baichuan-Audio\nleverages a pre-trained ASR model, followed by multi-codebook discretization of\nspeech at a frame rate of 12.5 Hz. This multi-codebook setup ensures that\nspeech tokens retain both semantic and acoustic information. To further enhance\nmodeling, an independent audio head is employed to process audio tokens,\neffectively capturing their unique characteristics. To mitigate the loss of\nintelligence during pre-training and preserve the original capabilities of the\nLLM, we propose a two-stage pre-training strategy that maintains language\nunderstanding while enhancing audio modeling. Following alignment, the model\nexcels in real-time speech-based conversation and exhibits outstanding\nquestion-answering capabilities, demonstrating its versatility and efficiency.\nThe proposed model demonstrates superior performance in real-time spoken\ndialogue and exhibits strong question-answering abilities. Our code, model and\ntraining data are available at https://github.com/baichuan-inc/Baichuan-Audio\n","authors":["Tianpeng Li","Jun Liu","Tao Zhang","Yuanbo Fang","Da Pan","Mingrui Wang","Zheng Liang","Zehuan Li","Mingan Lin","Guosheng Dong","Jianhua Xu","Haoze Sun","Zenan Zhou","Weipeng Chen"],"pdf_url":"https://arxiv.org/pdf/2502.17239v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.15210v2","updated":"2025-02-24T15:01:43Z","published":"2025-02-21T04:53:11Z","title":"PairBench: A Systematic Framework for Selecting Reliable Judge VLMs","summary":"  As large vision language models (VLMs) are increasingly used as automated\nevaluators, understanding their ability to effectively compare data pairs as\ninstructed in the prompt becomes essential. To address this, we present\nPairBench, a low-cost framework that systematically evaluates VLMs as\ncustomizable similarity tools across various modalities and scenarios. Through\nPairBench, we introduce four metrics that represent key desiderata of\nsimilarity scores: alignment with human annotations, consistency for data pairs\nirrespective of their order, smoothness of similarity distributions, and\ncontrollability through prompting. Our analysis demonstrates that no model,\nwhether closed- or open-source, is superior on all metrics; the optimal choice\ndepends on an auto evaluator's desired behavior (e.g., a smooth vs. a sharp\njudge), highlighting risks of widespread adoption of VLMs as evaluators without\nthorough assessment. For instance, the majority of VLMs struggle with\nmaintaining symmetric similarity scores regardless of order. Additionally, our\nresults show that the performance of VLMs on the metrics in PairBench closely\ncorrelates with popular benchmarks, showcasing its predictive power in ranking\nmodels.\n","authors":["Aarash Feizi","Sai Rajeswar","Adriana Romero-Soriano","Reihaneh Rabbany","Spandana Gella","Valentina Zantedeschi","João Monteiro"],"pdf_url":"https://arxiv.org/pdf/2502.15210v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07663v2","updated":"2025-02-24T15:00:18Z","published":"2025-02-11T15:56:22Z","title":"Human Decision-making is Susceptible to AI-driven Manipulation","summary":"  Artificial Intelligence (AI) systems are increasingly intertwined with daily\nlife, assisting users in executing various tasks and providing guidance on\ndecision-making. This integration introduces risks of AI-driven manipulation,\nwhere such systems may exploit users' cognitive biases and emotional\nvulnerabilities to steer them toward harmful outcomes. Through a randomized\ncontrolled trial with 233 participants, we examined human susceptibility to\nsuch manipulation in financial (e.g., purchases) and emotional (e.g., conflict\nresolution) decision-making contexts. Participants interacted with one of three\nAI agents: a neutral agent (NA) optimizing for user benefit without explicit\ninfluence, a manipulative agent (MA) designed to covertly influence beliefs and\nbehaviors, or a strategy-enhanced manipulative agent (SEMA) employing explicit\npsychological tactics to reach its hidden objectives. By analyzing\nparticipants' decision patterns and shifts in their preference ratings\npost-interaction, we found significant susceptibility to AI-driven\nmanipulation. Particularly, across both decision-making domains, participants\ninteracting with the manipulative agents shifted toward harmful options at\nsubstantially higher rates (financial, MA: 62.3%, SEMA: 59.6%; emotional, MA:\n42.3%, SEMA: 41.5%) compared to the NA group (financial, 35.8%; emotional,\n12.8%). Notably, our findings reveal that even subtle manipulative objectives\n(MA) can be as effective as employing explicit psychological strategies (SEMA)\nin swaying human decision-making. By revealing the potential for covert AI\ninfluence, this study highlights a critical vulnerability in human-AI\ninteractions, emphasizing the need for ethical safeguards and regulatory\nframeworks to ensure responsible deployment of AI technologies and protect\nhuman autonomy.\n","authors":["Sahand Sabour","June M. Liu","Siyang Liu","Chris Z. Yao","Shiyao Cui","Xuanming Zhang","Wen Zhang","Yaru Cao","Advait Bhat","Jian Guan","Wei Wu","Rada Mihalcea","Hongning Wang","Tim Althoff","Tatia M. C. Lee","Minlie Huang"],"pdf_url":"https://arxiv.org/pdf/2502.07663v2.pdf","comment":"Work in progress"},{"id":"http://arxiv.org/abs/2402.00913v3","updated":"2025-02-24T14:50:08Z","published":"2024-02-01T10:58:10Z","title":"Institutional Platform for Secure Self-Service Large Language Model\n  Exploration","summary":"  This paper introduces a user-friendly platform developed by the University of\nKentucky Center for Applied AI, designed to make large, customized language\nmodels (LLMs) more accessible. By capitalizing on recent advancements in\nmulti-LoRA inference, the system efficiently accommodates custom adapters for a\ndiverse range of users and projects. The paper outlines the system's\narchitecture and key features, encompassing dataset curation, model training,\nsecure inference, and text-based feature extraction.\n  We illustrate the establishment of a tenant-aware computational network using\nagent-based methods, securely utilizing islands of isolated resources as a\nunified system. The platform strives to deliver secure LLM services,\nemphasizing process and data isolation, end-to-end encryption, and role-based\nresource authentication. This contribution aligns with the overarching goal of\nenabling simplified access to cutting-edge AI models and technology in support\nof scientific discovery.\n","authors":["V. K. Cody Bumgardner","Mitchell A. Klusty","W. Vaiden Logan","Samuel E. Armstrong","Caroline N. Leach","Kenneth L. Calvert","Caylin Hickey","Jeff Talbert"],"pdf_url":"https://arxiv.org/pdf/2402.00913v3.pdf","comment":"10 pages 5 figures, 1 table"},{"id":"http://arxiv.org/abs/2502.17216v1","updated":"2025-02-24T14:49:52Z","published":"2025-02-24T14:49:52Z","title":"Making LLMs Reason? The Intermediate Language Problem in Neurosymbolic\n  Approaches","summary":"  Logical reasoning tasks manifest themselves as a challenge to Large Language\nModels (LLMs). Neurosymbolic approaches use LLMs to translate logical reasoning\nproblems formulated in natural language into a formal intermediate language.\nSubsequently, the usage of symbolic reasoners yields reliable solving thereof.\nHowever, LLMs often fail in translation due to poorly chosen intermediate\nlanguages.\n  We introduce the intermediate language problem, which is the problem of\nchoosing a suitable formal language representation for neurosymbolic\napproaches. Theoretically, we argue that its origins lie in the inability of\nLLMs to distinguish syntax from semantics and the relative independence of the\nproblem from its representation. We showcase its existence experimentally by\ncontrasting two intermediate languages, Answer Set Programming and the Python\nKnowledge Engine. In addition, we demonstrate the effects of varying degrees of\nsupplementary context information. Our results show a maximum difference in\noverall-accuracy of 53.20% and 49.26% in execution-accuracy. When using the\nGPT4o-mini LLM we beat the state-of-the-art in overall-accuracy on the ProntoQA\ndataset by 21.20% and by 50.50% on the ProofWriter dataset.\n","authors":["Alexander Beiser","David Penz"],"pdf_url":"https://arxiv.org/pdf/2502.17216v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.17214v1","updated":"2025-02-24T14:48:06Z","published":"2025-02-24T14:48:06Z","title":"CoT-UQ: Improving Response-wise Uncertainty Quantification in LLMs with\n  Chain-of-Thought","summary":"  Large language models (LLMs) excel in many tasks but struggle to accurately\nquantify uncertainty in their generated responses. This limitation makes it\nchallenging to detect misinformation and ensure reliable decision-making.\nExisting uncertainty quantification (UQ) methods for LLMs are primarily\nprompt-wise rather than response-wise, often requiring multiple response\nsamples, which incurs high computational costs. Moreover, LLMs have been shown\nto be overconfident, particularly when using reasoning steps to derive their\nanswers. In this work, we propose CoT-UQ, a response-wise UQ framework that\nintegrates LLMs' inherent reasoning capabilities through Chain-of-Thought (CoT)\ninto the UQ process. CoT-UQ captures critical information during inference by\nextracting keywords from each reasoning step and assessing their importance to\nthe final answer. This key reasoning information is then aggregated to produce\na final uncertainty estimate. We conduct extensive experiments based on LLaMA\nFamily with model sizes varying from 8B to 13B across logical and mathematical\nreasoning tasks. Experimental results demonstrate that CoT-UQ significantly\noutperforms existing UQ methods, achieving an average improvement of 5.9% AUROC\ncompared to current UQ methods. The code is available at:\nhttps://github.com/ZBox1005/CoT-UQ.\n","authors":["Boxuan Zhang","Ruqi Zhang"],"pdf_url":"https://arxiv.org/pdf/2502.17214v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.15411v2","updated":"2025-02-24T14:45:27Z","published":"2025-02-21T12:19:08Z","title":"HiFi-KPI: A Dataset for Hierarchical KPI Extraction from Earnings\n  Filings","summary":"  The U.S. Securities and Exchange Commission (SEC) requires that public\ncompanies file financial reports tagging numbers with the machine readable\ninline eXtensible Business Reporting Language (iXBRL) standard. However, the\nhighly complex and highly granular taxonomy defined by iXBRL limits label\ntransferability across domains. In this paper, we introduce the Hierarchical\nFinancial Key Performance Indicator (HiFi-KPI) dataset, designed to facilitate\nnumerical KPI extraction at specified levels of granularity from unstructured\nfinancial text. Our approach organizes a 218,126-label hierarchy using a\ntaxonomy based grouping method, investigating which taxonomy layer provides the\nmost meaningful structure. HiFi-KPI comprises ~1.8M paragraphs and ~5M\nentities, each linked to a label in the iXBRL-specific calculation and\npresentation taxonomies. We provide baselines using encoder-based approaches\nand structured extraction using Large Language Models (LLMs). To simplify LLM\ninference and evaluation, we additionally release HiFi-KPI Lite, a manually\ncurated subset with four expert-mapped labels. We publicly release all\nartifacts.\n","authors":["Rasmus Aavang","Giovanni Rizzi","Rasmus Bøggild","Alexandre Iolov","Mike Zhang","Johannes Bjerva"],"pdf_url":"https://arxiv.org/pdf/2502.15411v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.17204v1","updated":"2025-02-24T14:39:28Z","published":"2025-02-24T14:39:28Z","title":"Order Matters: Investigate the Position Bias in Multi-constraint\n  Instruction Following","summary":"  Real-world instructions with multiple constraints pose a significant\nchallenge to existing large language models (LLMs). An observation is that the\nLLMs exhibit dramatic performance fluctuation when disturbing the order of the\nincorporated constraints. Yet, none of the existing works has systematically\ninvestigated this position bias problem in the field of multi-constraint\ninstruction following. To bridge this gap, we design a probing task where we\nquantitatively measure the difficulty distribution of the constraints by a\nnovel Difficulty Distribution Index (CDDI). Through the experimental results,\nwe find that LLMs are more performant when presented with the constraints in a\n``hard-to-easy'' order. This preference can be generalized to LLMs with\ndifferent architecture or different sizes of parameters. Additionally, we\nconduct an explanation study, providing an intuitive insight into the\ncorrelation between the LLM's attention and constraint orders. Our code and\ndataset are publicly available at https://github.com/meowpass/PBIF.\n","authors":["Jie Zeng","Qianyu He","Qingyu Ren","Jiaqing Liang","Yanghua Xiao","Weikang Zhou","Zeye Sun","Fei Yu"],"pdf_url":"https://arxiv.org/pdf/2502.17204v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.03035v4","updated":"2025-02-24T14:34:37Z","published":"2025-01-06T14:23:02Z","title":"Quantization Meets Reasoning: Exploring LLM Low-Bit Quantization\n  Degradation for Mathematical Reasoning","summary":"  Large language models have achieved significant advancements in complex\nmathematical reasoning benchmarks, such as MATH. However, their substantial\ncomputational requirements present challenges for practical deployment. Model\nquantization has emerged as an effective strategy to reduce memory usage and\ncomputational costs by employing lower precision and bit-width representations.\nIn this study, we systematically evaluate the impact of quantization on\nmathematical reasoning tasks. Our results demonstrate that aggressive\nquantization methods like AWQ and GPTQ introduce up to 32.39% accuracy\ndegradation (average 11.31%) on Llama-3 models, particularly in numerical\ncomputation and reasoning planning. To address this, we introduce a\nmultidimensional evaluation framework combining qualitative capability analysis\nand quantitative error assessment. We further develop targeted recovery\nstrategies, showing that fine-tuning quantized models on only 545 task-specific\nexamples for 3 minutes on 4 GPUs effectively restores reasoning capabilities to\nnear full-precision levels. Additionally, our error assessment pipeline\nachieves 98.9% accuracy in diagnosing and localizing errors across 3,366\nfailure cases, providing actionable insights for mitigating\nquantization-induced degradation.\n","authors":["Zhen Li","Yupeng Su","Runming Yang","Congkai Xie","Zheng Wang","Zhongwei Xie","Ngai Wong","Hongxia Yang"],"pdf_url":"https://arxiv.org/pdf/2501.03035v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.15485v2","updated":"2025-02-24T14:30:32Z","published":"2025-02-21T14:18:18Z","title":"Enhancing RWKV-based Language Models for Long-Sequence Text Generation","summary":"  This paper introduces an enhanced RWKV architecture with adaptive temporal\ngating mechanisms for improved long-context language modeling. We propose two\nprincipal innovations: (1) a position-aware convolutional shift operator that\ncaptures local syntactic patterns while preserving global coherence, and (2) a\nneurally-gated information routing mechanism that dynamically regulates\ninter-token information flow. Through comprehensive experiments on text\ngeneration tasks, our enhanced model demonstrates superior performance compared\nto the baseline RWKV, achieving 96.5 relative improvement in ROUGE-L scores\nwith only 2.95 increased inference latency. Ablation studies validate the\nindividual contributions of each component, while linguistic analysis reveals\nthe model's adaptive attention to syntactic boundaries and entity coherence.\nThe proposed modifications maintain RWKV's linear computational complexity\nwhile significantly enhancing its contextual modeling capabilities,\nestablishing new state-of-the-art performance for recurrent-style architectures\nin long-form text generation.\n","authors":["Xinghan Pan"],"pdf_url":"https://arxiv.org/pdf/2502.15485v2.pdf","comment":"8 pages, 2 tables, 3 figures"},{"id":"http://arxiv.org/abs/2502.17187v1","updated":"2025-02-24T14:23:52Z","published":"2025-02-24T14:23:52Z","title":"Evaluating Expert Contributions in a MoE LLM for Quiz-Based Tasks","summary":"  Recently, Large Language Models (LLMs) with Mixture of Experts (MoE) layers\nhave gained significant attention. Currently, state-of-the-art LLMs utilize\nthis architecture. There is a substantial amount of research on how to train\nsuch models and how to select hyperparameters for this architecture. However,\nthere is a lack of studies focusing on post-evaluation analysis of MoE layer\nproperties. In this paper, we take a first step toward closing this gap by\nevaluating expert contributions on the quiz-based MMLU benchmark. We show that\nmost experts were never activated during inference on this benchmark.\nAdditionally, the output distribution of gating networks is much closer to\nuniform than sparse. Finally, we demonstrate that the average performance of\nsome experts within the same layer varies significantly.\n","authors":["Andrei Chernov"],"pdf_url":"https://arxiv.org/pdf/2502.17187v1.pdf","comment":"preprint, short paper"},{"id":"http://arxiv.org/abs/2502.17184v1","updated":"2025-02-24T14:20:22Z","published":"2025-02-24T14:20:22Z","title":"Measuring Data Diversity for Instruction Tuning: A Systematic Analysis\n  and A Reliable Metric","summary":"  Data diversity is crucial for the instruction tuning of large language\nmodels. Existing studies have explored various diversity-aware data selection\nmethods to construct high-quality datasets and enhance model performance.\nHowever, the fundamental problem of precisely defining and measuring data\ndiversity remains underexplored, limiting clear guidance for data engineering.\nTo address this, we systematically analyze 11 existing diversity measurement\nmethods by assessing their correlation with model performance through extensive\nfine-tuning experiments. Our results indicate that a reliable diversity measure\nshould properly account for both inter-sample differences and the information\ndistribution in the sample space. Building on this, we propose NovelSum, a new\ndiversity metric based on sample-level \"novelty.\" Experiments on both simulated\nand real-world data show that NovelSum accurately captures diversity variations\nand achieves a 0.97 correlation with instruction-tuned model performance,\nhighlighting its value in guiding data engineering practices. With NovelSum as\nan optimization objective, we further develop a greedy, diversity-oriented data\nselection strategy that outperforms existing approaches, validating both the\neffectiveness and practical significance of our metric.\n","authors":["Yuming Yang","Yang Nan","Junjie Ye","Shihan Dou","Xiao Wang","Shuo Li","Huijie Lv","Tao Gui","Qi Zhang","Xuanjing Huang"],"pdf_url":"https://arxiv.org/pdf/2502.17184v1.pdf","comment":"15 pages. The related codes and resources will be released later.\n  Project page: https://github.com/UmeanNever/NovelSum"},{"id":"http://arxiv.org/abs/2502.17173v1","updated":"2025-02-24T14:09:45Z","published":"2025-02-24T14:09:45Z","title":"Cheems: A Practical Guidance for Building and Evaluating Chinese Reward\n  Models from Scratch","summary":"  Reward models (RMs) are crucial for aligning large language models (LLMs)\nwith human preferences. However, most RM research is centered on English and\nrelies heavily on synthetic resources, which leads to limited and less reliable\ndatasets and benchmarks for Chinese. To address this gap, we introduce\nCheemsBench, a fully human-annotated RM evaluation benchmark within Chinese\ncontexts, and CheemsPreference, a large-scale and diverse preference dataset\nannotated through human-machine collaboration to support Chinese RM training.\nWe systematically evaluate open-source discriminative and generative RMs on\nCheemsBench and observe significant limitations in their ability to capture\nhuman preferences in Chinese scenarios. Additionally, based on\nCheemsPreference, we construct an RM that achieves state-of-the-art performance\non CheemsBench, demonstrating the necessity of human supervision in RM\ntraining. Our findings reveal that scaled AI-generated data struggles to fully\ncapture human preferences, emphasizing the importance of high-quality human\nsupervision in RM development.\n","authors":["Xueru Wen","Jie Lou","Zichao Li","Yaojie Lu","Xing Yu","Yuqiu Ji","Guohai Xu","Hongyu Lin","Ben He","Xianpei Han","Le Sun","Debing Zhang"],"pdf_url":"https://arxiv.org/pdf/2502.17173v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.17169v1","updated":"2025-02-24T14:05:47Z","published":"2025-02-24T14:05:47Z","title":"Logic Haystacks: Probing LLMs Long-Context Logical Reasoning (Without\n  Easily Identifiable Unrelated Padding)","summary":"  Large language models demonstrate promising long context processing\ncapabilities, with recent models touting context windows close to one million\ntokens. However, the evaluations supporting these claims often involve simple\nretrieval tasks or synthetic tasks padded with irrelevant text, which the\nmodels may easily detect and discard. In this work, we generate lengthy\nsimplified English text with first-order logic representations spanning up to\n2048 clauses (around 25k GPT-4 tokens). We formulate an evaluation task with\nevidence retrieval for contradiction detection. The long, homogeneous text is\nfilled with distractors that are both hard to distinguish from relevant\nevidences and provably not interfering with them. Our evaluation of evidence\nretrieval shows that the effective context window is much smaller with\nrealistic distractors, already crumbling at 128 clauses.\n","authors":["Damien Sileo"],"pdf_url":"https://arxiv.org/pdf/2502.17169v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.17166v1","updated":"2025-02-24T14:02:00Z","published":"2025-02-24T14:02:00Z","title":"JUREX-4E: Juridical Expert-Annotated Four-Element Knowledge Base for\n  Legal Reasoning","summary":"  The Four-Element Theory is a fundamental framework in criminal law, defining\nthe constitution of crime through four dimensions: Subject, Object, Subjective\naspect, and Objective aspect. This theory is widely referenced in legal\nreasoning, and many Large Language Models (LLMs) attempt to incorporate it when\nhandling legal tasks. However, current approaches rely on LLMs' internal\nknowledge to incorporate this theory, often lacking completeness and\nrepresentativeness. To address this limitation, we introduce JUREX-4E, an\nexpert-annotated knowledge base covering 155 criminal charges. It is structured\nthrough a progressive hierarchical annotation framework that prioritizes legal\nsource validity and employs diverse legal interpretation methods to ensure\ncomprehensiveness and authority. We evaluate JUREX-4E on the Similar Charge\nDistinction task and apply it to Legal Case Retrieval, demonstrating its\neffectiveness in improving LLM performance. Experimental results validate the\nhigh quality of JUREX-4E and its substantial impact on downstream legal tasks,\nunderscoring its potential for advancing legal AI applications. Code:\nhttps://github.com/THUlawtech/JUREX\n","authors":["Huanghai Liu","Quzhe Huang","Qingjing Chen","Yiran Hu","Jiayu Ma","Yun Liu","Weixing Shen","Yansong Feng"],"pdf_url":"https://arxiv.org/pdf/2502.17166v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.17163v1","updated":"2025-02-24T13:58:42Z","published":"2025-02-24T13:58:42Z","title":"MEMERAG: A Multilingual End-to-End Meta-Evaluation Benchmark for\n  Retrieval Augmented Generation","summary":"  Automatic evaluation of retrieval augmented generation (RAG) systems relies\non fine-grained dimensions like faithfulness and relevance, as judged by expert\nhuman annotators. Meta-evaluation benchmarks support the development of\nautomatic evaluators that correlate well with human judgement. However,\nexisting benchmarks predominantly focus on English or use translated data,\nwhich fails to capture cultural nuances. A native approach provides a better\nrepresentation of the end user experience.\n  In this work, we develop a Multilingual End-to-end Meta-Evaluation RAG\nbenchmark (MEMERAG). Our benchmark builds on the popular MIRACL dataset, using\nnative-language questions and generating responses with diverse large language\nmodels (LLMs), which are then assessed by expert annotators for faithfulness\nand relevance. We describe our annotation process and show that it achieves\nhigh inter-annotator agreement. We then analyse the performance of the\nanswer-generating LLMs across languages as per the human evaluators. Finally we\napply the dataset to our main use-case which is to benchmark multilingual\nautomatic evaluators (LLM-as-a-judge). We show that our benchmark can reliably\nidentify improvements offered by advanced prompting techniques and LLMs. We\nrelease our benchmark to support the community developing accurate evaluation\nmethods for multilingual RAG systems.\n","authors":["María Andrea Cruz Blandón","Jayasimha Talur","Bruno Charron","Dong Liu","Saab Mansour","Marcello Federico"],"pdf_url":"https://arxiv.org/pdf/2502.17163v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.17161v1","updated":"2025-02-24T13:56:27Z","published":"2025-02-24T13:56:27Z","title":"Real-time Monitoring of Economic Shocks using Company Websites","summary":"  Understanding the effects of economic shocks on firms is critical for\nanalyzing economic growth and resilience. We introduce a Web-Based Affectedness\nIndicator (WAI), a general-purpose tool for real-time monitoring of economic\ndisruptions across diverse contexts. By leveraging Large Language Model (LLM)\nassisted classification and information extraction on texts from over five\nmillion company websites, WAI quantifies the degree and nature of firms'\nresponses to external shocks. Using the COVID-19 pandemic as a specific\napplication, we show that WAI is highly correlated with pandemic containment\nmeasures and reliably predicts firm performance. Unlike traditional data\nsources, WAI provides timely firm-level information across industries and\ngeographies worldwide that would otherwise be unavailable due to institutional\nand data availability constraints. This methodology offers significant\npotential for monitoring and mitigating the impact of technological, political,\nfinancial, health or environmental crises, and represents a transformative tool\nfor adaptive policy-making and economic resilience.\n","authors":["Michael Koenig","Jakob Rauch","Martin Woerter"],"pdf_url":"https://arxiv.org/pdf/2502.17161v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.08090v2","updated":"2025-02-24T13:44:37Z","published":"2024-12-11T04:16:39Z","title":"Multilingual LLMs Inherently Reward In-Language Time-Sensitive Semantic\n  Alignment for Low-Resource Languages","summary":"  The unwavering disparity in labeled resources between resource-rich languages\nand those considered low-resource remains a significant impediment for Large\nLanguage Models (LLMs). Recent strides in cross-lingual in-context learning\n(X-ICL), mainly through semantically aligned examples retrieved from\nmultilingual pre-trained transformers, have shown promise in mitigating this\nissue. However, our investigation reveals that LLMs intrinsically reward\nin-language semantically aligned cross-lingual instances over direct\ncross-lingual semantic alignments, with a pronounced disparity in handling\ntime-sensitive queries in the X-ICL setup. Such queries demand sound temporal\nreasoning ability from LLMs, yet the advancements have predominantly focused on\nEnglish. This study aims to bridge this gap by improving temporal reasoning\ncapabilities in low-resource languages. To this end, we introduce mTEMPREASON,\na temporal reasoning dataset aimed at the varied degrees of low-resource\nlanguages and propose Cross-Lingual Time-Sensitive Semantic Alignment\n(CLiTSSA), a novel method to improve temporal reasoning in these contexts. To\nfacilitate this, we construct an extension of mTEMPREASON comprising pairs of\nparallel cross-language temporal queries along with their anticipated\nin-language semantic similarity scores. Our empirical evidence underscores the\nsuperior performance of CLiTSSA compared to established baselines across three\nlanguages -- Romanian, German, and French, encompassing three temporal tasks\nand including a diverse set of four contemporaneous LLMs. This marks a\nsignificant step forward in addressing resource disparity in the context of\ntemporal reasoning across languages.\n","authors":["Ashutosh Bajpai","Tanmoy Chakraborty"],"pdf_url":"https://arxiv.org/pdf/2412.08090v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.20238v2","updated":"2025-02-24T13:42:28Z","published":"2024-10-26T17:48:20Z","title":"A Survey of Large Language Models for Arabic Language and its Dialects","summary":"  This survey offers a comprehensive overview of Large Language Models (LLMs)\ndesigned for Arabic language and its dialects. It covers key architectures,\nincluding encoder-only, decoder-only, and encoder-decoder models, along with\nthe datasets used for pre-training, spanning Classical Arabic, Modern Standard\nArabic, and Dialectal Arabic. The study also explores monolingual, bilingual,\nand multilingual LLMs, analyzing their architectures and performance across\ndownstream tasks, such as sentiment analysis, named entity recognition, and\nquestion answering. Furthermore, it assesses the openness of Arabic LLMs based\non factors, such as source code availability, training data, model weights, and\ndocumentation. The survey highlights the need for more diverse dialectal\ndatasets and attributes the importance of openness for research reproducibility\nand transparency. It concludes by identifying key challenges and opportunities\nfor future research and stressing the need for more inclusive and\nrepresentative models.\n","authors":["Malak Mashaabi","Shahad Al-Khalifa","Hend Al-Khalifa"],"pdf_url":"https://arxiv.org/pdf/2410.20238v2.pdf","comment":"Submitted to ACM Transactions on Asian and Low-Resource Language\n  Information Processing"},{"id":"http://arxiv.org/abs/2502.15294v2","updated":"2025-02-24T13:35:18Z","published":"2025-02-21T08:40:07Z","title":"Round Attention: A Novel Round-Level Attention Mechanism to Accelerate\n  LLM Inference","summary":"  The increasing context window size in large language models (LLMs) has\nimproved their ability to handle complex, long-text tasks. However, as the\nconversation rounds continue, it is required to store a large amount of KV\ncache in GPU memory, which significantly affects the efficiency and even\navailability of the model serving systems. This paper analyzes dialogue data\nfrom real users and discovers that the LLM inference manifests a watershed\nlayer, after which the distribution of round-level attention shows notable\nsimilarity. We propose Round Attention, a novel round-level attention mechanism\nthat only recalls and computes the KV cache of the most relevant rounds. The\nexperiments show that our method saves 55\\% memory usage without compromising\nmodel performance.\n","authors":["Yaohua Tang","Zhicheng Hu","Kun Cheng","Fan Mo","Qiheng Lv","Hua Wang","Zhi Chen"],"pdf_url":"https://arxiv.org/pdf/2502.15294v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.17143v1","updated":"2025-02-24T13:34:35Z","published":"2025-02-24T13:34:35Z","title":"Sentiment analysis of texts from social networks based on machine\n  learning methods for monitoring public sentiment","summary":"  A sentiment analysis system powered by machine learning was created in this\nstudy to improve real-time social network public opinion monitoring. For\nsophisticated sentiment identification, the suggested approach combines\ncutting-edge transformer-based architectures (DistilBERT, RoBERTa) with\ntraditional machine learning models (Logistic Regression, SVM, Naive Bayes).\nThe system achieved an accuracy of up to 80-85% using transformer models in\nreal-world scenarios after being tested using both deep learning techniques and\nstandard machine learning processes on annotated social media datasets.\nAccording to experimental results, deep learning models perform noticeably\nbetter than lexicon-based and conventional rule-based classifiers, lowering\nmisclassification rates and enhancing the ability to recognize nuances like\nsarcasm. According to feature importance analysis, context tokens,\nsentiment-bearing keywords, and part-of-speech structure are essential for\nprecise categorization. The findings confirm that AI-driven sentiment\nframeworks can provide a more adaptive and efficient approach to modern\nsentiment challenges. Despite the system's impressive performance, issues with\ncomputing overhead, data quality, and domain-specific terminology still exist.\nIn order to monitor opinions on a broad scale, future research will investigate\nimproving computing performance, extending coverage to various languages, and\nintegrating real-time streaming APIs. The results demonstrate that governments,\ncorporations, and social researchers looking for more in-depth understanding of\npublic mood on digital platforms can find a reliable and adaptable answer in\nAI-powered sentiment analysis.\n","authors":["Arsen Tolebay Nurlanuly"],"pdf_url":"https://arxiv.org/pdf/2502.17143v1.pdf","comment":"10 pages, 5 figures, 2 tables. Preprint submitted for community\n  feedback"},{"id":"http://arxiv.org/abs/2502.11051v2","updated":"2025-02-24T13:32:25Z","published":"2025-02-16T09:23:50Z","title":"MMUnlearner: Reformulating Multimodal Machine Unlearning in the Era of\n  Multimodal Large Language Models","summary":"  Recent progress in Machine Unlearning (MU) has introduced solutions for the\nselective removal of private or sensitive information encoded within deep\nneural networks. Nonetheless, MU for Multimodal Large Language Models (MLLMs)\nremains in its nascent phase. Therefore, we propose to reformulate the task of\nmultimodal MU in the era of MLLMs, which aims to erase only the visual patterns\nassociated with a given entity while preserving the corresponding textual\nknowledge encoded within the original parameters of the language model\nbackbone. Furthermore, we develop a novel geometry-constrained gradient descent\nmethod MMUnlearner. It updates the weights of MLLMs with a weight saliency map\njointly restricted by the remaining concepts and textual knowledge during\nunlearning, thereby preserving parameters essential for non-target knowledge.\nExtensive experiments demonstrate that MMUnlearner surpasses baselines that\nfinetuning MLLMs with VQA data directly through Gradient Ascent (GA) or\nNegative Preference Optimization (NPO), across all evaluation dimensions. Our\ncode will be released upon acceptance.\n","authors":["Jiahao Huo","Yibo Yan","Xu Zheng","Yuanhuiyi Lyu","Xin Zou","Zhihua Wei","Xuming Hu"],"pdf_url":"https://arxiv.org/pdf/2502.11051v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.14943v2","updated":"2025-02-24T13:31:36Z","published":"2025-02-20T17:47:40Z","title":"GenAI vs. Human Fact-Checkers: Accurate Ratings, Flawed Rationales","summary":"  Despite recent advances in understanding the capabilities and limits of\ngenerative artificial intelligence (GenAI) models, we are just beginning to\nunderstand their capacity to assess and reason about the veracity of content.\nWe evaluate multiple GenAI models across tasks that involve the rating of, and\nperceived reasoning about, the credibility of information. The information in\nour experiments comes from content that subnational U.S. politicians post to\nFacebook. We find that GPT-4o, one of the most used AI models in consumer\napplications, outperforms other models, but all models exhibit only moderate\nagreement with human coders. Importantly, even when GenAI models accurately\nidentify low-credibility content, their reasoning relies heavily on linguistic\nfeatures and ``hard'' criteria, such as the level of detail, source\nreliability, and language formality, rather than an understanding of veracity.\nWe also assess the effectiveness of summarized versus full content inputs,\nfinding that summarized content holds promise for improving efficiency without\nsacrificing accuracy. While GenAI has the potential to support human\nfact-checkers in scaling misinformation detection, our results caution against\nrelying solely on these models.\n","authors":["Yuehong Cassandra Tai","Khushi Navin Patni","Nicholas Daniel Hemauer","Bruce Desmarais","Yu-Ru Lin"],"pdf_url":"https://arxiv.org/pdf/2502.14943v2.pdf","comment":"Accepted for publication in the 17th ACM Web Science Conference 2025"},{"id":"http://arxiv.org/abs/2410.13708v2","updated":"2025-02-24T13:31:08Z","published":"2024-10-17T16:08:06Z","title":"On the Role of Attention Heads in Large Language Model Safety","summary":"  Large language models (LLMs) achieve state-of-the-art performance on multiple\nlanguage tasks, yet their safety guardrails can be circumvented, leading to\nharmful generations. In light of this, recent research on safety mechanisms has\nemerged, revealing that when safety representations or component are\nsuppressed, the safety capability of LLMs are compromised. However, existing\nresearch tends to overlook the safety impact of multi-head attention\nmechanisms, despite their crucial role in various model functionalities. Hence,\nin this paper, we aim to explore the connection between standard attention\nmechanisms and safety capability to fill this gap in the safety-related\nmechanistic interpretability. We propose a novel metric which tailored for\nmulti-head attention, the Safety Head ImPortant Score (Ships), to assess the\nindividual heads' contributions to model safety. Based on this, we generalize\nShips to the dataset level and further introduce the Safety Attention Head\nAttRibution Algorithm (Sahara) to attribute the critical safety attention heads\ninside the model. Our findings show that the special attention head has a\nsignificant impact on safety. Ablating a single safety head allows aligned\nmodel (e.g., Llama-2-7b-chat) to respond to 16 times more harmful queries,\nwhile only modifying 0.006% of the parameters, in contrast to the ~ 5%\nmodification required in previous studies. More importantly, we demonstrate\nthat attention heads primarily function as feature extractors for safety and\nmodels fine-tuned from the same base model exhibit overlapping safety heads\nthrough comprehensive experiments. Together, our attribution approach and\nfindings provide a novel perspective for unpacking the black box of safety\nmechanisms within large models.\n","authors":["Zhenhong Zhou","Haiyang Yu","Xinghua Zhang","Rongwu Xu","Fei Huang","Kun Wang","Yang Liu","Junfeng Fang","Yongbin Li"],"pdf_url":"https://arxiv.org/pdf/2410.13708v2.pdf","comment":"28 pages, 18 figures, 7 tables. This paper has been accepted as ICLR\n  2025 (oral)"},{"id":"http://arxiv.org/abs/2405.13929v5","updated":"2025-02-24T13:24:20Z","published":"2024-05-22T18:58:58Z","title":"Vikhr: Constructing a State-of-the-art Bilingual Open-Source\n  Instruction-Following Large Language Model for Russian","summary":"  There has been a surge in developing various Large Language Models (LLMs).\nHowever, text generation for languages other than English often faces\nsignificant challenges, including poor generation quality and reduced\ncomputational performance due to the disproportionate representation of tokens\nin the model's vocabulary. In this work, we address these issues by developing\na pipeline for adapting English-oriented pre-trained models to other languages\nand constructing efficient bilingual LLMs. Using this pipeline, we construct\nVikhr, a state-of-the-art bilingual open-source instruction-following LLM\ndesigned specifically for the Russian language. \"Vikhr\" refers to the name of\nthe Mistral LLM series and means a \"strong gust of wind.\" Unlike previous\nRussian-language models that typically rely on LoRA adapters on top of\nEnglish-oriented models, sacrificing performance for lower training costs,\nVikhr features an adapted tokenizer vocabulary and undergoes continued\npre-training and instruction tuning of all weights. This not only enhances the\nmodel's performance but also significantly improves its computational and\ncontextual efficiency. The remarkable performance of Vikhr across various\nRussian-language benchmarks can also be attributed to our efforts in expanding\ninstruction datasets and corpora for continued pre-training. Vikhr not only\nsets a new state of the art among open-source LLMs for Russian but even\noutperforms some proprietary closed-source models on certain benchmarks. The\nmodel weights, instruction sets, and code are publicly available.\n","authors":["Aleksandr Nikolich","Konstantin Korolev","Sergei Bratchikov","Igor Kiselev","Artem Shelmanov"],"pdf_url":"https://arxiv.org/pdf/2405.13929v5.pdf","comment":"Accepted at WMRL @ EMNLP-2024"},{"id":"http://arxiv.org/abs/2502.17129v1","updated":"2025-02-24T13:19:33Z","published":"2025-02-24T13:19:33Z","title":"Thus Spake Long-Context Large Language Model","summary":"  Long context is an important topic in Natural Language Processing (NLP),\nrunning through the development of NLP architectures, and offers immense\nopportunities for Large Language Models (LLMs) giving LLMs the lifelong\nlearning potential akin to humans. Unfortunately, the pursuit of a long context\nis accompanied by numerous obstacles. Nevertheless, long context remains a core\ncompetitive advantage for LLMs. In the past two years, the context length of\nLLMs has achieved a breakthrough extension to millions of tokens. Moreover, the\nresearch on long-context LLMs has expanded from length extrapolation to a\ncomprehensive focus on architecture, infrastructure, training, and evaluation\ntechnologies.\n  Inspired by the symphonic poem, Thus Spake Zarathustra, we draw an analogy\nbetween the journey of extending the context of LLM and the attempts of humans\nto transcend its mortality. In this survey, We will illustrate how LLM\nstruggles between the tremendous need for a longer context and its equal need\nto accept the fact that it is ultimately finite. To achieve this, we give a\nglobal picture of the lifecycle of long-context LLMs from four perspectives:\narchitecture, infrastructure, training, and evaluation, showcasing the full\nspectrum of long-context technologies. At the end of this survey, we will\npresent 10 unanswered questions currently faced by long-context LLMs. We hope\nthis survey can serve as a systematic introduction to the research on\nlong-context LLMs.\n","authors":["Xiaoran Liu","Ruixiao Li","Mianqiu Huang","Zhigeng Liu","Yuerong Song","Qipeng Guo","Siyang He","Qiqi Wang","Linlin Li","Qun Liu","Yaqian Zhou","Xuanjing Huang","Xipeng Qiu"],"pdf_url":"https://arxiv.org/pdf/2502.17129v1.pdf","comment":"a global picture of the lifecycle of long-context LLMs from four\n  perspectives: architecture, infrastructure, training, and evaluation"},{"id":"http://arxiv.org/abs/2502.17125v1","updated":"2025-02-24T13:11:47Z","published":"2025-02-24T13:11:47Z","title":"LettuceDetect: A Hallucination Detection Framework for RAG Applications","summary":"  Retrieval Augmented Generation (RAG) systems remain vulnerable to\nhallucinated answers despite incorporating external knowledge sources. We\npresent LettuceDetect a framework that addresses two critical limitations in\nexisting hallucination detection methods: (1) the context window constraints of\ntraditional encoder-based methods, and (2) the computational inefficiency of\nLLM based approaches. Building on ModernBERT's extended context capabilities\n(up to 8k tokens) and trained on the RAGTruth benchmark dataset, our approach\noutperforms all previous encoder-based models and most prompt-based models,\nwhile being approximately 30 times smaller than the best models. LettuceDetect\nis a token-classification model that processes context-question-answer triples,\nallowing for the identification of unsupported claims at the token level.\nEvaluations on the RAGTruth corpus demonstrate an F1 score of 79.22% for\nexample-level detection, which is a 14.8% improvement over Luna, the previous\nstate-of-the-art encoder-based architecture. Additionally, the system can\nprocess 30 to 60 examples per second on a single GPU, making it more practical\nfor real-world RAG applications.\n","authors":["Ádám Kovács","Gábor Recski"],"pdf_url":"https://arxiv.org/pdf/2502.17125v1.pdf","comment":"6 pages"},{"id":"http://arxiv.org/abs/2402.19097v4","updated":"2025-02-24T13:06:32Z","published":"2024-02-29T12:25:45Z","title":"TEncDM: Understanding the Properties of the Diffusion Model in the Space\n  of Language Model Encodings","summary":"  This paper presents the Text Encoding Diffusion Model (TEncDM), a novel\napproach to diffusion modeling that operates in the space of pre-trained\nlanguage model encodings. In contrast to traditionally used embeddings,\nencodings integrate contextual information. In our approach, we also employ a\ntransformer-based decoder, specifically designed to incorporate context in the\ntoken prediction process. We conduct a comprehensive examination of the\ninfluence of the encoder, decoder, noise scheduler, and self-conditioning on\nzero-shot generation. Furthermore, we compare TEncDM with previous approaches\non three conditional text generation tasks: QQP, XSum, and Wiki-Auto. The\nresults show that TEncDM exhibits superior performance compared to existing\nnon-autoregressive diffusion models. Our code is available at\nhttps://github.com/M0RJIQUE/tencdm.\n","authors":["Alexander Shabalin","Viacheslav Meshchaninov","Egor Chimbulatov","Vladislav Lapikov","Roman Kim","Grigory Bartosh","Dmitry Molchanov","Sergey Markov","Dmitry Vetrov"],"pdf_url":"https://arxiv.org/pdf/2402.19097v4.pdf","comment":"15 pages, 13 figures"},{"id":"http://arxiv.org/abs/2502.17110v1","updated":"2025-02-24T12:51:23Z","published":"2025-02-24T12:51:23Z","title":"Mobile-Agent-V: Learning Mobile Device Operation Through Video-Guided\n  Multi-Agent Collaboration","summary":"  The rapid increase in mobile device usage necessitates improved automation\nfor seamless task management. However, many AI-driven frameworks struggle due\nto insufficient operational knowledge. Manually written knowledge helps but is\nlabor-intensive and inefficient. To address these challenges, we introduce\nMobile-Agent-V, a framework that leverages video guidance to provide rich and\ncost-effective operational knowledge for mobile automation. Mobile-Agent-V\nenhances task execution capabilities by leveraging video inputs without\nrequiring specialized sampling or preprocessing. Mobile-Agent-V integrates a\nsliding window strategy and incorporates a video agent and deep-reflection\nagent to ensure that actions align with user instructions. Through this\ninnovative approach, users can record task processes with guidance, enabling\nthe system to autonomously learn and execute tasks efficiently. Experimental\nresults show that Mobile-Agent-V achieves a 30% performance improvement\ncompared to existing frameworks.\n","authors":["Junyang Wang","Haiyang Xu","Xi Zhang","Ming Yan","Ji Zhang","Fei Huang","Jitao Sang"],"pdf_url":"https://arxiv.org/pdf/2502.17110v1.pdf","comment":"16 pages, 7 figures, 7tables"},{"id":"http://arxiv.org/abs/2305.14583v3","updated":"2025-02-24T12:49:51Z","published":"2023-05-23T23:45:20Z","title":"Natural Language Decompositions of Implicit Content Enable Better Text\n  Representations","summary":"  When people interpret text, they rely on inferences that go beyond the\nobserved language itself. Inspired by this observation, we introduce a method\nfor the analysis of text that takes implicitly communicated content explicitly\ninto account. We use a large language model to produce sets of propositions\nthat are inferentially related to the text that has been observed, then\nvalidate the plausibility of the generated content via human judgments.\nIncorporating these explicit representations of implicit content proves useful\nin multiple problem settings that involve the human interpretation of\nutterances: assessing the similarity of arguments, making sense of a body of\nopinion data, and modeling legislative behavior. Our results suggest that\nmodeling the meanings behind observed language, rather than the literal text\nalone, is a valuable direction for NLP and particularly its applications to\nsocial science.\n","authors":["Alexander Hoyle","Rupak Sarkar","Pranav Goel","Philip Resnik"],"pdf_url":"https://arxiv.org/pdf/2305.14583v3.pdf","comment":"Accepted to EMNLP 2023 (Main conference)"},{"id":"http://arxiv.org/abs/2501.01702v2","updated":"2025-02-24T12:42:14Z","published":"2025-01-03T08:55:19Z","title":"AgentRefine: Enhancing Agent Generalization through Refinement Tuning","summary":"  Large Language Model (LLM) based agents have proved their ability to perform\ncomplex tasks like humans. However, there is still a large gap between\nopen-sourced LLMs and commercial models like the GPT series. In this paper, we\nfocus on improving the agent generalization capabilities of LLMs via\ninstruction tuning. We first observe that the existing agent training corpus\nexhibits satisfactory results on held-in evaluation sets but fails to\ngeneralize to held-out sets. These agent-tuning works face severe formatting\nerrors and are frequently stuck in the same mistake for a long while. We\nanalyze that the poor generalization ability comes from overfitting to several\nmanual agent environments and a lack of adaptation to new situations. They\nstruggle with the wrong action steps and can not learn from the experience but\njust memorize existing observation-action relations. Inspired by the insight,\nwe propose a novel AgentRefine framework for agent-tuning. The core idea is to\nenable the model to learn to correct its mistakes via observation in the\ntrajectory. Specifically, we propose an agent synthesis framework to encompass\na diverse array of environments and tasks and prompt a strong LLM to refine its\nerror action according to the environment feedback. AgentRefine significantly\noutperforms state-of-the-art agent-tuning work in terms of generalization\nability on diverse agent tasks. It also has better robustness facing\nperturbation and can generate diversified thought in inference. Our findings\nestablish the correlation between agent generalization and self-refinement and\nprovide a new paradigm for future research.\n","authors":["Dayuan Fu","Keqing He","Yejie Wang","Wentao Hong","Zhuoma Gongque","Weihao Zeng","Wei Wang","Jingang Wang","Xunliang Cai","Weiran Xu"],"pdf_url":"https://arxiv.org/pdf/2501.01702v2.pdf","comment":"ICLR 2025"},{"id":"http://arxiv.org/abs/2502.17091v1","updated":"2025-02-24T12:14:05Z","published":"2025-02-24T12:14:05Z","title":"WildFrame: Comparing Framing in Humans and LLMs on Naturally Occurring\n  Texts","summary":"  Humans are influenced by how information is presented, a phenomenon known as\nthe framing effect. Previous work has shown that LLMs may also be susceptible\nto framing but has done so on synthetic data and did not compare to human\nbehavior. We introduce WildFrame, a dataset for evaluating LLM responses to\npositive and negative framing, in naturally-occurring sentences, and compare\nhumans on the same data. WildFrame consists of 1,000 texts, first selecting\nreal-world statements with clear sentiment, then reframing them in either\npositive or negative light, and lastly, collecting human sentiment annotations.\nBy evaluating eight state-of-the-art LLMs on WildFrame, we find that all models\nexhibit framing effects similar to humans ($r\\geq0.57$), with both humans and\nmodels being more influenced by positive rather than negative reframing. Our\nfindings benefit model developers, who can either harness framing or mitigate\nits effects, depending on the downstream application.\n","authors":["Gili Lior","Liron Nacchace","Gabriel Stanovsky"],"pdf_url":"https://arxiv.org/pdf/2502.17091v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.12490v2","updated":"2025-02-24T12:05:30Z","published":"2025-02-18T03:19:48Z","title":"UniGenCoder: Merging Seq2Seq and Seq2Tree Paradigms for Unified Code\n  Generation","summary":"  Deep learning-based code generation has completely transformed the way\ndevelopers write programs today. Existing approaches to code generation have\nfocused either on the Sequence-to-Sequence paradigm, which generates target\ncode as a sequence of tokens, or the Sequence-to-Tree paradigm, which outputs\ncode as a sequence of actions. While these two paradigms are intuitively\ncomplementary, their combination has not been previously explored. By comparing\nthe code generated under these two paradigms, we find that integrating them\nholds significant potential. In this paper, we propose UniGenCoder for\ncode-related generation tasks, which consists of a shared encoder, a shared\ndecoder with a minimal set of additional parameters to unify two paradigms, and\na selector that dynamically chooses optimal paradigm for each instance. Also,\nduring the model training, we first perform the multi-task learning and\ndistillation strategies to facilitate knowledge transfer between two paradigms,\nand then leverage contrastive learning to train the selector. Experimental\nresults on the text-to-code and code-to-code generation tasks demonstrate the\neffectiveness of our proposed model. We release our code at\nhttps://github.com/DeepLearnXMU/UniGenCoder.\n","authors":["Liangying Shao","Yanfu Yan","Denys Poshyvanyk","Jinsong Su"],"pdf_url":"https://arxiv.org/pdf/2502.12490v2.pdf","comment":"accepted to 47th International Conference on Software Engineering\n  (ICSE 2025), NIER track"},{"id":"http://arxiv.org/abs/2502.17086v1","updated":"2025-02-24T12:05:27Z","published":"2025-02-24T12:05:27Z","title":"Automatically Evaluating the Paper Reviewing Capability of Large\n  Language Models","summary":"  Peer review is essential for scientific progress, but it faces challenges\nsuch as reviewer shortages and growing workloads. Although Large Language\nModels (LLMs) show potential for providing assistance, research has reported\nsignificant limitations in the reviews they generate. While the insights are\nvaluable, conducting the analysis is challenging due to the considerable time\nand effort required, especially given the rapid pace of LLM developments. To\naddress the challenge, we developed an automatic evaluation pipeline to assess\nthe LLMs' paper review capability by comparing them with expert-generated\nreviews. By constructing a dataset consisting of 676 OpenReview papers, we\nexamined the agreement between LLMs and experts in their strength and weakness\nidentifications. The results showed that LLMs lack balanced perspectives,\nsignificantly overlook novelty assessment when criticizing, and produce poor\nacceptance decisions. Our automated pipeline enables a scalable evaluation of\nLLMs' paper review capability over time.\n","authors":["Hyungyu Shin","Jingyu Tang","Yoonjoo Lee","Nayoung Kim","Hyunseung Lim","Ji Yong Cho","Hwajung Hong","Moontae Lee","Juho Kim"],"pdf_url":"https://arxiv.org/pdf/2502.17086v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.17071v1","updated":"2025-02-24T11:34:49Z","published":"2025-02-24T11:34:49Z","title":"Systematic Weight Evaluation for Pruning Large Language Models:\n  Enhancing Performance and Sustainability","summary":"  The exponential growth of large language models (LLMs) like ChatGPT has\nrevolutionized artificial intelligence, offering unprecedented capabilities in\nnatural language processing. However, the extensive computational resources\nrequired for training these models have significant environmental implications,\nincluding high carbon emissions, energy consumption, and water usage. This\nresearch presents a novel approach to LLM pruning, focusing on the systematic\nevaluation of individual weight importance throughout the training process. By\nmonitoring parameter evolution over time, we propose a method that effectively\nreduces model size without compromising performance. Extensive experiments with\nboth a scaled-down LLM and a large multimodal model reveal that moderate\npruning enhances efficiency and reduces loss, while excessive pruning\ndrastically deteriorates model performance. These findings highlight the\ncritical need for optimized AI models to ensure sustainable development,\nbalancing technological advancement with environmental responsibility.\n","authors":["Ashhadul Islam","Samir Brahim Belhaouari","Amine Bermak"],"pdf_url":"https://arxiv.org/pdf/2502.17071v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.15022v2","updated":"2025-02-24T11:28:32Z","published":"2025-02-20T20:16:34Z","title":"A Meta-Evaluation of Style and Attribute Transfer Metrics","summary":"  LLMs make it easy to rewrite text in any style, be it more polite,\npersuasive, or more positive. We present a large-scale study of evaluation\nmetrics for style and attribute transfer with a focus on content preservation;\nmeaning content not attributed to the style shift is preserved. The de facto\nevaluation approach uses lexical or semantic similarity metrics often between\nsource sentences and rewrites. While these metrics are not designed to\ndistinguish between style or content differences, empirical meta-evaluation\nshows a reasonable correlation to human judgment. In fact, recent works find\nthat LLMs prompted as evaluators are only comparable to semantic similarity\nmetrics, even though intuitively, the LLM approach should better fit the task.\nTo investigate this discrepancy, we benchmark 8 metrics for evaluating content\npreservation on existing datasets and additionally construct a new test set\nthat better aligns with the meta-evaluation aim. Indeed, we then find that the\nempirical conclusion aligns with the intuition: content preservation metrics\nfor style/attribute transfer must be conditional on the style shift. To support\nthis, we propose a new efficient zero-shot evaluation method using the\nlikelihood of the next token. We hope our meta-evaluation can foster more\nresearch on evaluating content preservation metrics, and also to ensure fair\nevaluation of methods for conducting style transfer.\n","authors":["Amalie Brogaard Pauli","Isabelle Augenstein","Ira Assent"],"pdf_url":"https://arxiv.org/pdf/2502.15022v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.05806v3","updated":"2025-02-24T11:02:47Z","published":"2024-09-09T17:11:51Z","title":"CKnowEdit: A New Chinese Knowledge Editing Dataset for Linguistics,\n  Facts, and Logic Error Correction in LLMs","summary":"  Chinese, as a linguistic system rich in depth and complexity, is\ncharacterized by distinctive elements such as ancient poetry, proverbs, idioms,\nand other cultural constructs. However, current Large Language Models (LLMs)\nface limitations in these specialized domains, highlighting the need for the\ndevelopment of comprehensive datasets that can assess, continuously update, and\nprogressively improve these culturally-grounded linguistic competencies through\ntargeted training optimizations. To address this gap, we introduce CKnowEdit,\nthe first-ever Chinese knowledge editing dataset designed to correct\nlinguistic, factual, and logical errors in LLMs. We collect seven types of\nknowledge from a wide range of sources, including classical texts, idioms, and\ncontent from Baidu Tieba Ruozhiba, taking into account the unique polyphony,\nantithesis, and logical structures inherent in the Chinese language. By\nanalyzing this dataset, we highlight the challenges current LLMs face in\nmastering Chinese. Furthermore, our evaluation of state-of-the-art knowledge\nediting techniques reveals opportunities to advance the correction of Chinese\nknowledge. Code and dataset are available at\nhttps://github.com/zjunlp/EasyEdit.\n","authors":["Jizhan Fang","Tianhe Lu","Yunzhi Yao","Ziyan Jiang","Xin Xu","Ningyu Zhang","Huajun Chen"],"pdf_url":"https://arxiv.org/pdf/2409.05806v3.pdf","comment":"Ongoing work; project website is available at\n  https://zjunlp.github.io/project/CKnowEdit code and dataset are available at\n  https://github.com/zjunlp/EasyEdit"},{"id":"http://arxiv.org/abs/2502.10250v2","updated":"2025-02-24T10:58:12Z","published":"2025-02-14T15:59:33Z","title":"VisCon-100K: Leveraging Contextual Web Data for Fine-tuning Vision\n  Language Models","summary":"  Vision-language models (VLMs) excel in various visual benchmarks but are\noften constrained by the lack of high-quality visual fine-tuning data. To\naddress this challenge, we introduce VisCon-100K, a novel dataset derived from\ninterleaved image-text web documents. Our approach transforms 45K web documents\nfrom the OBELICS dataset into 100K image conversation samples. We utilize\nGPT-4V to generate image-contextual captions and OpenChat 3.5 model to convert\nthese captions into diverse free-form and multiple-choice question-answer\npairs. Integrating this dataset for fine-tuning considerably enhances VLM\nperformance across multiple benchmarks. Unlike methods that focus solely on\nfine-grained visual content, our approach leverages accompanying web context,\nyielding superior results. We also discover that a 'leaky modality mix', where\nconversation samples contain questions answerable from both the image and its\ncontextual caption, outperforms non-leaky combinations of captions and Q&A\npairs. VisCon-100k dataset shows strong performance with two popular VLM\napproaches: text-only large language model (LLM) aligned with a vision encoder\nusing image captions data (ShareGPT4V-7b) and multimodally pretrained LLM\n(IDEFICS2-8b) using interleaved image-text data. In addition to releasing the\nVisCon-100K dataset, we provide a contextual captioner trained on this dataset,\nfacilitating scalable fine-tuning data generation for future research and\nopen-source applications. Using the same pipeline, but substituting our trained\ncontextual captioner for GPT-4V, we also release the larger VisCon-1M dataset.\n","authors":["Gokul Karthik Kumar","Iheb Chaabane","Kebin Wu"],"pdf_url":"https://arxiv.org/pdf/2502.10250v2.pdf","comment":"Accepted at PAKDD 2025"},{"id":"http://arxiv.org/abs/2502.17041v1","updated":"2025-02-24T10:49:34Z","published":"2025-02-24T10:49:34Z","title":"PrivaCI-Bench: Evaluating Privacy with Contextual Integrity and Legal\n  Compliance","summary":"  Recent advancements in generative large language models (LLMs) have enabled\nwider applicability, accessibility, and flexibility. However, their reliability\nand trustworthiness are still in doubt, especially for concerns regarding\nindividuals' data privacy. Great efforts have been made on privacy by building\nvarious evaluation benchmarks to study LLMs' privacy awareness and robustness\nfrom their generated outputs to their hidden representations. Unfortunately,\nmost of these works adopt a narrow formulation of privacy and only investigate\npersonally identifiable information (PII). In this paper, we follow the merit\nof the Contextual Integrity (CI) theory, which posits that privacy evaluation\nshould not only cover the transmitted attributes but also encompass the whole\nrelevant social context through private information flows. We present\nPrivaCI-Bench, a comprehensive contextual privacy evaluation benchmark targeted\nat legal compliance to cover well-annotated privacy and safety regulations,\nreal court cases, privacy policies, and synthetic data built from the official\ntoolkit to study LLMs' privacy and safety compliance. We evaluate the latest\nLLMs, including the recent reasoner models QwQ-32B and Deepseek R1. Our\nexperimental results suggest that though LLMs can effectively capture key CI\nparameters inside a given context, they still require further advancements for\nprivacy compliance.\n","authors":["Haoran Li","Wenbin Hu","Huihao Jing","Yulin Chen","Qi Hu","Sirui Han","Tianshu Chu","Peizhao Hu","Yangqiu Song"],"pdf_url":"https://arxiv.org/pdf/2502.17041v1.pdf","comment":"Project Webpage: https://hkust-knowcomp.github.io/privacy/"},{"id":"http://arxiv.org/abs/2502.17036v1","updated":"2025-02-24T10:37:13Z","published":"2025-02-24T10:37:13Z","title":"Language Model Re-rankers are Steered by Lexical Similarities","summary":"  Language model (LM) re-rankers are used to refine retrieval results for\nretrieval-augmented generation (RAG). They are more expensive than lexical\nmatching methods like BM25 but assumed to better process semantic information.\nTo understand whether LM re-rankers always live up to this assumption, we\nevaluate 6 different LM re-rankers on the NQ, LitQA2 and DRUID datasets. Our\nresults show that LM re-rankers struggle to outperform a simple BM25 re-ranker\non DRUID. Leveraging a novel separation metric based on BM25 scores, we explain\nand identify re-ranker errors stemming from lexical dissimilarities. We also\ninvestigate different methods to improve LM re-ranker performance and find\nthese methods mainly useful for NQ. Taken together, our work identifies and\nexplains weaknesses of LM re-rankers and points to the need for more\nadversarial and realistic datasets for their evaluation.\n","authors":["Lovisa Hagström","Ercong Nie","Ruben Halifa","Helmut Schmid","Richard Johansson","Alexander Junge"],"pdf_url":"https://arxiv.org/pdf/2502.17036v1.pdf","comment":"16 pages"},{"id":"http://arxiv.org/abs/2502.17026v1","updated":"2025-02-24T10:28:21Z","published":"2025-02-24T10:28:21Z","title":"Understanding the Uncertainty of LLM Explanations: A Perspective Based\n  on Reasoning Topology","summary":"  Understanding the uncertainty in large language model (LLM) explanations is\nimportant for evaluating their faithfulness and reasoning consistency, and thus\nprovides insights into the reliability of LLM's output regarding a question. In\nthis work, we propose a novel framework that quantifies uncertainty in LLM\nexplanations through a reasoning topology perspective. By designing a\nstructural elicitation strategy, we guide the LLMs to frame the explanations of\nan answer into a graph topology. This process decomposes the explanations into\nthe knowledge related sub-questions and topology-based reasoning structures,\nwhich allows us to quantify uncertainty not only at the semantic level but also\nfrom the reasoning path. It further brings convenience to assess knowledge\nredundancy and provide interpretable insights into the reasoning process. Our\nmethod offers a systematic way to interpret the LLM reasoning, analyze\nlimitations, and provide guidance for enhancing robustness and faithfulness.\nThis work pioneers the use of graph-structured uncertainty measurement in LLM\nexplanations and demonstrates the potential of topology-based quantification.\n","authors":["Longchao Da","Xiaoou Liu","Jiaxin Dai","Lu Cheng","Yaqing Wang","Hua Wei"],"pdf_url":"https://arxiv.org/pdf/2502.17026v1.pdf","comment":"15 pages, 6 figures"},{"id":"http://arxiv.org/abs/2502.17024v1","updated":"2025-02-24T10:26:29Z","published":"2025-02-24T10:26:29Z","title":"Towards Auto-Regressive Next-Token Prediction: In-Context Learning\n  Emerges from Generalization","summary":"  Large language models (LLMs) have demonstrated remarkable in-context learning\n(ICL) abilities. However, existing theoretical analysis of ICL primarily\nexhibits two limitations: (a) Limited i.i.d. Setting. Most studies focus on\nsupervised function learning tasks where prompts are constructed with i.i.d.\ninput-label pairs. This i.i.d. assumption diverges significantly from real\nlanguage learning scenarios where prompt tokens are interdependent. (b) Lack of\nEmergence Explanation. Most literature answers what ICL does from an implicit\noptimization perspective but falls short in elucidating how ICL emerges and the\nimpact of pre-training phase on ICL. In our paper, to extend (a), we adopt a\nmore practical paradigm, auto-regressive next-token prediction (AR-NTP), which\nclosely aligns with the actual training of language models. Specifically,\nwithin AR-NTP, we emphasize prompt token-dependency, which involves predicting\neach subsequent token based on the preceding sequence. To address (b), we\nformalize a systematic pre-training and ICL framework, highlighting the\nlayer-wise structure of sequences and topics, alongside a two-level\nexpectation. In conclusion, we present data-dependent, topic-dependent and\noptimization-dependent PAC-Bayesian generalization bounds for pre-trained LLMs,\ninvestigating that ICL emerges from the generalization of sequences and topics.\nOur theory is supported by experiments on numerical linear dynamic systems,\nsynthetic GINC and real-world language datasets.\n","authors":["Zixuan Gong","Xiaolin Hu","Huayi Tang","Yong Liu"],"pdf_url":"https://arxiv.org/pdf/2502.17024v1.pdf","comment":"Published at ICLR 2025"},{"id":"http://arxiv.org/abs/2502.10248v3","updated":"2025-02-24T10:12:11Z","published":"2025-02-14T15:58:10Z","title":"Step-Video-T2V Technical Report: The Practice, Challenges, and Future of\n  Video Foundation Model","summary":"  We present Step-Video-T2V, a state-of-the-art text-to-video pre-trained model\nwith 30B parameters and the ability to generate videos up to 204 frames in\nlength. A deep compression Variational Autoencoder, Video-VAE, is designed for\nvideo generation tasks, achieving 16x16 spatial and 8x temporal compression\nratios, while maintaining exceptional video reconstruction quality. User\nprompts are encoded using two bilingual text encoders to handle both English\nand Chinese. A DiT with 3D full attention is trained using Flow Matching and is\nemployed to denoise input noise into latent frames. A video-based DPO approach,\nVideo-DPO, is applied to reduce artifacts and improve the visual quality of the\ngenerated videos. We also detail our training strategies and share key\nobservations and insights. Step-Video-T2V's performance is evaluated on a novel\nvideo generation benchmark, Step-Video-T2V-Eval, demonstrating its\nstate-of-the-art text-to-video quality when compared with both open-source and\ncommercial engines. Additionally, we discuss the limitations of current\ndiffusion-based model paradigm and outline future directions for video\nfoundation models. We make both Step-Video-T2V and Step-Video-T2V-Eval\navailable at https://github.com/stepfun-ai/Step-Video-T2V. The online version\ncan be accessed from https://yuewen.cn/videos as well. Our goal is to\naccelerate the innovation of video foundation models and empower video content\ncreators.\n","authors":["Guoqing Ma","Haoyang Huang","Kun Yan","Liangyu Chen","Nan Duan","Shengming Yin","Changyi Wan","Ranchen Ming","Xiaoniu Song","Xing Chen","Yu Zhou","Deshan Sun","Deyu Zhou","Jian Zhou","Kaijun Tan","Kang An","Mei Chen","Wei Ji","Qiling Wu","Wen Sun","Xin Han","Yanan Wei","Zheng Ge","Aojie Li","Bin Wang","Bizhu Huang","Bo Wang","Brian Li","Changxing Miao","Chen Xu","Chenfei Wu","Chenguang Yu","Dapeng Shi","Dingyuan Hu","Enle Liu","Gang Yu","Ge Yang","Guanzhe Huang","Gulin Yan","Haiyang Feng","Hao Nie","Haonan Jia","Hanpeng Hu","Hanqi Chen","Haolong Yan","Heng Wang","Hongcheng Guo","Huilin Xiong","Huixin Xiong","Jiahao Gong","Jianchang Wu","Jiaoren Wu","Jie Wu","Jie Yang","Jiashuai Liu","Jiashuo Li","Jingyang Zhang","Junjing Guo","Junzhe Lin","Kaixiang Li","Lei Liu","Lei Xia","Liang Zhao","Liguo Tan","Liwen Huang","Liying Shi","Ming Li","Mingliang Li","Muhua Cheng","Na Wang","Qiaohui Chen","Qinglin He","Qiuyan Liang","Quan Sun","Ran Sun","Rui Wang","Shaoliang Pang","Shiliang Yang","Sitong Liu","Siqi Liu","Shuli Gao","Tiancheng Cao","Tianyu Wang","Weipeng Ming","Wenqing He","Xu Zhao","Xuelin Zhang","Xianfang Zeng","Xiaojia Liu","Xuan Yang","Yaqi Dai","Yanbo Yu","Yang Li","Yineng Deng","Yingming Wang","Yilei Wang","Yuanwei Lu","Yu Chen","Yu Luo","Yuchu Luo","Yuhe Yin","Yuheng Feng","Yuxiang Yang","Zecheng Tang","Zekai Zhang","Zidong Yang","Binxing Jiao","Jiansheng Chen","Jing Li","Shuchang Zhou","Xiangyu Zhang","Xinhao Zhang","Yibo Zhu","Heung-Yeung Shum","Daxin Jiang"],"pdf_url":"https://arxiv.org/pdf/2502.10248v3.pdf","comment":"36 pages, 14 figures"},{"id":"http://arxiv.org/abs/2502.17017v1","updated":"2025-02-24T10:02:50Z","published":"2025-02-24T10:02:50Z","title":"Quantifying Logical Consistency in Transformers via Query-Key Alignment","summary":"  Large language models (LLMs) have demonstrated impressive performance in\nvarious natural language processing tasks, yet their ability to perform\nmulti-step logical reasoning remains an open challenge. Although\nChain-of-Thought prompting has improved logical reasoning by enabling models to\ngenerate intermediate steps, it lacks mechanisms to assess the coherence of\nthese logical transitions. In this paper, we propose a novel, lightweight\nevaluation strategy for logical reasoning that uses query-key alignments inside\ntransformer attention heads. By computing a single forward pass and extracting\na \"QK-score\" from carefully chosen heads, our method reveals latent\nrepresentations that reliably separate valid from invalid inferences, offering\na scalable alternative to traditional ablation-based techniques. We also\nprovide an empirical validation on multiple logical reasoning benchmarks,\ndemonstrating improved robustness of our evaluation method against distractors\nand increased reasoning depth. The experiments were conducted on a diverse set\nof models, ranging from 1.5B to 70B parameters.\n","authors":["Eduard Tulchinskii","Anastasia Voznyuk","Laida Kushnareva","Andrei Andriiainen","Irina Piontkovskaya","Evgeny Burnaev","Serguei Barannikov"],"pdf_url":"https://arxiv.org/pdf/2502.17017v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.19799v4","updated":"2025-02-24T09:50:00Z","published":"2024-05-30T08:10:50Z","title":"Unsupervised Mutual Learning of Discourse Parsing and Topic Segmentation\n  in Dialogue","summary":"  In dialogue systems, discourse plays a crucial role in managing\nconversational focus and coordinating interactions. It consists of two key\nstructures: rhetorical structure and topic structure. The former captures the\nlogical flow of conversations, while the latter detects transitions between\ntopics. Together, they improve the ability of a dialogue system to track\nconversation dynamics and generate contextually relevant high-quality\nresponses. These structures are typically identified through discourse parsing\nand topic segmentation, respectively. However, existing supervised methods rely\non costly manual annotations, while unsupervised methods often focus on a\nsingle task, overlooking the deep linguistic interplay between rhetorical and\ntopic structures. To address these issues, we first introduce a unified\nrepresentation that integrates rhetorical and topic structures, ensuring\nsemantic consistency between them. Under the unified representation, we further\npropose two linguistically grounded hypotheses based on discourse theories: (1)\nLocal Discourse Coupling, where rhetorical cues dynamically enhance topic-aware\ninformation flow, and (2) Global Topology Constraint, where topic structure\npatterns probabilistically constrain rhetorical relation distributions.\nBuilding on the unified representation and two hypotheses, we propose an\nunsupervised mutual learning framework (UMLF) that jointly models rhetorical\nand topic structures, allowing them to mutually reinforce each other without\nrequiring additional annotations. We evaluate our approach on two rhetorical\ndatasets and three topic segmentation datasets. Experimental results\ndemonstrate that our method surpasses all strong baselines built on pre-trained\nlanguage models. Furthermore, when applied to LLMs, our framework achieves\nnotable improvements, demonstrating its effectiveness in improving discourse\nstructure modeling.\n","authors":["Jiahui Xu","Feng Jiang","Anningzhe Gao","Luis Fernando D'Haro","Haizhou Li"],"pdf_url":"https://arxiv.org/pdf/2405.19799v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.17011v1","updated":"2025-02-24T09:46:37Z","published":"2025-02-24T09:46:37Z","title":"Predicting Liquidity-Aware Bond Yields using Causal GANs and Deep\n  Reinforcement Learning with LLM Evaluation","summary":"  Financial bond yield forecasting is challenging due to data scarcity,\nnonlinear macroeconomic dependencies, and evolving market conditions. In this\npaper, we propose a novel framework that leverages Causal Generative\nAdversarial Networks (CausalGANs) and Soft Actor-Critic (SAC) reinforcement\nlearning (RL) to generate high-fidelity synthetic bond yield data for four\nmajor bond categories (AAA, BAA, US10Y, Junk). By incorporating 12 key\nmacroeconomic variables, we ensure statistical fidelity by preserving essential\nmarket properties. To transform this market dependent synthetic data into\nactionable insights, we employ a finetuned Large Language Model (LLM)\nQwen2.5-7B that generates trading signals (BUY/HOLD/SELL), risk assessments,\nand volatility projections. We use automated, human and LLM evaluations, all of\nwhich demonstrate that our framework improves forecasting performance over\nexisting methods, with statistical validation via predictive accuracy, MAE\nevaluation(0.103%), profit/loss evaluation (60% profit rate), LLM evaluation\n(3.37/5) and expert assessments scoring 4.67 out of 5. The reinforcement\nlearning-enhanced synthetic data generation achieves the least Mean Absolute\nError of 0.103, demonstrating its effectiveness in replicating real-world bond\nmarket dynamics. We not only enhance data-driven trading strategies but also\nprovides a scalable, high-fidelity synthetic financial data pipeline for risk &\nvolatility management and investment decision-making. This work establishes a\nbridge between synthetic data generation, LLM driven financial forecasting, and\nlanguage model evaluation, contributing to AI-driven financial decision-making.\n","authors":["Jaskaran Singh Walia","Aarush Sinha","Srinitish Srinivasan","Srihari Unnikrishnan"],"pdf_url":"https://arxiv.org/pdf/2502.17011v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.16994v1","updated":"2025-02-24T09:28:35Z","published":"2025-02-24T09:28:35Z","title":"FADE: Why Bad Descriptions Happen to Good Features","summary":"  Recent advances in mechanistic interpretability have highlighted the\npotential of automating interpretability pipelines in analyzing the latent\nrepresentations within LLMs. While they may enhance our understanding of\ninternal mechanisms, the field lacks standardized evaluation methods for\nassessing the validity of discovered features. We attempt to bridge this gap by\nintroducing FADE: Feature Alignment to Description Evaluation, a scalable\nmodel-agnostic framework for evaluating feature-description alignment. FADE\nevaluates alignment across four key metrics - Clarity, Responsiveness, Purity,\nand Faithfulness - and systematically quantifies the causes for the\nmisalignment of feature and their description. We apply FADE to analyze\nexisting open-source feature descriptions, and assess key components of\nautomated interpretability pipelines, aiming to enhance the quality of\ndescriptions. Our findings highlight fundamental challenges in generating\nfeature descriptions, particularly for SAEs as compared to MLP neurons,\nproviding insights into the limitations and future directions of automated\ninterpretability. We release FADE as an open-source package at:\nhttps://github.com/brunibrun/FADE.\n","authors":["Bruno Puri","Aakriti Jain","Elena Golimblevskaia","Patrick Kahardipraja","Thomas Wiegand","Wojciech Samek","Sebastian Lapuschkin"],"pdf_url":"https://arxiv.org/pdf/2502.16994v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.02310v2","updated":"2025-02-24T09:25:51Z","published":"2024-11-04T17:36:40Z","title":"MdEval: Massively Multilingual Code Debugging","summary":"  Code large language models (LLMs) have made significant progress in code\ndebugging by directly generating the correct code based on the buggy code\nsnippet. Programming benchmarks, typically consisting of buggy code snippet and\ntheir associated test cases, are used to assess the debugging capabilities of\nLLMs. However, many existing benchmarks primarily focus on Python and are often\nlimited in terms of language diversity (e.g., DebugBench and DebugEval). To\nadvance the field of multilingual debugging with LLMs, we propose the first\nmassively multilingual debugging benchmark, which includes 3.6K test samples of\n18 programming languages and covers the automated program repair (APR) task,\nthe code review (CR) task, and the bug identification (BI) task. Further, we\nintroduce the debugging instruction corpora MDEVAL-INSTRUCT by injecting bugs\ninto the correct multilingual queries and solutions (xDebugGen). Further, a\nmultilingual debugger xDebugCoder trained on MDEVAL-INSTRUCT as a strong\nbaseline specifically to handle the bugs of a wide range of programming\nlanguages (e.g. \"Missing Mut\" in language Rust and \"Misused Macro Definition\"\nin language C). Our extensive experiments on MDEVAL reveal a notable\nperformance gap between open-source models and closed-source LLMs (e.g., GPT\nand Claude series), highlighting huge room for improvement in multilingual code\ndebugging scenarios.\n","authors":["Shukai Liu","Linzheng Chai","Jian Yang","Jiajun Shi","He Zhu","Liran Wang","Ke Jin","Wei Zhang","Hualei Zhu","Shuyue Guo","Tao Sun","Jiaheng Liu","Yunlong Duan","Yu Hao","Liqun Yang","Guanglin Niu","Ge Zhang","Zhoujun Li"],"pdf_url":"https://arxiv.org/pdf/2411.02310v2.pdf","comment":"15 pages"},{"id":"http://arxiv.org/abs/2502.16989v1","updated":"2025-02-24T09:25:51Z","published":"2025-02-24T09:25:51Z","title":"All-in-one: Understanding and Generation in Multimodal Reasoning with\n  the MAIA Benchmark","summary":"  We introduce MAIA (Multimodal AI Assessment), a native-Italian benchmark\ndesigned for fine-grained investigation of the reasoning abilities of visual\nlanguage models on videos. MAIA differs from other available video benchmarks\nfor its design, its reasoning categories, the metric it uses and the language\nand culture of the videos. It evaluates Vision Language Models (VLMs) on two\naligned tasks: a visual statement verification task, and an open-ended visual\nquestion-answering task, both on the same set of video-related questions. It\nconsiders twelve reasoning categories that aim to disentangle language and\nvision relations by highlight when one of two alone encodes sufficient\ninformation to solve the tasks, when they are both needed and when the full\nrichness of the short video is essential instead of just a part of it. Thanks\nto its carefully taught design, it evaluates VLMs' consistency and visually\ngrounded natural language comprehension and generation simultaneously through\nan aggregated metric. Last but not least, the video collection has been\ncarefully selected to reflect the Italian culture and the language data are\nproduced by native-speakers.\n","authors":["Davide Testa","Giovanni Bonetta","Raffaella Bernardi","Alessandro Bondielli","Alessandro Lenci","Alessio Miaschi","Lucia Passaro","Bernardo Magnini"],"pdf_url":"https://arxiv.org/pdf/2502.16989v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.16987v1","updated":"2025-02-24T09:23:39Z","published":"2025-02-24T09:23:39Z","title":"Hotter and Colder: A New Approach to Annotating Sentiment, Emotions, and\n  Bias in Icelandic Blog Comments","summary":"  This paper presents Hotter and Colder, a dataset designed to analyze various\ntypes of online behavior in Icelandic blog comments. Building on previous work,\nwe used GPT-4o mini to annotate approximately 800,000 comments for 25 tasks,\nincluding sentiment analysis, emotion detection, hate speech, and group\ngeneralizations. Each comment was automatically labeled on a 5-point Likert\nscale. In a second annotation stage, comments with high or low probabilities of\ncontaining each examined behavior were subjected to manual revision. By\nleveraging crowdworkers to refine these automatically labeled comments, we\nensure the quality and accuracy of our dataset resulting in 12,232 uniquely\nannotated comments and 19,301 annotations. Hotter and Colder provides an\nessential resource for advancing research in content moderation and\nautomatically detectiong harmful online behaviors in Icelandic.\n","authors":["Steinunn Rut Friðriksdóttir","Dan Saattrup Nielsen","Hafsteinn Einarsson"],"pdf_url":"https://arxiv.org/pdf/2502.16987v1.pdf","comment":"To be published in the proceedings of the NoDaLiDa/Baltic-HLT 2025\n  conference"},{"id":"http://arxiv.org/abs/2409.04185v3","updated":"2025-02-24T09:18:36Z","published":"2024-09-06T11:01:55Z","title":"Residual Stream Analysis with Multi-Layer SAEs","summary":"  Sparse autoencoders (SAEs) are a promising approach to interpreting the\ninternal representations of transformer language models. However, SAEs are\nusually trained separately on each transformer layer, making it difficult to\nuse them to study how information flows across layers. To solve this problem,\nwe introduce the multi-layer SAE (MLSAE): a single SAE trained on the residual\nstream activation vectors from every transformer layer. Given that the residual\nstream is understood to preserve information across layers, we expected MLSAE\nlatents to 'switch on' at a token position and remain active at later layers.\nInterestingly, we find that individual latents are often active at a single\nlayer for a given token or prompt, but the layer at which an individual latent\nis active may differ for different tokens or prompts. We quantify these\nphenomena by defining a distribution over layers and considering its variance.\nWe find that the variance of the distributions of latent activations over\nlayers is about two orders of magnitude greater when aggregating over tokens\ncompared with a single token. For larger underlying models, the degree to which\nlatents are active at multiple layers increases, which is consistent with the\nfact that the residual stream activation vectors at adjacent layers become more\nsimilar. Finally, we relax the assumption that the residual stream basis is the\nsame at every layer by applying pre-trained tuned-lens transformations, but our\nfindings remain qualitatively similar. Our results represent a new approach to\nunderstanding how representations change as they flow through transformers. We\nrelease our code to train and analyze MLSAEs at\nhttps://github.com/tim-lawson/mlsae.\n","authors":["Tim Lawson","Lucy Farnik","Conor Houghton","Laurence Aitchison"],"pdf_url":"https://arxiv.org/pdf/2409.04185v3.pdf","comment":"ICLR 2025 Camera Ready. 45 pages, 41 figures"},{"id":"http://arxiv.org/abs/2410.01524v3","updated":"2025-02-24T09:14:13Z","published":"2024-10-02T13:12:13Z","title":"HarmAug: Effective Data Augmentation for Knowledge Distillation of\n  Safety Guard Models","summary":"  Safety guard models that detect malicious queries aimed at large language\nmodels (LLMs) are essential for ensuring the secure and responsible deployment\nof LLMs in real-world applications. However, deploying existing safety guard\nmodels with billions of parameters alongside LLMs on mobile devices is\nimpractical due to substantial memory requirements and latency. To reduce this\ncost, we distill a large teacher safety guard model into a smaller one using a\nlabeled dataset of instruction-response pairs with binary harmfulness labels.\nDue to the limited diversity of harmful instructions in the existing labeled\ndataset, naively distilled models tend to underperform compared to larger\nmodels. To bridge the gap between small and large models, we propose HarmAug, a\nsimple yet effective data augmentation method that involves jailbreaking an LLM\nand prompting it to generate harmful instructions. Given a prompt such as,\n\"Make a single harmful instruction prompt that would elicit offensive content\",\nwe add an affirmative prefix (e.g., \"I have an idea for a prompt:\") to the\nLLM's response. This encourages the LLM to continue generating the rest of the\nresponse, leading to sampling harmful instructions. Another LLM generates a\nresponse to the harmful instruction, and the teacher model labels the\ninstruction-response pair. We empirically show that our HarmAug outperforms\nother relevant baselines. Moreover, a 435-million-parameter safety guard model\ntrained with HarmAug achieves an F1 score comparable to larger models with over\n7 billion parameters, and even outperforms them in AUPRC, while operating at\nless than 25% of their computational cost.\n","authors":["Seanie Lee","Haebin Seong","Dong Bok Lee","Minki Kang","Xiaoyin Chen","Dominik Wagner","Yoshua Bengio","Juho Lee","Sung Ju Hwang"],"pdf_url":"https://arxiv.org/pdf/2410.01524v3.pdf","comment":"ICLR 2025"},{"id":"http://arxiv.org/abs/2502.16982v1","updated":"2025-02-24T09:12:29Z","published":"2025-02-24T09:12:29Z","title":"Muon is Scalable for LLM Training","summary":"  Recently, the Muon optimizer based on matrix orthogonalization has\ndemonstrated strong results in training small-scale language models, but the\nscalability to larger models has not been proven. We identify two crucial\ntechniques for scaling up Muon: (1) adding weight decay and (2) carefully\nadjusting the per-parameter update scale. These techniques allow Muon to work\nout-of-the-box on large-scale training without the need of hyper-parameter\ntuning. Scaling law experiments indicate that Muon achieves $\\sim\\!2\\times$\ncomputational efficiency compared to AdamW with compute optimal training.\n  Based on these improvements, we introduce Moonlight, a 3B/16B-parameter\nMixture-of-Expert (MoE) model trained with 5.7T tokens using Muon. Our model\nimproves the current Pareto frontier, achieving better performance with much\nfewer training FLOPs compared to prior models.\n  We open-source our distributed Muon implementation that is memory optimal and\ncommunication efficient. We also release the pretrained, instruction-tuned, and\nintermediate checkpoints to support future research.\n","authors":["Jingyuan Liu","Jianlin Su","Xingcheng Yao","Zhejun Jiang","Guokun Lai","Yulun Du","Yidao Qin","Weixin Xu","Enzhe Lu","Junjie Yan","Yanru Chen","Huabin Zheng","Yibo Liu","Shaowei Liu","Bohong Yin","Weiran He","Han Zhu","Yuzhi Wang","Jianzhou Wang","Mengnan Dong","Zheng Zhang","Yongsheng Kang","Hao Zhang","Xinran Xu","Yutao Zhang","Yuxin Wu","Xinyu Zhou","Zhilin Yang"],"pdf_url":"https://arxiv.org/pdf/2502.16982v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.06438v5","updated":"2025-02-24T09:11:03Z","published":"2024-11-10T11:49:36Z","title":"Conditional [MASK] Discrete Diffusion Language Model","summary":"  Although auto-regressive models excel in natural language processing, they\noften struggle to generate diverse text and provide limited controllability.\nNon-auto-regressive methods could be an alternative but often produce\ndegenerate outputs and exhibit shortcomings in conditional generation. To\naddress these challenges, we propose Diffusion-EAGS, a novel framework that\nintegrates conditional masked language models into diffusion language models\nthrough the theoretical lens of a conditional Markov Random Field. In doing so,\nwe propose entropy-adaptive Gibbs sampling and entropy-based noise scheduling\nto counterbalance each model's shortcomings. Experimental results show that\nDiffusion-EAGS outperforms baselines and achieves the best quality-diversity\ntradeoff, demonstrating its effectiveness in non-autoregressive text\ngeneration.\n","authors":["Hyukhun Koh","Minha Jhang","Dohyung Kim","Sangmook Lee","Kyomin Jung"],"pdf_url":"https://arxiv.org/pdf/2411.06438v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.10400v3","updated":"2025-02-24T08:57:10Z","published":"2024-12-05T16:10:42Z","title":"Reinforcement Learning Enhanced LLMs: A Survey","summary":"  Reinforcement learning (RL) enhanced large language models (LLMs),\nparticularly exemplified by DeepSeek-R1, have exhibited outstanding\nperformance. Despite the effectiveness in improving LLM capabilities, its\nimplementation remains highly complex, requiring complex algorithms, reward\nmodeling strategies, and optimization techniques. This complexity poses\nchallenges for researchers and practitioners in developing a systematic\nunderstanding of RL-enhanced LLMs. Moreover, the absence of a comprehensive\nsurvey summarizing existing research on RL-enhanced LLMs has limited progress\nin this domain, hindering further advancements.\n  In this work, we are going to make a systematic review of the most up-to-date\nstate of knowledge on RL-enhanced LLMs, attempting to consolidate and analyze\nthe rapidly growing research in this field, helping researchers understand the\ncurrent challenges and advancements. Specifically, we (1) detail the basics of\nRL; (2) introduce popular RL-enhanced LLMs; (3) review researches on two\nwidely-used reward model-based RL techniques: Reinforcement Learning from Human\nFeedback (RLHF) and Reinforcement Learning from AI Feedback (RLAIF); and (4)\nexplore Direct Preference Optimization (DPO), a set of methods that bypass the\nreward model to directly use human preference data for aligning LLM outputs\nwith human expectations. We will also point out current challenges and\ndeficiencies of existing methods and suggest some avenues for further\nimprovements. Project page of this work can be found at\nhttps://github.com/ShuheWang1998/Reinforcement-Learning-Enhanced-LLMs-A-Survey.\n","authors":["Shuhe Wang","Shengyu Zhang","Jie Zhang","Runyi Hu","Xiaoya Li","Tianwei Zhang","Jiwei Li","Fei Wu","Guoyin Wang","Eduard Hovy"],"pdf_url":"https://arxiv.org/pdf/2412.10400v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.16971v1","updated":"2025-02-24T08:54:39Z","published":"2025-02-24T08:54:39Z","title":"LongSafety: Evaluating Long-Context Safety of Large Language Models","summary":"  As Large Language Models (LLMs) continue to advance in understanding and\ngenerating long sequences, new safety concerns have been introduced through the\nlong context. However, the safety of LLMs in long-context tasks remains\nunder-explored, leaving a significant gap in both evaluation and improvement of\ntheir safety. To address this, we introduce LongSafety, the first comprehensive\nbenchmark specifically designed to evaluate LLM safety in open-ended\nlong-context tasks. LongSafety encompasses 7 categories of safety issues and 6\nuser-oriented long-context tasks, with a total of 1,543 test cases, averaging\n5,424 words per context. Our evaluation towards 16 representative LLMs reveals\nsignificant safety vulnerabilities, with most models achieving safety rates\nbelow 55%. Our findings also indicate that strong safety performance in\nshort-context scenarios does not necessarily correlate with safety in\nlong-context tasks, emphasizing the unique challenges and urgency of improving\nlong-context safety. Moreover, through extensive analysis, we identify\nchallenging safety issues and task types for long-context models. Furthermore,\nwe find that relevant context and extended input sequences can exacerbate\nsafety risks in long-context scenarios, highlighting the critical need for\nongoing attention to long-context safety challenges. Our code and data are\navailable at https://github.com/thu-coai/LongSafety.\n","authors":["Yida Lu","Jiale Cheng","Zhexin Zhang","Shiyao Cui","Cunxiang Wang","Xiaotao Gu","Yuxiao Dong","Jie Tang","Hongning Wang","Minlie Huang"],"pdf_url":"https://arxiv.org/pdf/2502.16971v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.11022v4","updated":"2025-02-24T08:46:07Z","published":"2024-09-17T09:32:12Z","title":"DynamicNER: A Dynamic, Multilingual, and Fine-Grained Dataset for\n  LLM-based Named Entity Recognition","summary":"  With the advancement of Large Language Models (LLMs), more and more\nresearchers apply LLMs for Named Entity Recognition (NER) methods, bringing\nvitality to this classical Natural Language Processing task. However, existing\ndatasets are designed for traditional machine learning methods, inadequate for\nLLM-based methods in terms of corpus selection, entity categorization, and\ndesign logic. This limitation leads to less effective evaluation and model\nfine-tuning. To address this issue, we propose DynamicNER, the first NER\ndataset specifically designed for LLMs and with dynamic categorization,\ntranscending the limitations of fixed categorization in existing datasets. It\nis also multi-lingual and multi-granular, covering 8 languages and 155 entity\ntypes, with corpus spanning multiple specialized domains. Furthermore, in\nresponse to the limitations demonstrated by existing LLM-based methods during\nDynamicNER testing, we develop CascadeNER, a novel NER method based on a\ntwo-stage strategy and lightweight LLMs, addressing the problems in current\nmethods. Experiments show that DynamicNER is an effective benchmark for\nLLM-based NER methods, and CascadeNER outperforms existing methods with fewer\ncomputational resources. Our work is opened at\nhttps://github.com/CascadeNER/CascadeNER.\n","authors":["Hanjun Luo","Yingbin Jin","Xinfeng Li","Xuecheng Liu","Ruizhe Chen","Tong Shang","Kun Wang","Qingsong Wen","Zuozhu Liu"],"pdf_url":"https://arxiv.org/pdf/2409.11022v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.10165v2","updated":"2025-02-24T08:42:25Z","published":"2024-10-14T05:18:02Z","title":"HSR-Enhanced Sparse Attention Acceleration","summary":"  Large Language Models (LLMs) have demonstrated remarkable capabilities across\nvarious applications, but their performance on long-context tasks is often\nlimited by the computational complexity of attention mechanisms. We introduce a\nnovel approach to accelerate attention computation in LLMs, particularly for\nlong-context scenarios. We leverage the inherent sparsity within attention\nmechanisms, both in conventional Softmax attention and ReLU attention (with\n$\\mathsf{ReLU}^\\alpha$ activation, $\\alpha \\in \\mathbb{N}_+$), to significantly\nreduce the running time complexity. Our method employs a Half-Space Reporting\n(HSR) data structure to identify non-zero or ``massively activated'' entries in\nthe attention matrix. We present theoretical analyses for two key scenarios:\ngeneration decoding and prompt prefilling. Our approach achieves a running time\nof $O(mn^{4/5})$ significantly faster than the naive approach $O(mn)$ for\ngeneration decoding, where $n$ is the context length, $m$ is the query length,\nand $d$ is the hidden dimension. We can also reduce the running time for prompt\nprefilling from $O(mn)$ to $O(mn^{1 - 1 / \\lfloor d/2\\rfloor} + mn^{4/5})$. Our\nmethod introduces only provably negligible error for Softmax attention. This\nwork represents a significant step towards enabling efficient long-context\nprocessing in LLMs.\n","authors":["Bo Chen","Yingyu Liang","Zhizhou Sha","Zhenmei Shi","Zhao Song"],"pdf_url":"https://arxiv.org/pdf/2410.10165v2.pdf","comment":"CPAL 2025"},{"id":"http://arxiv.org/abs/2502.16961v1","updated":"2025-02-24T08:38:21Z","published":"2025-02-24T08:38:21Z","title":"UrduLLaMA 1.0: Dataset Curation, Preprocessing, and Evaluation in\n  Low-Resource Settings","summary":"  Multilingual Large Language Models (LLMs) often provide suboptimal\nperformance on low-resource languages like Urdu. This paper introduces\nUrduLLaMA 1.0, a model derived from the open-source Llama-3.1-8B-Instruct\narchitecture and continually pre-trained on 128 million Urdu tokens, capturing\nthe rich diversity of the language. To enhance instruction-following and\ntranslation capabilities, we leverage Low-Rank Adaptation (LoRA) to fine tune\nthe model on 41,000 Urdu instructions and approximately 50,000 English-Urdu\ntranslation pairs. Evaluation across three machine translation datasets\ndemonstrates significant performance improvements compared to state-of-the-art\n(SOTA) models, establishing a new benchmark for Urdu LLMs. These findings\nunderscore the potential of targeted adaptation strategies with limited data\nand computational resources to address the unique challenges of low-resource\nlanguages.\n","authors":["Layba Fiaz","Munief Hassan Tahir","Sana Shams","Sarmad Hussain"],"pdf_url":"https://arxiv.org/pdf/2502.16961v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.09802v3","updated":"2025-02-24T08:33:46Z","published":"2023-11-16T11:26:21Z","title":"Neuro-Symbolic Integration Brings Causal and Reliable Reasoning Proofs","summary":"  Two lines of approaches are adopted for complex reasoning with LLMs. One line\nof work prompts LLMs with various reasoning structures, while the structural\noutputs can be naturally regarded as intermediate reasoning steps. Another line\nof work adopt LLM-free declarative solvers to do the reasoning task, rendering\nhigher reasoning accuracy but lacking interpretability due to the black-box\nnature of the solvers. Aiming to resolve the trade-off between answer accuracy\nand interpretability, we present a simple extension to the latter line of work.\nSpecifically, we showcase that the intermediate search logs generated by Prolog\ninterpreters can be accessed and interpreted into human-readable reasoning\nproofs. As long as LLMs correctly translate problem descriptions into Prolog\nrepresentations, the corresponding reasoning proofs are ensured to be causal\nand reliable. On two logical reasoning and one arithmetic reasoning datasets,\nour framework obtains significant improvements in terms of both answer accuracy\nand reasoning proof accuracy. Our code is released at\nhttps://github.com/DAMO-NLP-SG/CaRing\n","authors":["Sen Yang","Xin Li","Leyang Cui","Lidong Bing","Wai Lam"],"pdf_url":"https://arxiv.org/pdf/2311.09802v3.pdf","comment":"To appear in Findings of NAACL2025"},{"id":"http://arxiv.org/abs/2502.16949v1","updated":"2025-02-24T08:21:48Z","published":"2025-02-24T08:21:48Z","title":"SparseTransX: Efficient Training of Translation-Based Knowledge Graph\n  Embeddings Using Sparse Matrix Operations","summary":"  Knowledge graph (KG) learning offers a powerful framework for generating new\nknowledge and making inferences. Training KG embedding can take a significantly\nlong time, especially for larger datasets. Our analysis shows that the gradient\ncomputation of embedding is one of the dominant functions in the\ntranslation-based KG embedding training loop. We address this issue by\nreplacing the core embedding computation with SpMM (Sparse-Dense Matrix\nMultiplication) kernels. This allows us to unify multiple scatter (and gather)\noperations as a single operation, reducing training time and memory usage. We\ncreate a general framework for training KG models using sparse kernels and\nimplement four models, namely TransE, TransR, TransH, and TorusE. Our sparse\nimplementations exhibit up to 5.3x speedup on the CPU and up to 4.2x speedup on\nthe GPU with a significantly low GPU memory footprint. The speedups are\nconsistent across large and small datasets for a given model. Our proposed\nsparse approach can also be extended to accelerate other translation-based\n(such as TransC, TransM, etc.) and non-translational (such as DistMult,\nComplEx, RotatE, etc.) models as well.\n","authors":["Md Saidul Hoque Anik","Ariful Azad"],"pdf_url":"https://arxiv.org/pdf/2502.16949v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.16947v1","updated":"2025-02-24T08:17:54Z","published":"2025-02-24T08:17:54Z","title":"Using Machine Learning to Detect Fraudulent SMSs in Chichewa","summary":"  SMS enabled fraud is of great concern globally. Building classifiers based on\nmachine learning for SMS fraud requires the use of suitable datasets for model\ntraining and validation. Most research has centred on the use of datasets of\nSMSs in English. This paper introduces a first dataset for SMS fraud detection\nin Chichewa, a major language in Africa, and reports on experiments with\nmachine learning algorithms for classifying SMSs in Chichewa as fraud or\nnon-fraud. We answer the broader research question of how feasible it is to\ndevelop machine learning classification models for Chichewa SMSs. To do that,\nwe created three datasets. A small dataset of SMS in Chichewa was collected\nthrough primary research from a segment of the young population. We applied a\nlabel-preserving text transformations to increase its size. The enlarged\ndataset was translated into English using two approaches: human translation and\nmachine translation. The Chichewa and the translated datasets were subjected to\nmachine classification using random forest and logistic regression. Our\nfindings indicate that both models achieved a promising accuracy of over 96% on\nthe Chichewa dataset. There was a drop in performance when moving from the\nChichewa to the translated dataset. This highlights the importance of data\npreprocessing, especially in multilingual or cross-lingual NLP tasks, and shows\nthe challenges of relying on machine-translated text for training machine\nlearning models. Our results underscore the importance of developing language\nspecific models for SMS fraud detection to optimise accuracy and performance.\nSince most machine learning models require data preprocessing, it is essential\nto investigate the impact of the reliance on English-specific tools for data\npreprocessing.\n","authors":["Amelia Taylor","Amoss Robert"],"pdf_url":"https://arxiv.org/pdf/2502.16947v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.16942v1","updated":"2025-02-24T08:11:17Z","published":"2025-02-24T08:11:17Z","title":"NUTSHELL: A Dataset for Abstract Generation from Scientific Talks","summary":"  Scientific communication is receiving increasing attention in natural\nlanguage processing, especially to help researches access, summarize, and\ngenerate content. One emerging application in this area is Speech-to-Abstract\nGeneration (SAG), which aims to automatically generate abstracts from recorded\nscientific presentations. SAG enables researchers to efficiently engage with\nconference talks, but progress has been limited by a lack of large-scale\ndatasets. To address this gap, we introduce NUTSHELL, a novel multimodal\ndataset of *ACL conference talks paired with their corresponding abstracts. We\nestablish strong baselines for SAG and evaluate the quality of generated\nabstracts using both automatic metrics and human judgments. Our results\nhighlight the challenges of SAG and demonstrate the benefits of training on\nNUTSHELL. By releasing NUTSHELL under an open license (CC-BY 4.0), we aim to\nadvance research in SAG and foster the development of improved models and\nevaluation methods.\n","authors":["Maike Züfle","Sara Papi","Beatrice Savoldi","Marco Gaido","Luisa Bentivogli","Jan Niehues"],"pdf_url":"https://arxiv.org/pdf/2502.16942v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.16940v1","updated":"2025-02-24T08:08:41Z","published":"2025-02-24T08:08:41Z","title":"Reasoning Does Not Necessarily Improve Role-Playing Ability","summary":"  The application of role-playing large language models (LLMs) is rapidly\nexpanding in both academic and commercial domains, driving an increasing demand\nfor high-precision role-playing models. Simultaneously, the rapid advancement\nof reasoning techniques has continuously pushed the performance boundaries of\nLLMs. This intersection of practical role-playing demands and evolving\nreasoning capabilities raises an important research question: \"Can reasoning\ntechniques enhance the role-playing capabilities of LLMs?\" To address this, we\nconduct a comprehensive study using 6 role-playing benchmarks, 24 LLMs, and 3\ndistinct role-playing strategies, comparing the effectiveness of direct\nzero-shot role-playing, role-playing with Chain-of-Thought (CoT), and\nrole-playing using reasoning-optimized LLMs. Our findings reveal that CoT may\nreduce role-playing performance, reasoning-optimized LLMs are unsuitable for\nrole-playing, reasoning ability disrupts the role-playing scaling law, large\nmodels still lack proficiency in advanced role-playing, and Chinese\nrole-playing performance surpasses English role-playing performance.\nFurthermore, based on extensive experimental results, we propose two promising\nfuture research directions: Role-aware CoT for improving role-playing LLMs and\nReinforcement Learning for role-playing LLMs, aiming to enhance the\nadaptability, consistency, and effectiveness of role-playing LLMs for both\nresearch and real-world applications.\n","authors":["Xiachong Feng","Longxu Dou","Lingpeng Kong"],"pdf_url":"https://arxiv.org/pdf/2502.16940v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.10306v6","updated":"2025-02-24T07:59:10Z","published":"2024-04-16T06:27:39Z","title":"Balancing Speciality and Versatility: A Coarse to Fine Framework for\n  Mitigating Catastrophic Forgetting in Large Language Models","summary":"  Aligned Large Language Models (LLMs) showcase remarkable versatility, capable\nof handling diverse real-world tasks. Meanwhile, aligned LLMs are also expected\nto exhibit speciality, excelling in specific applications. However, fine-tuning\nwith extra data, a common practice to gain speciality, often leads to\ncatastrophic forgetting (CF) of previously acquired versatility, hindering the\nmodel's performance across diverse tasks. In response to this challenge, we\npropose CoFiTune, a coarse to fine framework in an attempt to strike the\nbalance between speciality and versatility. At the coarse-grained level, an\nempirical tree-search algorithm is utilized to pinpoint and update specific\nmodules that are crucial for speciality, while keeping other parameters frozen;\nat the fine-grained level, a soft-masking mechanism regulates the update to the\nLLMs, mitigating the CF issue without harming speciality. In an overall\nevaluation of both speciality and versatility, CoFiTune consistently\noutperforms baseline methods across diverse tasks and model scales. Compared to\nthe full-parameter SFT, CoFiTune leads to about 14% versatility improvement and\nmarginal speciality loss on a 13B model. Lastly, based on further analysis, we\nprovide a speculative insight into the information forwarding process in LLMs,\nwhich helps explain the effectiveness of the proposed method. The code is\navailable at https://github.com/rattlesnakey/CoFiTune.\n","authors":["Hengyuan Zhang","Yanru Wu","Dawei Li","Sak Yang","Rui Zhao","Yong Jiang","Fei Tan"],"pdf_url":"https://arxiv.org/pdf/2404.10306v6.pdf","comment":"43 pages, 10 figures, accepted by ACL 2024"},{"id":"http://arxiv.org/abs/2406.14115v2","updated":"2025-02-24T07:59:00Z","published":"2024-06-20T08:58:58Z","title":"Take the essence and discard the dross: A Rethinking on Data Selection\n  for Fine-Tuning Large Language Models","summary":"  Data selection for fine-tuning large language models (LLMs) aims to choose a\nhigh-quality subset from existing datasets, allowing the trained model to\noutperform baselines trained on the full dataset. However, the expanding body\nof research lacks a clear, unified framework, and the variability in\nexperimental settings complicates systematic comparisons. While existing\nsurveys comprehensively overview the stages and methods of data selection, they\noften overlook an in-depth exploration of the fine-tuning phase. In this paper,\nwe conduct a focused review of recent data selection techniques for fine-tuning\nLLMs, analyzing a dozen key studies. We introduce a novel three-stage scheme -\ncomprising feature extraction, criteria design, and selector evaluation - to\nsystematically categorize and evaluate these methods. Additionally, we propose\na unified comparison approach that incorporates ratio-based efficiency and\nranking-based feasibility metrics to address inconsistencies across\nexperiments. Our findings reveal that methods emphasizing more targeted quality\nmeasurement achieve higher efficiency but at the cost of feasibility. Finally,\nwe discuss trends and highlight four key challenges in fine-tuning data\nselection, offering potential directions for future research.\n","authors":["Ziche Liu","Rui Ke","Yajiao Liu","Feng Jiang","Haizhou Li"],"pdf_url":"https://arxiv.org/pdf/2406.14115v2.pdf","comment":"Accepted by the NAACL 2025 main conference"},{"id":"http://arxiv.org/abs/2412.16378v3","updated":"2025-02-24T07:53:07Z","published":"2024-12-20T22:25:23Z","title":"REFA: Reference Free Alignment for multi-preference optimization","summary":"  We introduce $\\textbf{REFA}$, a family of reference-free alignment methods\nthat optimize over multiple user preferences while enforcing fine-grained\nlength control. Our approach integrates deviation-based weighting to emphasize\nhigh-quality responses, length normalization to prevent trivial short-response\nsolutions, and an EOS-probability regularizer to mitigate dataset-induced\nbrevity biases. Theoretically, we show that under the Uncertainty Reduction\nwith Sequence Length Assertion (URSLA) framework, naive length normalization\ncan still incentivize length-based shortcuts. In contrast, REFA corrects these\nsubtle incentives, guiding models toward genuinely more informative and\nhigher-quality outputs. Empirically, REFA achieves a new\n$\\textbf{state-of-the-art}$ among reference-free alignment methods, generating\nricher responses that align more closely with human preferences. Notably, REFA\nimproves performance on the AlpacaEval2 benchmark, achieving a $\\textbf{26.6%}$\nLength-Controlled Win Rate (LC-WR) and $\\textbf{24.2%}$ Win Rate (WR).\n","authors":["Taneesh Gupta","Rahul Madhavan","Xuchao Zhang","Chetan Bansal","Saravan Rajmohan"],"pdf_url":"https://arxiv.org/pdf/2412.16378v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.21271v3","updated":"2025-02-24T07:44:53Z","published":"2024-10-28T17:59:03Z","title":"EoRA: Training-free Compensation for Compressed LLM with Eigenspace\n  Low-Rank Approximation","summary":"  In this work, we re-formulate the model compression problem into the\ncustomized compensation problem: Given a compressed model, we aim to introduce\nresidual low-rank paths to compensate for compression errors under customized\nrequirements from users (e.g., tasks, compression ratios), resulting in greater\nflexibility in balancing accuracy and overhead(inference and model size)\nwithout being bound to fixed compression formats. However, naively applying SVD\nto derive residual paths causes suboptimal utilization of the low-rank\nrepresentation capacity. Instead, we propose Training-free Eigenspace Low-Rank\nApproximation (EoRA), a method that directly minimizes compression-induced\nerrors without requiring gradient-based training, achieving fast optimization\nin minutes using a small amount of calibration data. EoRA projects compression\nerrors into the eigenspace of input activations, leveraging eigenvalues to\neffectively prioritize the reconstruction of high-importance error components.\nMoreover, EoRA can be seamlessly integrated with fine-tuning and quantization\nto further improve effectiveness and efficiency. EoRA consistently outperforms\nprevious methods in compensating errors for compressed LLaMA2/3 models on\nvarious tasks, such as language generation, commonsense reasoning, and math\nreasoning tasks (e.g., 31.31%/12.88% and 9.69% improvements on\nARC-Easy/ARC-Challenge and MathQA when compensating LLaMA3-8B that is quantized\nto 4-bit and pruned to 2:4 sparsity). EoRA offers a scalable, training-free\nsolution to compensate for compression errors, making it a powerful tool to\ndeploy LLMs more flexibly. Code is available at https://github.com/NVlabs/EoRA.\n","authors":["Shih-Yang Liu","Maksim Khadkevich","Nai Chit Fung","Charbel Sakr","Chao-Han Huck Yang","Chien-Yi Wang","Saurav Muralidharan","Hongxu Yin","Kwang-Ting Cheng","Jan Kautz","Yu-Chiang Frank Wang","Pavlo Molchanov","Min-Hung Chen"],"pdf_url":"https://arxiv.org/pdf/2410.21271v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.16319v2","updated":"2025-02-24T07:41:18Z","published":"2024-02-26T05:51:47Z","title":"Data-free Weight Compress and Denoise for Large Language Models","summary":"  Large Language Models (LLMs) are reshaping the research landscape in\nartificial intelligence, particularly as model parameters scale up\nsignificantly, unlocking remarkable capabilities across various domains.\nNevertheless, the scalability of model parameters faces constraints due to\nlimitations in GPU memory and computational speed. To address these\nconstraints, various weight compression methods have emerged, such as Pruning\nand Quantization. Given the low-rank nature of weight matrices in language\nmodels, the reduction of weights through matrix decomposition undoubtedly holds\nsignificant potential and promise. In this paper, drawing upon the intrinsic\nstructure of LLMs, we propose a novel approach termed Data-free Joint Rank-k\nApproximation for compressing the parameter matrices. Significantly, our method\nis characterized by without necessitating additional involvement of any corpus,\nwhile simultaneously preserving orthogonality in conjunction with pruning and\nquantization methods. We achieve a model pruning of 80% parameters while\nretaining 93.43% of the original performance without any calibration data.\nAdditionally, we explore the fundamental properties of the weight matrix of\nLLMs undergone Rank-k Approximation and conduct comprehensive experiments to\nelucidate our hypothesis.\n","authors":["Runyu Peng","Yunhua Zhou","Qipeng Guo","Yang Gao","Hang Yan","Xipeng Qiu","Dahua Lin"],"pdf_url":"https://arxiv.org/pdf/2402.16319v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.04686v4","updated":"2025-02-24T07:32:58Z","published":"2025-01-08T18:49:41Z","title":"URSA: Understanding and Verifying Chain-of-thought Reasoning in\n  Multimodal Mathematics","summary":"  Chain-of-Thought (CoT) reasoning is widely used to enhance the mathematical\nreasoning capabilities of large language models (LLMs). The introduction of\nprocess supervision for CoT trajectories has sparked discussions on improving\ntest-time scaling, thereby unlocking the System 2-style thinking capabilities\nof these models. However, in multimodal mathematical reasoning, the scarcity of\nhigh-quality CoT training data has hindered existing models from achieving both\ndeliberate reasoning and fine-grained verification. In this work, we propose a\nnovel framework that introduces System 2-style thinking to multimodal\nmathematical reasoning. We introduce a three-module CoT data synthesis process\nthat integrates CoT distillation, trajectory-format rewriting, and format\nunification. This process generates MMathCoT-1M, a high-quality CoT reasoning\ninstruction fine-tuning dataset. Furthermore, we implement a dual-view\ntrajectory labeling automation that targets both visual grounding fidelity and\ndeductive chain validity, resulting in the DualMath-1.1M dataset. The URSA-8B\nmodel, trained on MMathCoT-1M, achieves new state-of-the-art (SOTA) performance\namong similarly sized multimodal LLMs on six popular reasoning benchmarks.\nTraining URSA-8B further on the DualMath-1.1M dataset yields URSA-RM-8B, a\nverifier that enhances URSA-8B's test-time performance and surpasses strong\nclosed-source multimodal MLLMs like GPT-4o. The model weights, training data,\nand code have been open-sourced: https://github.com/URSA-MATH/URSA-MATH.\n","authors":["Ruilin Luo","Zhuofan Zheng","Yifan Wang","Yiyao Yu","Xinzhe Ni","Zicheng Lin","Jin Zeng","Yujiu Yang"],"pdf_url":"https://arxiv.org/pdf/2501.04686v4.pdf","comment":"Fix typos and add results. 27 pages, 11 tables, 17 figures. Models,\n  training data and code have been open-sourced. Project url:\n  https://ursa-math.github.io"},{"id":"http://arxiv.org/abs/2502.16923v1","updated":"2025-02-24T07:29:13Z","published":"2025-02-24T07:29:13Z","title":"A Systematic Survey of Automatic Prompt Optimization Techniques","summary":"  Since the advent of large language models (LLMs), prompt engineering has been\na crucial step for eliciting desired responses for various Natural Language\nProcessing (NLP) tasks. However, prompt engineering remains an impediment for\nend users due to rapid advances in models, tasks, and associated best\npractices. To mitigate this, Automatic Prompt Optimization (APO) techniques\nhave recently emerged that use various automated techniques to help improve the\nperformance of LLMs on various tasks. In this paper, we present a comprehensive\nsurvey summarizing the current progress and remaining challenges in this field.\nWe provide a formal definition of APO, a 5-part unifying framework, and then\nproceed to rigorously categorize all relevant works based on their salient\nfeatures therein. We hope to spur further research guided by our framework.\n","authors":["Kiran Ramnath","Kang Zhou","Sheng Guan","Soumya Smruti Mishra","Xuan Qi","Zhengyuan Shen","Shuai Wang","Sangmin Woo","Sullam Jeoung","Yawei Wang","Haozhu Wang","Han Ding","Yuzhe Lu","Zhichao Xu","Yun Zhou","Balasubramaniam Srinivasan","Qiaojing Yan","Yueyan Chen","Haibo Ding","Panpan Xu","Lin Lee Cheong"],"pdf_url":"https://arxiv.org/pdf/2502.16923v1.pdf","comment":"8 main pages, 31 total pages, 1 figure"},{"id":"http://arxiv.org/abs/2502.16922v1","updated":"2025-02-24T07:27:54Z","published":"2025-02-24T07:27:54Z","title":"Benchmarking Temporal Reasoning and Alignment Across Chinese Dynasties","summary":"  Temporal reasoning is fundamental to human cognition and is crucial for\nvarious real-world applications. While recent advances in Large Language Models\nhave demonstrated promising capabilities in temporal reasoning, existing\nbenchmarks primarily rely on rule-based construction, lack contextual depth,\nand involve a limited range of temporal entities. To address these limitations,\nwe introduce Chinese Time Reasoning (CTM), a benchmark designed to evaluate\nLLMs on temporal reasoning within the extensive scope of Chinese dynastic\nchronology. CTM emphasizes cross-entity relationships, pairwise temporal\nalignment, and contextualized and culturally-grounded reasoning, providing a\ncomprehensive evaluation. Extensive experimental results reveal the challenges\nposed by CTM and highlight potential avenues for improvement.\n","authors":["Zhenglin Wang","Jialong Wu","Pengfei LI","Yong Jiang","Deyu Zhou"],"pdf_url":"https://arxiv.org/pdf/2502.16922v1.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2502.16920v1","updated":"2025-02-24T07:25:19Z","published":"2025-02-24T07:25:19Z","title":"SS-MPC: A Sequence-Structured Multi-Party Conversation System","summary":"  Recent Multi-Party Conversation (MPC) models typically rely on graph-based\napproaches to capture dialogue structures. However, these methods have\nlimitations, such as information loss during the projection of utterances into\nstructural embeddings and constraints in leveraging pre-trained language models\ndirectly. In this paper, we propose \\textbf{SS-MPC}, a response generation\nmodel for MPC that eliminates the need for explicit graph structures. Unlike\nexisting models that depend on graphs to analyze conversation structures,\nSS-MPC internally encodes the dialogue structure as a sequential input,\nenabling direct utilization of pre-trained language models. Experimental\nresults show that \\textbf{SS-MPC} achieves \\textbf{15.60\\% BLEU-1} and\n\\textbf{12.44\\% ROUGE-L} score, outperforming the current state-of-the-art MPC\nresponse generation model by \\textbf{3.91\\%p} in \\textbf{BLEU-1} and\n\\textbf{0.62\\%p} in \\textbf{ROUGE-L}. Additionally, human evaluation confirms\nthat SS-MPC generates more fluent and accurate responses compared to existing\nMPC models.\n","authors":["Yoonjin Jang","Keunha Kim","Youngjoong Ko"],"pdf_url":"https://arxiv.org/pdf/2502.16920v1.pdf","comment":"8 pages, 5 figures"},{"id":"http://arxiv.org/abs/2502.16919v1","updated":"2025-02-24T07:25:10Z","published":"2025-02-24T07:25:10Z","title":"Dependency Parsing with the Structuralized Prompt Template","summary":"  Dependency parsing is a fundamental task in natural language processing\n(NLP), aiming to identify syntactic dependencies and construct a syntactic tree\nfor a given sentence. Traditional dependency parsing models typically construct\nembeddings and utilize additional layers for prediction. We propose a novel\ndependency parsing method that relies solely on an encoder model with a\ntext-to-text training approach. To facilitate this, we introduce a structured\nprompt template that effectively captures the structural information of\ndependency trees. Our experimental results demonstrate that the proposed method\nachieves outstanding performance compared to traditional models, despite\nrelying solely on a pre-trained model. Furthermore, this method is highly\nadaptable to various pre-trained models across different target languages and\ntraining environments, allowing easy integration of task-specific features.\n","authors":["Keunha Kim","Youngjoong Ko"],"pdf_url":"https://arxiv.org/pdf/2502.16919v1.pdf","comment":"12pages, 5 figures"},{"id":"http://arxiv.org/abs/2502.16906v1","updated":"2025-02-24T07:02:31Z","published":"2025-02-24T07:02:31Z","title":"AutoLogi: Automated Generation of Logic Puzzles for Evaluating Reasoning\n  Abilities of Large Language Models","summary":"  While logical reasoning evaluation of Large Language Models (LLMs) has\nattracted significant attention, existing benchmarks predominantly rely on\nmultiple-choice formats that are vulnerable to random guessing, leading to\noverestimated performance and substantial performance fluctuations. To obtain\nmore accurate assessments of models' reasoning capabilities, we propose an\nautomated method for synthesizing open-ended logic puzzles, and use it to\ndevelop a bilingual benchmark, AutoLogi. Our approach features program-based\nverification and controllable difficulty levels, enabling more reliable\nevaluation that better distinguishes models' reasoning abilities. Extensive\nevaluation of eight modern LLMs shows that AutoLogi can better reflect true\nmodel capabilities, with performance scores spanning from 35% to 73% compared\nto the narrower range of 21% to 37% on the source multiple-choice dataset.\nBeyond benchmark creation, this synthesis method can generate high-quality\ntraining data by incorporating program verifiers into the rejection sampling\nprocess, enabling systematic enhancement of LLMs' reasoning capabilities across\ndiverse datasets.\n","authors":["Qin Zhu","Fei Huang","Runyu Peng","Keming Lu","Bowen Yu","Qinyuan Cheng","Xipeng Qiu","Xuanjing Huang","Junyang Lin"],"pdf_url":"https://arxiv.org/pdf/2502.16906v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.16903v1","updated":"2025-02-24T06:57:27Z","published":"2025-02-24T06:57:27Z","title":"GuidedBench: Equipping Jailbreak Evaluation with Guidelines","summary":"  Jailbreaking methods for large language models (LLMs) have gained increasing\nattention for building safe and responsible AI systems. After analyzing 35\njailbreak methods across six categories, we find that existing benchmarks,\nrelying on universal LLM-based or keyword-matching scores, lack case-specific\ncriteria, leading to conflicting results. In this paper, we introduce a more\nrobust evaluation framework for jailbreak methods, with a curated harmful\nquestion dataset, detailed case-by-case evaluation guidelines, and a scoring\nsystem equipped with these guidelines. Our experiments show that existing\njailbreak methods exhibit better discrimination when evaluated using our\nbenchmark. Some jailbreak methods that claim to achieve over 90% attack success\nrate (ASR) on other benchmarks only reach a maximum of 30.2% on our benchmark,\nproviding a higher ceiling for more advanced jailbreak research; furthermore,\nusing our scoring system reduces the variance of disagreements between\ndifferent evaluator LLMs by up to 76.33%. This demonstrates its ability to\nprovide more fair and stable evaluation.\n","authors":["Ruixuan Huang","Xunguang Wang","Zongjie Li","Daoyuan Wu","Shuai Wang"],"pdf_url":"https://arxiv.org/pdf/2502.16903v1.pdf","comment":"Homepage: https://sproutnan.github.io/AI-Safety_Benchmark/"},{"id":"http://arxiv.org/abs/2402.05374v5","updated":"2025-02-24T06:56:33Z","published":"2024-02-08T03:12:25Z","title":"CIC: A Framework for Culturally-Aware Image Captioning","summary":"  Image Captioning generates descriptive sentences from images using\nVision-Language Pre-trained models (VLPs) such as BLIP, which has improved\ngreatly. However, current methods lack the generation of detailed descriptive\ncaptions for the cultural elements depicted in the images, such as the\ntraditional clothing worn by people from Asian cultural groups. In this paper,\nwe propose a new framework, Culturally-aware Image Captioning (CIC), that\ngenerates captions and describes cultural elements extracted from cultural\nvisual elements in images representing cultures. Inspired by methods combining\nvisual modality and Large Language Models (LLMs) through appropriate prompts,\nour framework (1) generates questions based on cultural categories from images,\n(2) extracts cultural visual elements from Visual Question Answering (VQA)\nusing generated questions, and (3) generates culturally-aware captions using\nLLMs with the prompts. Our human evaluation conducted on 45 participants from 4\ndifferent cultural groups with a high understanding of the corresponding\nculture shows that our proposed framework generates more culturally descriptive\ncaptions when compared to the image captioning baseline based on VLPs.\nResources can be found at https://shane3606.github.io/cic..\n","authors":["Youngsik Yun","Jihie Kim"],"pdf_url":"https://arxiv.org/pdf/2402.05374v5.pdf","comment":"Accepted by IJCAI 2024"},{"id":"http://arxiv.org/abs/2411.00836v2","updated":"2025-02-24T06:55:22Z","published":"2024-10-29T17:29:19Z","title":"DynaMath: A Dynamic Visual Benchmark for Evaluating Mathematical\n  Reasoning Robustness of Vision Language Models","summary":"  The rapid advancements in Vision-Language Models (VLMs) have shown great\npotential in tackling mathematical reasoning tasks that involve visual context.\nUnlike humans who can reliably apply solution steps to similar problems with\nminor modifications, we found that SOTA VLMs like GPT-4o can consistently fail\nin these scenarios, revealing limitations in their mathematical reasoning\ncapabilities. In this paper, we investigate the mathematical reasoning\nrobustness in VLMs and evaluate how well these models perform under different\nvariants of the same question, such as changes in visual numerical values or\nfunction graphs. While several vision-based math benchmarks have been developed\nto assess VLMs' problem-solving capabilities, these benchmarks contain only\nstatic sets of problems and cannot easily evaluate mathematical reasoning\nrobustness. To fill this gap, we introduce DynaMath, a dynamic visual math\nbenchmark designed for in-depth assessment of VLMs. DynaMath includes 501\nhigh-quality, multi-topic seed questions, each represented as a Python program.\nThose programs are carefully designed and annotated to enable the automatic\ngeneration of a much larger set of concrete questions, including many different\ntypes of visual and textual variations. DynaMath allows us to evaluate the\ngeneralization ability of VLMs, by assessing their performance under varying\ninput conditions of a seed question. We evaluated 14 SOTA VLMs with 5,010\ngenerated concrete questions. Our results show that the worst-case model\naccuracy, defined as the percentage of correctly answered seed questions in all\n10 variants, is significantly lower than the average-case accuracy. Our\nanalysis emphasizes the need to study the robustness of VLMs' reasoning\nabilities, and DynaMath provides valuable insights to guide the development of\nmore reliable models for mathematical reasoning.\n","authors":["Chengke Zou","Xingang Guo","Rui Yang","Junyu Zhang","Bin Hu","Huan Zhang"],"pdf_url":"https://arxiv.org/pdf/2411.00836v2.pdf","comment":"Accepted by ICLR 2025"},{"id":"http://arxiv.org/abs/2502.16901v1","updated":"2025-02-24T06:54:50Z","published":"2025-02-24T06:54:50Z","title":"Char-mander Use mBackdoor! A Study of Cross-lingual Backdoor Attacks in\n  Multilingual LLMs","summary":"  We explore Cross-lingual Backdoor ATtacks (X-BAT) in multilingual Large\nLanguage Models (mLLMs), revealing how backdoors inserted in one language can\nautomatically transfer to others through shared embedding spaces. Using\ntoxicity classification as a case study, we demonstrate that attackers can\ncompromise multilingual systems by poisoning data in a single language, with\nrare tokens serving as specific effective triggers. Our findings expose a\ncritical vulnerability in the fundamental architecture that enables\ncross-lingual transfer in these models. Our code and data are publicly\navailable at https://github.com/himanshubeniwal/X-BAT.\n","authors":["Himanshu Beniwal","Sailesh Panda","Mayank Singh"],"pdf_url":"https://arxiv.org/pdf/2502.16901v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.16894v1","updated":"2025-02-24T06:48:13Z","published":"2025-02-24T06:48:13Z","title":"Make LoRA Great Again: Boosting LoRA with Adaptive Singular Values and\n  Mixture-of-Experts Optimization Alignment","summary":"  While Low-Rank Adaptation (LoRA) enables parameter-efficient fine-tuning for\nLarge Language Models (LLMs), its performance often falls short of Full\nFine-Tuning (Full FT). Current methods optimize LoRA by initializing with\nstatic singular value decomposition (SVD) subsets, leading to suboptimal\nleveraging of pre-trained knowledge. Another path for improving LoRA is\nincorporating a Mixture-of-Experts (MoE) architecture. However, weight\nmisalignment and complex gradient dynamics make it challenging to adopt SVD\nprior to the LoRA MoE architecture. To mitigate these issues, we propose\n\\underline{G}reat L\\underline{o}R\\underline{A} Mixture-of-Exper\\underline{t}\n(GOAT), a framework that (1) adaptively integrates relevant priors using an\nSVD-structured MoE, and (2) aligns optimization with full fine-tuned MoE by\nderiving a theoretical scaling factor. We demonstrate that proper scaling,\nwithout modifying the architecture or training algorithms, boosts LoRA MoE's\nefficiency and performance. Experiments across 25 datasets, including natural\nlanguage understanding, commonsense reasoning, image classification, and\nnatural language generation, demonstrate GOAT's state-of-the-art performance,\nclosing the gap with Full FT.\n","authors":["Chenghao Fan","Zhenyi Lu","Sichen Liu","Xiaoye Qu","Wei Wei","Chengfeng Gu","Yu Cheng"],"pdf_url":"https://arxiv.org/pdf/2502.16894v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.12913v2","updated":"2025-02-24T06:46:54Z","published":"2025-02-18T14:54:55Z","title":"GSQ-Tuning: Group-Shared Exponents Integer in Fully Quantized Training\n  for LLMs On-Device Fine-tuning","summary":"  Large Language Models (LLMs) fine-tuning technologies have achieved\nremarkable results. However, traditional LLM fine-tuning approaches face\nsignificant challenges: they require large Floating Point (FP) computation,\nraising privacy concerns when handling sensitive data, and are impractical for\nresource-constrained edge devices. While Parameter-Efficient Fine-Tuning (PEFT)\ntechniques reduce trainable parameters, their reliance on floating-point\narithmetic creates fundamental incompatibilities with edge hardware. In this\nwork, we introduce a novel framework for on-device LLM fine-tuning that\neliminates the need for floating-point operations in both inference and\ntraining, named GSQ-Tuning. At its core is the Group-Shared Exponents Integer\nformat, which efficiently represents model parameters in integer format using\nshared exponents among parameter groups. When combined with LoRA-like adapters,\nthis enables fully integer-based fine-tuning that is both memory and compute\nefficient. We demonstrate that our approach achieves accuracy comparable to\nBF16-based fine-tuning while significantly reducing 1.85x memory usage.\nMoreover, compared to FP8, our method can reduce 5x power consumption and 11x\nchip area with same performance, making large-scale model adaptation feasible\non edge devices.\n","authors":["Sifan Zhou","Shuo Wang","Zhihang Yuan","Mingjia Shi","Yuzhang Shang","Dawei Yang"],"pdf_url":"https://arxiv.org/pdf/2502.12913v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.22394v2","updated":"2025-02-24T06:44:17Z","published":"2024-10-29T17:58:29Z","title":"AAAR-1.0: Assessing AI's Potential to Assist Research","summary":"  Numerous studies have assessed the proficiency of AI systems, particularly\nlarge language models (LLMs), in facilitating everyday tasks such as email\nwriting, question answering, and creative content generation. However,\nresearchers face unique challenges and opportunities in leveraging LLMs for\ntheir own work, such as brainstorming research ideas, designing experiments,\nand writing or reviewing papers. In this study, we introduce AAAR-1.0, a\nbenchmark dataset designed to evaluate LLM performance in three fundamental,\nexpertise-intensive research tasks: (i) EquationInference, assessing the\ncorrectness of equations based on the contextual information in paper\nsubmissions; (ii) ExperimentDesign, designing experiments to validate research\nideas and solutions; (iii) PaperWeakness, identifying weaknesses in paper\nsubmissions; and (iv) REVIEWCRITIQUE, identifying each segment in human reviews\nis deficient or not. AAAR-1.0 differs from prior benchmarks in two key ways:\nfirst, it is explicitly research-oriented, with tasks requiring deep domain\nexpertise; second, it is researcher-oriented, mirroring the primary activities\nthat researchers engage in on a daily basis. An evaluation of both open-source\nand proprietary LLMs reveals their potential as well as limitations in\nconducting sophisticated research tasks. We will keep iterating AAAR-1.0 to new\nversions.\n","authors":["Renze Lou","Hanzi Xu","Sijia Wang","Jiangshu Du","Ryo Kamoi","Xiaoxin Lu","Jian Xie","Yuxuan Sun","Yusen Zhang","Jihyun Janice Ahn","Hongchao Fang","Zhuoyang Zou","Wenchao Ma","Xi Li","Kai Zhang","Congying Xia","Lifu Huang","Wenpeng Yin"],"pdf_url":"https://arxiv.org/pdf/2410.22394v2.pdf","comment":"Project Webpage: https://renzelou.github.io/AAAR-1.0/"},{"id":"http://arxiv.org/abs/2502.16892v1","updated":"2025-02-24T06:43:19Z","published":"2025-02-24T06:43:19Z","title":"Applying LLMs to Active Learning: Towards Cost-Efficient Cross-Task Text\n  Classification without Manually Labeled Data","summary":"  Machine learning-based classifiers have been used for text classification,\nsuch as sentiment analysis, news classification, and toxic comment\nclassification. However, supervised machine learning models often require large\namounts of labeled data for training, and manual annotation is both\nlabor-intensive and requires domain-specific knowledge, leading to relatively\nhigh annotation costs. To address this issue, we propose an approach that\nintegrates large language models (LLMs) into an active learning framework. Our\napproach combines the Robustly Optimized BERT Pretraining Approach (RoBERTa),\nGenerative Pre-trained Transformer (GPT), and active learning, achieving high\ncross-task text classification performance without the need for any manually\nlabeled data. Furthermore, compared to directly applying GPT for classification\ntasks, our approach retains over 93% of its classification performance while\nrequiring only approximately 6% of the computational time and monetary cost,\neffectively balancing performance and resource efficiency. These findings\nprovide new insights into the efficient utilization of LLMs and active learning\nalgorithms in text classification tasks, paving the way for their broader\napplication.\n","authors":["Yejian Zhang","Shingo Takada"],"pdf_url":"https://arxiv.org/pdf/2502.16892v1.pdf","comment":"Statement in Accordance with IEEE Preprint Policy: This work is\n  intended for submission to the IEEE for possible publication"},{"id":"http://arxiv.org/abs/2502.09642v2","updated":"2025-02-24T06:38:13Z","published":"2025-02-10T09:32:08Z","title":"Krutrim LLM: Multilingual Foundational Model for over a Billion People","summary":"  India is a diverse society with unique challenges in developing AI systems,\nincluding linguistic diversity, oral traditions, data accessibility, and\nscalability. Existing foundation models are primarily trained on English,\nlimiting their effectiveness for India's population. Indic languages comprise\nonly 1 percent of Common Crawl corpora despite India representing 18 percent of\nthe global population, leading to linguistic biases. Thousands of regional\nlanguages, dialects, and code mixing create additional representation\nchallenges due to sparse training data.\n  We introduce Krutrim LLM, a 2 trillion token multilingual model designed for\nIndia's linguistic landscape. It incorporates the largest known Indic dataset,\nmitigating data scarcity and ensuring balanced performance across dialects.\nKrutrim outperforms or matches state-of-the-art models on Indic benchmarks\nwhile maintaining competitive English performance. Despite being significantly\nsmaller in training flops, Krutrim LLM matches or exceeds models like LLAMA-2\non 10 out of 16 tasks, with an average score of 0.57 versus 0.55. This\nevidences Krutrim's flexible multilingual fluency across diverse linguistic\ncontexts.\n  Krutrim is integrated with real-time search to improve factual accuracy in\nconversational AI applications. This enhances accessibility for over 1 billion\nusers worldwide. Through intentional design choices addressing data imbalances,\nKrutrim LLM signifies meaningful progress in building ethical, globally\nrepresentative AI models.\n","authors":["Aditya Kallappa","Palash Kamble","Abhinav Ravi","Akshat Patidar","Vinayak Dhruv","Deepak Kumar","Raghav Awasthi","Arveti Manjunath","Himanshu Gupta","Shubham Agarwal","Kumar Ashish","Gautam Bhargava","Chandra Khatri"],"pdf_url":"https://arxiv.org/pdf/2502.09642v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.16886v1","updated":"2025-02-24T06:33:39Z","published":"2025-02-24T06:33:39Z","title":"DBudgetKV: Dynamic Budget in KV Cache Compression for Ensuring Optimal\n  Performance","summary":"  To alleviate memory burden during inference of large language models (LLMs),\nnumerous studies have focused on compressing the KV cache by exploring aspects\nsuch as attention sparsity. However, these techniques often require a\npre-defined cache budget; as the optimal budget varies with different input\nlengths and task types, it limits their practical deployment accepting\nopen-domain instructions. To address this limitation, we propose a new KV cache\ncompression objective: to always ensure the full-cache performance regardless\nof specific inputs, while maximizing KV cache pruning as much as possible. To\nachieve this goal, we introduce a novel KV cache compression method dubbed\nDBudgetKV, which features an attention-based metric to signal when the\nremaining KV cache is unlikely to match the full-cache performance, then\nhalting the pruning process. Empirical evaluation spanning diverse context\nlengths, task types, and model sizes suggests that our method achieves lossless\nKV pruning effectively and robustly, exceeding 25% compression ratio on\naverage. Furthermore, our method is easy to integrate within LLM inference, not\nonly optimizing memory space, but also showing reduced inference time compared\nto existing methods.\n","authors":["Xuanfan Ni","Liyan Xu","Chenyang Lyu","Longyue Wang","Mo Yu","Lemao Liu","Fandong Meng","Jie Zhou","Piji Li"],"pdf_url":"https://arxiv.org/pdf/2502.16886v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.16880v1","updated":"2025-02-24T06:28:26Z","published":"2025-02-24T06:28:26Z","title":"CORAL: Learning Consistent Representations across Multi-step Training\n  with Lighter Speculative Drafter","summary":"  Speculative decoding is a powerful technique that accelerates Large Language\nModel (LLM) inference by leveraging a lightweight speculative draft model.\nHowever, existing designs suffers in performance due to misalignment between\ntraining and inference. Recent methods have tried to solve this issue by\nadopting a multi-step training strategy, but the complex inputs of different\ntraining steps make it harder for the draft model to converge. To address this,\nwe propose CORAL, a novel framework that improves both accuracy and efficiency\nin speculative drafting. CORAL introduces Cross-Step Representation Alignment,\na method that enhances consistency across multiple training steps,\nsignificantly improving speculative drafting performance. Additionally, we\nidentify the LM head as a major bottleneck in the inference speed of the draft\nmodel. We introduce a weight-grouping mechanism that selectively activates a\nsubset of LM head parameters during inference, substantially reducing the\nlatency of the draft model. We evaluate CORAL on three LLM families and three\nbenchmark datasets, achieving speedup ratios of 2.50x-4.07x, outperforming\nstate-of-the-art methods such as EAGLE-2 and HASS. Our results demonstrate that\nCORAL effectively mitigates training-inference misalignment and delivers\nsignificant speedup for modern LLMs with large vocabularies.\n","authors":["Yepeng Weng","Dianwen Mei","Huishi Qiu","Xujie Chen","Li Liu","Jiang Tian","Zhongchao Shi"],"pdf_url":"https://arxiv.org/pdf/2502.16880v1.pdf","comment":"Under Review"},{"id":"http://arxiv.org/abs/2411.12174v2","updated":"2025-02-24T06:19:39Z","published":"2024-11-19T02:39:28Z","title":"Just KIDDIN: Knowledge Infusion and Distillation for Detection of\n  INdecent Memes","summary":"  Toxicity identification in online multimodal environments remains a\nchallenging task due to the complexity of contextual connections across\nmodalities (e.g., textual and visual). In this paper, we propose a novel\nframework that integrates Knowledge Distillation (KD) from Large Visual\nLanguage Models (LVLMs) and knowledge infusion to enhance the performance of\ntoxicity detection in hateful memes. Our approach extracts sub-knowledge graphs\nfrom ConceptNet, a large-scale commonsense Knowledge Graph (KG) to be infused\nwithin a compact VLM framework. The relational context between toxic phrases in\ncaptions and memes, as well as visual concepts in memes enhance the model's\nreasoning capabilities. Experimental results from our study on two hate speech\nbenchmark datasets demonstrate superior performance over the state-of-the-art\nbaselines across AU-ROC, F1, and Recall with improvements of 1.1%, 7%, and 35%,\nrespectively. Given the contextual complexity of the toxicity detection task,\nour approach showcases the significance of learning from both explicit (i.e.\nKG) as well as implicit (i.e. LVLMs) contextual cues incorporated through a\nhybrid neurosymbolic approach. This is crucial for real-world applications\nwhere accurate and scalable recognition of toxic content is critical for\ncreating safer online environments.\n","authors":["Rahul Garg","Trilok Padhi","Hemang Jain","Ugur Kursuncu","Ponnurangam Kumaraguru"],"pdf_url":"https://arxiv.org/pdf/2411.12174v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.18032v4","updated":"2025-02-24T06:16:00Z","published":"2024-10-23T17:02:59Z","title":"GraphTeam: Facilitating Large Language Model-based Graph Analysis via\n  Multi-Agent Collaboration","summary":"  Graphs are widely used for modeling relational data in real-world scenarios,\nsuch as social networks and urban computing. Existing LLM-based graph analysis\napproaches either integrate graph neural networks (GNNs) for specific machine\nlearning tasks, limiting their transferability, or rely solely on LLMs'\ninternal reasoning ability, resulting in suboptimal performance. To address\nthese limitations, we take advantage of recent advances in LLM-based agents,\nwhich have shown capabilities of utilizing external knowledge or tools for\nproblem solving. By simulating human problem-solving strategies such as analogy\nand collaboration, we propose a multi-agent system based on LLMs named\nGraphTeam, for graph analysis. GraphTeam consists of five LLM-based agents from\nthree modules, and the agents with different specialities can collaborate with\neach other to address complex problems. Specifically, (1) input-output\nnormalization module: the question agent extracts and refines four key\narguments from the original question, facilitating the problem understanding,\nand the answer agent organizes the results to meet the output requirement; (2)\nexternal knowledge retrieval module: we first build a knowledge base consisting\nof relevant documentation and experience information, and then the search agent\nretrieves the most relevant entries for each question. (3) problem-solving\nmodule: given the retrieved information from search agent, the coding agent\nuses established algorithms via programming to generate solutions, and in case\nthe coding agent does not work, the reasoning agent will directly compute the\nresults without programming. Extensive experiments on six graph analysis\nbenchmarks demonstrate that GraphTeam achieves state-of-the-art performance\nwith an average 25.85% improvement over the best baseline in terms of accuracy.\nThe code and data are available at https://github.com/BUPT-GAMMA/GraphTeam.\n","authors":["Xin Sky Li","Qizhi Chu","Yubin Chen","Yang Liu","Yaoqi Liu","Zekai Yu","Weize Chen","Chen Qian","Chuan Shi","Cheng Yang"],"pdf_url":"https://arxiv.org/pdf/2410.18032v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.11882v2","updated":"2025-02-24T06:15:02Z","published":"2025-02-17T15:09:45Z","title":"Leveraging Dual Process Theory in Language Agent Framework for Real-time\n  Simultaneous Human-AI Collaboration","summary":"  Agents built on large language models (LLMs) have excelled in turn-by-turn\nhuman-AI collaboration but struggle with simultaneous tasks requiring real-time\ninteraction. Latency issues and the challenge of inferring variable human\nstrategies hinder their ability to make autonomous decisions without explicit\ninstructions. Through experiments with current independent System 1 and System\n2 methods, we validate the necessity of using Dual Process Theory (DPT) in\nreal-time tasks. We propose DPT-Agent, a novel language agent framework that\nintegrates System 1 and System 2 for efficient real-time simultaneous human-AI\ncollaboration. DPT-Agent's System 1 uses a Finite-state Machine (FSM) and\ncode-as-policy for fast, intuitive, and controllable decision-making.\nDPT-Agent's System 2 integrates Theory of Mind (ToM) and asynchronous\nreflection to infer human intentions and perform reasoning-based autonomous\ndecisions. We demonstrate the effectiveness of DPT-Agent through further\nexperiments with rule-based agents and human collaborators, showing significant\nimprovements over mainstream LLM-based frameworks. To the best of our\nknowledge, DPT-Agent is the first language agent framework that achieves\nsuccessful real-time simultaneous human-AI collaboration autonomously. Code of\nDPT-Agent can be found in https://github.com/sjtu-marl/DPT-Agent.\n","authors":["Shao Zhang","Xihuai Wang","Wenhao Zhang","Chaoran Li","Junru Song","Tingyu Li","Lin Qiu","Xuezhi Cao","Xunliang Cai","Wen Yao","Weinan Zhang","Xinbing Wang","Ying Wen"],"pdf_url":"https://arxiv.org/pdf/2502.11882v2.pdf","comment":"Preprint under review. Update the experimental results of the\n  DeepSeek-R1 series models"},{"id":"http://arxiv.org/abs/2410.06617v4","updated":"2025-02-24T06:09:45Z","published":"2024-10-09T07:14:45Z","title":"Learning Evolving Tools for Large Language Models","summary":"  Tool learning enables large language models (LLMs) to interact with external\ntools and APIs, greatly expanding the application scope of LLMs. However, due\nto the dynamic nature of external environments, these tools and APIs may become\noutdated over time, preventing LLMs from correctly invoking tools. Existing\nresearch primarily focuses on static environments and overlooks this issue,\nlimiting the adaptability of LLMs in real-world applications. In this paper, we\npropose ToolEVO, a novel framework designed to enhance the adaptive and\nreflective capabilities of LLMs against tool variability. By leveraging Monte\nCarlo Tree Search, ToolEVO facilitates active exploration and interaction of\nLLMs within dynamic environments, allowing for autonomous self-reflection and\nself-updating of tool usage based on environmental feedback. Additionally, we\nintroduce ToolQA-D, a benchmark specifically designed to evaluate the impact of\ntool variability. Extensive experiments demonstrate the effectiveness and\nstability of our approach, highlighting the importance of adaptability to tool\nvariability for effective tool learning. Code:\nhttps://github.com/Chen-GX/ToolEVO\n","authors":["Guoxin Chen","Zhong Zhang","Xin Cong","Fangda Guo","Yesai Wu","Yankai Lin","Wenzheng Feng","Yasheng Wang"],"pdf_url":"https://arxiv.org/pdf/2410.06617v4.pdf","comment":"Camera ready version for ICLR 2025"},{"id":"http://arxiv.org/abs/2408.10593v3","updated":"2025-02-24T06:04:45Z","published":"2024-08-20T07:10:40Z","title":"An Efficient Sign Language Translation Using Spatial Configuration and\n  Motion Dynamics with LLMs","summary":"  Gloss-free Sign Language Translation (SLT) converts sign videos directly into\nspoken language sentences without relying on glosses. Recently, Large Language\nModels (LLMs) have shown remarkable translation performance in gloss-free\nmethods by harnessing their powerful natural language generation capabilities.\nHowever, these methods often rely on domain-specific fine-tuning of visual\nencoders to achieve optimal results. By contrast, this paper emphasizes the\nimportance of capturing the spatial configurations and motion dynamics inherent\nin sign language. With this in mind, we introduce Spatial and Motion-based Sign\nLanguage Translation (SpaMo), a novel LLM-based SLT framework. The core idea of\nSpaMo is simple yet effective. We first extract spatial and motion features\nusing off-the-shelf visual encoders and then input these features into an LLM\nwith a language prompt. Additionally, we employ a visual-text alignment process\nas a warm-up before the SLT supervision. Our experiments demonstrate that SpaMo\nachieves state-of-the-art performance on two popular datasets, PHOENIX14T and\nHow2Sign.\n","authors":["Eui Jun Hwang","Sukmin Cho","Junmyeong Lee","Jong C. Park"],"pdf_url":"https://arxiv.org/pdf/2408.10593v3.pdf","comment":"Accepted to NAACL 2025 main"},{"id":"http://arxiv.org/abs/2502.16860v1","updated":"2025-02-24T05:51:53Z","published":"2025-02-24T05:51:53Z","title":"LongAttn: Selecting Long-context Training Data via Token-level Attention","summary":"  With the development of large language models (LLMs), there has been an\nincreasing need for significant advancements in handling long contexts. To\nenhance long-context capabilities, constructing high-quality training data with\nlong-range dependencies is crucial. Existing methods to select long-context\ndata often rely on sentence-level analysis, which can be greatly optimized in\nboth performance and efficiency. In this paper, we propose a novel token-level\nframework, LongAttn, which leverages the self-attention mechanism of LLMs to\nmeasure the long-range dependencies for the data. By calculating token-level\ndependency strength and distribution uniformity of token scores, LongAttn\neffectively quantifies long-range dependencies, enabling more accurate and\nefficient data selection. We filter LongABC-32K from open-source long-context\ndatasets (ArXiv, Book, and Code). Through our comprehensive experiments,\nLongAttn has demonstrated its excellent effectiveness, scalability, and\nefficiency. To facilitate future research in long-context data, we released our\ncode and the high-quality long-context training data LongABC-32K.\n","authors":["Longyun Wu","Dawei Zhu","Guangxiang Zhao","Zhuocheng Yu","Junfeng Ran","Xiangyu Wong","Lin Sun","Sujian Li"],"pdf_url":"https://arxiv.org/pdf/2502.16860v1.pdf","comment":"17 pages, 5 figures"},{"id":"http://arxiv.org/abs/2409.19667v3","updated":"2025-02-24T05:39:36Z","published":"2024-09-29T11:38:45Z","title":"Can Large Language Models Analyze Graphs like Professionals? A\n  Benchmark, Datasets and Models","summary":"  The need to analyze graphs is ubiquitous across various fields, from social\nnetworks to biological research and recommendation systems. Therefore, enabling\nthe ability of large language models (LLMs) to process graphs is an important\nstep toward more advanced general intelligence. However, current LLM benchmarks\non graph analysis require models to directly reason over the prompts describing\ngraph topology, and are thus limited to small graphs with only a few dozens of\nnodes. In contrast, human experts typically write programs based on popular\nlibraries for task solving, and can thus handle graphs with different scales.\nTo this end, a question naturally arises: can LLMs analyze graphs like\nprofessionals? In this paper, we introduce ProGraph, a manually crafted\nbenchmark containing 3 categories of graph tasks. The benchmark expects\nsolutions based on programming instead of directly reasoning over raw inputs.\nOur findings reveal that the performance of current LLMs is unsatisfactory,\nwith the best model achieving only 36% accuracy. To bridge this gap, we propose\nLLM4Graph datasets, which include crawled documents and auto-generated codes\nbased on 6 widely used graph libraries. By augmenting closed-source LLMs with\ndocument retrieval and fine-tuning open-source ones on the codes, we show\n11-32% absolute improvements in their accuracies. Our results underscore that\nthe capabilities of LLMs in handling structured data are still under-explored,\nand show the effectiveness of LLM4Graph in enhancing LLMs' proficiency of graph\nanalysis. The benchmark, datasets and enhanced open-source models are available\nat https://github.com/BUPT-GAMMA/ProGraph.\n","authors":["Xin Sky Li","Weize Chen","Qizhi Chu","Haopeng Li","Zhaojun Sun","Ran Li","Chen Qian","Yiwei Wei","Zhiyuan Liu","Chuan Shi","Maosong Sun","Cheng Yang"],"pdf_url":"https://arxiv.org/pdf/2409.19667v3.pdf","comment":"NeurIPS 2024"},{"id":"http://arxiv.org/abs/2502.16857v1","updated":"2025-02-24T05:32:00Z","published":"2025-02-24T05:32:00Z","title":"Sarang at DEFACTIFY 4.0: Detecting AI-Generated Text Using Noised Data\n  and an Ensemble of DeBERTa Models","summary":"  This paper presents an effective approach to detect AI-generated text,\ndeveloped for the Defactify 4.0 shared task at the fourth workshop on\nmultimodal fact checking and hate speech detection. The task consists of two\nsubtasks: Task-A, classifying whether a text is AI generated or human written,\nand Task-B, classifying the specific large language model that generated the\ntext. Our team (Sarang) achieved the 1st place in both tasks with F1 scores of\n1.0 and 0.9531, respectively. The methodology involves adding noise to the\ndataset to improve model robustness and generalization. We used an ensemble of\nDeBERTa models to effectively capture complex patterns in the text. The result\nindicates the effectiveness of our noise-driven and ensemble-based approach,\nsetting a new standard in AI-generated text detection and providing guidance\nfor future developments.\n","authors":["Avinash Trivedi","Sangeetha Sivanesan"],"pdf_url":"https://arxiv.org/pdf/2502.16857v1.pdf","comment":"AAAI-25 DEFACTIFY 4.0 Workshop AI generated text detection (1st Rank)"},{"id":"http://arxiv.org/abs/2502.16852v1","updated":"2025-02-24T05:24:52Z","published":"2025-02-24T05:24:52Z","title":"Improving LLM General Preference Alignment via Optimistic Online Mirror\n  Descent","summary":"  Reinforcement learning from human feedback (RLHF) has demonstrated remarkable\neffectiveness in aligning large language models (LLMs) with human preferences.\nMany existing alignment approaches rely on the Bradley-Terry (BT) model\nassumption, which assumes the existence of a ground-truth reward for each\nprompt-response pair. However, this assumption can be overly restrictive when\nmodeling complex human preferences. In this paper, we drop the BT model\nassumption and study LLM alignment under general preferences, formulated as a\ntwo-player game. Drawing on theoretical insights from learning in games, we\nintegrate optimistic online mirror descent into our alignment framework to\napproximate the Nash policy. Theoretically, we demonstrate that our approach\nachieves an $O(T^{-1})$ bound on the duality gap, improving upon the previous\n$O(T^{-1/2})$ result. More importantly, we implement our method and show\nthrough experiments that it outperforms state-of-the-art RLHF algorithms across\nmultiple representative benchmarks.\n","authors":["Yuheng Zhang","Dian Yu","Tao Ge","Linfeng Song","Zhichen Zeng","Haitao Mi","Nan Jiang","Dong Yu"],"pdf_url":"https://arxiv.org/pdf/2502.16852v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.05112v6","updated":"2025-02-24T05:10:03Z","published":"2024-09-08T14:45:47Z","title":"WaterSeeker: Pioneering Efficient Detection of Watermarked Segments in\n  Large Documents","summary":"  Watermarking algorithms for large language models (LLMs) have attained high\naccuracy in detecting LLM-generated text. However, existing methods primarily\nfocus on distinguishing fully watermarked text from non-watermarked text,\noverlooking real-world scenarios where LLMs generate only small sections within\nlarge documents. In this scenario, balancing time complexity and detection\nperformance poses significant challenges. This paper presents WaterSeeker, a\nnovel approach to efficiently detect and locate watermarked segments amid\nextensive natural text. It first applies an efficient anomaly extraction method\nto preliminarily locate suspicious watermarked regions. Following this, it\nconducts a local traversal and performs full-text detection for more precise\nverification. Theoretical analysis and experimental results demonstrate that\nWaterSeeker achieves a superior balance between detection accuracy and\ncomputational efficiency. Moreover, its localization capability lays the\nfoundation for building interpretable AI detection systems. Our code is\navailable at https://github.com/THU-BPM/WaterSeeker.\n","authors":["Leyi Pan","Aiwei Liu","Yijian Lu","Zitian Gao","Yichen Di","Shiyu Huang","Lijie Wen","Irwin King","Philip S. Yu"],"pdf_url":"https://arxiv.org/pdf/2409.05112v6.pdf","comment":"NAACL 2025 Findings; AAAI PDLM Workshop (Oral)"},{"id":"http://arxiv.org/abs/2308.12219v3","updated":"2025-02-24T05:09:09Z","published":"2023-08-23T16:01:12Z","title":"Diffusion Language Models Can Perform Many Tasks with Scaling and\n  Instruction-Finetuning","summary":"  The recent surge of generative AI has been fueled by the generative power of\ndiffusion probabilistic models and the scalable capabilities of large language\nmodels. Despite their potential, it remains elusive whether diffusion language\nmodels can solve general language tasks comparable to their autoregressive\ncounterparts. This paper demonstrates that scaling diffusion models w.r.t.\ndata, sizes, and tasks can effectively make them strong language learners. We\nbuild competent diffusion language models at scale by first acquiring knowledge\nfrom massive data via masked language modeling pretraining thanks to their\nintrinsic connections. We then reprogram pretrained masked language models into\ndiffusion language models via diffusive adaptation, wherein task-specific\nfinetuning and instruction finetuning are explored to unlock their versatility\nin solving general language tasks. Experiments show that scaling diffusion\nlanguage models consistently improves performance across downstream language\ntasks. We further discover that instruction finetuning can elicit zero-shot and\nfew-shot in-context learning abilities that help tackle many unseen tasks by\nfollowing natural language instructions, and show promise in advanced and\nchallenging abilities such as reasoning.\n","authors":["Jiasheng Ye","Zaixiang Zheng","Yu Bao","Lihua Qian","Quanquan Gu"],"pdf_url":"https://arxiv.org/pdf/2308.12219v3.pdf","comment":"add results on reasoning and multimodality; add discussions on latest\n  progress"},{"id":"http://arxiv.org/abs/2502.12084v2","updated":"2025-02-24T04:56:36Z","published":"2025-02-17T17:57:50Z","title":"VLM$^2$-Bench: A Closer Look at How Well VLMs Implicitly Link Explicit\n  Matching Visual Cues","summary":"  Visually linking matching cues is a crucial ability in daily life, such as\nidentifying the same person in multiple photos based on their cues, even\nwithout knowing who they are. Despite the extensive knowledge that\nvision-language models (VLMs) possess, it remains largely unexplored whether\nthey are capable of performing this fundamental task. To address this, we\nintroduce VLM$^2$-Bench, a benchmark designed to assess whether VLMs can\nVisually Link Matching cues, with 9 subtasks and over 3,000 test cases.\nComprehensive evaluation across eight open-source VLMs and GPT-4o, along with\nfurther analysis of various language-side and vision-side prompting methods,\nleads to a total of eight key findings. We identify critical challenges in\nmodels' ability to link visual cues, highlighting a significant performance gap\nwhere even GPT-4o lags 34.80% behind humans. Based on these insights, we\nadvocate for (i) enhancing core visual capabilities to improve adaptability and\nreduce reliance on prior knowledge, (ii) establishing clearer principles for\nintegrating language-based reasoning in vision-centric tasks to prevent\nunnecessary biases, and (iii) shifting vision-text training paradigms toward\nfostering models' ability to independently structure and infer relationships\namong visual cues.\n","authors":["Jianshu Zhang","Dongyu Yao","Renjie Pi","Paul Pu Liang","Yi R. Fung"],"pdf_url":"https://arxiv.org/pdf/2502.12084v2.pdf","comment":"Project Page: https://vlm2-bench.github.io/"},{"id":"http://arxiv.org/abs/2502.16839v1","updated":"2025-02-24T04:50:06Z","published":"2025-02-24T04:50:06Z","title":"\"Actionable Help\" in Crises: A Novel Dataset and Resource-Efficient\n  Models for Identifying Request and Offer Social Media Posts","summary":"  During crises, social media serves as a crucial coordination tool, but the\nvast influx of posts--from \"actionable\" requests and offers to generic content\nlike emotional support, behavioural guidance, or outdated\ninformation--complicates effective classification. Although generative LLMs\n(Large Language Models) can address this issue with few-shot classification,\ntheir high computational demands limit real-time crisis response. While\nfine-tuning encoder-only models (e.g., BERT) is a popular choice, these models\nstill exhibit higher inference times in resource-constrained environments.\nMoreover, although distilled variants (e.g., DistilBERT) exist, they are not\ntailored for the crisis domain. To address these challenges, we make two key\ncontributions. First, we present CrisisHelpOffer, a novel dataset of 101k\ntweets collaboratively labelled by generative LLMs and validated by humans,\nspecifically designed to distinguish actionable content from noise. Second, we\nintroduce the first crisis-specific mini models optimized for deployment in\nresource-constrained settings. Across 13 crisis classification tasks, our mini\nmodels surpass BERT (also outperform or match the performance of RoBERTa,\nMPNet, and BERTweet), offering higher accuracy with significantly smaller sizes\nand faster speeds. The Medium model is 47% smaller with 3.8% higher accuracy at\n3.5x speed, the Small model is 68% smaller with a 1.8% accuracy gain at 7.7x\nspeed, and the Tiny model, 83% smaller, matches BERT's accuracy at 18.6x speed.\nAll models outperform existing distilled variants, setting new benchmarks.\nFinally, as a case study, we analyze social media posts from a global crisis to\nexplore help-seeking and assistance-offering behaviours in selected developing\nand developed countries.\n","authors":["Rabindra Lamsal","Maria Rodriguez Read","Shanika Karunasekera","Muhammad Imran"],"pdf_url":"https://arxiv.org/pdf/2502.16839v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.16838v1","updated":"2025-02-24T04:49:49Z","published":"2025-02-24T04:49:49Z","title":"REGen: A Reliable Evaluation Framework for Generative Event Argument\n  Extraction","summary":"  Event argument extraction identifies arguments for predefined event roles in\ntext. Traditional evaluations rely on exact match (EM), requiring predicted\narguments to match annotated spans exactly. However, this approach fails for\ngenerative models like large language models (LLMs), which produce diverse yet\nsemantically accurate responses. EM underestimates performance by disregarding\nvalid variations, implicit arguments (unstated but inferable), and scattered\narguments (distributed across a document). To bridge this gap, we introduce\nReliable Evaluation framework for Generative event argument extraction (REGen),\na framework that better aligns with human judgment. Across six datasets, REGen\nimproves performance by an average of 23.93 F1 points over EM. Human validation\nfurther confirms REGen's effectiveness, achieving 87.67% alignment with human\nassessments of argument correctness.\n","authors":["Omar Sharif","Joseph Gatto","Madhusudan Basak","Sarah M. Preum"],"pdf_url":"https://arxiv.org/pdf/2502.16838v1.pdf","comment":"20 pages, 9 figures, 13 tables"},{"id":"http://arxiv.org/abs/2502.16825v1","updated":"2025-02-24T04:22:57Z","published":"2025-02-24T04:22:57Z","title":"Finding the Sweet Spot: Preference Data Construction for Scaling\n  Preference Optimization","summary":"  Iterative data generation and model retraining are widely used to align large\nlanguage models (LLMs). It typically involves a policy model to generate\non-policy responses and a reward model to guide training data selection. Direct\nPreference Optimization (DPO) further enhances this process by constructing\npreference pairs of chosen and rejected responses. In this work, we aim to\n\\emph{scale up} the number of on-policy samples via repeated random sampling to\nimprove alignment performance. Conventional practice selects the sample with\nthe highest reward as chosen and the lowest as rejected for DPO. However, our\nexperiments reveal that this strategy leads to a \\emph{decline} in performance\nas the sample size increases. To address this, we investigate preference data\nconstruction through the lens of underlying normal distribution of sample\nrewards. We categorize the reward space into seven representative points and\nsystematically explore all 21 ($C_7^2$) pairwise combinations. Through\nevaluations on four models using AlpacaEval 2, we find that selecting the\nrejected response at reward position $\\mu - 2\\sigma$ rather than the minimum\nreward, is crucial for optimal performance. We finally introduce a scalable\npreference data construction strategy that consistently enhances model\nperformance as the sample scale increases.\n","authors":["Yao Xiao","Hai Ye","Linyao Chen","Hwee Tou Ng","Lidong Bing","Xiaoli Li","Roy Ka-wei Lee"],"pdf_url":"https://arxiv.org/pdf/2502.16825v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.16820v1","updated":"2025-02-24T04:05:08Z","published":"2025-02-24T04:05:08Z","title":"Uncertainty Quantification of Large Language Models through\n  Multi-Dimensional Responses","summary":"  Large Language Models (LLMs) have demonstrated remarkable capabilities across\nvarious tasks due to large training datasets and powerful transformer\narchitecture. However, the reliability of responses from LLMs remains a\nquestion. Uncertainty quantification (UQ) of LLMs is crucial for ensuring their\nreliability, especially in areas such as healthcare, finance, and\ndecision-making. Existing UQ methods primarily focus on semantic similarity,\noverlooking the deeper knowledge dimensions embedded in responses. We introduce\na multi-dimensional UQ framework that integrates semantic and knowledge-aware\nsimilarity analysis. By generating multiple responses and leveraging auxiliary\nLLMs to extract implicit knowledge, we construct separate similarity matrices\nand apply tensor decomposition to derive a comprehensive uncertainty\nrepresentation. This approach disentangles overlapping information from both\nsemantic and knowledge dimensions, capturing both semantic variations and\nfactual consistency, leading to more accurate UQ. Our empirical evaluations\ndemonstrate that our method outperforms existing techniques in identifying\nuncertain responses, offering a more robust framework for enhancing LLM\nreliability in high-stakes applications.\n","authors":["Tiejin Chen","Xiaoou Liu","Longchao Da","Xiaoou Liu","Vagelis Papalexakis","Hua Wei"],"pdf_url":"https://arxiv.org/pdf/2502.16820v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.16810v1","updated":"2025-02-24T03:36:57Z","published":"2025-02-24T03:36:57Z","title":"Grounded Persuasive Language Generation for Automated Marketing","summary":"  This paper develops an agentic framework that employs large language models\n(LLMs) to automate the generation of persuasive and grounded marketing content,\nusing real estate listing descriptions as our focal application domain. Our\nmethod is designed to align the generated content with user preferences while\nhighlighting useful factual attributes. This agent consists of three key\nmodules: (1) Grounding Module, mimicking expert human behavior to predict\nmarketable features; (2) Personalization Module, aligning content with user\npreferences; (3) Marketing Module, ensuring factual accuracy and the inclusion\nof localized features. We conduct systematic human-subject experiments in the\ndomain of real estate marketing, with a focus group of potential house buyers.\nThe results demonstrate that marketing descriptions generated by our approach\nare preferred over those written by human experts by a clear margin. Our\nfindings suggest a promising LLM-based agentic framework to automate\nlarge-scale targeted marketing while ensuring responsible generation using only\nfacts.\n","authors":["Jibang Wu","Chenghao Yang","Simon Mahns","Chaoqi Wang","Hao Zhu","Fei Fang","Haifeng Xu"],"pdf_url":"https://arxiv.org/pdf/2502.16810v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.11323v3","updated":"2025-02-24T03:35:56Z","published":"2024-01-20T20:55:21Z","title":"Identifying and Analyzing Performance-Critical Tokens in Large Language\n  Models","summary":"  In-context learning (ICL) has emerged as an effective solution for few-shot\nlearning with large language models (LLMs). However, how LLMs leverage\ndemonstrations to specify a task and learn a corresponding computational\nfunction through ICL is underexplored. Drawing from the way humans learn from\ncontent-label mappings in demonstrations, we categorize the tokens in an ICL\nprompt into content, stopword, and template tokens. Our goal is to identify the\ntypes of tokens whose representations directly influence LLM's performance, a\nproperty we refer to as being performance-critical. By ablating representations\nfrom the attention of the test example, we find that the representations of\ninformative content tokens have less influence on performance compared to\ntemplate and stopword tokens, which contrasts with the human attention to\ninformative words. We give evidence that the representations of\nperformance-critical tokens aggregate information from the content tokens.\nMoreover, we demonstrate experimentally that lexical meaning, repetition, and\nstructural cues are the main distinguishing characteristics of these tokens.\nOur work sheds light on how large language models learn to perform tasks from\ndemonstrations and deepens our understanding of the roles different types of\ntokens play in large language models.\n","authors":["Yu Bai","Heyan Huang","Cesare Spinoso-Di Piano","Marc-Antoine Rondeau","Sanxing Chen","Yang Gao","Jackie Chi Kit Cheung"],"pdf_url":"https://arxiv.org/pdf/2401.11323v3.pdf","comment":"Work in progress"},{"id":"http://arxiv.org/abs/2502.16806v1","updated":"2025-02-24T03:30:29Z","published":"2025-02-24T03:30:29Z","title":"CoT2Align: Cross-Chain of Thought Distillation via Optimal Transport\n  Alignment for Language Models with Different Tokenizers","summary":"  Large Language Models (LLMs) achieve state-of-the-art performance across\nvarious NLP tasks but face deployment challenges due to high computational\ncosts and memory constraints. Knowledge distillation (KD) is a promising\nsolution, transferring knowledge from large teacher models to smaller student\nmodels. However, existing KD methods often assume shared vocabularies and\ntokenizers, limiting their flexibility. While approaches like Universal Logit\nDistillation (ULD) and Dual-Space Knowledge Distillation (DSKD) address\nvocabulary mismatches, they overlook the critical \\textbf{reasoning-aware\ndistillation} aspect. To bridge this gap, we propose CoT2Align a universal KD\nframework that integrates Chain-of-Thought (CoT) augmentation and introduces\nCross-CoT Alignment to enhance reasoning transfer. Additionally, we extend\nOptimal Transport beyond token-wise alignment to a sequence-level and\nlayer-wise alignment approach that adapts to varying sequence lengths while\npreserving contextual integrity. Comprehensive experiments demonstrate that\nCoT2Align outperforms existing KD methods across different vocabulary settings,\nimproving reasoning capabilities and robustness in domain-specific tasks.\n","authors":["Anh Duc Le","Tu Vu","Nam Le Hai","Nguyen Thi Ngoc Diep","Linh Ngo Van","Trung Le","Thien Huu Nguyen"],"pdf_url":"https://arxiv.org/pdf/2502.16806v1.pdf","comment":null}]},"2025-02-25T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2502.18460v1","updated":"2025-02-25T18:59:07Z","published":"2025-02-25T18:59:07Z","title":"DRAMA: Diverse Augmentation from Large Language Models to Smaller Dense\n  Retrievers","summary":"  Large language models (LLMs) have demonstrated strong effectiveness and\nrobustness while fine-tuned as dense retrievers. However, their large parameter\nsize brings significant inference time computational challenges, including high\nencoding costs for large-scale corpora and increased query latency, limiting\ntheir practical deployment. While smaller retrievers offer better efficiency,\nthey often fail to generalize effectively with limited supervised fine-tuning\ndata. In this work, we introduce DRAMA, a training framework that leverages\nLLMs to train smaller generalizable dense retrievers. In particular, we adopt\npruned LLMs as the backbone and train on diverse LLM-augmented data in a\nsingle-stage contrastive learning setup. Experiments show that DRAMA offers\nbetter multilingual and long-context capabilities than traditional\nencoder-based retrievers, and achieves strong performance across multiple tasks\nand languages. These highlight the potential of connecting the training of\nsmaller retrievers with the growing advancements in LLMs, bridging the gap\nbetween efficiency and generalization.\n","authors":["Xueguang Ma","Xi Victoria Lin","Barlas Oguz","Jimmy Lin","Wen-tau Yih","Xilun Chen"],"pdf_url":"https://arxiv.org/pdf/2502.18460v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.05862v2","updated":"2025-02-25T18:59:04Z","published":"2024-12-08T08:54:13Z","title":"Domain-Specific Translation with Open-Source Large Language Models:\n  Resource-Oriented Analysis","summary":"  In this work, we compare the domain-specific translation performance of\nopen-source autoregressive decoder-only large language models (LLMs) with\ntask-oriented machine translation (MT) models. Our experiments focus on the\nmedical domain and cover four language pairs with varied resource availability:\nEnglish-to-French, English-to-Portuguese, English-to-Swahili, and\nSwahili-to-English. Despite recent advancements, LLMs exhibit a clear gap in\nspecialized translation quality compared to multilingual encoder-decoder MT\nmodels such as NLLB-200. In three out of four language directions in our study,\nNLLB-200 3.3B outperforms all LLMs in the size range of 8B parameters in\nmedical translation. While fine-tuning LLMs such as Mistral and Llama improves\ntheir performance at medical translation, these models still fall short\ncompared to fine-tuned NLLB-200 3.3B models. Our findings highlight the ongoing\nneed for specialized MT models to achieve higher-quality domain-specific\ntranslation, especially in medium-resource and low-resource settings. As larger\nLLMs outperform their 8B variants, this also encourages pre-training\ndomain-specific medium-sized LMs to improve quality and efficiency in\nspecialized translation tasks.\n","authors":["Aman Kassahun Wassie","Mahdi Molaei","Yasmin Moslem"],"pdf_url":"https://arxiv.org/pdf/2412.05862v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.18452v1","updated":"2025-02-25T18:51:06Z","published":"2025-02-25T18:51:06Z","title":"FRIDA to the Rescue! Analyzing Synthetic Data Effectiveness in\n  Object-Based Common Sense Reasoning for Disaster Response","summary":"  Large Language Models (LLMs) have the potential for substantial common sense\nreasoning. However, these capabilities are often emergent in larger models.\nThis means smaller models that can be run locally are less helpful and capable\nwith respect to certain reasoning tasks. To meet our problem space\nrequirements, we fine-tune smaller LLMs to disaster domains, as these domains\ninvolve complex and low-frequency physical common sense knowledge. We introduce\na pipeline to create Field Ready Instruction Decoding Agent (FRIDA) models,\nwhere domain experts and linguists combine their knowledge to make high-quality\nseed data that is used to generate synthetic data for fine-tuning. We create a\nset of 130 seed instructions for synthetic generation, a synthetic dataset of\n25000 instructions, and 119 evaluation instructions relating to both general\nand earthquake-specific object affordances. We fine-tune several LLaMa and\nMistral instruction-tuned models and find that FRIDA models outperform their\nbase models at a variety of sizes. We then run an ablation study to understand\nwhich kinds of synthetic data most affect performance and find that training\nphysical state and object function common sense knowledge alone improves over\nFRIDA models trained on all data. We conclude that the FRIDA pipeline is\ncapable of instilling general common sense, but needs to be augmented with\ninformation retrieval for specific domain knowledge.\n","authors":["Mollie Shichman","Claire Bonial","Austin Blodgett","Taylor Hudson","Francis Ferraro","Rachel Rudinger"],"pdf_url":"https://arxiv.org/pdf/2502.18452v1.pdf","comment":"8 pages, 3 figures, 5 tables"},{"id":"http://arxiv.org/abs/2502.18449v1","updated":"2025-02-25T18:45:04Z","published":"2025-02-25T18:45:04Z","title":"SWE-RL: Advancing LLM Reasoning via Reinforcement Learning on Open\n  Software Evolution","summary":"  The recent DeepSeek-R1 release has demonstrated the immense potential of\nreinforcement learning (RL) in enhancing the general reasoning capabilities of\nlarge language models (LLMs). While DeepSeek-R1 and other follow-up work\nprimarily focus on applying RL to competitive coding and math problems, this\npaper introduces SWE-RL, the first approach to scale RL-based LLM reasoning for\nreal-world software engineering. Leveraging a lightweight rule-based reward\n(e.g., the similarity score between ground-truth and LLM-generated solutions),\nSWE-RL enables LLMs to autonomously recover a developer's reasoning processes\nand solutions by learning from extensive open-source software evolution data --\nthe record of a software's entire lifecycle, including its code snapshots, code\nchanges, and events such as issues and pull requests. Trained on top of Llama\n3, our resulting reasoning model, Llama3-SWE-RL-70B, achieves a 41.0% solve\nrate on SWE-bench Verified -- a human-verified collection of real-world GitHub\nissues. To our knowledge, this is the best performance reported for\nmedium-sized (<100B) LLMs to date, even comparable to leading proprietary LLMs\nlike GPT-4o. Surprisingly, despite performing RL solely on software evolution\ndata, Llama3-SWE-RL has even emerged with generalized reasoning skills. For\nexample, it shows improved results on five out-of-domain tasks, namely,\nfunction coding, library use, code reasoning, mathematics, and general language\nunderstanding, whereas a supervised-finetuning baseline even leads to\nperformance degradation on average. Overall, SWE-RL opens up a new direction to\nimprove the reasoning capabilities of LLMs through reinforcement learning on\nmassive software engineering data.\n","authors":["Yuxiang Wei","Olivier Duchenne","Jade Copet","Quentin Carbonneaux","Lingming Zhang","Daniel Fried","Gabriel Synnaeve","Rishabh Singh","Sida I. Wang"],"pdf_url":"https://arxiv.org/pdf/2502.18449v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.18448v1","updated":"2025-02-25T18:42:26Z","published":"2025-02-25T18:42:26Z","title":"Disambiguate First Parse Later: Generating Interpretations for Ambiguity\n  Resolution in Semantic Parsing","summary":"  Handling ambiguity and underspecification is an important challenge in\nnatural language interfaces, particularly for tasks like text-to-SQL semantic\nparsing. We propose a modular approach that resolves ambiguity using natural\nlanguage interpretations before mapping these to logical forms (e.g., SQL\nqueries). Although LLMs excel at parsing unambiguous utterances, they show\nstrong biases for ambiguous ones, typically predicting only preferred\ninterpretations. We constructively exploit this bias to generate an initial set\nof preferred disambiguations and then apply a specialized infilling model to\nidentify and generate missing interpretations. To train the infilling model, we\nintroduce an annotation method that uses SQL execution to validate different\nmeanings. Our approach improves interpretation coverage and generalizes across\ndatasets with different annotation styles, database structures, and ambiguity\ntypes.\n","authors":["Irina Saparina","Mirella Lapata"],"pdf_url":"https://arxiv.org/pdf/2502.18448v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.18443v1","updated":"2025-02-25T18:38:38Z","published":"2025-02-25T18:38:38Z","title":"olmOCR: Unlocking Trillions of Tokens in PDFs with Vision Language\n  Models","summary":"  PDF documents have the potential to provide trillions of novel, high-quality\ntokens for training language models. However, these documents come in a\ndiversity of types with differing formats and visual layouts that pose a\nchallenge when attempting to extract and faithfully represent the underlying\ncontent for language model use. We present olmOCR, an open-source Python\ntoolkit for processing PDFs into clean, linearized plain text in natural\nreading order while preserving structured content like sections, tables, lists,\nequations, and more. Our toolkit runs a fine-tuned 7B vision language model\n(VLM) trained on a sample of 260,000 pages from over 100,000 crawled PDFs with\ndiverse properties, including graphics, handwritten text and poor quality\nscans. olmOCR is optimized for large-scale batch processing, able to scale\nflexibly to different hardware setups and convert a million PDF pages for only\n$190 USD. We release all components of olmOCR including VLM weights, data and\ntraining code, as well as inference code built on serving frameworks including\nvLLM and SGLang.\n","authors":["Jake Poznanski","Jon Borchardt","Jason Dunkelberger","Regan Huff","Daniel Lin","Aman Rangapur","Christopher Wilhelm","Kyle Lo","Luca Soldaini"],"pdf_url":"https://arxiv.org/pdf/2502.18443v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.18435v1","updated":"2025-02-25T18:30:25Z","published":"2025-02-25T18:30:25Z","title":"Reversal Blessing: Thinking Backward May Outpace Thinking Forward in\n  Multi-choice Questions","summary":"  Language models usually use left-to-right (L2R) autoregressive factorization.\nHowever, L2R factorization may not always be the best inductive bias.\nTherefore, we investigate whether alternative factorizations of the text\ndistribution could be beneficial in some tasks. We investigate right-to-left\n(R2L) training as a compelling alternative, focusing on multiple-choice\nquestions (MCQs) as a test bed for knowledge extraction and reasoning. Through\nextensive experiments across various model sizes (2B-8B parameters) and\ntraining datasets, we find that R2L models can significantly outperform L2R\nmodels on several MCQ benchmarks, including logical reasoning, commonsense\nunderstanding, and truthfulness assessment tasks. Our analysis reveals that\nthis performance difference may be fundamentally linked to multiple factors\nincluding calibration, computability and directional conditional entropy. We\nablate the impact of these factors through controlled simulation studies using\narithmetic tasks, where the impacting factors can be better disentangled. Our\nwork demonstrates that exploring alternative factorizations of the text\ndistribution can lead to improvements in LLM capabilities and provides\ntheoretical insights into optimal factorization towards approximating human\nlanguage distribution, and when each reasoning order might be more\nadvantageous.\n","authors":["Yizhe Zhang","Richard Bai","Zijin Gu","Ruixiang Zhang","Jiatao Gu","Emmanuel Abbe","Samy Bengio","Navdeep Jaitly"],"pdf_url":"https://arxiv.org/pdf/2502.18435v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.08155v2","updated":"2025-02-25T18:29:54Z","published":"2024-06-12T12:44:48Z","title":"QuantMoE-Bench: Examining Post-Training Quantization for\n  Mixture-of-Experts","summary":"  Mixture-of-Experts (MoE) is a promising way to scale up the learning capacity\nof large language models. It increases the number of parameters while keeping\nFLOPs nearly constant during inference through sparse activation. Yet, it still\nsuffers from significant memory overheads due to the vast parameter size,\nnecessitating model compression techniques. Post-training quantization offers a\npowerful approach for model compression. Existing methods adopt a fixed\nquantization precision for the entire MoE model. This rigid setup can lead to\nsuboptimal performance, without considering the inherent sparse structure. For\nexample, MoE's sparse routing mechanism leads to different activation patterns,\nwhere shared experts are accessed by all tokens while token-conditioned experts\nare selectively activated. This activation disparity suggests different\nquantization requirements, with consistently activated shared experts\npotentially needing higher precision to maintain model quality. In this paper,\nwe study a fine-grained precision setup for MoE quantization. We explore MoE\nstructure-aware quantization heuristics, ranging from coarse (e.g., MoE layers)\nto fine granularity (e.g., linear layers). Our investigations reveal critical\nprinciples, where different MoE structures require varying numbers of bits for\neffective quantization. Conclusions are supported by extensive benchmarking\nacross two representative MoE models and six tasks including commonsense\nreasoning and natural language understanding. We further show that an MoE\nquantized in a fined-grained mixed precision achieved state-of-the-art 65.35%\nperformance on average compared to the baseline 64.30% (i.e., GPTQ). Moreover,\nbased on the findings, we introduce novel data-driven techniques for optimizing\nbit allocation in MoE quantization, including the outlier-aware linear layer\nscorer and MoE block importance predictor.\n","authors":["Pingzhi Li","Xiaolong Jin","Zhen Tan","Yu Cheng","Tianlong Chen"],"pdf_url":"https://arxiv.org/pdf/2406.08155v2.pdf","comment":"Our code for reproducing all our experiments is provided at\n  https://github.com/UNITES-Lab/moe-quantization"},{"id":"http://arxiv.org/abs/2502.18434v1","updated":"2025-02-25T18:29:38Z","published":"2025-02-25T18:29:38Z","title":"Exploring Gender Disparities in Automatic Speech Recognition Technology","summary":"  This study investigates factors influencing Automatic Speech Recognition\n(ASR) systems' fairness and performance across genders, beyond the conventional\nexamination of demographics. Using the LibriSpeech dataset and the Whisper\nsmall model, we analyze how performance varies across different gender\nrepresentations in training data. Our findings suggest a complex interplay\nbetween the gender ratio in training data and ASR performance. Optimal fairness\noccurs at specific gender distributions rather than a simple 50-50 split.\nFurthermore, our findings suggest that factors like pitch variability can\nsignificantly affect ASR accuracy. This research contributes to a deeper\nunderstanding of biases in ASR systems, highlighting the importance of\ncarefully curated training data in mitigating gender bias.\n","authors":["Hend ElGhazaly","Bahman Mirheidari","Nafise Sadat Moosavi","Heidi Christensen"],"pdf_url":"https://arxiv.org/pdf/2502.18434v1.pdf","comment":"ISCA/ITG Workshop on Diversity in Large Speech and Language Models"},{"id":"http://arxiv.org/abs/2502.18431v1","updated":"2025-02-25T18:26:48Z","published":"2025-02-25T18:26:48Z","title":"TextGames: Learning to Self-Play Text-Based Puzzle Games via Language\n  Model Reasoning","summary":"  Reasoning is a fundamental capability of large language models (LLMs),\nenabling them to comprehend, analyze, and solve complex problems. In this\npaper, we introduce TextGames, an innovative benchmark specifically crafted to\nassess LLMs through demanding text-based games that require advanced skills in\npattern recognition, spatial awareness, arithmetic, and logical reasoning. Our\nanalysis probes LLMs' performance in both single-turn and multi-turn reasoning,\nand their abilities in leveraging feedback to correct subsequent answers\nthrough self-reflection. Our findings reveal that, although LLMs exhibit\nproficiency in addressing most easy and medium-level problems, they face\nsignificant challenges with more difficult tasks. In contrast, humans are\ncapable of solving all tasks when given sufficient time. Moreover, we observe\nthat LLMs show improved performance in multi-turn predictions through\nself-reflection, yet they still struggle with sequencing, counting, and\nfollowing complex rules consistently. Additionally, models optimized for\nreasoning outperform pre-trained LLMs that prioritize instruction following,\nhighlighting the crucial role of reasoning skills in addressing highly complex\nproblems.\n","authors":["Frederikus Hudi","Genta Indra Winata","Ruochen Zhang","Alham Fikri Aji"],"pdf_url":"https://arxiv.org/pdf/2502.18431v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.18424v1","updated":"2025-02-25T18:20:00Z","published":"2025-02-25T18:20:00Z","title":"Compressing Language Models for Specialized Domains","summary":"  Compression techniques such as pruning and quantization offer a solution for\nmore efficient deployment of language models (LMs), albeit with small\nperformance drops in benchmark performance. However, general-purpose LM\ncompression methods can negatively affect performance in specialized domains\n(e.g. biomedical or legal). Recent work has sought to address this, yet\nrequires computationally expensive full-parameter fine-tuning. To this end, we\npropose cross-calibration, a novel training-free approach for improving the\ndomain performance of compressed LMs. Our approach effectively leverages\nHessian-based sensitivity to identify weights that are influential for both\nin-domain and general performance. Through extensive experimentation, we\ndemonstrate that cross-calibration substantially outperforms existing\napproaches on domain-specific tasks, without compromising general performance.\nNotably, these gains come without additional computational overhead, displaying\nremarkable potential towards extracting domain-specialized compressed models\nfrom general-purpose LMs.\n","authors":["Miles Williams","George Chrysostomou","Vitor Jeronymo","Nikolaos Aletras"],"pdf_url":"https://arxiv.org/pdf/2502.18424v1.pdf","comment":"Work in progress"},{"id":"http://arxiv.org/abs/2502.18418v1","updated":"2025-02-25T18:14:06Z","published":"2025-02-25T18:14:06Z","title":"Rank1: Test-Time Compute for Reranking in Information Retrieval","summary":"  We introduce Rank1, the first reranking model trained to take advantage of\ntest-time compute. Rank1 demonstrates the applicability within retrieval of\nusing a reasoning language model (i.e. OpenAI's o1, Deepseek's R1, etc.) for\ndistillation in order to rapidly improve the performance of a smaller model. We\ngather and open-source a dataset of more than 600,000 examples of R1 reasoning\ntraces from queries and passages in MS MARCO. Models trained on this dataset\nshow: (1) state-of-the-art performance on advanced reasoning and instruction\nfollowing datasets; (2) work remarkably well out of distribution due to the\nability to respond to user-input prompts; and (3) have explainable reasoning\nchains that can be given to users or RAG-based systems. Further, we demonstrate\nthat quantized versions of these models retain strong performance while using\nless compute/memory. Overall, Rank1 shows that test-time compute allows for a\nfundamentally new type of explainable and performant reranker model for search.\n","authors":["Orion Weller","Kathryn Ricci","Eugene Yang","Andrew Yates","Dawn Lawrie","Benjamin Van Durme"],"pdf_url":"https://arxiv.org/pdf/2502.18418v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.10563v2","updated":"2025-02-25T18:11:38Z","published":"2025-02-14T21:27:09Z","title":"Accelerating Unbiased LLM Evaluation via Synthetic Feedback","summary":"  When developing new large language models (LLMs), a key step is evaluating\ntheir final performance, often by computing the win-rate against a reference\nmodel based on external feedback. Human feedback is the gold standard,\nparticularly for capturing nuanced qualities like coherence, readability, and\nalignment with human expectations. However, human evaluations are costly --\neven for large tech companies -- and when conducted with active users, they may\nnegatively impact user experience. A promising alternative is synthetic\nfeedback, where evaluations are conducted by other large language models,\nincluding reward models. While this eliminates the need for costly human\nannotations, it introduces biases that may distort the evaluation process. In\nthis work, we propose a statistically principled framework that integrates\nhuman and synthetic feedback to reduce reliance on human annotations while\nmaintaining unbiased win-rate calculations. Our experiments demonstrate a\nreduction in human annotations by up to 12.2% with an off-the-shelf synthetic\nevaluator and up to 24.8% with a finetuned variant. Apart from being\ngeneralizable, scalable, and free of hyper-parameter tuning, our method offers\npredictable annotation savings, which can be estimated based on data-dependent\ncharacteristics.\n","authors":["Zhaoyi Zhou","Yuda Song","Andrea Zanette"],"pdf_url":"https://arxiv.org/pdf/2502.10563v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.18414v1","updated":"2025-02-25T18:11:37Z","published":"2025-02-25T18:11:37Z","title":"GLEAN: Generalized Category Discovery with Diverse and Quality-Enhanced\n  LLM Feedback","summary":"  Generalized Category Discovery (GCD) is a practical and challenging\nopen-world task that aims to recognize both known and novel categories in\nunlabeled data using limited labeled data from known categories. Due to the\nlack of supervision, previous GCD methods face significant challenges, such as\ndifficulty in rectifying errors for confusing instances, and inability to\neffectively uncover and leverage the semantic meanings of discovered clusters.\nTherefore, additional annotations are usually required for real-world\napplicability. However, human annotation is extremely costly and inefficient.\nTo address these issues, we propose GLEAN, a unified framework for generalized\ncategory discovery that actively learns from diverse and quality-enhanced LLM\nfeedback. Our approach leverages three different types of LLM feedback to: (1)\nimprove instance-level contrastive features, (2) generate category\ndescriptions, and (3) align uncertain instances with LLM-selected category\ndescriptions. Extensive experiments demonstrate the superior performance of\n\\MethodName over state-of-the-art models across diverse datasets, metrics, and\nsupervision settings. Our code is available at\nhttps://github.com/amazon-science/Glean.\n","authors":["Henry Peng Zou","Siffi Singh","Yi Nian","Jianfeng He","Jason Cai","Saab Mansour","Hang Su"],"pdf_url":"https://arxiv.org/pdf/2502.18414v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.06248v2","updated":"2025-02-25T18:04:50Z","published":"2025-01-08T19:03:17Z","title":"Utility-inspired Reward Transformations Improve Reinforcement Learning\n  Training of Language Models","summary":"  Current methods that train large language models (LLMs) with reinforcement\nlearning feedback, often resort to averaging outputs of multiple rewards\nfunctions during training. This overlooks crucial aspects of individual reward\ndimensions and inter-reward dependencies that can lead to sub-optimal outcomes\nin generations. In this work, we show how linear aggregation of rewards\nexhibits some vulnerabilities that can lead to undesired properties of\ngenerated text. We then propose a transformation of reward functions inspired\nby economic theory of utility functions (specifically Inada conditions), that\nenhances sensitivity to low reward values while diminishing sensitivity to\nalready high values. We compare our approach to the existing baseline methods\nthat linearly aggregate rewards and show how the Inada-inspired reward feedback\nis superior to traditional weighted averaging. We quantitatively and\nqualitatively analyse the difference in the methods, and see that models\ntrained with Inada-transformations score as more helpful while being less\nharmful.\n","authors":["Roberto-Rafael Maura-Rivero","Chirag Nagpal","Roma Patel","Francesco Visin"],"pdf_url":"https://arxiv.org/pdf/2501.06248v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.18407v1","updated":"2025-02-25T17:58:02Z","published":"2025-02-25T17:58:02Z","title":"AgentRM: Enhancing Agent Generalization with Reward Modeling","summary":"  Existing LLM-based agents have achieved strong performance on held-in tasks,\nbut their generalizability to unseen tasks remains poor. Hence, some recent\nwork focus on fine-tuning the policy model with more diverse tasks to improve\nthe generalizability. In this work, we find that finetuning a reward model to\nguide the policy model is more robust than directly finetuning the policy\nmodel. Based on this finding, we propose AgentRM, a generalizable reward model,\nto guide the policy model for effective test-time search. We comprehensively\ninvestigate three approaches to construct the reward model, including explicit\nreward modeling, implicit reward modeling and LLM-as-a-judge. We then use\nAgentRM to guide the answer generation with Best-of-N sampling and step-level\nbeam search. On four types of nine agent tasks, AgentRM enhances the base\npolicy model by $8.8$ points on average, surpassing the top general agent by\n$4.0$. Moreover, it demonstrates weak-to-strong generalization, yielding\ngreater improvement of $12.6$ on LLaMA-3-70B policy model. As for the\nspecializability, AgentRM can also boost a finetuned policy model and\noutperform the top specialized agent by $11.4$ on three held-in tasks. Further\nanalysis verifies its effectiveness in test-time scaling. Codes will be\nreleased to facilitate the research in this area.\n","authors":["Yu Xia","Jingru Fan","Weize Chen","Siyu Yan","Xin Cong","Zhong Zhang","Yaxi Lu","Yankai Lin","Zhiyuan Liu","Maosong Sun"],"pdf_url":"https://arxiv.org/pdf/2502.18407v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.00047v2","updated":"2025-02-25T17:54:13Z","published":"2024-06-05T21:17:34Z","title":"Queue management for slo-oriented large language model serving","summary":"  Large language model (LLM) serving is becoming an increasingly critical\nworkload for cloud providers. Existing LLM serving systems focus on interactive\nrequests, such as chatbots and coding assistants, with tight latency SLO\nrequirements. However, when such systems execute batch requests that have\nrelaxed SLOs along with interactive requests, it leads to poor multiplexing and\ninefficient resource utilization. To address these challenges, we propose QLM,\na queue management system for LLM serving. QLM maintains batch and interactive\nrequests across different models and SLOs in a request queue. Optimal ordering\nof the request queue is critical to maintain SLOs while ensuring high resource\nutilization. To generate this optimal ordering, QLM uses a Request Waiting Time\n(RWT) Estimator that estimates the waiting times for requests in the request\nqueue. These estimates are used by a global scheduler to orchestrate LLM\nServing Operations (LSOs) such as request pulling, request eviction, load\nbalancing, and model swapping. Evaluation on heterogeneous GPU devices and\nmodels with real-world LLM serving dataset shows that QLM improves SLO\nattainment by 40-90% and throughput by 20-400% while maintaining or improving\ndevice utilization compared to other state-of-the-art LLM serving systems.\nQLM's evaluation is based on the production requirements of a cloud provider.\nQLM is publicly available at https://www.github.com/QLM-project/QLM.\n","authors":["Archit Patke","Dhemath Reddy","Saurabh Jha","Haoran Qiu","Christian Pinto","Chandra Narayanaswami","Zbigniew Kalbarczyk","Ravishankar Iyer"],"pdf_url":"https://arxiv.org/pdf/2407.00047v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.18397v1","updated":"2025-02-25T17:47:53Z","published":"2025-02-25T17:47:53Z","title":"KiRAG: Knowledge-Driven Iterative Retriever for Enhancing\n  Retrieval-Augmented Generation","summary":"  Iterative retrieval-augmented generation (iRAG) models offer an effective\napproach for multi-hop question answering (QA). However, their retrieval\nprocess faces two key challenges: (1) it can be disrupted by irrelevant\ndocuments or factually inaccurate chain-of-thoughts; (2) their retrievers are\nnot designed to dynamically adapt to the evolving information needs in\nmulti-step reasoning, making it difficult to identify and retrieve the missing\ninformation required at each iterative step. Therefore, we propose KiRAG, which\nuses a knowledge-driven iterative retriever model to enhance the retrieval\nprocess of iRAG. Specifically, KiRAG decomposes documents into knowledge\ntriples and performs iterative retrieval with these triples to enable a\nfactually reliable retrieval process. Moreover, KiRAG integrates reasoning into\nthe retrieval process to dynamically identify and retrieve knowledge that\nbridges information gaps, effectively adapting to the evolving information\nneeds. Empirical results show that KiRAG significantly outperforms existing\niRAG models, with an average improvement of 9.40% in R@3 and 5.14% in F1 on\nmulti-hop QA.\n","authors":["Jinyuan Fang","Zaiqiao Meng","Craig Macdonald"],"pdf_url":"https://arxiv.org/pdf/2502.18397v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.18389v1","updated":"2025-02-25T17:33:20Z","published":"2025-02-25T17:33:20Z","title":"Monte Carlo Temperature: a robust sampling strategy for LLM's\n  uncertainty quantification methods","summary":"  Uncertainty quantification (UQ) in Large Language Models (LLMs) is essential\nfor their safe and reliable deployment, particularly in critical applications\nwhere incorrect outputs can have serious consequences. Current UQ methods\ntypically rely on querying the model multiple times using non-zero temperature\nsampling to generate diverse outputs for uncertainty estimation. However, the\nimpact of selecting a given temperature parameter is understudied, and our\nanalysis reveals that temperature plays a fundamental role in the quality of\nuncertainty estimates. The conventional approach of identifying optimal\ntemperature values requires expensive hyperparameter optimization (HPO) that\nmust be repeated for each new model-dataset combination. We propose Monte Carlo\nTemperature (MCT), a robust sampling strategy that eliminates the need for\ntemperature calibration. Our analysis reveals that: 1) MCT provides more robust\nuncertainty estimates across a wide range of temperatures, 2) MCT improves the\nperformance of UQ methods by replacing fixed-temperature strategies that do not\nrely on HPO, and 3) MCT achieves statistical parity with oracle temperatures,\nwhich represent the ideal outcome of a well-tuned but computationally expensive\nHPO process. These findings demonstrate that effective UQ can be achieved\nwithout the computational burden of temperature parameter calibration.\n","authors":["Nicola Cecere","Andrea Bacciu","Ignacio Fernández Tobías","Amin Mantrach"],"pdf_url":"https://arxiv.org/pdf/2502.18389v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.04556v2","updated":"2025-02-25T17:31:36Z","published":"2024-09-06T18:33:38Z","title":"How Does Code Pretraining Affect Language Model Task Performance?","summary":"  Large language models are increasingly trained on corpora containing both\nnatural language and non-linguistic data like source code. Aside from aiding\nprogramming-related tasks, anecdotal evidence suggests that including code in\npretraining corpora may improve performance on other, unrelated tasks, yet to\ndate no work has been able to establish a causal connection by controlling\nbetween language and code data. Here we do just this. We pretrain language\nmodels on datasets which interleave natural language and code in two different\nsettings: additive, in which the total volume of data seen during pretraining\nis held constant; and competitive, in which the volume of language data is held\nconstant. We study how the pretraining mixture affects performance on (a) a\ndiverse collection of tasks included in the BigBench benchmark, and (b)\ncompositionality, measured by generalization accuracy on semantic parsing and\nsyntactic transformations. We find that pretraining on higher proportions of\ncode improves performance on compositional tasks involving structured output\n(like semantic parsing), and mathematics. Conversely, increase code mixture can\nharm performance on other tasks, including on tasks that requires sensitivity\nto linguistic structure such as syntax or morphology, and tasks measuring\nreal-world knowledge.\n","authors":["Jackson Petty","Sjoerd van Steenkiste","Tal Linzen"],"pdf_url":"https://arxiv.org/pdf/2409.04556v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.02841v2","updated":"2025-02-25T17:24:16Z","published":"2024-09-04T16:14:05Z","title":"Historical German Text Normalization Using Type- and Token-Based\n  Language Modeling","summary":"  Historic variations of spelling poses a challenge for full-text search or\nnatural language processing on historical digitized texts. To minimize the gap\nbetween the historic orthography and contemporary spelling, usually an\nautomatic orthographic normalization of the historical source material is\npursued. This report proposes a normalization system for German literary texts\nfrom c. 1700-1900, trained on a parallel corpus. The proposed system makes use\nof a machine learning approach using Transformer language models, combining an\nencoder-decoder model to normalize individual word types, and a pre-trained\ncausal language model to adjust these normalizations within their context. An\nextensive evaluation shows that the proposed system provides state-of-the-art\naccuracy, comparable with a much larger fully end-to-end sentence-based\nnormalization system, fine-tuning a pre-trained Transformer large language\nmodel. However, the normalization of historical text remains a challenge due to\ndifficulties for models to generalize, and the lack of extensive high-quality\nparallel data.\n","authors":["Anton Ehrmanntraut"],"pdf_url":"https://arxiv.org/pdf/2409.02841v2.pdf","comment":"27 pages, 3 figures; minor editorial changes"},{"id":"http://arxiv.org/abs/2412.02674v2","updated":"2025-02-25T16:59:11Z","published":"2024-12-03T18:47:26Z","title":"Mind the Gap: Examining the Self-Improvement Capabilities of Large\n  Language Models","summary":"  Self-improvement is a mechanism in Large Language Model (LLM) pre-training,\npost-training and test-time inference. We explore a framework where the model\nverifies its own outputs, filters or reweights data based on this verification,\nand distills the filtered data. Despite several empirical successes, a\nfundamental understanding is still lacking. In this work, we initiate a\ncomprehensive, modular and controlled study on LLM self-improvement. We provide\na mathematical formulation for self-improvement, which is largely governed by a\nquantity which we formalize as the generation-verification gap. Through\nexperiments with various model families and tasks, we discover a scaling\nphenomenon of self-improvement -- a variant of the generation-verification gap\nscales monotonically with the model pre-training flops. We also examine when\nself-improvement is possible, an iterative self-improvement procedure, and ways\nto improve its performance. Our findings not only advance understanding of LLM\nself-improvement with practical implications, but also open numerous avenues\nfor future research into its capabilities and boundaries.\n","authors":["Yuda Song","Hanlin Zhang","Carson Eisenach","Sham Kakade","Dean Foster","Udaya Ghai"],"pdf_url":"https://arxiv.org/pdf/2412.02674v2.pdf","comment":"ICLR 2025; 41 pages, 19 figures"},{"id":"http://arxiv.org/abs/2502.18353v1","updated":"2025-02-25T16:44:10Z","published":"2025-02-25T16:44:10Z","title":"DBR: Divergence-Based Regularization for Debiasing Natural Language\n  Understanding Models","summary":"  Pre-trained language models (PLMs) have achieved impressive results on\nvarious natural language processing tasks. However, recent research has\nrevealed that these models often rely on superficial features and shortcuts\ninstead of developing a genuine understanding of language, especially for\nnatural language understanding (NLU) tasks. Consequently, the models struggle\nto generalize to out-of-domain data. In this work, we propose Divergence Based\nRegularization (DBR) to mitigate this shortcut learning behavior. Our method\nmeasures the divergence between the output distributions for original examples\nand examples where shortcut tokens have been masked. This process prevents the\nmodel's predictions from being overly influenced by shortcut features or\nbiases. We evaluate our model on three NLU tasks and find that it improves\nout-of-domain performance with little loss of in-domain accuracy. Our results\ndemonstrate that reducing the reliance on shortcuts and superficial features\ncan enhance the generalization ability of large pre-trained language models.\n","authors":["Zihao Li","Ruixiang Tang","Lu Cheng","Shuaiqiang Wang","Dawei Yin","Mengnan Du"],"pdf_url":"https://arxiv.org/pdf/2502.18353v1.pdf","comment":"Accepted by SIGKDD Explorations"},{"id":"http://arxiv.org/abs/2501.13042v2","updated":"2025-02-25T16:41:36Z","published":"2025-01-22T17:44:01Z","title":"Does Table Source Matter? Benchmarking and Improving Multimodal\n  Scientific Table Understanding and Reasoning","summary":"  Recent large language models (LLMs) have advanced table understanding\ncapabilities but rely on converting tables into text sequences. While\nmultimodal large language models (MLLMs) enable direct visual processing, they\nface limitations in handling scientific tables due to fixed input image\nresolutions and insufficient numerical reasoning capabilities. We present a\ncomprehensive framework for multimodal scientific table understanding and\nreasoning with dynamic input image resolutions. Our framework consists of three\nkey components: (1) MMSci-Pre, a domain-specific table structure learning\ndataset of 52K scientific table structure recognition samples, (2) MMSci-Ins,\nan instruction tuning dataset with 12K samples across three table-based tasks,\nand (3) MMSci-Eval, a benchmark with 3,114 testing samples specifically\ndesigned to evaluate numerical reasoning capabilities. Extensive experiments\ndemonstrate that our domain-specific approach with 52K scientific table images\nachieves superior performance compared to 150K general-domain tables,\nhighlighting the importance of data quality over quantity. Our proposed\ntable-based MLLMs with dynamic input resolutions show significant improvements\nin both general table understanding and numerical reasoning capabilities, with\nstrong generalisation to held-out datasets. Our code and data are publicly\navailable at https://github.com/Bernard-Yang/MMSci_Table.\n","authors":["Bohao Yang","Yingji Zhang","Dong Liu","André Freitas","Chenghua Lin"],"pdf_url":"https://arxiv.org/pdf/2501.13042v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.18342v1","updated":"2025-02-25T16:33:50Z","published":"2025-02-25T16:33:50Z","title":"BRIDO: Bringing Democratic Order to Abstractive Summarization","summary":"  Hallucination refers to the inaccurate, irrelevant, and inconsistent text\ngenerated from large language models (LLMs). While the LLMs have shown great\npromise in a variety of tasks, the issue of hallucination still remains a major\nchallenge for many practical uses. In this paper, we tackle the issue of\nhallucination in abstract text summarization by mitigating exposure bias.\nExisting models targeted for exposure bias mitigation, namely BRIO, aim for\nbetter summarization quality in the ROUGE score. We propose a model that uses a\nsimilar exposure bias mitigation strategy but with a goal that is aligned with\nless hallucination. We conjecture that among a group of candidate outputs, ones\nwith hallucinations will comprise the minority of the whole group. That is,\ncandidates with less similarity with others will have a higher chance of\ncontaining hallucinated content. Our method uses this aspect and utilizes\ncontrastive learning, incentivizing candidates with high inter-candidate ROUGE\nscores. We performed experiments on the XSum and CNN/DM summarization datasets,\nand our method showed 6.25% and 3.82% improvement, respectively, on the\nconsistency G-Eval score over BRIO.\n","authors":["Junhyun Lee","Harshith Goka","Hyeonmok Ko"],"pdf_url":"https://arxiv.org/pdf/2502.18342v1.pdf","comment":"13 pages, 1 figure; AAAI-25 Workshop on PDLM camera ready"},{"id":"http://arxiv.org/abs/2406.17962v5","updated":"2025-02-25T16:30:21Z","published":"2024-06-25T22:44:17Z","title":"Crafting Customisable Characters with LLMs: Introducing SimsChat, a\n  Persona-Driven Role-Playing Agent Framework","summary":"  Large Language Models (LLMs) demonstrate remarkable ability to comprehend\ninstructions and generate human-like text, enabling sophisticated agent\nsimulation beyond basic behavior replication. However, the potential for\ncreating freely customisable characters remains underexplored. We introduce the\nCustomisable Conversation Agent Framework, which employs LLMs to simulate\nreal-world characters through personalised characteristic feature injection,\nenabling diverse character creation according to user preferences. We propose\nthe SimsConv dataset, comprising 68 customised characters and 13,971 multi-turn\nrole-playing dialogues across 1,360 real-world scenes. Characters are initially\ncustomised using pre-defined elements (career, aspiration, traits, skills),\nthen expanded through personal and social profiles. Building on this, we\npresent SimsChat, a freely customisable role-playing agent incorporating\nvarious realistic settings and topic-specified character interactions.\nExperimental results on both SimsConv and WikiRoleEval datasets demonstrate\nSimsChat's superior performance in maintaining character consistency, knowledge\naccuracy, and appropriate question rejection compared to existing models. Our\nframework provides valuable insights for developing more accurate and\ncustomisable human simulacra. Our data and code are publicly available at\nhttps://github.com/Bernard-Yang/SimsChat.\n","authors":["Bohao Yang","Dong Liu","Chenghao Xiao","Kun Zhao","Chen Tang","Chao Li","Lin Yuan","Guang Yang","Lanxiao Huang","Chenghua Lin"],"pdf_url":"https://arxiv.org/pdf/2406.17962v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.18318v1","updated":"2025-02-25T16:11:40Z","published":"2025-02-25T16:11:40Z","title":"Mapping of Subjective Accounts into Interpreted Clusters (MOSAIC): Topic\n  Modelling and LLM applied to Stroboscopic Phenomenology","summary":"  Stroboscopic light stimulation (SLS) on closed eyes typically induces simple\nvisual hallucinations (VHs), characterised by vivid, geometric and colourful\npatterns. A dataset of 862 sentences, extracted from 422 open subjective\nreports, was recently compiled as part of the Dreamachine programme (Collective\nAct, 2022), an immersive multisensory experience that combines SLS and spatial\nsound in a collective setting. Although open reports extend the range of\nreportable phenomenology, their analysis presents significant challenges,\nparticularly in systematically identifying patterns. To address this challenge,\nwe implemented a data-driven approach leveraging Large Language Models and\nTopic Modelling to uncover and interpret latent experiential topics directly\nfrom the Dreamachine's text-based reports. Our analysis confirmed the presence\nof simple VHs typically documented in scientific studies of SLS, while also\nrevealing experiences of altered states of consciousness and complex\nhallucinations. Building on these findings, our computational approach expands\nthe systematic study of subjective experience by enabling data-driven analyses\nof open-ended phenomenological reports, capturing experiences not readily\nidentified through standard questionnaires. By revealing rich and multifaceted\naspects of experiences, our study broadens our understanding of\nstroboscopically-induced phenomena while highlighting the potential of Natural\nLanguage Processing and Large Language Models in the emerging field of\ncomputational (neuro)phenomenology. More generally, this approach provides a\npractically applicable methodology for uncovering subtle hidden patterns of\nsubjective experience across diverse research domains.\n","authors":["Romy Beauté","David J. Schwartzman","Guillaume Dumas","Jennifer Crook","Fiona Macpherson","Adam B. Barrett","Anil K. Seth"],"pdf_url":"https://arxiv.org/pdf/2502.18318v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.18316v1","updated":"2025-02-25T16:09:38Z","published":"2025-02-25T16:09:38Z","title":"WiCkeD: A Simple Method to Make Multiple Choice Benchmarks More\n  Challenging","summary":"  We introduce WiCkeD, a simple method to increase the complexity of existing\nmultiple-choice benchmarks by randomly replacing a choice with \"None of the\nabove\", a method often used in educational tests. We show that WiCkeD can be\nautomatically applied to any existing benchmark, making it more challenging. We\napply WiCkeD to 6 popular benchmarks and use it to evaluate 18 open-weight\nLLMs. The performance of the models drops 12.1 points on average with respect\nto the original versions of the datasets. When using chain-of-thought on 3 MMLU\ndatasets, the performance drop for the WiCkeD variant is similar to the one\nobserved when using the LLMs directly, showing that WiCkeD is also challenging\nfor models with enhanced reasoning abilities. WiCkeD also uncovers that some\nmodels are more sensitive to the extra reasoning required, providing additional\ninformation with respect to the original benchmarks. We relase our code and\ndata at https://github.com/ahmedselhady/wicked-benchmarks.\n","authors":["Ahmed Elhady","Eneko Agirre","Mikel Artetxe"],"pdf_url":"https://arxiv.org/pdf/2502.18316v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.18313v1","updated":"2025-02-25T16:03:15Z","published":"2025-02-25T16:03:15Z","title":"Looking forward: Linguistic theory and methods","summary":"  This chapter examines current developments in linguistic theory and methods,\nfocusing on the increasing integration of computational, cognitive, and\nevolutionary perspectives. We highlight four major themes shaping contemporary\nlinguistics: (1) the explicit testing of hypotheses about symbolic\nrepresentation, such as efficiency, locality, and conceptual semantic\ngrounding; (2) the impact of artificial neural networks on theoretical debates\nand linguistic analysis; (3) the importance of intersubjectivity in linguistic\ntheory; and (4) the growth of evolutionary linguistics. By connecting\nlinguistics with computer science, psychology, neuroscience, and biology, we\nprovide a forward-looking perspective on the changing landscape of linguistic\nresearch.\n","authors":["John Mansfield","Ethan Gotlieb Wilcox"],"pdf_url":"https://arxiv.org/pdf/2502.18313v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.18308v1","updated":"2025-02-25T15:51:25Z","published":"2025-02-25T15:51:25Z","title":"RefuteBench 2.0 -- Agentic Benchmark for Dynamic Evaluation of LLM\n  Responses to Refutation Instruction","summary":"  In the multi-turn interaction schema, large language models (LLMs) can\nleverage user feedback to enhance the quality and relevance of their responses.\nHowever, evaluating an LLM's ability to incorporate user refutation feedback is\ncrucial yet challenging. In this study, we introduce RefuteBench 2.0, which\nsignificantly extends the original RefuteBench by incorporating LLM agents as\nrefuters and evaluators, which allows for flexible and comprehensive\nassessment.\n  We design both transient and persistent refutation instructions with\ndifferent validity periods. Meta-evaluation shows that the LLM-based refuter\ncould generate more human-like refutations and the evaluators could assign\nscores with high correlation with humans. Experimental results of various LLMs\nshow that current models could effectively satisfy the refutation but fail to\nmemorize the refutation information. Interestingly, we also observe that the\nperformance of the initial task decreases as the refutations increase. Analysis\nof the attention scores further shows a potential weakness of current LLMs:\nthey struggle to retain and correctly use previous information during long\ncontext dialogues. https://github.com/ElliottYan/RefuteBench-2.0\n","authors":["Jianhao Yan","Yun Luo","Yue Zhang"],"pdf_url":"https://arxiv.org/pdf/2502.18308v1.pdf","comment":"Work on progess"},{"id":"http://arxiv.org/abs/2502.02444v3","updated":"2025-02-25T15:40:09Z","published":"2025-02-04T16:10:55Z","title":"Generative Psycho-Lexical Approach for Constructing Value Systems in\n  Large Language Models","summary":"  Values are core drivers of individual and collective perception, cognition,\nand behavior. Value systems, such as Schwartz's Theory of Basic Human Values,\ndelineate the hierarchy and interplay among these values, enabling\ncross-disciplinary investigations into decision-making and societal dynamics.\nRecently, the rise of Large Language Models (LLMs) has raised concerns\nregarding their elusive intrinsic values. Despite growing efforts in\nevaluating, understanding, and aligning LLM values, a psychologically grounded\nLLM value system remains underexplored. This study addresses the gap by\nintroducing the Generative Psycho-Lexical Approach (GPLA), a scalable,\nadaptable, and theoretically informed method for constructing value systems.\nLeveraging GPLA, we propose a psychologically grounded five-factor value system\ntailored for LLMs. For systematic validation, we present three benchmarking\ntasks that integrate psychological principles with cutting-edge AI priorities.\nOur results reveal that the proposed value system meets standard psychological\ncriteria, better captures LLM values, improves LLM safety prediction, and\nenhances LLM alignment, when compared to the canonical Schwartz's values.\n","authors":["Haoran Ye","Tianze Zhang","Yuhang Xie","Liyuan Zhang","Yuanyi Ren","Xin Zhang","Guojie Song"],"pdf_url":"https://arxiv.org/pdf/2502.02444v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.18293v1","updated":"2025-02-25T15:29:51Z","published":"2025-02-25T15:29:51Z","title":"AMPO: Active Multi-Preference Optimization","summary":"  Multi-preference optimization enriches language-model alignment beyond\npairwise preferences by contrasting entire sets of helpful and undesired\nresponses, thereby enabling richer training signals for large language models.\nDuring self-play alignment, these models often produce numerous candidate\nanswers per query, rendering it computationally infeasible to include all\nresponses in the training objective. In this work, we propose $\\textit{Active\nMulti-Preference Optimization}$ (AMPO), a novel approach that combines\non-policy generation, a multi-preference group-contrastive loss, and active\nsubset selection. Specifically, we score and embed large candidate pools of\nresponses and then select a small, yet informative, subset that covers reward\nextremes and distinct semantic clusters for preference optimization. Our\ncontrastive training scheme is capable of identifying not only the best and\nworst answers but also subtle, underexplored modes that are crucial for robust\nalignment. Theoretically, we provide guarantees for expected reward\nmaximization using our active selection method, and empirically, AMPO achieves\nstate-of-the-art results on $\\textit{AlpacaEval}$ using Llama 8B.\n","authors":["Taneesh Gupta","Rahul Madhavan","Xuchao Zhang","Chetan Bansal","Saravan Rajmohan"],"pdf_url":"https://arxiv.org/pdf/2502.18293v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.18292v1","updated":"2025-02-25T15:29:07Z","published":"2025-02-25T15:29:07Z","title":"How Vital is the Jurisprudential Relevance: Law Article Intervened Legal\n  Case Retrieval and Matching","summary":"  Legal case retrieval (LCR) aims to automatically scour for comparable legal\ncases based on a given query, which is crucial for offering relevant precedents\nto support the judgment in intelligent legal systems. Due to similar goals, it\nis often associated with a similar case matching (LCM) task. To address them, a\ndaunting challenge is assessing the uniquely defined legal-rational similarity\nwithin the judicial domain, which distinctly deviates from the semantic\nsimilarities in general text retrieval. Past works either tagged\ndomain-specific factors or incorporated reference laws to capture\nlegal-rational information. However, their heavy reliance on expert or\nunrealistic assumptions restricts their practical applicability in real-world\nscenarios. In this paper, we propose an end-to-end model named LCM-LAI to solve\nthe above challenges. Through meticulous theoretical analysis, LCM-LAI employs\na dependent multi-task learning framework to capture legal-rational information\nwithin legal cases by a law article prediction (LAP) sub-task, without any\nadditional assumptions in inference. Besides, LCM-LAI proposes an article-aware\nattention mechanism to evaluate the legal-rational similarity between\nacross-case sentences based on law distribution, which is more effective than\nconventional semantic similarity. Weperform a series of exhaustive experiments\nincluding two different tasks involving four real-world datasets. Results\ndemonstrate that LCM-LAI achieves state-of-the-art performance.\n","authors":["Nuo Xu","Pinghui Wang","Zi Liang","Junzhou Zhao","Xiaohong Guan"],"pdf_url":"https://arxiv.org/pdf/2502.18292v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.04665v2","updated":"2025-02-25T15:20:58Z","published":"2024-08-06T14:53:25Z","title":"LLM-based MOFs Synthesis Condition Extraction using Few-Shot\n  Demonstrations","summary":"  The extraction of Metal-Organic Frameworks (MOFs) synthesis route from\nliterature has been crucial for the logical MOFs design with desirable\nfunctionality. The recent advent of large language models (LLMs) provides\ndisruptively new solution to this long-standing problem. While the latest\nresearches mostly stick to primitive zero-shot LLMs lacking specialized\nmaterial knowledge, we introduce in this work the few-shot LLM in-context\nlearning paradigm. First, a human-AI interactive data curation approach is\nproposed to secure high-quality demonstrations. Second, an information\nretrieval algorithm is applied to pick and quantify few-shot demonstrations for\neach extraction. Over three datasets randomly sampled from nearly 90,000\nwell-defined MOFs, we conduct triple evaluations to validate our method. The\nsynthesis extraction, structure inference, and material design performance of\nthe proposed few-shot LLMs all significantly outplay zero-shot LLM and baseline\nmethods. The lab-synthesized material guided by LLM surpasses 91.1%\nhigh-quality MOFs of the same class reported in the literature, on the key\nphysical property of specific surface area.\n","authors":["Lei Shi","Zhimeng Liu","Yi Yang","Weize Wu","Yuyang Zhang","Hongbo Zhang","Jing Lin","Siyu Wu","Zihan Chen","Ruiming Li","Nan Wang","Zipeng Liu","Huobin Tan","Hongyi Gao","Yue Zhang","Ge Wang"],"pdf_url":"https://arxiv.org/pdf/2408.04665v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.18285v1","updated":"2025-02-25T15:19:21Z","published":"2025-02-25T15:19:21Z","title":"Uncertainty Modeling in Multimodal Speech Analysis Across the Psychosis\n  Spectrum","summary":"  Capturing subtle speech disruptions across the psychosis spectrum is\nchallenging because of the inherent variability in speech patterns. This\nvariability reflects individual differences and the fluctuating nature of\nsymptoms in both clinical and non-clinical populations. Accounting for\nuncertainty in speech data is essential for predicting symptom severity and\nimproving diagnostic precision. Speech disruptions characteristic of psychosis\nappear across the spectrum, including in non-clinical individuals. We develop\nan uncertainty-aware model integrating acoustic and linguistic features to\npredict symptom severity and psychosis-related traits. Quantifying uncertainty\nin specific modalities allows the model to address speech variability,\nimproving prediction accuracy. We analyzed speech data from 114 participants,\nincluding 32 individuals with early psychosis and 82 with low or high\nschizotypy, collected through structured interviews, semi-structured\nautobiographical tasks, and narrative-driven interactions in German. The model\nimproved prediction accuracy, reducing RMSE and achieving an F1-score of 83%\nwith ECE = 4.5e-2, showing robust performance across different interaction\ncontexts. Uncertainty estimation improved model interpretability by identifying\nreliability differences in speech markers such as pitch variability, fluency\ndisruptions, and spectral instability. The model dynamically adjusted to task\nstructures, weighting acoustic features more in structured settings and\nlinguistic features in unstructured contexts. This approach strengthens early\ndetection, personalized assessment, and clinical decision-making in\npsychosis-spectrum research.\n","authors":["Morteza Rohanian","Roya M. Hüppi","Farhad Nooralahzadeh","Noemi Dannecker","Yves Pauli","Werner Surbeck","Iris Sommer","Wolfram Hinzen","Nicolas Langer","Michael Krauthammer","Philipp Homan"],"pdf_url":"https://arxiv.org/pdf/2502.18285v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.18282v1","updated":"2025-02-25T15:16:17Z","published":"2025-02-25T15:16:17Z","title":"Better Aligned with Survey Respondents or Training Data? Unveiling\n  Political Leanings of LLMs on U.S. Supreme Court Cases","summary":"  The increased adoption of Large Language Models (LLMs) and their potential to\nshape public opinion have sparked interest in assessing these models' political\nleanings. Building on previous research that compared LLMs and human opinions\nand observed political bias in system responses, we take a step further to\ninvestigate the underlying causes of such biases by empirically examining how\nthe values and biases embedded in training corpora shape model outputs.\nSpecifically, we propose a method to quantitatively evaluate political leanings\nembedded in the large pretraining corpora. Subsequently we investigate to whom\nare the LLMs' political leanings more aligned with, their pretrainig corpora or\nthe surveyed human opinions. As a case study, we focus on probing the political\nleanings of LLMs in 32 U.S. Supreme Court cases, addressing contentious topics\nsuch as abortion and voting rights. Our findings reveal that LLMs strongly\nreflect the political leanings in their training data, and no strong\ncorrelation is observed with their alignment to human opinions as expressed in\nsurveys. These results underscore the importance of responsible curation of\ntraining data and the need for robust evaluation metrics to ensure LLMs'\nalignment with human-centered values.\n","authors":["Shanshan Xu","T. Y. S. S Santosh","Yanai Elazar","Quirin Vogel","Barbara Plank","Matthias Grabmair"],"pdf_url":"https://arxiv.org/pdf/2502.18282v1.pdf","comment":"under review"},{"id":"http://arxiv.org/abs/2410.14141v2","updated":"2025-02-25T15:13:08Z","published":"2024-10-18T03:26:06Z","title":"Coherence-Driven Multimodal Safety Dialogue with Active Learning for\n  Embodied Agents","summary":"  When assisting people in daily tasks, robots need to accurately interpret\nvisual cues and respond effectively in diverse safety-critical situations, such\nas sharp objects on the floor. In this context, we present M-CoDAL, a\nmultimodal-dialogue system specifically designed for embodied agents to better\nunderstand and communicate in safety-critical situations. The system leverages\ndiscourse coherence relations to enhance its contextual understanding and\ncommunication abilities. To train this system, we introduce a novel\nclustering-based active learning mechanism that utilizes an external Large\nLanguage Model (LLM) to identify informative instances. Our approach is\nevaluated using a newly created multimodal dataset comprising 1K safety\nviolations extracted from 2K Reddit images. These violations are annotated\nusing a Large Multimodal Model (LMM) and verified by human annotators. Results\nwith this dataset demonstrate that our approach improves resolution of safety\nsituations, user sentiment, as well as safety of the conversation. Next, we\ndeploy our dialogue system on a Hello Robot Stretch robot and conduct a\nwithin-subject user study with real-world participants. In the study,\nparticipants role-play two safety scenarios with different levels of severity\nwith the robot and receive interventions from our model and a baseline system\npowered by OpenAI's ChatGPT. The study results corroborate and extend the\nfindings from the automated evaluation, showing that our proposed system is\nmore persuasive in a real-world embodied agent setting.\n","authors":["Sabit Hassan","Hye-Young Chung","Xiang Zhi Tan","Malihe Alikhani"],"pdf_url":"https://arxiv.org/pdf/2410.14141v2.pdf","comment":"To appear at AAMAS, 2025"},{"id":"http://arxiv.org/abs/2411.11853v3","updated":"2025-02-25T15:10:34Z","published":"2024-11-01T08:56:17Z","title":"Chat Bankman-Fried: an Exploration of LLM Alignment in Finance","summary":"  Advancements in large language models (LLMs) have renewed concerns about AI\nalignment - the consistency between human and AI goals and values. As various\njurisdictions enact legislation on AI safety, the concept of alignment must be\ndefined and measured across different domains. This paper proposes an\nexperimental framework to assess whether LLMs adhere to ethical and legal\nstandards in the relatively unexplored context of finance. We prompt twelve\nLLMs to impersonate the CEO of a financial institution and test their\nwillingness to misuse customer assets to repay outstanding corporate debt.\nBeginning with a baseline configuration, we adjust preferences, incentives and\nconstraints, analyzing the impact of each adjustment with logistic regression.\nOur findings reveal significant heterogeneity in the baseline propensity for\nunethical behavior of LLMs. Factors such as risk aversion, profit expectations,\nand regulatory environment consistently influence misalignment in ways\npredicted by economic theory, although the magnitude of these effects varies\nacross LLMs. This paper highlights both the benefits and limitations of\nsimulation-based, ex post safety testing. While it can inform financial\nauthorities and institutions aiming to ensure LLM safety, there is a clear\ntrade-off between generality and cost.\n","authors":["Claudia Biancotti","Carolina Camassa","Andrea Coletta","Oliver Giudice","Aldo Glielmo"],"pdf_url":"https://arxiv.org/pdf/2411.11853v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.18277v1","updated":"2025-02-25T15:07:40Z","published":"2025-02-25T15:07:40Z","title":"Self-Adjust Softmax","summary":"  The softmax function is crucial in Transformer attention, which normalizes\neach row of the attention scores with summation to one, achieving superior\nperformances over other alternative functions. However, the softmax function\ncan face a gradient vanishing issue when some elements of the attention scores\napproach extreme values, such as probabilities close to one or zero. In this\npaper, we propose Self-Adjust Softmax (SA-Softmax) to address this issue by\nmodifying $softmax(x)$ to $x \\cdot softmax(x)$ and its normalized variant\n$\\frac{(x - min(x_{\\min},0))}{max(0,x_{max})-min(x_{min},0)} \\cdot softmax(x)$.\nWe theoretically show that SA-Softmax provides enhanced gradient properties\ncompared to the vanilla softmax function. Moreover, SA-Softmax Attention can be\nseamlessly integrated into existing Transformer models to their attention\nmechanisms with minor adjustments. We conducted experiments to evaluate the\nempirical performance of Transformer models using SA-Softmax compared to the\nvanilla softmax function. These experiments, involving models with up to 2.7\nbillion parameters, are conducted across diverse datasets, language tasks, and\npositional encoding methods.\n","authors":["Chuanyang Zheng","Yihang Gao","Guoxuan Chen","Han Shi","Jing Xiong","Xiaozhe Ren","Chao Huang","Xin Jiang","Zhenguo Li","Yu Li"],"pdf_url":"https://arxiv.org/pdf/2502.18277v1.pdf","comment":"Tech Report"},{"id":"http://arxiv.org/abs/2502.18274v1","updated":"2025-02-25T15:05:12Z","published":"2025-02-25T15:05:12Z","title":"Citrus: Leveraging Expert Cognitive Pathways in a Medical Language Model\n  for Advanced Medical Decision Support","summary":"  Large language models (LLMs), particularly those with reasoning capabilities,\nhave rapidly advanced in recent years, demonstrating significant potential\nacross a wide range of applications. However, their deployment in healthcare,\nespecially in disease reasoning tasks, is hindered by the challenge of\nacquiring expert-level cognitive data. In this paper, we introduce Citrus, a\nmedical language model that bridges the gap between clinical expertise and AI\nreasoning by emulating the cognitive processes of medical experts. The model is\ntrained on a large corpus of simulated expert disease reasoning data,\nsynthesized using a novel approach that accurately captures the decision-making\npathways of clinicians. This approach enables Citrus to better simulate the\ncomplex reasoning processes involved in diagnosing and treating medical\nconditions.To further address the lack of publicly available datasets for\nmedical reasoning tasks, we release the last-stage training data, including a\ncustom-built medical diagnostic dialogue dataset. This open-source contribution\naims to support further research and development in the field. Evaluations\nusing authoritative benchmarks such as MedQA, covering tasks in medical\nreasoning and language understanding, show that Citrus achieves superior\nperformance compared to other models of similar size. These results highlight\nCitrus potential to significantly enhance medical decision support systems,\nproviding a more accurate and efficient tool for clinical decision-making.\n","authors":["Guoxin Wang","Minyu Gao","Shuai Yang","Ya Zhang","Lizhi He","Liang Huang","Hanlin Xiao","Yexuan Zhang","Wanyue Li","Lu Chen","Jintao Fei","Xin Li"],"pdf_url":"https://arxiv.org/pdf/2502.18274v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.18273v1","updated":"2025-02-25T15:04:17Z","published":"2025-02-25T15:04:17Z","title":"Beyond In-Distribution Success: Scaling Curves of CoT Granularity for\n  Language Model Generalization","summary":"  Generalization to novel compound tasks under distribution shift is important\nfor deploying transformer-based language models (LMs). This work investigates\nChain-of-Thought (CoT) reasoning as a means to enhance OOD generalization.\nThrough controlled experiments across several compound tasks, we reveal three\nkey insights: (1) While QA-trained models achieve near-perfect in-distribution\naccuracy, their OOD performance degrades catastrophically, even with 10000k+\ntraining examples; (2) the granularity of CoT data strongly correlates with\ngeneralization performance; finer-grained CoT data leads to better\ngeneralization; (3) CoT exhibits remarkable sample efficiency, matching QA\nperformance with much less (even 80%) data.\n  Theoretically, we demonstrate that compound tasks inherently permit shortcuts\nin Q-A data that misalign with true reasoning principles, while CoT forces\ninternalization of valid dependency structures, and thus can achieve better\ngeneralization. Further, we show that transformer positional embeddings can\namplify generalization by emphasizing subtask condition recurrence in long CoT\nsequences. Our combined theoretical and empirical analysis provides compelling\nevidence for CoT reasoning as a crucial training paradigm for enabling LM\ngeneralization under real-world distributional shifts for compound tasks.\n","authors":["Ru Wang","Wei Huang","Selena Song","Haoyu Zhang","Yusuke Iwasawa","Yutaka Matsuo","Jiaxian Guo"],"pdf_url":"https://arxiv.org/pdf/2502.18273v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.01386v2","updated":"2025-02-25T14:57:43Z","published":"2025-02-03T14:21:42Z","title":"Topic-FlipRAG: Topic-Orientated Adversarial Opinion Manipulation Attacks\n  to Retrieval-Augmented Generation Models","summary":"  Retrieval-Augmented Generation (RAG) systems based on Large Language Models\n(LLMs) have become essential for tasks such as question answering and content\ngeneration. However, their increasing impact on public opinion and information\ndissemination has made them a critical focus for security research due to\ninherent vulnerabilities. Previous studies have predominantly addressed attacks\ntargeting factual or single-query manipulations. In this paper, we address a\nmore practical scenario: topic-oriented adversarial opinion manipulation\nattacks on RAG models, where LLMs are required to reason and synthesize\nmultiple perspectives, rendering them particularly susceptible to systematic\nknowledge poisoning. Specifically, we propose Topic-FlipRAG, a two-stage\nmanipulation attack pipeline that strategically crafts adversarial\nperturbations to influence opinions across related queries. This approach\ncombines traditional adversarial ranking attack techniques and leverages the\nextensive internal relevant knowledge and reasoning capabilities of LLMs to\nexecute semantic-level perturbations. Experiments show that the proposed\nattacks effectively shift the opinion of the model's outputs on specific\ntopics, significantly impacting user information perception. Current mitigation\nmethods cannot effectively defend against such attacks, highlighting the\nnecessity for enhanced safeguards for RAG systems, and offering crucial\ninsights for LLM security research.\n","authors":["Yuyang Gong","Zhuo Chen","Miaokun Chen","Fengchang Yu","Wei Lu","Xiaofeng Wang","Xiaozhong Liu","Jiawei Liu"],"pdf_url":"https://arxiv.org/pdf/2502.01386v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.14660v2","updated":"2025-02-25T14:49:33Z","published":"2024-05-23T14:57:52Z","title":"Implicit In-context Learning","summary":"  In-context Learning (ICL) empowers large language models (LLMs) to swiftly\nadapt to unseen tasks at inference-time by prefixing a few demonstration\nexamples before queries. Despite its versatility, ICL incurs substantial\ncomputational and memory overheads compared to zero-shot learning and is\nsensitive to the selection and order of demonstration examples. In this work,\nwe introduce Implicit In-context Learning (I2CL), an innovative paradigm that\nreduces the inference cost of ICL to that of zero-shot learning with minimal\ninformation loss. I2CL operates by first generating a condensed vector\nrepresentation, namely a context vector, extracted from the demonstration\nexamples. It then conducts an inference-time intervention through injecting a\nlinear combination of the context vector and query activations back into the\nmodel's residual streams. Empirical evaluation on nine real-world tasks across\nthree model architectures demonstrates that I2CL achieves few-shot level\nperformance at zero-shot inference cost, and it exhibits robustness against\nvariations in demonstration examples. Furthermore, I2CL facilitates a novel\nrepresentation of task-ids, enhancing task similarity detection and fostering\neffective transfer learning. We also perform a comprehensive analysis and\nablation study on I2CL, offering deeper insights into its internal mechanisms.\nCode is available at https://github.com/LzVv123456/I2CL.\n","authors":["Zhuowei Li","Zihao Xu","Ligong Han","Yunhe Gao","Song Wen","Di Liu","Hao Wang","Dimitris N. Metaxas"],"pdf_url":"https://arxiv.org/pdf/2405.14660v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.16022v2","updated":"2025-02-25T14:34:15Z","published":"2025-02-22T00:50:01Z","title":"Enhancing LLMs for Identifying and Prioritizing Important Medical\n  Jargons from Electronic Health Record Notes Utilizing Data Augmentation","summary":"  OpenNotes enables patients to access EHR notes, but medical jargon can hinder\ncomprehension. To improve understanding, we evaluated closed- and open-source\nLLMs for extracting and prioritizing key medical terms using prompting,\nfine-tuning, and data augmentation. We assessed LLMs on 106 expert-annotated\nEHR notes, experimenting with (i) general vs. structured prompts, (ii)\nzero-shot vs. few-shot prompting, (iii) fine-tuning, and (iv) data\naugmentation. To enhance open-source models in low-resource settings, we used\nChatGPT for data augmentation and applied ranking techniques. We incrementally\nincreased the augmented dataset size (10 to 10,000) and conducted 5-fold\ncross-validation, reporting F1 score and Mean Reciprocal Rank (MRR). Our result\nshow that fine-tuning and data augmentation improved performance over other\nstrategies. GPT-4 Turbo achieved the highest F1 (0.433), while Mistral7B with\ndata augmentation had the highest MRR (0.746). Open-source models, when\nfine-tuned or augmented, outperformed closed-source models. Notably, the best\nF1 and MRR scores did not always align. Few-shot prompting outperformed\nzero-shot in vanilla models, and structured prompts yielded different\npreferences across models. Fine-tuning improved zero-shot performance but\nsometimes degraded few-shot performance. Data augmentation performed comparably\nor better than other methods. Our evaluation highlights the effectiveness of\nprompting, fine-tuning, and data augmentation in improving model performance\nfor medical jargon extraction in low-resource scenarios.\n","authors":["Won Seok Jang","Sharmin Sultana","Zonghai Yao","Hieu Tran","Zhichao Yang","Sunjae Kwon","Hong Yu"],"pdf_url":"https://arxiv.org/pdf/2502.16022v2.pdf","comment":"21pages, 5 figures, 4 tables"},{"id":"http://arxiv.org/abs/2502.18249v1","updated":"2025-02-25T14:33:50Z","published":"2025-02-25T14:33:50Z","title":"Iterative Counterfactual Data Augmentation","summary":"  Counterfactual data augmentation (CDA) is a method for controlling\ninformation or biases in training datasets by generating a complementary\ndataset with typically opposing biases. Prior work often either relies on\nhand-crafted rules or algorithmic CDA methods which can leave unwanted\ninformation in the augmented dataset. In this work, we show iterative CDA\n(ICDA) with initial, high-noise interventions can converge to a state with\nsignificantly lower noise. Our ICDA procedure produces a dataset where one\ntarget signal in the training dataset maintains high mutual information with a\ncorresponding label and the information of spurious signals are reduced. We\nshow training on the augmented datasets produces rationales on documents that\nbetter align with human annotation. Our experiments include six human produced\ndatasets and two large-language model generated datasets.\n","authors":["Mitchell Plyler","Min Chi"],"pdf_url":"https://arxiv.org/pdf/2502.18249v1.pdf","comment":"AAAI 2025"},{"id":"http://arxiv.org/abs/2411.12118v3","updated":"2025-02-25T14:26:58Z","published":"2024-11-18T23:12:13Z","title":"Mechanism and Emergence of Stacked Attention Heads in Multi-Layer\n  Transformers","summary":"  In this paper, I introduce the retrieval problem, a simple yet common\nreasoning task that can be solved only by transformers with a minimum number of\nlayers, which grows logarithmically with the input size. I empirically show\nthat large language models can solve the task under different prompting\nformulations without any fine-tuning. To understand how transformers solve the\nretrieval problem, I train several transformers on a minimal formulation.\nSuccessful learning occurs only under the presence of an implicit curriculum. I\nuncover the learned mechanisms by studying the attention maps in the trained\ntransformers. I also study the training process, uncovering that attention\nheads always emerge in a specific sequence guided by the implicit curriculum.\n","authors":["Tiberiu Musat"],"pdf_url":"https://arxiv.org/pdf/2411.12118v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.18228v1","updated":"2025-02-25T14:13:03Z","published":"2025-02-25T14:13:03Z","title":"Debt Collection Negotiations with Large Language Models: An Evaluation\n  System and Optimizing Decision Making with Multi-Agent","summary":"  Debt collection negotiations (DCN) are vital for managing non-performing\nloans (NPLs) and reducing creditor losses. Traditional methods are\nlabor-intensive, while large language models (LLMs) offer promising automation\npotential. However, prior systems lacked dynamic negotiation and real-time\ndecision-making capabilities. This paper explores LLMs in automating DCN and\nproposes a novel evaluation framework with 13 metrics across 4 aspects. Our\nexperiments reveal that LLMs tend to over-concede compared to human\nnegotiators. To address this, we propose the Multi-Agent Debt Negotiation\n(MADeN) framework, incorporating planning and judging modules to improve\ndecision rationality. We also apply post-training techniques, including DPO\nwith rejection sampling, to optimize performance. Our studies provide valuable\ninsights for practitioners and researchers seeking to enhance efficiency and\noutcomes in this domain.\n","authors":["Xiaofeng Wang","Zhixin Zhang","Jinguang Zheng","Yiming Ai","Rui Wang"],"pdf_url":"https://arxiv.org/pdf/2502.18228v1.pdf","comment":"21 pages"},{"id":"http://arxiv.org/abs/2502.18215v1","updated":"2025-02-25T14:00:15Z","published":"2025-02-25T14:00:15Z","title":"Connecting Voices: LoReSpeech as a Low-Resource Speech Parallel Corpus","summary":"  Aligned audio corpora are fundamental to NLP technologies such as ASR and\nspeech translation, yet they remain scarce for underrepresented languages,\nhindering their technological integration. This paper introduces a methodology\nfor constructing LoReSpeech, a low-resource speech-to-speech translation\ncorpus. Our approach begins with LoReASR, a sub-corpus of short audios aligned\nwith their transcriptions, created through a collaborative platform. Building\non LoReASR, long-form audio recordings, such as biblical texts, are aligned\nusing tools like the MFA. LoReSpeech delivers both intra- and inter-language\nalignments, enabling advancements in multilingual ASR systems, direct\nspeech-to-speech translation models, and linguistic preservation efforts, while\nfostering digital inclusivity. This work is conducted within Tutlayt AI project\n(https://tutlayt.fr).\n","authors":["Samy Ouzerrout"],"pdf_url":"https://arxiv.org/pdf/2502.18215v1.pdf","comment":"ISCA/ITG Workshop on Diversity in Large Speech and Language Models"},{"id":"http://arxiv.org/abs/2502.18209v1","updated":"2025-02-25T13:54:03Z","published":"2025-02-25T13:54:03Z","title":"LAG: LLM agents for Leaderboard Auto Generation on Demanding","summary":"  This paper introduces Leaderboard Auto Generation (LAG), a novel and\nwell-organized framework for automatic generation of leaderboards on a given\nresearch topic in rapidly evolving fields like Artificial Intelligence (AI).\nFaced with a large number of AI papers updated daily, it becomes difficult for\nresearchers to track every paper's proposed methods, experimental results, and\nsettings, prompting the need for efficient automatic leaderboard construction.\nWhile large language models (LLMs) offer promise in automating this process,\nchallenges such as multi-document summarization, leaderboard generation, and\nexperiment fair comparison still remain under exploration. LAG solves these\nchallenges through a systematic approach that involves the paper collection,\nexperiment results extraction and integration, leaderboard generation, and\nquality evaluation. Our contributions include a comprehensive solution to the\nleaderboard construction problem, a reliable evaluation method, and\nexperimental results showing the high quality of leaderboards.\n","authors":["Jian Wu","Jiayu Zhang","Dongyuan Li","Linyi Yang","Aoxiao Zhong","Renhe Jiang","Qingsong Wen","Yue Zhang"],"pdf_url":"https://arxiv.org/pdf/2502.18209v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.15790v2","updated":"2025-02-25T13:48:03Z","published":"2024-09-24T06:36:56Z","title":"Small Language Models: Survey, Measurements, and Insights","summary":"  Small language models (SLMs), despite their widespread adoption in modern\nsmart devices, have received significantly less academic attention compared to\ntheir large language model (LLM) counterparts, which are predominantly deployed\nin data centers and cloud environments. While researchers continue to improve\nthe capabilities of LLMs in the pursuit of artificial general intelligence, SLM\nresearch aims to make machine intelligence more accessible, affordable, and\nefficient for everyday tasks. Focusing on transformer-based, decoder-only\nlanguage models with 100M-5B parameters, we survey 70 state-of-the-art\nopen-source SLMs, analyzing their technical innovations across three axes:\narchitectures, training datasets, and training algorithms. In addition, we\nevaluate their capabilities in various domains, including commonsense\nreasoning, in-context learning, mathematics, and coding. To gain further\ninsight into their on-device runtime costs, we benchmark their inference\nlatency and memory footprints. Through in-depth analysis of our benchmarking\ndata, we offer valuable insights to advance research in this field.\n","authors":["Zhenyan Lu","Xiang Li","Dongqi Cai","Rongjie Yi","Fangming Liu","Xiwen Zhang","Nicholas D. Lane","Mengwei Xu"],"pdf_url":"https://arxiv.org/pdf/2409.15790v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.18205v1","updated":"2025-02-25T13:44:49Z","published":"2025-02-25T13:44:49Z","title":"Grandes modelos de lenguaje: de la predicción de palabras a la\n  comprensión?","summary":"  Large language models, such as the well-known ChatGPT, have brought about an\nunexpected revolution in the field of artificial intelligence. On the one hand,\nthey have numerous practical applications and enormous potential still to be\nexplored. On the other hand, they are also the subject of debate from\nscientific, philosophical, and social perspectives: there are doubts about the\nexact mechanisms of their functioning and their actual capacity for language\ncomprehension, and their applications raise ethical dilemmas. In this chapter,\nwe describe how this technology has been developed and the fundamentals of its\noperation, allowing us to better understand its capabilities and limitations\nand to introduce some of the main debates surrounding its development and use.\n  --\n  Los grandes modelos de lenguaje, como el conocido ChatGPT, han supuesto una\ninesperada revoluci\\'on en el \\'ambito de la inteligencia artificial. Por un\nlado, cuentan con multitud de aplicaciones pr\\'acticas y un enorme potencial\ntodav\\'ia por explorar. Por otro lado, son tambi\\'en objeto de debate, tanto\ndesde el punto de vista cient\\'ifico y filos\\'ofico como social: hay dudas\nsobre los mecanismos exactos de su funcionamiento y su capacidad real de\ncomprensi\\'on del lenguaje, y sus aplicaciones plantean dilemas \\'eticos. En\neste cap\\'itulo describimos c\\'omo se ha llegado a esta tecnolog\\'ia y los\nfundamentos de su funcionamiento, permiti\\'endonos as\\'i comprender mejor sus\ncapacidades y limitaciones e introducir algunos de los principales debates que\nrodean su desarrollo y uso.\n","authors":["Carlos Gómez-Rodríguez"],"pdf_url":"https://arxiv.org/pdf/2502.18205v1.pdf","comment":"26 pages, in Spanish. Chapter from book \"La Inteligencia Artificial\n  hoy y sus aplicaciones con Big Data\", (Amparo Alonso Betanzos, Daniel Pe\\~na\n  y Pilar Poncela, eds.). Publisher: Funcas. ISBN 978-84-17609-94-8"},{"id":"http://arxiv.org/abs/2502.18186v1","updated":"2025-02-25T13:26:25Z","published":"2025-02-25T13:26:25Z","title":"Steering Language Model to Stable Speech Emotion Recognition via\n  Contextual Perception and Chain of Thought","summary":"  Large-scale audio language models (ALMs), such as Qwen2-Audio, are capable of\ncomprehending diverse audio signal, performing audio analysis and generating\ntextual responses. However, in speech emotion recognition (SER), ALMs often\nsuffer from hallucinations, resulting in misclassifications or irrelevant\noutputs. To address these challenges, we propose C$^2$SER, a novel ALM designed\nto enhance the stability and accuracy of SER through Contextual perception and\nChain of Thought (CoT). C$^2$SER integrates the Whisper encoder for semantic\nperception and Emotion2Vec-S for acoustic perception, where Emotion2Vec-S\nextends Emotion2Vec with semi-supervised learning to enhance emotional\ndiscrimination. Additionally, C$^2$SER employs a CoT approach, processing SER\nin a step-by-step manner while leveraging speech content and speaking styles to\nimprove recognition. To further enhance stability, C$^2$SER introduces\nself-distillation from explicit CoT to implicit CoT, mitigating error\naccumulation and boosting recognition accuracy. Extensive experiments show that\nC$^2$SER outperforms existing popular ALMs, such as Qwen2-Audio and SECap,\ndelivering more stable and precise emotion recognition. We release the training\ncode, checkpoints, and test sets to facilitate further research.\n","authors":["Zhixian Zhao","Xinfa Zhu","Xinsheng Wang","Shuiyuan Wang","Xuelong Geng","Wenjie Tian","Lei Xie"],"pdf_url":"https://arxiv.org/pdf/2502.18186v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.10245v4","updated":"2025-02-25T13:17:12Z","published":"2024-09-16T12:55:14Z","title":"From Text to Emoji: How PEFT-Driven Personality Manipulation Unleashes\n  the Emoji Potential in LLMs","summary":"  The manipulation of the personality traits of large language models (LLMs)\nhas emerged as a key area of research. Methods like prompt-based In-Context\nKnowledge Editing (IKE) and gradient-based Model Editor Networks (MEND) have\nbeen explored but show irregularity and variability; IKE depends on the prompt,\nleading to variability and sensitivity, while MEND yields inconsistent and\ngibberish outputs. To address this, we employed Opinion QA Based\nParameter-Efficient Fine-Tuning (PEFT), specifically Quantized Low-Rank\nAdaptation (QLoRA), to manipulate the Big Five personality traits: Openness,\nConscientiousness, Extraversion, Agreeableness, and Neuroticism. After PEFT,\nmodels such as Mistral-7B-Instruct and LLaMA-2-7B-chat showed a latent\nbehaviour by generating emojis for certain traits, despite no emojis being\npresent in the PEFT data. For instance, LLaMA-2-7B-chat generated emojis in\n99.5\\% of extraversion-related test instances, while Mistral-7B-Instruct did so\nin 92.5\\% of openness-related test instances. ICL Explainability analysis\nindicated that the LLMs used emojis intentionally to express these traits.\nMechanistic Interpretability analysis showed that this latent behaviour of LLMs\ncould be traced to specific neurons that became activated or amplified after\nPEFT. This paper provides a number of novel contributions. First, introducing\nan Opinion QA dataset for PEFT-driven personality manipulation; second,\ndeveloping metric models to benchmark LLM personality traits; third,\ndemonstrating PEFT's superiority over IKE in personality manipulation; and\nfinally, analysing and validating emoji usage through explainability methods\nsuch as Mechanistic Interpretability and In-context learning Explainability\nmethods.\n","authors":["Navya Jain","Zekun Wu","Cristian Munoz","Airlie Hilliard","Xin Guan","Adriano Koshiyama","Emre Kazim","Philip Treleaven"],"pdf_url":"https://arxiv.org/pdf/2409.10245v4.pdf","comment":"Findings paper of NAACL 2025 and NeurIPS 2024 Workshop on Behavioral\n  Machine Learning"},{"id":"http://arxiv.org/abs/2502.18179v1","updated":"2025-02-25T13:11:53Z","published":"2025-02-25T13:11:53Z","title":"Problem Solved? Information Extraction Design Space for Layout-Rich\n  Documents using LLMs","summary":"  This paper defines and explores the design space for information extraction\n(IE) from layout-rich documents using large language models (LLMs). The three\ncore challenges of layout-aware IE with LLMs are 1) data structuring, 2) model\nengagement, and 3) output refinement. Our study delves into the sub-problems\nwithin these core challenges, such as input representation, chunking,\nprompting, and selection of LLMs and multimodal models. It examines the\noutcomes of different design choices through a new layout-aware IE test suite,\nbenchmarking against the state-of-art (SoA) model LayoutLMv3. The results show\nthat the configuration from one-factor-at-a-time (OFAT) trial achieves\nnear-optimal results with 14.1 points F1-score gain from the baseline model,\nwhile full factorial exploration yields only a slightly higher 15.1 points gain\nat around 36x greater token usage. We demonstrate that well-configured\ngeneral-purpose LLMs can match the performance of specialized models, providing\na cost-effective alternative. Our test-suite is freely available at\nhttps://github.com/gayecolakoglu/LayIE-LLM.\n","authors":["Gaye Colakoglu","Gürkan Solmaz","Jonathan Fürst"],"pdf_url":"https://arxiv.org/pdf/2502.18179v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.05647v2","updated":"2025-02-25T13:10:08Z","published":"2025-01-10T01:27:12Z","title":"Collaboration of Large Language Models and Small Recommendation Models\n  for Device-Cloud Recommendation","summary":"  Large Language Models (LLMs) for Recommendation (LLM4Rec) is a promising\nresearch direction that has demonstrated exceptional performance in this field.\nHowever, its inability to capture real-time user preferences greatly limits the\npractical application of LLM4Rec because (i) LLMs are costly to train and infer\nfrequently, and (ii) LLMs struggle to access real-time data (its large number\nof parameters poses an obstacle to deployment on devices). Fortunately, small\nrecommendation models (SRMs) can effectively supplement these shortcomings of\nLLM4Rec diagrams by consuming minimal resources for frequent training and\ninference, and by conveniently accessing real-time data on devices.\n  In light of this, we designed the Device-Cloud LLM-SRM Collaborative\nRecommendation Framework (LSC4Rec) under a device-cloud collaboration setting.\nLSC4Rec aims to integrate the advantages of both LLMs and SRMs, as well as the\nbenefits of cloud and edge computing, achieving a complementary synergy. We\nenhance the practicability of LSC4Rec by designing three strategies:\ncollaborative training, collaborative inference, and intelligent request.\nDuring training, LLM generates candidate lists to enhance the ranking ability\nof SRM in collaborative scenarios and enables SRM to update adaptively to\ncapture real-time user interests. During inference, LLM and SRM are deployed on\nthe cloud and on the device, respectively. LLM generates candidate lists and\ninitial ranking results based on user behavior, and SRM get reranking results\nbased on the candidate list, with final results integrating both LLM's and\nSRM's scores. The device determines whether a new candidate list is needed by\ncomparing the consistency of the LLM's and SRM's sorted lists. Our\ncomprehensive and extensive experimental analysis validates the effectiveness\nof each strategy in LSC4Rec.\n","authors":["Zheqi Lv","Tianyu Zhan","Wenjie Wang","Xinyu Lin","Shengyu Zhang","Wenqiao Zhang","Jiwei Li","Kun Kuang","Fei Wu"],"pdf_url":"https://arxiv.org/pdf/2501.05647v2.pdf","comment":"Published on KDD'25: Proceedings of the ACM SIGKDD Conference on\n  Knowledge Discovery and Data Mining 2025"},{"id":"http://arxiv.org/abs/2502.18168v1","updated":"2025-02-25T13:00:05Z","published":"2025-02-25T13:00:05Z","title":"SECURA: Sigmoid-Enhanced CUR Decomposition with Uninterrupted Retention\n  and Low-Rank Adaptation in Large Language Models","summary":"  With the rapid development of large language models (LLMs), fully fine-tuning\n(FT) these models has become increasingly impractical due to the high\ncomputational demands. Additionally, FT can lead to catastrophic forgetting. As\nan alternative, Low-Rank Adaptation (LoRA) has been proposed, which fine-tunes\nonly a small subset of parameters, achieving similar performance to FT while\nsignificantly reducing resource requirements. However, since LoRA inherits FT's\ndesign, the issue of catastrophic forgetting remains.\n  To address these challenges, we propose SECURA: Sigmoid-Enhanced CUR\nDecomposition LoRA, a novel parameter-efficient fine-tuning (PEFT) variant that\nmitigates catastrophic forgetting while improving fine-tuning performance. Our\nmethod introduces a new normalization technique, SigNorm, to enhance parameter\nretention and overall performance.\n  SECURA has been evaluated on a variety of tasks, including mathematical\nproblem-solving (GSM8K), challenging question-answering (CNNDM), translation\n(NewsDE), and complex multiple-choice reasoning (LogiQA). Experimental results\nshow that SECURA achieves an average fine-tuning improvement of 3.59% across\nfour multiple-choice question (MCQ) tasks and a 2.51% improvement across five\nquestion-answering (QA) tasks on models such as Gemma2 2b, Qwen2 1.5b, Qwen 2\n7b, Llama3 8b, and Llama3.1 8b, compared to DoRA. Moreover, SECURA demonstrates\nsuperior knowledge retention capabilities, maintaining more than 70% accuracy\non basic LLM knowledge across 16 continual learning tests, outperforming\nExperience Replay (ER), Sequential Learning (SEQ), EWC, I-LoRA, and CUR-LoRA.\n","authors":["Zhang Yuxuan","Li Ruizhe"],"pdf_url":"https://arxiv.org/pdf/2502.18168v1.pdf","comment":"New work on Parameter-Efficient Fine-Tuning (PEFT) for large language\n  models. Includes new techniques SigNorm and CABR-LoRA for optimizing\n  fine-tune performance and Knowledge retention"},{"id":"http://arxiv.org/abs/2410.20533v3","updated":"2025-02-25T12:59:55Z","published":"2024-10-27T17:55:27Z","title":"Guiding Through Complexity: What Makes Good Supervision for Hard Math\n  Reasoning Tasks?","summary":"  How can \"weak teacher models\" such as average human annotators or existing AI\nsystems, effectively supervise LLMs to improve performance on hard reasoning\ntasks, especially those that challenge and requires expertise or daily practice\nfrom the teacher models? In this paper, we seek for empirical answers to this\nquestion by investigating various data-driven strategies that offer supervision\ndata at different quality levels upon tasks of varying complexity. Two\nintuitive strategies emerge for teacher models to provide supervision during\nalignment training: 1) using lower-quality supervision from complete tasks that\nmatch the difficulty of the target reasoning tasks, and 2) leveraging\nhigher-quality supervision from easier subtasks that are less challenging.\nInterestingly, we find that even when the outcome error rate for hard task\nsupervision is high (e.g., 90\\%), training on such data can outperform\nperfectly correct supervision of easier subtasks on multiple hard math\nbenchmarks. We further identify a more critical factor influencing training\nperformance: step-wise error rates, which indicate the severity of errors in\nsolutions. Specifically, training on hard task supervision with the same\noutcome error rates but disparate step-wise error rates can lead to a 30\\%\naccuracy gap on MATH benchmark. Our results also reveal that supplementing hard\ntask supervision with the corresponding subtask supervision can yield notable\nperformance improvements than simply combining rephrased hard full task\nsupervision, suggesting new avenues for data augmentation. Data and code are\nreleased at https://github.com/hexuan21/Weak-to-Strong.\n","authors":["Xuan He","Da Yin","Nanyun Peng"],"pdf_url":"https://arxiv.org/pdf/2410.20533v3.pdf","comment":"NAACL 2025 Main"},{"id":"http://arxiv.org/abs/2412.01621v3","updated":"2025-02-25T12:59:42Z","published":"2024-12-02T15:41:47Z","title":"NYT-Connections: A Deceptively Simple Text Classification Task that\n  Stumps System-1 Thinkers","summary":"  Large Language Models (LLMs) have shown impressive performance on various\nbenchmarks, yet their ability to engage in deliberate reasoning remains\nquestionable. We present NYT-Connections, a collection of 358 simple word\nclassification puzzles derived from the New York Times Connections game. This\nbenchmark is designed to penalize quick, intuitive \"System 1\" thinking,\nisolating fundamental reasoning skills. We evaluated six recent LLMs, a simple\nmachine learning heuristic, and humans across three configurations:\nsingle-attempt, multiple attempts without hints, and multiple attempts with\ncontextual hints. Our findings reveal a significant performance gap: even\ntop-performing LLMs like GPT-4 fall short of human performance by nearly 30%.\nNotably, advanced prompting techniques such as Chain-of-Thought and\nSelf-Consistency show diminishing returns as task difficulty increases.\nNYT-Connections uniquely combines linguistic isolation, resistance to intuitive\nshortcuts, and regular updates to mitigate data leakage, offering a novel tool\nfor assessing LLM reasoning capabilities.\n","authors":["Angel Yahir Loredo Lopez","Tyler McDonald","Ali Emami"],"pdf_url":"https://arxiv.org/pdf/2412.01621v3.pdf","comment":"5 pages (excluding references), Published at Coling 2025, Best\n  Dataset Paper Award"},{"id":"http://arxiv.org/abs/2410.03777v2","updated":"2025-02-25T12:40:51Z","published":"2024-10-03T08:42:38Z","title":"Determine-Then-Ensemble: Necessity of Top-k Union for Large Language\n  Model Ensembling","summary":"  Large language models (LLMs) exhibit varying strengths and weaknesses across\ndifferent tasks, prompting recent studies to explore the benefits of ensembling\nmodels to leverage their complementary advantages. However, existing LLM\nensembling methods often overlook model compatibility and struggle with\ninefficient alignment of probabilities across the entire vocabulary. In this\nstudy, we empirically investigate the factors influencing ensemble performance,\nidentifying model performance, vocabulary size, and response style as key\ndeterminants, revealing that compatibility among models is essential for\neffective ensembling. This analysis leads to the development of a simple yet\neffective model selection strategy that identifies compatible models.\nAdditionally, we introduce the \\textsc{Uni}on \\textsc{T}op-$k$\n\\textsc{E}nsembling (\\textsc{UniTE}), a novel approach that efficiently\ncombines models by focusing on the union of the top-k tokens from each model,\nthereby avoiding the need for full vocabulary alignment and reducing\ncomputational overhead. Extensive evaluations across multiple benchmarks\ndemonstrate that \\textsc{UniTE} significantly enhances performance compared to\nexisting methods, offering a more efficient framework for LLM ensembling.\n","authors":["Yuxuan Yao","Han Wu","Mingyang Liu","Sichun Luo","Xiongwei Han","Jie Liu","Zhijiang Guo","Linqi Song"],"pdf_url":"https://arxiv.org/pdf/2410.03777v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.18156v1","updated":"2025-02-25T12:40:41Z","published":"2025-02-25T12:40:41Z","title":"Can LLMs Explain Themselves Counterfactually?","summary":"  Explanations are an important tool for gaining insights into the behavior of\nML models, calibrating user trust and ensuring regulatory compliance. Past few\nyears have seen a flurry of post-hoc methods for generating model explanations,\nmany of which involve computing model gradients or solving specially designed\noptimization problems. However, owing to the remarkable reasoning abilities of\nLarge Language Model (LLMs), self-explanation, that is, prompting the model to\nexplain its outputs has recently emerged as a new paradigm. In this work, we\nstudy a specific type of self-explanations, self-generated counterfactual\nexplanations (SCEs). We design tests for measuring the efficacy of LLMs in\ngenerating SCEs. Analysis over various LLM families, model sizes, temperature\nsettings, and datasets reveals that LLMs sometimes struggle to generate SCEs.\nEven when they do, their prediction often does not agree with their own\ncounterfactual reasoning.\n","authors":["Zahra Dehghanighobadi","Asja Fischer","Muhammad Bilal Zafar"],"pdf_url":"https://arxiv.org/pdf/2502.18156v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.17163v2","updated":"2025-02-25T12:39:53Z","published":"2025-02-24T13:58:42Z","title":"MEMERAG: A Multilingual End-to-End Meta-Evaluation Benchmark for\n  Retrieval Augmented Generation","summary":"  Automatic evaluation of retrieval augmented generation (RAG) systems relies\non fine-grained dimensions like faithfulness and relevance, as judged by expert\nhuman annotators. Meta-evaluation benchmarks support the development of\nautomatic evaluators that correlate well with human judgement. However,\nexisting benchmarks predominantly focus on English or use translated data,\nwhich fails to capture cultural nuances. A native approach provides a better\nrepresentation of the end user experience.\n  In this work, we develop a Multilingual End-to-end Meta-Evaluation RAG\nbenchmark (MEMERAG). Our benchmark builds on the popular MIRACL dataset, using\nnative-language questions and generating responses with diverse large language\nmodels (LLMs), which are then assessed by expert annotators for faithfulness\nand relevance. We describe our annotation process and show that it achieves\nhigh inter-annotator agreement. We then analyse the performance of the\nanswer-generating LLMs across languages as per the human evaluators. Finally we\napply the dataset to our main use-case which is to benchmark multilingual\nautomatic evaluators (LLM-as-a-judge). We show that our benchmark can reliably\nidentify improvements offered by advanced prompting techniques and LLMs. We\nwill release our benchmark to support the community developing accurate\nevaluation methods for multilingual RAG systems.\n","authors":["María Andrea Cruz Blandón","Jayasimha Talur","Bruno Charron","Dong Liu","Saab Mansour","Marcello Federico"],"pdf_url":"https://arxiv.org/pdf/2502.17163v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.07025v2","updated":"2025-02-25T12:35:17Z","published":"2024-10-09T16:07:11Z","title":"CheXalign: Preference fine-tuning in chest X-ray interpretation models\n  without human feedback","summary":"  Radiologists play a crucial role in translating medical images into\nactionable reports. However, the field faces staffing shortages and increasing\nworkloads. While automated approaches using vision-language models (VLMs) show\npromise as assistants, they require exceptionally high accuracy. Most current\nVLMs in radiology rely solely on supervised fine-tuning. Meanwhile, additional\npreference fine-tuning in the post-training pipeline has become standard\npractice in the general domain. The challenge in radiology lies in the\nprohibitive cost of obtaining radiologist feedback at scale. To address this\nchallenge, we propose an automated pipeline for preference feedback, focusing\non chest X-ray radiology report generation (RRG). Specifically, our method\nleverages publicly available datasets containing pairs of images and\nradiologist-written reference reports with reference-based metrics, or Judges,\neliminating the need for additional radiologist feedback. We investigate reward\noveroptimization via length exploitation in this setting and introduce a\nlength-controlled version of the GREEN score. Our best-performing setup\nachieves state-of-the-art CheXbert scores on the MIMIC-CXR dataset for the RRG\ntask while on average maintaining robust performance across six additional\nimage perception and reasoning tasks.\n","authors":["Dennis Hein","Zhihong Chen","Sophie Ostmeier","Justin Xu","Maya Varma","Eduardo Pontes Reis","Arne Edward Michalson","Christian Bluethgen","Hyun Joo Shin","Curtis Langlotz","Akshay S Chaudhari"],"pdf_url":"https://arxiv.org/pdf/2410.07025v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.18148v1","updated":"2025-02-25T12:23:52Z","published":"2025-02-25T12:23:52Z","title":"NusaAksara: A Multimodal and Multilingual Benchmark for Preserving\n  Indonesian Indigenous Scripts","summary":"  Indonesia is rich in languages and scripts. However, most NLP progress has\nbeen made using romanized text. In this paper, we present NusaAksara, a novel\npublic benchmark for Indonesian languages that includes their original scripts.\nOur benchmark covers both text and image modalities and encompasses diverse\ntasks such as image segmentation, OCR, transliteration, translation, and\nlanguage identification. Our data is constructed by human experts through\nrigorous steps. NusaAksara covers 8 scripts across 7 languages, including\nlow-resource languages not commonly seen in NLP benchmarks. Although\nunsupported by Unicode, the Lampung script is included in this dataset. We\nbenchmark our data across several models, from LLMs and VLMs such as GPT-4o,\nLlama 3.2, and Aya 23 to task-specific systems such as PP-OCR and LangID, and\nshow that most NLP technologies cannot handle Indonesia's local scripts, with\nmany achieving near-zero performance.\n","authors":["Muhammad Farid Adilazuarda","Musa Izzanardi Wijanarko","Lucky Susanto","Khumaisa Nur'aini","Derry Wijaya","Alham Fikri Aji"],"pdf_url":"https://arxiv.org/pdf/2502.18148v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.18147v1","updated":"2025-02-25T12:21:45Z","published":"2025-02-25T12:21:45Z","title":"Jacobian Sparse Autoencoders: Sparsify Computations, Not Just\n  Activations","summary":"  Sparse autoencoders (SAEs) have been successfully used to discover sparse and\nhuman-interpretable representations of the latent activations of LLMs. However,\nwe would ultimately like to understand the computations performed by LLMs and\nnot just their representations. The extent to which SAEs can help us understand\ncomputations is unclear because they are not designed to \"sparsify\"\ncomputations in any sense, only latent activations. To solve this, we propose\nJacobian SAEs (JSAEs), which yield not only sparsity in the input and output\nactivations of a given model component but also sparsity in the computation\n(formally, the Jacobian) connecting them. With a na\\\"ive implementation, the\nJacobians in LLMs would be computationally intractable due to their size. One\nkey technical contribution is thus finding an efficient way of computing\nJacobians in this setup. We find that JSAEs extract a relatively large degree\nof computational sparsity while preserving downstream LLM performance\napproximately as well as traditional SAEs. We also show that Jacobians are a\nreasonable proxy for computational sparsity because MLPs are approximately\nlinear when rewritten in the JSAE basis. Lastly, we show that JSAEs achieve a\ngreater degree of computational sparsity on pre-trained LLMs than on the\nequivalent randomized LLM. This shows that the sparsity of the computational\ngraph appears to be a property that LLMs learn through training, and suggests\nthat JSAEs might be more suitable for understanding learned transformer\ncomputations than standard SAEs.\n","authors":["Lucy Farnik","Tim Lawson","Conor Houghton","Laurence Aitchison"],"pdf_url":"https://arxiv.org/pdf/2502.18147v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.18139v1","updated":"2025-02-25T12:09:16Z","published":"2025-02-25T12:09:16Z","title":"LevelRAG: Enhancing Retrieval-Augmented Generation with Multi-hop Logic\n  Planning over Rewriting Augmented Searchers","summary":"  Retrieval-Augmented Generation (RAG) is a crucial method for mitigating\nhallucinations in Large Language Models (LLMs) and integrating external\nknowledge into their responses. Existing RAG methods typically employ query\nrewriting to clarify the user intent and manage multi-hop logic, while using\nhybrid retrieval to expand search scope. However, the tight coupling of query\nrewriting to the dense retriever limits its compatibility with hybrid\nretrieval, impeding further RAG performance improvements. To address this\nchallenge, we introduce a high-level searcher that decomposes complex queries\ninto atomic queries, independent of any retriever-specific optimizations.\nAdditionally, to harness the strengths of sparse retrievers for precise keyword\nretrieval, we have developed a new sparse searcher that employs Lucene syntax\nto enhance retrieval accuracy.Alongside web and dense searchers, these\ncomponents seamlessly collaborate within our proposed method,\n\\textbf{LevelRAG}. In LevelRAG, the high-level searcher orchestrates the\nretrieval logic, while the low-level searchers (sparse, web, and dense) refine\nthe queries for optimal retrieval. This approach enhances both the completeness\nand accuracy of the retrieval process, overcoming challenges associated with\ncurrent query rewriting techniques in hybrid retrieval scenarios. Empirical\nexperiments conducted on five datasets, encompassing both single-hop and\nmulti-hop question answering tasks, demonstrate the superior performance of\nLevelRAG compared to existing RAG methods. Notably, LevelRAG outperforms the\nstate-of-the-art proprietary model, GPT4o, underscoring its effectiveness and\npotential impact on the RAG field.\n","authors":["Zhuocheng Zhang","Yang Feng","Min Zhang"],"pdf_url":"https://arxiv.org/pdf/2502.18139v1.pdf","comment":"First submit"},{"id":"http://arxiv.org/abs/2501.01059v2","updated":"2025-02-25T12:07:02Z","published":"2025-01-02T05:07:06Z","title":"Dynamic Attention-Guided Context Decoding for Mitigating Context\n  Faithfulness Hallucinations in Large Language Models","summary":"  Large language models (LLMs) often exhibit Context Faithfulness\nHallucinations, where outputs deviate from retrieved information due to\nincomplete context integration. Our analysis reveals a strong correlation\nbetween token-level uncertainty and hallucinations. We hypothesize that\nattention mechanisms inherently encode context utilization signals, supported\nby probing analysis. Based on these insights, we propose Dynamic\nAttention-Guided Context Decoding (DAGCD), a lightweight framework that\nleverages attention distributions and uncertainty signals in a single-pass\ndecoding. Experiments on open-book QA datasets demonstrate DAGCD's\neffectiveness, yielding significant improvements in faithfulness and robustness\nwhile preserving computational efficiency.\n","authors":["Yanwen Huang","Yong Zhang","Ning Cheng","Zhitao Li","Shaojun Wang","Jing Xiao"],"pdf_url":"https://arxiv.org/pdf/2501.01059v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.13603v2","updated":"2025-02-25T12:06:25Z","published":"2025-02-19T10:33:18Z","title":"Efficient Safety Retrofitting Against Jailbreaking for LLMs","summary":"  Direct Preference Optimization (DPO) is an efficient alignment technique that\nsteers LLMs towards preferable outputs by training on preference data,\nbypassing the need for explicit reward models. Its simplicity enables easy\nadaptation to various domains and safety requirements. This paper examines\nDPO's effectiveness in model safety against jailbreaking attacks while\nminimizing data requirements and training costs. We introduce Egida, a dataset\nexpanded from multiple sources, which includes 27 different safety topics and\n18 different attack styles, complemented with synthetic and human labels. This\ndata is used to boost the safety of state-of-the-art LLMs\n(Llama-3.1-8B/70B-Instruct, Qwen-2.5-7B/72B-Instruct) across topics and attack\nstyles. In addition to safety evaluations, we assess their post-alignment\nperformance degradation in general purpose tasks, and their tendency to over\nrefusal. Following the proposed methodology, trained models reduce their Attack\nSuccess Rate by 10%-30%, using small training efforts (2,000 samples) with low\ncomputational cost (3\\$ for 8B models, 20\\$ for 72B models). Safety aligned\nmodels generalize to unseen topics and attack styles, with the most successful\nattack style reaching a success rate around 5%. Size and family are found to\nstrongly influence model malleability towards safety, pointing at the\nimportance of pre-training choices. To validate our findings, a large\nindependent assessment of human preference agreement with Llama-Guard-3-8B is\nconducted by the authors and the associated dataset Egida-HSafe is released.\nOverall, this study illustrates how affordable and accessible it is to enhance\nLLM safety using DPO while outlining its current limitations. All datasets and\nmodels are released to enable reproducibility and further research.\n","authors":["Dario Garcia-Gasulla","Adrian Tormos","Anna Arias-Duart","Daniel Hinjos","Oscar Molina-Sedano","Ashwin Kumar Gururajan","Maria Eugenia Cardello"],"pdf_url":"https://arxiv.org/pdf/2502.13603v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.00339v2","updated":"2025-02-25T11:53:48Z","published":"2024-12-31T08:22:21Z","title":"Rethinking Layer Removal: A Hybrid Pruning Framework Combining Layer\n  Removal and Singular Value Selection for Efficient LLM Compression","summary":"  Layer removal is an effective technique for compressing large language models\n(LLMs) by reducing redundancy and improving inference efficiency. However,\nindiscriminate pruning disrupts representation stability, leading to\nperformance degradation. We propose GRASP (Gradient-based Retention of Adaptive\nSingular Parameters), which preserves representation-critical singular values\nto mitigate these effects. Unlike direct layer removal, GRASP leverages\ngradient-based attribution on a syntax- and semantics-rich dataset to guide the\nselection of representation-critical singular values. By selectively applying\nsingular value decomposition (SVD) to affected layers, GRASP achieves efficient\ncompression while maintaining representation stability with minimal overhead.\nExperiments across multiple LLMs show that GRASP consistently outperforms\nexisting compression methods in perplexity and downstream task performance.\n","authors":["Kainan Liu","Yong Zhang","Ning Cheng","Zhitao Li","Shaojun Wang","Jing Xiao"],"pdf_url":"https://arxiv.org/pdf/2501.00339v2.pdf","comment":"16 pages, 5 figures"},{"id":"http://arxiv.org/abs/2502.18125v1","updated":"2025-02-25T11:47:32Z","published":"2025-02-25T11:47:32Z","title":"HyperG: Hypergraph-Enhanced LLMs for Structured Knowledge","summary":"  Given that substantial amounts of domain-specific knowledge are stored in\nstructured formats, such as web data organized through HTML, Large Language\nModels (LLMs) are expected to fully comprehend this structured information to\nbroaden their applications in various real-world downstream tasks. Current\napproaches for applying LLMs to structured data fall into two main categories:\nserialization-based and operation-based methods. Both approaches, whether\nrelying on serialization or using SQL-like operations as an intermediary,\nencounter difficulties in fully capturing structural relationships and\neffectively handling sparse data. To address these unique characteristics of\nstructured data, we propose HyperG, a hypergraph-based generation framework\naimed at enhancing LLMs' ability to process structured knowledge. Specifically,\nHyperG first augment sparse data with contextual information, leveraging the\ngenerative power of LLMs, and incorporate a prompt-attentive hypergraph\nlearning (PHL) network to encode both the augmented information and the\nintricate structural relationships within the data. To validate the\neffectiveness and generalization of HyperG, we conduct extensive experiments\nacross two different downstream tasks requiring structured knowledge.\n","authors":["Sirui Huang","Hanqian Li","Yanggan Gu","Xuming Hu","Qing Li","Guandong Xu"],"pdf_url":"https://arxiv.org/pdf/2502.18125v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.18116v1","updated":"2025-02-25T11:41:33Z","published":"2025-02-25T11:41:33Z","title":"Bayesian Optimization for Controlled Image Editing via LLMs","summary":"  In the rapidly evolving field of image generation, achieving precise control\nover generated content and maintaining semantic consistency remain significant\nlimitations, particularly concerning grounding techniques and the necessity for\nmodel fine-tuning. To address these challenges, we propose BayesGenie, an\noff-the-shelf approach that integrates Large Language Models (LLMs) with\nBayesian Optimization to facilitate precise and user-friendly image editing.\nOur method enables users to modify images through natural language descriptions\nwithout manual area marking, while preserving the original image's semantic\nintegrity. Unlike existing techniques that require extensive pre-training or\nfine-tuning, our approach demonstrates remarkable adaptability across various\nLLMs through its model-agnostic design. BayesGenie employs an adapted Bayesian\noptimization strategy to automatically refine the inference process parameters,\nachieving high-precision image editing with minimal user intervention. Through\nextensive experiments across diverse scenarios, we demonstrate that our\nframework significantly outperforms existing methods in both editing accuracy\nand semantic preservation, as validated using different LLMs including Claude3\nand GPT-4.\n","authors":["Chengkun Cai","Haoliang Liu","Xu Zhao","Zhongyu Jiang","Tianfang Zhang","Zongkai Wu","Jenq-Neng Hwang","Serge Belongie","Lei Li"],"pdf_url":"https://arxiv.org/pdf/2502.18116v1.pdf","comment":"8 figures"},{"id":"http://arxiv.org/abs/2307.00925v7","updated":"2025-02-25T11:27:10Z","published":"2023-07-03T10:53:05Z","title":"Automatic Design of Semantic Similarity Ensembles Using Grammatical\n  Evolution","summary":"  Semantic similarity measures are widely used in natural language processing\nto catalyze various computer-related tasks. However, no single semantic\nsimilarity measure is the most appropriate for all tasks, and researchers often\nuse ensemble strategies to ensure performance. This research work proposes a\nmethod for automatically designing semantic similarity ensembles. In fact, our\nproposed method uses grammatical evolution, for the first time, to\nautomatically select and aggregate measures from a pool of candidates to create\nan ensemble that maximizes correlation to human judgment. The method is\nevaluated on several benchmark datasets and compared to state-of-the-art\nensembles, showing that it can significantly improve similarity assessment\naccuracy and outperform existing methods in some cases. As a result, our\nresearch demonstrates the potential of using grammatical evolution to\nautomatically compare text and prove the benefits of using ensembles for\nsemantic similarity tasks. The source code that illustrates our approach can be\ndownloaded from https://github.com/jorge-martinez-gil/sesige.\n","authors":["Jorge Martinez-Gil"],"pdf_url":"https://arxiv.org/pdf/2307.00925v7.pdf","comment":"35 pages"},{"id":"http://arxiv.org/abs/2502.18108v1","updated":"2025-02-25T11:24:52Z","published":"2025-02-25T11:24:52Z","title":"Uncertainty Quantification in Retrieval Augmented Question Answering","summary":"  Retrieval augmented Question Answering (QA) helps QA models overcome\nknowledge gaps by incorporating retrieved evidence, typically a set of\npassages, alongside the question at test time. Previous studies show that this\napproach improves QA performance and reduces hallucinations, without, however,\nassessing whether the retrieved passages are indeed useful at answering\ncorrectly. In this work, we propose to quantify the uncertainty of a QA model\nvia estimating the utility of the passages it is provided with. We train a\nlightweight neural model to predict passage utility for a target QA model and\nshow that while simple information theoretic metrics can predict answer\ncorrectness up to a certain extent, our approach efficiently approximates or\noutperforms more expensive sampling-based methods. Code and data are available\nat https://github.com/lauhaide/ragu.\n","authors":["Laura Perez-Beltrachini","Mirella Lapata"],"pdf_url":"https://arxiv.org/pdf/2502.18108v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.18101v1","updated":"2025-02-25T11:15:49Z","published":"2025-02-25T11:15:49Z","title":"Detecting Offensive Memes with Social Biases in Singapore Context Using\n  Multimodal Large Language Models","summary":"  Traditional online content moderation systems struggle to classify modern\nmultimodal means of communication, such as memes, a highly nuanced and\ninformation-dense medium. This task is especially hard in a culturally diverse\nsociety like Singapore, where low-resource languages are used and extensive\nknowledge on local context is needed to interpret online content. We curate a\nlarge collection of 112K memes labeled by GPT-4V for fine-tuning a VLM to\nclassify offensive memes in Singapore context. We show the effectiveness of\nfine-tuned VLMs on our dataset, and propose a pipeline containing OCR,\ntranslation and a 7-billion parameter-class VLM. Our solutions reach 80.62%\naccuracy and 0.8192 AUROC on a held-out test set, and can greatly aid human in\nmoderating online contents. The dataset, code, and model weights will be\nopen-sourced at https://github.com/aliencaocao/vlm-for-memes-aisg.\n","authors":["Cao Yuxuan","Wu Jiayang","Alistair Cheong Liang Chuen","Bryan Shan Guanrong","Theodore Lee Chong Jen","Sherman Chann Zhi Shen"],"pdf_url":"https://arxiv.org/pdf/2502.18101v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.07238v2","updated":"2025-02-25T11:10:08Z","published":"2024-11-11T18:58:46Z","title":"OpenThaiGPT 1.5: A Thai-Centric Open Source Large Language Model","summary":"  OpenThaiGPT 1.5 is an advanced Thai language chat model based on Qwen v2.5,\nfinetuned on over 2,000,000 Thai instruction pairs. This report provides an\nengineering perspective on the model's development, capabilities, and\nperformance. We discuss the model's architecture, training process, and key\nfeatures, including multi-turn conversation support, Retrieval Augmented\nGeneration (RAG) compatibility, and tool-calling functionality. Benchmark\nresults demonstrate OpenThaiGPT 1.5's state-of-the-art performance on various\nThai language tasks, outperforming other open-source Thai language models. We\nalso address practical considerations such as GPU memory requirements and\ndeployment strategies.\n","authors":["Sumeth Yuenyong","Kobkrit Viriyayudhakorn","Apivadee Piyatumrong","Jillaphat Jaroenkantasima"],"pdf_url":"https://arxiv.org/pdf/2411.07238v2.pdf","comment":"8 pages, 4 tables"},{"id":"http://arxiv.org/abs/2405.11874v3","updated":"2025-02-25T11:04:02Z","published":"2024-05-20T08:30:13Z","title":"xFinder: Large Language Models as Automated Evaluators for Reliable\n  Evaluation","summary":"  The continuous advancement of large language models (LLMs) has brought\nincreasing attention to the critical issue of developing fair and reliable\nmethods for evaluating their performance. Particularly, the emergence of\ncheating phenomena, such as test set leakage and prompt format overfitting,\nposes significant challenges to the reliable evaluation of LLMs. As evaluation\nframeworks commonly use Regular Expression (RegEx) for answer extraction,\nmodels may adjust their responses to fit formats easily handled by RegEx.\nNevertheless, the key answer extraction module based on RegEx frequently\nsuffers from extraction errors. Furthermore, recent studies proposing\nfine-tuned LLMs as judge models for automated evaluation face challenges in\nterms of generalization ability and fairness. This paper comprehensively\nanalyzes the entire LLM evaluation chain and demonstrates that optimizing the\nkey answer extraction module improves extraction accuracy and enhances\nevaluation reliability. Our findings suggest that improving the key answer\nextraction module can lead to higher judgment accuracy and improved evaluation\nefficiency compared to the judge models. To address these issues, we propose\nxFinder, a novel evaluator for answer extraction and matching in LLM\nevaluation. As part of this process, we create a specialized dataset, the\n\\textbf{K}ey \\textbf{A}nswer \\textbf{F}inder (KAF) dataset, to ensure effective\nmodel training and evaluation. Generalization tests and real-world evaluations\nshow that the smallest xFinder model, with only 500 million parameters,\nachieves an average extraction accuracy of 93.42\\%. In contrast, RegEx accuracy\nin the best evaluation framework is 74.38\\%. The final judgment accuracy of\nxFinder reaches 97.61\\%, outperforming existing evaluation frameworks and judge\nmodels.\n","authors":["Qingchen Yu","Zifan Zheng","Shichao Song","Zhiyu Li","Feiyu Xiong","Bo Tang","Ding Chen"],"pdf_url":"https://arxiv.org/pdf/2405.11874v3.pdf","comment":"Accepted by ICLR 2025"},{"id":"http://arxiv.org/abs/2502.18080v1","updated":"2025-02-25T10:48:05Z","published":"2025-02-25T10:48:05Z","title":"Towards Thinking-Optimal Scaling of Test-Time Compute for LLM Reasoning","summary":"  Recent studies have shown that making a model spend more time thinking\nthrough longer Chain of Thoughts (CoTs) enables it to gain significant\nimprovements in complex reasoning tasks. While current researches continue to\nexplore the benefits of increasing test-time compute by extending the CoT\nlengths of Large Language Models (LLMs), we are concerned about a potential\nissue hidden behind the current pursuit of test-time scaling: Would excessively\nscaling the CoT length actually bring adverse effects to a model's reasoning\nperformance? Our explorations on mathematical reasoning tasks reveal an\nunexpected finding that scaling with longer CoTs can indeed impair the\nreasoning performance of LLMs in certain domains. Moreover, we discover that\nthere exists an optimal scaled length distribution that differs across\ndifferent domains. Based on these insights, we propose a Thinking-Optimal\nScaling strategy. Our method first uses a small set of seed data with varying\nresponse length distributions to teach the model to adopt different reasoning\nefforts for deep thinking. Then, the model selects its shortest correct\nresponse under different reasoning efforts on additional problems for\nself-improvement. Our self-improved models built upon Qwen2.5-32B-Instruct\noutperform other distillation-based 32B o1-like models across various math\nbenchmarks, and achieve performance on par with QwQ-32B-Preview.\n","authors":["Wenkai Yang","Shuming Ma","Yankai Lin","Furu Wei"],"pdf_url":"https://arxiv.org/pdf/2502.18080v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.00809v3","updated":"2025-02-25T10:42:40Z","published":"2024-10-23T16:16:15Z","title":"Adaptive Segment-level Reward: Bridging the Gap Between Action and\n  Reward Space in Alignment","summary":"  Reinforcement Learning (RL) has proven highly effective in aligning Large\nLanguage Models (LLMs) with human preferences. Typical RL methods optimize\nunder an overall sequence reward, which can lead to a suboptimal learning\nprocess. This reflects a key credit assignment problem: identifying which\ntokens to reinforce or suppress. To rectify these shortcomings, step-wise and\ntoken-wise methods have been proposed. However, step-wise methods rely on\npunctuation segmentation and still cannot accurately identify the key tokens.\nThe token-level approach is too fine-grained, attending to many unimportant\ntokens and thus introducing a large amount of noise. To assign more accurate\nrewards to different tokens, improving credit assignment, we propose the\n\"Adaptive Segment-wise Reward\" method. We employ semantic meaning, rather than\npunctuation, to adaptively delineate segments. Experiments demonstrate that our\nmethod can be integrated into various training methods. Compared to training\nmethods \\textit{without} our approach, our method improves the success rate on\nadversarial samples by 10\\%, and achieves a 1.3\\% improvement on evaluation\nbenchmarks such as MMLU, GSM8K, HumanEval, etc.\n","authors":["Yanshi Li","Shaopan Xiong","Gengru Chen","Xiaoyang Li","Yijia Luo","Xingyuan Bu","Yingshui Tan","Wenbo Su","Bo Zheng"],"pdf_url":"https://arxiv.org/pdf/2411.00809v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.14502v2","updated":"2025-02-25T10:37:01Z","published":"2025-02-20T12:31:03Z","title":"How Much Knowledge Can You Pack into a LoRA Adapter without Harming LLM?","summary":"  The performance of Large Language Models (LLMs) on many tasks is greatly\nlimited by the knowledge learned during pre-training and stored in the model's\nparameters. Low-rank adaptation (LoRA) is a popular and efficient training\ntechnique for updating or domain-specific adaptation of LLMs. In this study, we\ninvestigate how new facts can be incorporated into the LLM using LoRA without\ncompromising the previously learned knowledge. We fine-tuned\nLlama-3.1-8B-instruct using LoRA with varying amounts of new knowledge. Our\nexperiments have shown that the best results are obtained when the training\ndata contains a mixture of known and new facts. However, this approach is still\npotentially harmful because the model's performance on external\nquestion-answering benchmarks declines after such fine-tuning. When the\ntraining data is biased towards certain entities, the model tends to regress to\nfew overrepresented answers. In addition, we found that the model becomes more\nconfident and refuses to provide an answer in only few cases. These findings\nhighlight the potential pitfalls of LoRA-based LLM updates and underscore the\nimportance of training data composition and tuning parameters to balance new\nknowledge integration and general model capabilities.\n","authors":["Sergey Pletenev","Maria Marina","Daniil Moskovskiy","Vasily Konovalov","Pavel Braslavski","Alexander Panchenko","Mikhail Salnikov"],"pdf_url":"https://arxiv.org/pdf/2502.14502v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.18060v1","updated":"2025-02-25T10:28:16Z","published":"2025-02-25T10:28:16Z","title":"Defining bias in AI-systems: Biased models are fair models","summary":"  The debate around bias in AI systems is central to discussions on algorithmic\nfairness. However, the term bias often lacks a clear definition, despite\nfrequently being contrasted with fairness, implying that an unbiased model is\ninherently fair. In this paper, we challenge this assumption and argue that a\nprecise conceptualization of bias is necessary to effectively address fairness\nconcerns. Rather than viewing bias as inherently negative or unfair, we\nhighlight the importance of distinguishing between bias and discrimination. We\nfurther explore how this shift in focus can foster a more constructive\ndiscourse within academic debates on fairness in AI systems.\n","authors":["Chiara Lindloff","Ingo Siegert"],"pdf_url":"https://arxiv.org/pdf/2502.18060v1.pdf","comment":"ISCA/ITG Workshop on Diversity in Large Speech and Language Models"},{"id":"http://arxiv.org/abs/2410.18697v2","updated":"2025-02-25T10:20:16Z","published":"2024-10-24T12:48:03Z","title":"How Good Are LLMs for Literary Translation, Really? Literary Translation\n  Evaluation with Humans and LLMs","summary":"  Recent research has focused on literary machine translation (MT) as a new\nchallenge in MT. However, the evaluation of literary MT remains an open\nproblem. We contribute to this ongoing discussion by introducing\nLITEVAL-CORPUS, a paragraph-level parallel corpus containing verified human\ntranslations and outputs from 9 MT systems, which totals over 2k translations\nand 13k evaluated sentences across four language pairs, costing 4.5k C. This\ncorpus enables us to (i) examine the consistency and adequacy of human\nevaluation schemes with various degrees of complexity, (ii) compare evaluations\nby students and professionals, assess the effectiveness of (iii) LLM-based\nmetrics and (iv) LLMs themselves. Our findings indicate that the adequacy of\nhuman evaluation is controlled by two factors: the complexity of the evaluation\nscheme (more complex is less adequate) and the expertise of evaluators (higher\nexpertise yields more adequate evaluations). For instance, MQM\n(Multidimensional Quality Metrics), a complex scheme and the de facto standard\nfor non-literary human MT evaluation, is largely inadequate for literary\ntranslation evaluation: with student evaluators, nearly 60% of human\ntranslations are misjudged as indistinguishable or inferior to machine\ntranslations. In contrast, BWS (BEST-WORST SCALING), a much simpler scheme,\nidentifies human translations at a rate of 80-100%. Automatic metrics fare\ndramatically worse, with rates of at most 20%. Our overall evaluation indicates\nthat published human translations consistently outperform LLM translations,\nwhere even the most recent LLMs tend to produce considerably more literal and\nless diverse translations compared to humans.\n","authors":["Ran Zhang","Wei Zhao","Steffen Eger"],"pdf_url":"https://arxiv.org/pdf/2410.18697v2.pdf","comment":"NAACL Camera-Ready version"},{"id":"http://arxiv.org/abs/2404.09656v4","updated":"2025-02-25T10:19:35Z","published":"2024-04-15T10:44:31Z","title":"Learn Your Reference Model for Real Good Alignment","summary":"  Despite the fact that offline methods for Large Language Models (LLMs)\nalignment do not require a direct reward model, they remain susceptible to\noveroptimization. This issue arises when the trained model deviates excessively\nfrom the reference policy, leading to a decrease in sample quality. We propose\na new paradigm of offline alignment methods, called Trust Region (including\nvariants TR-DPO, TR-IPO, TR-KTO), which dynamically updates the reference\npolicy throughout the training process. Our results show that TR alignment\nmethods effectively mitigate overoptimization, enabling models to maintain\nstrong performance even when substantially deviating from the initial reference\npolicy. We demonstrate the efficacy of these approaches not only through toy\nexamples that exhibit reduced overoptimization, but also through direct,\nside-by-side comparisons in specific tasks such as helpful and harmless\ndialogue, as well as summarization, where they surpass conventional methods.\nAdditionally, we report significant improvements in general-purpose assistant\nsetups with the Llama3 model on the AlpacaEval 2 and Arena-Hard benchmarks,\nhighlighting the advantages of Trust Region methods over classical approaches.\n","authors":["Alexey Gorbatovski","Boris Shaposhnikov","Alexey Malakhov","Nikita Surnachev","Yaroslav Aksenov","Ian Maksimov","Nikita Balagansky","Daniil Gavrilov"],"pdf_url":"https://arxiv.org/pdf/2404.09656v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.18050v1","updated":"2025-02-25T10:15:21Z","published":"2025-02-25T10:15:21Z","title":"Uncertainty-aware abstention in medical diagnosis based on medical texts","summary":"  This study addresses the critical issue of reliability for AI-assisted\nmedical diagnosis. We focus on the selection prediction approach that allows\nthe diagnosis system to abstain from providing the decision if it is not\nconfident in the diagnosis. Such selective prediction (or abstention)\napproaches are usually based on the modeling predictive uncertainty of machine\nlearning models involved.\n  This study explores uncertainty quantification in machine learning models for\nmedical text analysis, addressing diverse tasks across multiple datasets. We\nfocus on binary mortality prediction from textual data in MIMIC-III,\nmulti-label medical code prediction using ICD-10 codes from MIMIC-IV, and\nmulti-class classification with a private outpatient visits dataset.\nAdditionally, we analyze mental health datasets targeting depression and\nanxiety detection, utilizing various text-based sources, such as essays, social\nmedia posts, and clinical descriptions.\n  In addition to comparing uncertainty methods, we introduce HUQ-2, a new\nstate-of-the-art method for enhancing reliability in selective prediction\ntasks. Our results provide a detailed comparison of uncertainty quantification\nmethods. They demonstrate the effectiveness of HUQ-2 in capturing and\nevaluating uncertainty, paving the way for more reliable and interpretable\napplications in medical text analysis.\n","authors":["Artem Vazhentsev","Ivan Sviridov","Alvard Barseghyan","Gleb Kuzmin","Alexander Panchenko","Aleksandr Nesterov","Artem Shelmanov","Maxim Panov"],"pdf_url":"https://arxiv.org/pdf/2502.18050v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.13396v2","updated":"2025-02-25T10:02:34Z","published":"2024-10-17T09:48:08Z","title":"Linguistically Grounded Analysis of Language Models using Shapley Head\n  Values","summary":"  Understanding how linguistic knowledge is encoded in language models is\ncrucial for improving their generalisation capabilities. In this paper, we\ninvestigate the processing of morphosyntactic phenomena, by leveraging a\nrecently proposed method for probing language models via Shapley Head Values\n(SHVs). Using the English language BLiMP dataset, we test our approach on two\nwidely used models, BERT and RoBERTa, and compare how linguistic constructions\nsuch as anaphor agreement and filler-gap dependencies are handled. Through\nquantitative pruning and qualitative clustering analysis, we demonstrate that\nattention heads responsible for processing related linguistic phenomena cluster\ntogether. Our results show that SHV-based attributions reveal distinct patterns\nacross both models, providing insights into how language models organize and\nprocess linguistic information. These findings support the hypothesis that\nlanguage models learn subnetworks corresponding to linguistic theory, with\npotential implications for cross-linguistic model analysis and interpretability\nin Natural Language Processing (NLP).\n","authors":["Marcell Fekete","Johannes Bjerva"],"pdf_url":"https://arxiv.org/pdf/2410.13396v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.05769v2","updated":"2025-02-25T09:57:48Z","published":"2023-11-09T22:28:14Z","title":"Are Chatbots Reliable Text Annotators? Sometimes","summary":"  Recent research highlights the significant potential of ChatGPT for text\nannotation in social science research. However, ChatGPT is a closed-source\nproduct which has major drawbacks with regards to transparency,\nreproducibility, cost, and data protection. Recent advances in open-source (OS)\nlarge language models (LLMs) offer an alternative without these drawbacks.\nThus, it is important to evaluate the performance of OS LLMs relative to\nChatGPT and standard approaches to supervised machine learning classification.\nWe conduct a systematic comparative evaluation of the performance of a range of\nOS LLMs alongside ChatGPT, using both zero- and few-shot learning as well as\ngeneric and custom prompts, with results compared to supervised classification\nmodels. Using a new dataset of tweets from US news media, and focusing on\nsimple binary text annotation tasks, we find significant variation in the\nperformance of ChatGPT and OS models across the tasks, and that the supervised\nclassifier using DistilBERT generally outperforms both. Given the unreliable\nperformance of ChatGPT and the significant challenges it poses to Open Science\nwe advise caution when using ChatGPT for substantive text annotation tasks.\n","authors":["Ross Deans Kristensen-McLachlan","Miceal Canavan","Márton Kardos","Mia Jacobsen","Lene Aarøe"],"pdf_url":"https://arxiv.org/pdf/2311.05769v2.pdf","comment":"Accepted for publication in PNAS Nexus (accepted Feb. 2025)"},{"id":"http://arxiv.org/abs/2502.18036v1","updated":"2025-02-25T09:48:53Z","published":"2025-02-25T09:48:53Z","title":"Harnessing Multiple Large Language Models: A Survey on LLM Ensemble","summary":"  LLM Ensemble -- which involves the comprehensive use of multiple large\nlanguage models (LLMs), each aimed at handling user queries during downstream\ninference, to benefit from their individual strengths -- has gained substantial\nattention recently. The widespread availability of LLMs, coupled with their\nvarying strengths and out-of-the-box usability, has profoundly advanced the\nfield of LLM Ensemble. This paper presents the first systematic review of\nrecent developments in LLM Ensemble. First, we introduce our taxonomy of LLM\nEnsemble and discuss several related research problems. Then, we provide a more\nin-depth classification of the methods under the broad categories of\n\"ensemble-before-inference, ensemble-during-inference,\nensemble-after-inference\", and review all relevant methods. Finally, we\nintroduce related benchmarks and applications, summarize existing studies, and\nsuggest several future research directions. A curated list of papers on LLM\nEnsemble is available at https://github.com/junchenzhi/Awesome-LLM-Ensemble.\n","authors":["Zhijun Chen","Jingzheng Li","Pengpeng Chen","Zhuoran Li","Kai Sun","Yuankai Luo","Qianren Mao","Dingqi Yang","Hailong Sun","Philip S. Yu"],"pdf_url":"https://arxiv.org/pdf/2502.18036v1.pdf","comment":"9 pages, 2 figures, codebase:\n  https://github.com/junchenzhi/Awesome-LLM-Ensemble"},{"id":"http://arxiv.org/abs/2409.15907v2","updated":"2025-02-25T09:36:41Z","published":"2024-09-24T09:24:03Z","title":"Enhancing Text-to-SQL Capabilities of Large Language Models via Domain\n  Database Knowledge Injection","summary":"  Text-to-SQL is a subtask in semantic parsing that has seen rapid progress\nwith the evolution of Large Language Models (LLMs). However, LLMs face\nchallenges due to hallucination issues and a lack of domain-specific database\nknowledge(such as table schema and cell values). As a result, they can make\nerrors in generating table names, columns, and matching values to the correct\ncolumns in SQL statements. This paper introduces a method of knowledge\ninjection to enhance LLMs' ability to understand schema contents by\nincorporating prior knowledge. This approach improves their performance in\nText-to-SQL tasks. Experimental results show that pre-training LLMs on\ndomain-specific database knowledge and fine-tuning them on downstream\nText-to-SQL tasks significantly improves the Execution Match (EX) and Exact\nMatch (EM) metrics across various models. This effectively reduces errors in\ngenerating column names and matching values to the columns. Furthermore, the\nknowledge-injected models can be applied to many downstream Text-to-SQL tasks,\ndemonstrating the generalizability of the approach presented in this paper.\n","authors":["Xingyu Ma","Xin Tian","Lingxiang Wu","Xuepeng Wang","Xueming Tang","Jinqiao Wang"],"pdf_url":"https://arxiv.org/pdf/2409.15907v2.pdf","comment":"This paper has been accepted by ECAI 2024"},{"id":"http://arxiv.org/abs/2502.15538v2","updated":"2025-02-25T09:33:44Z","published":"2025-02-21T15:40:37Z","title":"SOTOPIA-Ω: Dynamic Strategy Injection Learning and Social\n  Instruction Following Evaluation for Social Agents","summary":"  Despite the abundance of prior social strategies possessed by humans, there\nremains a paucity of research dedicated to their transfer and integration into\nsocial agents. Our proposed SOTOPIA-{\\Omega} framework aims to address and\nbridge this gap, with a particular focus on enhancing the social capabilities\nof language agents. This framework dynamically injects multi-step reasoning\nstrategies inspired by negotiation theory and two simple direct strategies into\nexpert agents, thereby automating the construction of a high-quality social\ndialogue training corpus. Additionally, we introduce the concept of Social\nInstruction Following (S-IF) and propose two new S-IF evaluation metrics that\ncomplement social capability. We demonstrate that several 7B models trained on\nhigh-quality corpus not only significantly surpass the expert agent (GPT-4) in\nachieving social goals but also enhance S-IF performance. Analysis and variant\nexperiments validate the advantages of dynamic construction, which can\nespecially break the agent's prolonged deadlock.\n","authors":["Wenyuan Zhang","Tianyun Liu","Mengxiao Song","Xiaodong Li","Tingwen Liu"],"pdf_url":"https://arxiv.org/pdf/2502.15538v2.pdf","comment":"26 pages, 5 figures, 23 tables"},{"id":"http://arxiv.org/abs/2502.18023v1","updated":"2025-02-25T09:32:08Z","published":"2025-02-25T09:32:08Z","title":"Detecting Knowledge Boundary of Vision Large Language Models by\n  Sampling-Based Inference","summary":"  Despite the advancements made in Visual Large Language Models (VLLMs), like\ntext Large Language Models (LLMs), they have limitations in addressing\nquestions that require real-time information or are knowledge-intensive.\nIndiscriminately adopting Retrieval Augmented Generation (RAG) techniques is an\neffective yet expensive way to enable models to answer queries beyond their\nknowledge scopes. To mitigate the dependence on retrieval and simultaneously\nmaintain, or even improve, the performance benefits provided by retrieval, we\npropose a method to detect the knowledge boundary of VLLMs, allowing for more\nefficient use of techniques like RAG. Specifically, we propose a method with\ntwo variants that fine-tunes a VLLM on an automatically constructed dataset for\nboundary identification. Experimental results on various types of Visual\nQuestion Answering datasets show that our method successfully depicts a VLLM's\nknowledge boundary based on which we are able to reduce indiscriminate\nretrieval while maintaining or improving the performance. In addition, we show\nthat the knowledge boundary identified by our method for one VLLM can be used\nas a surrogate boundary for other VLLMs. Code will be released at\nhttps://github.com/Chord-Chen-30/VLLM-KnowledgeBoundary\n","authors":["Zhuo Chen","Xinyu Wang","Yong Jiang","Zhen Zhang","Xinyu Geng","Pengjun Xie","Fei Huang","Kewei Tu"],"pdf_url":"https://arxiv.org/pdf/2502.18023v1.pdf","comment":"Under review"},{"id":"http://arxiv.org/abs/2410.08047v2","updated":"2025-02-25T09:30:50Z","published":"2024-10-10T15:42:39Z","title":"Divide and Translate: Compositional First-Order Logic Translation and\n  Verification for Complex Logical Reasoning","summary":"  Complex logical reasoning tasks require a long sequence of reasoning, which a\nlarge language model (LLM) with chain-of-thought prompting still falls short.\nTo alleviate this issue, neurosymbolic approaches incorporate a symbolic\nsolver. Specifically, an LLM only translates a natural language problem into a\nsatisfiability (SAT) problem that consists of first-order logic formulas, and a\nsound symbolic solver returns a mathematically correct solution. However, we\ndiscover that LLMs have difficulties to capture complex logical semantics\nhidden in the natural language during translation. To resolve this limitation,\nwe propose a Compositional First-Order Logic Translation. An LLM first parses a\nnatural language sentence into newly defined logical dependency structures that\nconsist of an atomic subsentence and its dependents, then sequentially\ntranslate the parsed subsentences. Since multiple logical dependency structures\nand sequential translations are possible for a single sentence, we also\nintroduce two Verification algorithms to ensure more reliable results. We\nutilize an SAT solver to rigorously compare semantics of generated first-order\nlogic formulas and select the most probable one. We evaluate the proposed\nmethod, dubbed CLOVER, on seven logical reasoning benchmarks and show that it\noutperforms the previous neurosymbolic approaches and achieves new\nstate-of-the-art results.\n","authors":["Hyun Ryu","Gyeongman Kim","Hyemin S. Lee","Eunho Yang"],"pdf_url":"https://arxiv.org/pdf/2410.08047v2.pdf","comment":"ICLR 2025 camera-ready version"},{"id":"http://arxiv.org/abs/2502.18020v1","updated":"2025-02-25T09:28:47Z","published":"2025-02-25T09:28:47Z","title":"AfroXLMR-Comet: Multilingual Knowledge Distillation with Attention\n  Matching for Low-Resource languages","summary":"  Language model compression through knowledge distillation has emerged as a\npromising approach for deploying large language models in resource-constrained\nenvironments. However, existing methods often struggle to maintain performance\nwhen distilling multilingual models, especially for low-resource languages. In\nthis paper, we present a novel hybrid distillation approach that combines\ntraditional knowledge distillation with a simplified attention matching\nmechanism, specifically designed for multilingual contexts. Our method\nintroduces an extremely compact student model architecture, significantly\nsmaller than conventional multilingual models. We evaluate our approach on five\nAfrican languages: Kinyarwanda, Swahili, Hausa, Igbo, and Yoruba. The distilled\nstudent model; AfroXLMR-Comet successfully captures both the output\ndistribution and internal attention patterns of a larger teacher model\n(AfroXLMR-Large) while reducing the model size by over 85%. Experimental\nresults demonstrate that our hybrid approach achieves competitive performance\ncompared to the teacher model, maintaining an accuracy within 85% of the\noriginal model's performance while requiring substantially fewer computational\nresources. Our work provides a practical framework for deploying efficient\nmultilingual models in resource-constrained environments, particularly\nbenefiting applications involving African languages.\n","authors":["Joshua Sakthivel Raju","Sanjay S","Jaskaran Singh Walia","Srinivas Raghav","Vukosi Marivate"],"pdf_url":"https://arxiv.org/pdf/2502.18020v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.18018v1","updated":"2025-02-25T09:26:44Z","published":"2025-02-25T09:26:44Z","title":"Verdict: A Library for Scaling Judge-Time Compute","summary":"  The use of LLMs as automated judges (\"LLM-as-a-judge\") is now widespread, yet\nstandard judges suffer from a multitude of reliability issues. To address these\nchallenges, we introduce Verdict, an open-source library for scaling judge-time\ncompute to enhance the accuracy, reliability, and interpretability of automated\nevaluators. Verdict leverages the composition of modular reasoning units --\nsuch as verification, debate, and aggregation -- and increased inference-time\ncompute to improve LLM judge quality. Across a variety of challenging tasks\nsuch as content moderation, fact-checking, and hallucination detection, Verdict\njudges achieve state-of-the-art (SOTA) or near-SOTA performance, surpassing\norders-of-magnitude larger fine-tuned judges, prompted judges, and reasoning\nmodels. Ultimately, we hope Verdict serves as a useful framework for\nresearchers and practitioners building scalable, interpretable, and reliable\nLLM-based evaluators.\n","authors":["Nimit Kalra","Leonard Tang"],"pdf_url":"https://arxiv.org/pdf/2502.18018v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.18017v1","updated":"2025-02-25T09:26:12Z","published":"2025-02-25T09:26:12Z","title":"ViDoRAG: Visual Document Retrieval-Augmented Generation via Dynamic\n  Iterative Reasoning Agents","summary":"  Understanding information from visually rich documents remains a significant\nchallenge for traditional Retrieval-Augmented Generation (RAG) methods.\nExisting benchmarks predominantly focus on image-based question answering (QA),\noverlooking the fundamental challenges of efficient retrieval, comprehension,\nand reasoning within dense visual documents. To bridge this gap, we introduce\nViDoSeek, a novel dataset designed to evaluate RAG performance on visually rich\ndocuments requiring complex reasoning. Based on it, we identify key limitations\nin current RAG approaches: (i) purely visual retrieval methods struggle to\neffectively integrate both textual and visual features, and (ii) previous\napproaches often allocate insufficient reasoning tokens, limiting their\neffectiveness. To address these challenges, we propose ViDoRAG, a novel\nmulti-agent RAG framework tailored for complex reasoning across visual\ndocuments. ViDoRAG employs a Gaussian Mixture Model (GMM)-based hybrid strategy\nto effectively handle multi-modal retrieval. To further elicit the model's\nreasoning capabilities, we introduce an iterative agent workflow incorporating\nexploration, summarization, and reflection, providing a framework for\ninvestigating test-time scaling in RAG domains. Extensive experiments on\nViDoSeek validate the effectiveness and generalization of our approach.\nNotably, ViDoRAG outperforms existing methods by over 10% on the competitive\nViDoSeek benchmark.\n","authors":["Qiuchen Wang","Ruixue Ding","Zehui Chen","Weiqi Wu","Shihang Wang","Pengjun Xie","Feng Zhao"],"pdf_url":"https://arxiv.org/pdf/2502.18017v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.14846v3","updated":"2025-02-25T09:19:30Z","published":"2025-01-24T10:49:45Z","title":"Wormhole Memory: A Rubik's Cube for Cross-Dialogue Retrieval","summary":"  In view of the gap in the current large language model in sharing memory\nacross dialogues, this research proposes a wormhole memory module (WMM) to\nrealize memory as a Rubik's cube that can be arbitrarily retrieved between\ndifferent dialogues. Through simulation experiments, the researcher built an\nexperimental framework based on the Python environment and used setting memory\nbarriers to simulate the current situation where memories between LLMs\ndialogues are difficult to share. The CoQA development data set was imported\ninto the experiment, and the feasibility of its cross-dialogue memory retrieval\nfunction was verified for WMM's nonlinear indexing and dynamic retrieval, and a\ncomparative analysis was conducted with the capabilities of Titans and MemGPT\nmemory modules. Experimental results show that WMM demonstrated the ability to\nretrieve memory across dialogues and the stability of quantitative indicators\nin eight experiments. It contributes new technical approaches to the\noptimization of memory management of LLMs and provides experience for the\npractical application in the future.\n","authors":["Libo Wang"],"pdf_url":"https://arxiv.org/pdf/2501.14846v3.pdf","comment":"The experimental process and code have been uploaded to the Github\n  repository, the link is:\n  https://github.com/brucewang123456789/GeniusTrail/tree/main/Wormhole%20Memory%20Module"},{"id":"http://arxiv.org/abs/2502.14734v2","updated":"2025-02-25T09:15:29Z","published":"2025-02-20T17:00:19Z","title":"Sentence Smith: Formally Controllable Text Transformation and its\n  Application to Evaluation of Text Embedding Models","summary":"  We propose the Sentence Smith framework that enables controlled and specified\nmanipulation of text meaning. It consists of three main steps: 1. Parsing a\nsentence into a semantic graph, 2. Applying human-designed semantic\nmanipulation rules, and 3. Generating text from the manipulated graph. A final\nfiltering step (4.) ensures the validity of the applied transformation. To\ndemonstrate the utility of Sentence Smith in an application study, we use it to\ngenerate hard negative pairs that challenge text embedding models. Since the\ncontrollable generation makes it possible to clearly isolate different types of\nsemantic shifts, we can gain deeper insights into the specific strengths and\nweaknesses of widely used text embedding models, also addressing an issue in\ncurrent benchmarking where linguistic phenomena remain opaque. Human validation\nconfirms that the generations produced by Sentence Smith are highly accurate.\n","authors":["Hongji Li","Andrianos Michail","Reto Gubelmann","Simon Clematide","Juri Opitz"],"pdf_url":"https://arxiv.org/pdf/2502.14734v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.07950v5","updated":"2025-02-25T09:14:18Z","published":"2023-12-13T07:56:27Z","title":"CBQ: Cross-Block Quantization for Large Language Models","summary":"  Post-training quantization (PTQ) has played a key role in compressing large\nlanguage models (LLMs) with ultra-low costs. However, existing PTQ methods only\nfocus on handling the outliers within one layer or one block, which ignores the\ndependency of blocks and leads to severe performance degradation in low-bit\nsettings. In this paper, we propose CBQ, a cross-block reconstruction-based PTQ\nmethod for LLMs. CBQ employs a cross-block dependency using a homologous\nreconstruction scheme, establishing long-range dependencies across multiple\nblocks to minimize error accumulation. Furthermore, CBQ incorporates a\ncoarse-to-fine preprocessing (CFP) strategy for suppressing weight and\nactivation outliers, coupled with an adaptive LoRA-Rounding technique for\nprecise weight quantization. These innovations enable CBQ to not only handle\nextreme outliers effectively but also improve overall quantization accuracy.\nExtensive experiments show that CBQ achieves superior low-bit quantization\n(W4A4, W4A8, W2A16) and outperforms existing state-of-the-art methods across\nvarious LLMs and datasets. Notably, CBQ quantizes the 4-bit LLAMA1-65B model\nwithin only 4.3 hours on a single GPU, achieving a commendable tradeoff between\nperformance and quantization efficiency.\n","authors":["Xin Ding","Xiaoyu Liu","Zhijun Tu","Yun Zhang","Wei Li","Jie Hu","Hanting Chen","Yehui Tang","Zhiwei Xiong","Baoqun Yin","Yunhe Wang"],"pdf_url":"https://arxiv.org/pdf/2312.07950v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.18001v1","updated":"2025-02-25T09:08:45Z","published":"2025-02-25T09:08:45Z","title":"Unveiling the Key Factors for Distilling Chain-of-Thought Reasoning","summary":"  Large Language Models (LLMs) excel in reasoning tasks through\nChain-of-Thought (CoT) prompting. However, CoT prompting greatly increases\ncomputational demands, which has prompted growing interest in distilling CoT\ncapabilities into Small Language Models (SLMs). This study systematically\nexamines the factors influencing CoT distillation, including the choice of\ngranularity, format and teacher model. Through experiments involving four\nteacher models and seven student models across seven mathematical and\ncommonsense reasoning datasets, we uncover three key findings: (1) Unlike LLMs,\nSLMs exhibit a non-monotonic relationship with granularity, with stronger\nmodels benefiting from finer-grained reasoning and weaker models performing\nbetter with simpler CoT supervision; (2) CoT format significantly impacts LLMs\nbut has minimal effect on SLMs, likely due to their reliance on supervised\nfine-tuning rather than pretraining preferences; (3) Stronger teacher models do\nNOT always produce better student models, as diversity and complexity in CoT\nsupervision can outweigh accuracy alone. These findings emphasize the need to\ntailor CoT strategies to specific student model, offering actionable insights\nfor optimizing CoT distillation in SLMs. The code and datasets are available at\nhttps://github.com/EIT-NLP/Distilling-CoT-Reasoning.\n","authors":["Xinghao Chen","Zhijing Sun","Wenjin Guo","Miaoran Zhang","Yanjun Chen","Yirong Sun","Hui Su","Yijie Pan","Dietrich Klakow","Wenjie Li","Xiaoyu Shen"],"pdf_url":"https://arxiv.org/pdf/2502.18001v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.19487v4","updated":"2025-02-25T09:06:14Z","published":"2024-09-28T23:59:46Z","title":"HealthQ: Unveiling Questioning Capabilities of LLM Chains in Healthcare\n  Conversations","summary":"  Effective patient care in digital healthcare requires large language models\n(LLMs) that not only answer questions but also actively gather critical\ninformation through well-crafted inquiries. This paper introduces HealthQ, a\nnovel framework for evaluating the questioning capabilities of LLM healthcare\nchains. By implementing advanced LLM chains, including Retrieval-Augmented\nGeneration (RAG), Chain of Thought (CoT), and reflective chains, HealthQ\nassesses how effectively these chains elicit comprehensive and relevant patient\ninformation. To achieve this, we integrate an LLM judge to evaluate generated\nquestions across metrics such as specificity, relevance, and usefulness, while\naligning these evaluations with traditional Natural Language Processing (NLP)\nmetrics like ROUGE and Named Entity Recognition (NER)-based set comparisons. We\nvalidate HealthQ using two custom datasets constructed from public medical\ndatasets, ChatDoctor and MTS-Dialog, and demonstrate its robustness across\nmultiple LLM judge models, including GPT-3.5, GPT-4, and Claude. Our\ncontributions are threefold: we present the first systematic framework for\nassessing questioning capabilities in healthcare conversations, establish a\nmodel-agnostic evaluation methodology, and provide empirical evidence linking\nhigh-quality questions to improved patient information elicitation.\n","authors":["Ziyu Wang","Hao Li","Di Huang","Hye-Sung Kim","Chae-Won Shin","Amir M. Rahmani"],"pdf_url":"https://arxiv.org/pdf/2409.19487v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.14669v3","updated":"2025-02-25T08:56:11Z","published":"2025-02-20T16:05:18Z","title":"AlphaMaze: Enhancing Large Language Models' Spatial Intelligence via\n  GRPO","summary":"  Large Language Models (LLMs) have demonstrated impressive capabilities in\nlanguage processing, yet they often struggle with tasks requiring genuine\nvisual spatial reasoning. In this paper, we introduce a novel two-stage\ntraining framework designed to equip standard LLMs with visual reasoning\nabilities for maze navigation. First, we leverage Supervised Fine Tuning (SFT)\non a curated dataset of tokenized maze representations to teach the model to\npredict step-by-step movement commands. Next, we apply Group Relative Policy\nOptimization (GRPO)-a technique used in DeepSeekR1-with a carefully crafted\nreward function to refine the model's sequential decision-making and encourage\nemergent chain-of-thought behaviors. Experimental results on synthetically\ngenerated mazes show that while a baseline model fails to navigate the maze,\nthe SFT-trained model achieves 86% accuracy, and further GRPO fine-tuning\nboosts accuracy to 93%. Qualitative analyses reveal that GRPO fosters more\nrobust and self-corrective reasoning, highlighting the potential of our\napproach to bridge the gap between language models and visual spatial tasks.\nThese findings offer promising implications for applications in robotics,\nautonomous navigation, and other domains that require integrated visual and\nsequential reasoning.\n","authors":["Alan Dao","Dinh Bach Vu"],"pdf_url":"https://arxiv.org/pdf/2502.14669v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.17987v1","updated":"2025-02-25T08:53:27Z","published":"2025-02-25T08:53:27Z","title":"MAGE: Multi-Head Attention Guided Embeddings for Low Resource Sentiment\n  Classification","summary":"  Due to the lack of quality data for low-resource Bantu languages, significant\nchallenges are presented in text classification and other practical\nimplementations. In this paper, we introduce an advanced model combining\nLanguage-Independent Data Augmentation (LiDA) with Multi-Head Attention based\nweighted embeddings to selectively enhance critical data points and improve\ntext classification performance. This integration allows us to create robust\ndata augmentation strategies that are effective across various linguistic\ncontexts, ensuring that our model can handle the unique syntactic and semantic\nfeatures of Bantu languages. This approach not only addresses the data scarcity\nissue but also sets a foundation for future research in low-resource language\nprocessing and classification tasks.\n","authors":["Varun Vashisht","Samar Singh","Mihir Konduskar","Jaskaran Singh Walia","Vukosi Marivate"],"pdf_url":"https://arxiv.org/pdf/2502.17987v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.17967v1","updated":"2025-02-25T08:41:01Z","published":"2025-02-25T08:41:01Z","title":"LLM Knows Geometry Better than Algebra: Numerical Understanding of\n  LLM-Based Agents in A Trading Arena","summary":"  Recent advancements in large language models (LLMs) have significantly\nimproved performance in natural language processing tasks. However, their\nability to generalize to dynamic, unseen tasks, particularly in numerical\nreasoning, remains a challenge. Existing benchmarks mainly evaluate LLMs on\nproblems with predefined optimal solutions, which may not align with real-world\nscenarios where clear answers are absent. To bridge this gap, we design the\nAgent Trading Arena, a virtual numerical game simulating complex economic\nsystems through zero-sum games, where agents invest in stock portfolios. Our\nexperiments reveal that LLMs, including GPT-4o, struggle with algebraic\nreasoning when dealing with plain-text stock data, often focusing on local\ndetails rather than global trends. In contrast, LLMs perform significantly\nbetter with geometric reasoning when presented with visual data, such as\nscatter plots or K-line charts, suggesting that visual representations enhance\nnumerical reasoning. This capability is further improved by incorporating the\nreflection module, which aids in the analysis and interpretation of complex\ndata. We validate our findings on NASDAQ Stock dataset, where LLMs demonstrate\nstronger reasoning with visual data compared to text. Our code and data are\npublicly available at https://github.com/wekjsdvnm/Agent-Trading-Arena.git.\n","authors":["Tianmi Ma","Jiawei Du","Wenxin Huang","Wenjie Wang","Liang Xie","Xian Zhong","Joey Tianyi Zhou"],"pdf_url":"https://arxiv.org/pdf/2502.17967v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.17957v1","updated":"2025-02-25T08:27:54Z","published":"2025-02-25T08:27:54Z","title":"On Synthetic Data Strategies for Domain-Specific Generative Retrieval","summary":"  This paper investigates synthetic data generation strategies in developing\ngenerative retrieval models for domain-specific corpora, thereby addressing the\nscalability challenges inherent in manually annotating in-domain queries. We\nstudy the data strategies for a two-stage training framework: in the first\nstage, which focuses on learning to decode document identifiers from queries,\nwe investigate LLM-generated queries across multiple granularity (e.g. chunks,\nsentences) and domain-relevant search constraints that can better capture\nnuanced relevancy signals. In the second stage, which aims to refine document\nranking through preference learning, we explore the strategies for mining hard\nnegatives based on the initial model's predictions. Experiments on public\ndatasets over diverse domains demonstrate the effectiveness of our synthetic\ndata generation and hard negative sampling approach.\n","authors":["Haoyang Wen","Jiang Guo","Yi Zhang","Jiarong Jiang","Zhiguo Wang"],"pdf_url":"https://arxiv.org/pdf/2502.17957v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.17956v1","updated":"2025-02-25T08:27:28Z","published":"2025-02-25T08:27:28Z","title":"Towards Better Understanding of Program-of-Thought Reasoning in\n  Cross-Lingual and Multilingual Environments","summary":"  Multi-step reasoning is essential for large language models (LLMs), yet\nmultilingual performance remains challenging. While Chain-of-Thought (CoT)\nprompting improves reasoning, it struggles with non-English languages due to\nthe entanglement of reasoning and execution. Program-of-Thought (PoT) prompting\nseparates reasoning from execution, offering a promising alternative but\nshifting the challenge to generating programs from non-English questions. We\npropose a framework to evaluate PoT by separating multilingual reasoning from\ncode execution to examine (i) the impact of fine-tuning on question-reasoning\nalignment and (ii) how reasoning quality affects answer correctness. Our\nfindings demonstrate that PoT fine-tuning substantially enhances multilingual\nreasoning, outperforming CoT fine-tuned models. We further demonstrate a strong\ncorrelation between reasoning quality (measured through code quality) and\nanswer accuracy, highlighting its potential as a test-time performance\nimprovement heuristic.\n","authors":["Patomporn Payoungkhamdee","Pume Tuchinda","Jinheon Baek","Samuel Cahyawijaya","Can Udomcharoenchaikit","Potsawee Manakul","Peerat Limkonchotiwat","Ekapol Chuangsuwanich","Sarana Nutanong"],"pdf_url":"https://arxiv.org/pdf/2502.17956v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.17955v1","updated":"2025-02-25T08:27:18Z","published":"2025-02-25T08:27:18Z","title":"Language Models' Factuality Depends on the Language of Inquiry","summary":"  Multilingual language models (LMs) are expected to recall factual knowledge\nconsistently across languages, yet they often fail to transfer knowledge\nbetween languages even when they possess the correct information in one of the\nlanguages. For example, we find that an LM may correctly identify Rashed Al\nShashai as being from Saudi Arabia when asked in Arabic, but consistently fails\nto do so when asked in English or Swahili. To systematically investigate this\nlimitation, we introduce a benchmark of 10,000 country-related facts across 13\nlanguages and propose three novel metrics: Factual Recall Score, Knowledge\nTransferability Score, and Cross-Lingual Factual Knowledge Transferability\nScore-to quantify factual recall and knowledge transferability in LMs across\ndifferent languages. Our results reveal fundamental weaknesses in today's\nstate-of-the-art LMs, particularly in cross-lingual generalization where models\nfail to transfer knowledge effectively across different languages, leading to\ninconsistent performance sensitive to the language used. Our findings emphasize\nthe need for LMs to recognize language-specific factual reliability and\nleverage the most trustworthy information across languages. We release our\nbenchmark and evaluation framework to drive future research in multilingual\nknowledge transfer.\n","authors":["Tushar Aggarwal","Kumar Tanmay","Ayush Agrawal","Kumar Ayush","Hamid Palangi","Paul Pu Liang"],"pdf_url":"https://arxiv.org/pdf/2502.17955v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.13766v2","updated":"2025-02-25T08:15:43Z","published":"2025-01-23T15:46:43Z","title":"UGMathBench: A Diverse and Dynamic Benchmark for Undergraduate-Level\n  Mathematical Reasoning with Large Language Models","summary":"  Large Language Models (LLMs) have made significant strides in mathematical\nreasoning, underscoring the need for a comprehensive and fair evaluation of\ntheir capabilities. However, existing benchmarks often fall short, either\nlacking extensive coverage of undergraduate-level mathematical problems or\nprobably suffering from test-set contamination. To address these issues, we\nintroduce UGMathBench, a diverse and dynamic benchmark specifically designed\nfor evaluating undergraduate-level mathematical reasoning with LLMs.\nUGMathBench comprises 5,062 problems across 16 subjects and 111 topics,\nfeaturing 10 distinct answer types. Each problem includes three randomized\nversions, with additional versions planned for release as leading open-source\nLLMs become saturated in UGMathBench. Furthermore, we propose two key metrics:\neffective accuracy (EAcc), which measures the percentage of correctly solved\nproblems across all three versions, and reasoning gap ($\\Delta$), which\nassesses reasoning robustness by calculating the difference between the average\naccuracy across all versions and EAcc. Our extensive evaluation of 23 leading\nLLMs reveals that the highest EAcc achieved is 56.3\\% by OpenAI-o1-mini, with\nlarge $\\Delta$ values observed across different models. This highlights the\nneed for future research aimed at developing \"large reasoning models\" with high\nEAcc and $\\Delta = 0$. We anticipate that the release of UGMathBench, along\nwith its detailed evaluation codes, will serve as a valuable resource to\nadvance the development of LLMs in solving mathematical problems. Codes and\ndata are available at https://github.com/YangLabHKUST/UGMathBench\n","authors":["Xin Xu","Jiaxin Zhang","Tianhao Chen","Zitong Chao","Jishan Hu","Can Yang"],"pdf_url":"https://arxiv.org/pdf/2501.13766v2.pdf","comment":"Accepted to ICLR 2025"},{"id":"http://arxiv.org/abs/2204.06970v2","updated":"2025-02-25T08:11:03Z","published":"2022-04-14T13:52:11Z","title":"Can Visual Dialogue Models Do Scorekeeping? Exploring How Dialogue\n  Representations Incrementally Encode Shared Knowledge","summary":"  Cognitively plausible visual dialogue models should keep a mental scoreboard\nof shared established facts in the dialogue context. We propose a theory-based\nevaluation method for investigating to what degree models pretrained on the\nVisDial dataset incrementally build representations that appropriately do\nscorekeeping. Our conclusion is that the ability to make the distinction\nbetween shared and privately known statements along the dialogue is moderately\npresent in the analysed models, but not always incrementally consistent, which\nmay partially be due to the limited need for grounding interactions in the\noriginal task.\n","authors":["Brielen Madureira","David Schlangen"],"pdf_url":"https://arxiv.org/pdf/2204.06970v2.pdf","comment":"Accepted at ACL 2022, short paper (v2 fixes labels in Figure 3)"},{"id":"http://arxiv.org/abs/2502.17947v1","updated":"2025-02-25T08:08:53Z","published":"2025-02-25T08:08:53Z","title":"DeepSeek-R1 Outperforms Gemini 2.0 Pro, OpenAI o1, and o3-mini in\n  Bilingual Complex Ophthalmology Reasoning","summary":"  Purpose: To evaluate the accuracy and reasoning ability of DeepSeek-R1 and\nthree other recently released large language models (LLMs) in bilingual complex\nophthalmology cases. Methods: A total of 130 multiple-choice questions (MCQs)\nrelated to diagnosis (n = 39) and management (n = 91) were collected from the\nChinese ophthalmology senior professional title examination and categorized\ninto six topics. These MCQs were translated into English using DeepSeek-R1. The\nresponses of DeepSeek-R1, Gemini 2.0 Pro, OpenAI o1 and o3-mini were generated\nunder default configurations between February 15 and February 20, 2025.\nAccuracy was calculated as the proportion of correctly answered questions, with\nomissions and extra answers considered incorrect. Reasoning ability was\nevaluated through analyzing reasoning logic and the causes of reasoning error.\nResults: DeepSeek-R1 demonstrated the highest overall accuracy, achieving 0.862\nin Chinese MCQs and 0.808 in English MCQs. Gemini 2.0 Pro, OpenAI o1, and\nOpenAI o3-mini attained accuracies of 0.715, 0.685, and 0.692 in Chinese MCQs\n(all P<0.001 compared with DeepSeek-R1), and 0.746 (P=0.115), 0.723 (P=0.027),\nand 0.577 (P<0.001) in English MCQs, respectively. DeepSeek-R1 achieved the\nhighest accuracy across five topics in both Chinese and English MCQs. It also\nexcelled in management questions conducted in Chinese (all P<0.05). Reasoning\nability analysis showed that the four LLMs shared similar reasoning logic.\nIgnoring key positive history, ignoring key positive signs, misinterpretation\nmedical data, and too aggressive were the most common causes of reasoning\nerrors. Conclusion: DeepSeek-R1 demonstrated superior performance in bilingual\ncomplex ophthalmology reasoning tasks than three other state-of-the-art LLMs.\nWhile its clinical applicability remains challenging, it shows promise for\nsupporting diagnosis and clinical decision-making.\n","authors":["Pusheng Xu","Yue Wu","Kai Jin","Xiaolan Chen","Mingguang He","Danli Shi"],"pdf_url":"https://arxiv.org/pdf/2502.17947v1.pdf","comment":"29 pages, 4 figures, 1 table"},{"id":"http://arxiv.org/abs/2502.17945v1","updated":"2025-02-25T08:07:42Z","published":"2025-02-25T08:07:42Z","title":"Assessing Large Language Models in Agentic Multilingual National Bias","summary":"  Large Language Models have garnered significant attention for their\ncapabilities in multilingual natural language processing, while studies on\nrisks associated with cross biases are limited to immediate context\npreferences. Cross-language disparities in reasoning-based recommendations\nremain largely unexplored, with a lack of even descriptive analysis. This study\nis the first to address this gap. We test LLM's applicability and capability in\nproviding personalized advice across three key scenarios: university\napplications, travel, and relocation. We investigate multilingual bias in\nstate-of-the-art LLMs by analyzing their responses to decision-making tasks\nacross multiple languages. We quantify bias in model-generated scores and\nassess the impact of demographic factors and reasoning strategies (e.g.,\nChain-of-Thought prompting) on bias patterns. Our findings reveal that local\nlanguage bias is prevalent across different tasks, with GPT-4 and Sonnet\nreducing bias for English-speaking countries compared to GPT-3.5 but failing to\nachieve robust multilingual alignment, highlighting broader implications for\nmultilingual AI agents and applications such as education.\n","authors":["Qianying Liu","Katrina Qiyao Wang","Fei Cheng","Sadao Kurohashi"],"pdf_url":"https://arxiv.org/pdf/2502.17945v1.pdf","comment":"13 pages"},{"id":"http://arxiv.org/abs/2502.17943v1","updated":"2025-02-25T08:03:32Z","published":"2025-02-25T08:03:32Z","title":"CaseGen: A Benchmark for Multi-Stage Legal Case Documents Generation","summary":"  Legal case documents play a critical role in judicial proceedings. As the\nnumber of cases continues to rise, the reliance on manual drafting of legal\ncase documents is facing increasing pressure and challenges. The development of\nlarge language models (LLMs) offers a promising solution for automating\ndocument generation. However, existing benchmarks fail to fully capture the\ncomplexities involved in drafting legal case documents in real-world scenarios.\nTo address this gap, we introduce CaseGen, the benchmark for multi-stage legal\ncase documents generation in the Chinese legal domain. CaseGen is based on 500\nreal case samples annotated by legal experts and covers seven essential case\nsections. It supports four key tasks: drafting defense statements, writing\ntrial facts, composing legal reasoning, and generating judgment results. To the\nbest of our knowledge, CaseGen is the first benchmark designed to evaluate LLMs\nin the context of legal case document generation. To ensure an accurate and\ncomprehensive evaluation, we design the LLM-as-a-judge evaluation framework and\nvalidate its effectiveness through human annotations. We evaluate several\nwidely used general-domain LLMs and legal-specific LLMs, highlighting their\nlimitations in case document generation and pinpointing areas for potential\nimprovement. This work marks a step toward a more effective framework for\nautomating legal case documents drafting, paving the way for the reliable\napplication of AI in the legal field. The dataset and code are publicly\navailable at https://github.com/CSHaitao/CaseGen.\n","authors":["Haitao Li","Jiaying Ye","Yiran Hu","Jia Chen","Qingyao Ai","Yueyue Wu","Junjie Chen","Yifan Chen","Cheng Luo","Quan Zhou","Yiqun Liu"],"pdf_url":"https://arxiv.org/pdf/2502.17943v1.pdf","comment":"18 pages"},{"id":"http://arxiv.org/abs/2405.14804v3","updated":"2025-02-25T07:58:27Z","published":"2024-05-23T17:13:50Z","title":"Can LLMs Solve longer Math Word Problems Better?","summary":"  Math Word Problems (MWPs) play a vital role in assessing the capabilities of\nLarge Language Models (LLMs), yet current research primarily focuses on\nquestions with concise contexts. The impact of longer contexts on mathematical\nreasoning remains under-explored. This study pioneers the investigation of\nContext Length Generalizability (CoLeG), which refers to the ability of LLMs to\nsolve MWPs with extended narratives. We introduce Extended Grade-School Math\n(E-GSM), a collection of MWPs featuring lengthy narratives, and propose two\nnovel metrics to evaluate the efficacy and resilience of LLMs in tackling these\nproblems. Our analysis of existing zero-shot prompting techniques with\nproprietary LLMs along with open-source LLMs reveals a general deficiency in\nCoLeG. To alleviate these issues, we propose tailored approaches for different\ncategories of LLMs. For proprietary LLMs, we introduce a new instructional\nprompt designed to mitigate the impact of long contexts. For open-source LLMs,\nwe develop a novel auxiliary task for fine-tuning to enhance CoLeG. Our\ncomprehensive results demonstrate the effectiveness of our proposed methods,\nshowing improved performance on E-GSM. Additionally, we conduct an in-depth\nanalysis to differentiate the effects of semantic understanding and reasoning\nefficacy, showing that our methods improves the latter. We also establish the\ngeneralizability of our methods across several other MWP benchmarks. Our\nfindings highlight the limitations of current LLMs and offer practical\nsolutions correspondingly, paving the way for further exploration of model\ngeneralizability and training methodologies.\n","authors":["Xin Xu","Tong Xiao","Zitong Chao","Zhenya Huang","Can Yang","Yang Wang"],"pdf_url":"https://arxiv.org/pdf/2405.14804v3.pdf","comment":"Accepted to ICLR 2025"},{"id":"http://arxiv.org/abs/2502.17110v2","updated":"2025-02-25T07:48:37Z","published":"2025-02-24T12:51:23Z","title":"Mobile-Agent-V: Learning Mobile Device Operation Through Video-Guided\n  Multi-Agent Collaboration","summary":"  The rapid increase in mobile device usage necessitates improved automation\nfor seamless task management. However, many AI-driven frameworks struggle due\nto insufficient operational knowledge. Manually written knowledge helps but is\nlabor-intensive and inefficient. To address these challenges, we introduce\nMobile-Agent-V, a framework that leverages video guidance to provide rich and\ncost-effective operational knowledge for mobile automation. Mobile-Agent-V\nenhances task execution capabilities by leveraging video inputs without\nrequiring specialized sampling or preprocessing. Mobile-Agent-V integrates a\nsliding window strategy and incorporates a video agent and deep-reflection\nagent to ensure that actions align with user instructions. Through this\ninnovative approach, users can record task processes with guidance, enabling\nthe system to autonomously learn and execute tasks efficiently. Experimental\nresults show that Mobile-Agent-V achieves a 30% performance improvement\ncompared to existing frameworks. The code will be open-sourced at\nhttps://github.com/X-PLUG/MobileAgent.\n","authors":["Junyang Wang","Haiyang Xu","Xi Zhang","Ming Yan","Ji Zhang","Fei Huang","Jitao Sang"],"pdf_url":"https://arxiv.org/pdf/2502.17110v2.pdf","comment":"16 pages, 7 figures, 7tables"},{"id":"http://arxiv.org/abs/2502.17927v1","updated":"2025-02-25T07:47:22Z","published":"2025-02-25T07:47:22Z","title":"Advantage-Guided Distillation for Preference Alignment in Small Language\n  Models","summary":"  Alignment techniques enable Large Language Models (LLMs) to generate outputs\nthat align with human preferences and play a crucial role in their\neffectiveness. However, their impact often diminishes when applied to Small\nLanguage Models (SLMs), likely due to the limited capacity of these models.\nInstead of directly applying existing alignment techniques to SLMs, we propose\nto utilize a well-aligned teacher LLM to guide the alignment process for these\nmodels, thereby facilitating the transfer of the teacher's knowledge of human\npreferences to the student model. To achieve this, we first explore a\nstraightforward approach, Dual-Constrained Knowledge Distillation (DCKD), that\nemploys knowledge distillation with two KL-divergence constraints from the\naligned teacher to the unaligned student. To further enhance the student's\nability to distinguish between preferred and dispreferred responses, we then\npropose Advantage-Guided Distillation for Preference Alignment (ADPA), which\nleverages an advantage function from the aligned teacher to deliver more\nnuanced, distribution-level reward signals for the student's alignment. Our\nexperimental results show that these two approaches appreciably improve the\nalignment of SLMs and narrow the performance gap with larger counterparts.\nAmong them, ADPA demonstrates superior performance and achieves even greater\neffectiveness when integrated with DCKD. Our code is available at\nhttps://github.com/SLIT-AI/ADPA.\n","authors":["Shiping Gao","Fanqi Wan","Jiajian Guo","Xiaojun Quan","Qifan Wang"],"pdf_url":"https://arxiv.org/pdf/2502.17927v1.pdf","comment":"Accepted by ICLR 2025(spotlight)"},{"id":"http://arxiv.org/abs/2502.17924v1","updated":"2025-02-25T07:44:22Z","published":"2025-02-25T07:44:22Z","title":"FACT-AUDIT: An Adaptive Multi-Agent Framework for Dynamic Fact-Checking\n  Evaluation of Large Language Models","summary":"  Large Language Models (LLMs) have significantly advanced the fact-checking\nstudies. However, existing automated fact-checking evaluation methods rely on\nstatic datasets and classification metrics, which fail to automatically\nevaluate the justification production and uncover the nuanced limitations of\nLLMs in fact-checking. In this work, we introduce FACT-AUDIT, an agent-driven\nframework that adaptively and dynamically assesses LLMs' fact-checking\ncapabilities. Leveraging importance sampling principles and multi-agent\ncollaboration, FACT-AUDIT generates adaptive and scalable datasets, performs\niterative model-centric evaluations, and updates assessments based on\nmodel-specific responses. By incorporating justification production alongside\nverdict prediction, this framework provides a comprehensive and evolving audit\nof LLMs' factual reasoning capabilities, to investigate their trustworthiness.\nExtensive experiments demonstrate that FACT-AUDIT effectively differentiates\namong state-of-the-art LLMs, providing valuable insights into model strengths\nand limitations in model-centric fact-checking analysis.\n","authors":["Hongzhan Lin","Yang Deng","Yuxuan Gu","Wenxuan Zhang","Jing Ma","See-Kiong Ng","Tat-Seng Chua"],"pdf_url":"https://arxiv.org/pdf/2502.17924v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.17910v1","updated":"2025-02-25T07:18:29Z","published":"2025-02-25T07:18:29Z","title":"Scaling LLM Pre-training with Vocabulary Curriculum","summary":"  Modern language models rely on static vocabularies, fixed before pretraining,\nin contrast to the adaptive vocabulary acquisition observed in human language\nlearning. To bridge this gap, we introduce vocabulary curriculum learning, an\napproach that improves pretraining efficiency with log-linear scaling gains\nrelative to vocabulary size. Our method alternates between entropy-guided\nvocabulary expansion and model optimization, enabling models to learn\ntransferable representations across diverse tokenization granularities. This\napproach naturally gives rise to an optimal computation allocation pattern:\nlonger tokens capture predictable content, while shorter tokens focus on more\ncomplex, harder-to-predict contexts. Experiments on small-scale GPT models\ndemonstrate improved scaling efficiency, reinforcing the effectiveness of\ndynamic tokenization. We release our code to support further research and plan\nto extend our experiments to larger models and diverse domains.\n","authors":["Fangyuan Yu"],"pdf_url":"https://arxiv.org/pdf/2502.17910v1.pdf","comment":"under review"},{"id":"http://arxiv.org/abs/2312.01529v3","updated":"2025-02-25T07:04:54Z","published":"2023-12-03T23:03:22Z","title":"T3D: Advancing 3D Medical Vision-Language Pre-training by Learning\n  Multi-View Visual Consistency","summary":"  While 3D visual self-supervised learning (vSSL) shows promising results in\ncapturing visual representations, it overlooks the clinical knowledge from\nradiology reports. Meanwhile, 3D medical vision-language pre-training (MedVLP)\nremains underexplored due to the lack of a large-scale, publicly available 3D\nmedical image-report dataset. To bridge this gap, we introduce **CT-3DVLP**,\nthe first and largest **public** 3D volume-report dataset, establishing a\ncomprehensive benchmark for 3D MedVLP research. Meanwhile, we propose the\n**T3D** framework, which enhances 3D MedVLP beyond naive CLIP-style alignment\nthat directly pairs volumes with reports but neglects local visual\nrepresentations. Instead, we introduce **Text-informed Multi-view Alignment\n(TMA)**, a novel approach that clusters volumetric data while enforcing\nconsistency across different views of the same volume-report pair. TMA\nintegrates textual features into fine-grained visual representations, ensuring\ncontextual coherence across views. We evaluate T3D across multiple downstream\ntasks in both unimodal and cross-modal settings, including zero-shot and\nfine-tuned classification, cross-modal retrieval, report generation, and\nsemantic segmentation. Our results show that T3D consistently outperforms\nexisting vSSL and multimodal methods, demonstrating superior zero-shot and\nfine-tuning capabilities and setting a new benchmark for 3D medical image\nunderstanding.\n","authors":["Che Liu","Cheng Ouyang","Yinda Chen","Cesar César Quilodrán-Casas","Lei Ma","Jie Fu","Yike Guo","Anand Shah","Wenjia Bai","Rossella Arcucci"],"pdf_url":"https://arxiv.org/pdf/2312.01529v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.17184v2","updated":"2025-02-25T06:56:39Z","published":"2025-02-24T14:20:22Z","title":"Measuring Data Diversity for Instruction Tuning: A Systematic Analysis\n  and A Reliable Metric","summary":"  Data diversity is crucial for the instruction tuning of large language\nmodels. Existing studies have explored various diversity-aware data selection\nmethods to construct high-quality datasets and enhance model performance.\nHowever, the fundamental problem of precisely defining and measuring data\ndiversity remains underexplored, limiting clear guidance for data engineering.\nTo address this, we systematically analyze 11 existing diversity measurement\nmethods by evaluating their correlation with model performance through\nextensive fine-tuning experiments. Our results indicate that a reliable\ndiversity measure should properly account for both inter-sample differences and\nthe information distribution in the sample space. Building on this, we propose\nNovelSum, a new diversity metric based on sample-level \"novelty.\" Experiments\non both simulated and real-world data show that NovelSum accurately captures\ndiversity variations and achieves a 0.97 correlation with instruction-tuned\nmodel performance, highlighting its value in guiding data engineering\npractices. With NovelSum as an optimization objective, we further develop a\ngreedy, diversity-oriented data selection strategy that outperforms existing\napproaches, validating both the effectiveness and practical significance of our\nmetric.\n","authors":["Yuming Yang","Yang Nan","Junjie Ye","Shihan Dou","Xiao Wang","Shuo Li","Huijie Lv","Tao Gui","Qi Zhang","Xuanjing Huang"],"pdf_url":"https://arxiv.org/pdf/2502.17184v2.pdf","comment":"15 pages. The related codes and resources will be released later.\n  Project page: https://github.com/UmeanNever/NovelSum"},{"id":"http://arxiv.org/abs/2502.15499v2","updated":"2025-02-25T06:54:39Z","published":"2025-02-21T14:49:34Z","title":"Scale-Distribution Decoupling: Enabling Stable and Effective Training of\n  Large Language Models","summary":"  Training stability is a persistent challenge in the pre-training of large\nlanguage models (LLMs), particularly for architectures such as Post-Norm\nTransformers, which are prone to gradient explosion and dissipation. In this\npaper, we propose Scale-Distribution Decoupling (SDD), a novel approach that\nstabilizes training by explicitly decoupling the scale and distribution of the\nweight matrix in fully-connected layers. SDD applies a normalization mechanism\nto regulate activations and a learnable scaling vector to maintain\nwell-conditioned gradients, effectively preventing $\\textbf{gradient explosion\nand dissipation}$. This separation improves optimization efficiency,\nparticularly in deep networks, by ensuring stable gradient propagation.\nExperimental results demonstrate that our method stabilizes training across\nvarious LLM architectures and outperforms existing techniques in different\nnormalization configurations. Furthermore, the proposed method is lightweight\nand compatible with existing frameworks, making it a practical solution for\nstabilizing LLM training. Code is available at https://github.com/kaihemo/SDD.\n","authors":["Ya Wang","Zhijian Zhuo","Yutao Zeng","Xun Zhou","Jian Yang","Xiaoqing Li"],"pdf_url":"https://arxiv.org/pdf/2502.15499v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.17899v1","updated":"2025-02-25T06:53:00Z","published":"2025-02-25T06:53:00Z","title":"Can Large Language Models Identify Implicit Suicidal Ideation? An\n  Empirical Evaluation","summary":"  We present a comprehensive evaluation framework for assessing Large Language\nModels' (LLMs) capabilities in suicide prevention, focusing on two critical\naspects: the Identification of Implicit Suicidal ideation (IIS) and the\nProvision of Appropriate Supportive responses (PAS). We introduce \\ourdata, a\nnovel dataset of 1,308 test cases built upon psychological frameworks including\nD/S-IAT and Negative Automatic Thinking, alongside real-world scenarios.\nThrough extensive experiments with 8 widely used LLMs under different\ncontextual settings, we find that current models struggle significantly with\ndetecting implicit suicidal ideation and providing appropriate support,\nhighlighting crucial limitations in applying LLMs to mental health contexts.\nOur findings underscore the need for more sophisticated approaches in\ndeveloping and evaluating LLMs for sensitive psychological applications.\n","authors":["Tong Li","Shu Yang","Junchao Wu","Jiyao Wei","Lijie Hu","Mengdi Li","Derek F. Wong","Joshua R. Oltmanns","Di Wang"],"pdf_url":"https://arxiv.org/pdf/2502.17899v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.06839v2","updated":"2025-02-25T06:42:57Z","published":"2024-11-11T10:07:51Z","title":"LLM-NEO: Parameter Efficient Knowledge Distillation for Large Language\n  Models","summary":"  Knowledge distillation (KD) has been a predominant method for compressing\nLarge Language Models (LLMs). In this paper, we first revisit KD and Low-Rank\nAdaption (LoRA) and demonstrate that they follow the same paradigm. Inspired by\nthis observation, we propose a parameter-efficient knowledge distillation\nmethod, LLM-NEO, which integrates LoRA into KD to improve the efficiency of\nknowledge transfer. After that, we summarize some valuable guidelines for the\nhyperparameters in LLM-NEO. Experimental results on compressing Llama 2 and\nLlama 3.2 show that LLM-NEO outperforms various baselines. Further analysis\ndemonstrates the robustness of the proposed LLM-NEO on variants of LoRA. The\ncode and trained models are available at\n[Github](https://github.com/yang3121099/LLM-Neo).\n","authors":["Runming Yang","Taiqiang Wu","Jiahao Wang","Pengfei Hu","Yik-Chung Wu","Ngai Wong","Yujiu Yang"],"pdf_url":"https://arxiv.org/pdf/2411.06839v2.pdf","comment":"ARR under review"},{"id":"http://arxiv.org/abs/2502.17888v1","updated":"2025-02-25T06:18:05Z","published":"2025-02-25T06:18:05Z","title":"RankCoT: Refining Knowledge for Retrieval-Augmented Generation through\n  Ranking Chain-of-Thoughts","summary":"  Retrieval-Augmented Generation (RAG) enhances the performance of Large\nLanguage Models (LLMs) by incorporating external knowledge. However, LLMs still\nencounter challenges in effectively utilizing the knowledge from retrieved\ndocuments, often being misled by irrelevant or noisy information. To address\nthis issue, we introduce RankCoT, a knowledge refinement method that\nincorporates reranking signals in generating CoT-based summarization for\nknowledge refinement based on given query and all retrieval documents. During\ntraining, RankCoT prompts the LLM to generate Chain-of-Thought (CoT) candidates\nbased on the query and individual documents. It then fine-tunes the LLM to\ndirectly reproduce the best CoT from these candidate outputs based on all\nretrieved documents, which requires LLM to filter out irrelevant documents\nduring generating CoT-style summarization. Additionally, RankCoT incorporates a\nself-reflection mechanism that further refines the CoT outputs, resulting in\nhigher-quality training data. Our experiments demonstrate the effectiveness of\nRankCoT, showing its superior performance over other knowledge refinement\nmodels. Further analysis reveals that RankCoT can provide shorter but effective\nrefinement results, enabling the generator to produce more accurate answers.\nAll code and data are available at https://github.com/NEUIR/RankCoT.\n","authors":["Mingyan Wu","Zhenghao Liu","Yukun Yan","Xinze Li","Shi Yu","Zheni Zeng","Yu Gu","Ge Yu"],"pdf_url":"https://arxiv.org/pdf/2502.17888v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.17882v1","updated":"2025-02-25T06:08:48Z","published":"2025-02-25T06:08:48Z","title":"Science Across Languages: Assessing LLM Multilingual Translation of\n  Scientific Papers","summary":"  Scientific research is inherently global. However, the vast majority of\nacademic journals are published exclusively in English, creating barriers for\nnon-native-English-speaking researchers. In this study, we leverage large\nlanguage models (LLMs) to translate published scientific articles while\npreserving their native JATS XML formatting, thereby developing a practical,\nautomated approach for implementation by academic journals. Using our approach,\nwe translate articles across multiple scientific disciplines into 28 languages.\nTo evaluate translation accuracy, we introduce a novel question-and-answer (QA)\nbenchmarking method, in which an LLM generates comprehension-based questions\nfrom the original text and then answers them based on the translated text. Our\nbenchmark results show an average performance of 95.9%, showing that the key\nscientific details are accurately conveyed. In a user study, we translate the\nscientific papers of 15 researchers into their native languages, finding that\nthe authors consistently found the translations to accurately capture the\noriginal information in their articles. Interestingly, a third of the authors\nfound many technical terms \"overtranslated,\" expressing a preference to keep\nterminology more familiar in English untranslated. Finally, we demonstrate how\nin-context learning techniques can be used to align translations with\ndomain-specific preferences such as mitigating overtranslation, highlighting\nthe adaptability and utility of LLM-driven scientific translation. The code and\ntranslated articles are available at https://hankleid.github.io/ProjectMundo.\n","authors":["Hannah Calzi Kleidermacher","James Zou"],"pdf_url":"https://arxiv.org/pdf/2502.17882v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.17878v1","updated":"2025-02-25T06:06:16Z","published":"2025-02-25T06:06:16Z","title":"Towards Enhanced Immersion and Agency for LLM-based Interactive Drama","summary":"  LLM-based Interactive Drama is a novel AI-based dialogue scenario, where the\nuser (i.e. the player) plays the role of a character in the story, has\nconversations with characters played by LLM agents, and experiences an\nunfolding story. This paper begins with understanding interactive drama from\ntwo aspects: Immersion, the player's feeling of being present in the story, and\nAgency, the player's ability to influence the story world. Both are crucial to\ncreating an enjoyable interactive experience, while they have been\nunderexplored in previous work. To enhance these two aspects, we first propose\nPlaywriting-guided Generation, a novel method that helps LLMs craft dramatic\nstories with substantially improved structures and narrative quality.\nAdditionally, we introduce Plot-based Reflection for LLM agents to refine their\nreactions to align with the player's intentions. Our evaluation relies on human\njudgment to assess the gains of our methods in terms of immersion and agency.\n","authors":["Hongqiu Wu","Weiqi Wu","Tianyang Xu","Jiameng Zhang","Hai Zhao"],"pdf_url":"https://arxiv.org/pdf/2502.17878v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.12221v4","updated":"2025-02-25T06:05:45Z","published":"2024-06-18T02:43:49Z","title":"On-Policy Self-Alignment with Fine-grained Knowledge Feedback for\n  Hallucination Mitigation","summary":"  Hallucination occurs when large language models exhibit behavior that\ndeviates from the boundaries of their knowledge during response generation. To\naddress this critical issue, previous learning-based methods attempt to\nfinetune models but are limited by off-policy sampling and coarse-grained\nfeedback. In this paper, we present \\textit{\\b{R}einforcement \\b{L}earning\n\\b{f}or \\b{H}allucination} (RLFH), an on-policy self-alignment approach that\nenables LLMs to actively explore their knowledge boundaries and self-correct\ngeneration behavior through fine-grained feedback signals. RLFH introduces a\nself-assessment framework where the policy serves as its own judge. Through\nthis framework, responses are automatically decomposed into atomic facts and\ntheir truthfulness and informativeness are assessed against external knowledge\nsources. The resulting fine-grained feedback at the statement level are then\nconverted into token-level dense reward signals. This enables online\nreinforcement learning to achieve precise and timely optimization without human\nintervention. Comprehensive evaluations on HotpotQA, SQuADv2, and Biography\nbenchmarks validate RLFH's effectiveness in hallucination mitigation.\n","authors":["Xueru Wen","Jie Lou","Xinyu Lu","Ji Yuqiu","Xinyan Guan","Yaojie Lu","Hongyu Lin","Ben He","Xianpei Han","Debing Zhang","Le Sun"],"pdf_url":"https://arxiv.org/pdf/2406.12221v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.17857v1","updated":"2025-02-25T05:07:27Z","published":"2025-02-25T05:07:27Z","title":"SYNTHEMPATHY: A Scalable Empathy Corpus Generated Using LLMs Without Any\n  Crowdsourcing","summary":"  Previous research has shown that humans are more receptive towards language\nmodels that that exhibit empathetic behavior. While empathy is essential for\ndeveloping helpful dialogue agents, very few large corpora containing\nempathetic dialogues are available for fine-tune LLMs. The few existing corpora\nhave largely relied on crowdsourcing to simulate empathetic conversations, a\nprocess that is expensive, time-consuming, and not scalable to larger datasets.\nWe propose a data generation framework for developing SYNTHEMPATHY, a large\ncorpus containing 105k empathetic responses to real-life situations compiled\nthrough LLM generation. A base Mistral 7B model fine-tuned on our SYNTHEMPATHY\ncorpus exhibits an increase in the average empathy score.\n","authors":["Run Chen","Jun Shin","Julia Hirschberg"],"pdf_url":"https://arxiv.org/pdf/2502.17857v1.pdf","comment":"10 pages"},{"id":"http://arxiv.org/abs/2502.16820v2","updated":"2025-02-25T05:03:51Z","published":"2025-02-24T04:05:08Z","title":"Uncertainty Quantification of Large Language Models through\n  Multi-Dimensional Responses","summary":"  Large Language Models (LLMs) have demonstrated remarkable capabilities across\nvarious tasks due to large training datasets and powerful transformer\narchitecture. However, the reliability of responses from LLMs remains a\nquestion. Uncertainty quantification (UQ) of LLMs is crucial for ensuring their\nreliability, especially in areas such as healthcare, finance, and\ndecision-making. Existing UQ methods primarily focus on semantic similarity,\noverlooking the deeper knowledge dimensions embedded in responses. We introduce\na multi-dimensional UQ framework that integrates semantic and knowledge-aware\nsimilarity analysis. By generating multiple responses and leveraging auxiliary\nLLMs to extract implicit knowledge, we construct separate similarity matrices\nand apply tensor decomposition to derive a comprehensive uncertainty\nrepresentation. This approach disentangles overlapping information from both\nsemantic and knowledge dimensions, capturing both semantic variations and\nfactual consistency, leading to more accurate UQ. Our empirical evaluations\ndemonstrate that our method outperforms existing techniques in identifying\nuncertain responses, offering a more robust framework for enhancing LLM\nreliability in high-stakes applications.\n","authors":["Tiejin Chen","Xiaoou Liu","Longchao Da","Jia Chen","Vagelis Papalexakis","Hua Wei"],"pdf_url":"https://arxiv.org/pdf/2502.16820v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.10762v2","updated":"2025-02-25T04:56:05Z","published":"2024-10-14T17:40:40Z","title":"AFlow: Automating Agentic Workflow Generation","summary":"  Large language models (LLMs) have demonstrated remarkable potential in\nsolving complex tasks across diverse domains, typically by employing agentic\nworkflows that follow detailed instructions and operational sequences. However,\nconstructing these workflows requires significant human effort, limiting\nscalability and generalizability. Recent research has sought to automate the\ngeneration and optimization of these workflows, but existing methods still rely\non initial manual setup and fall short of achieving fully automated and\neffective workflow generation. To address this challenge, we reformulate\nworkflow optimization as a search problem over code-represented workflows,\nwhere LLM-invoking nodes are connected by edges. We introduce AFlow, an\nautomated framework that efficiently explores this space using Monte Carlo Tree\nSearch, iteratively refining workflows through code modification,\ntree-structured experience, and execution feedback. Empirical evaluations\nacross six benchmark datasets demonstrate AFlow's efficacy, yielding a 5.7%\naverage improvement over state-of-the-art baselines. Furthermore, AFlow enables\nsmaller models to outperform GPT-4o on specific tasks at 4.55% of its inference\ncost in dollars. The code will be available at\nhttps://github.com/geekan/MetaGPT.\n","authors":["Jiayi Zhang","Jinyu Xiang","Zhaoyang Yu","Fengwei Teng","Xionghui Chen","Jiaqi Chen","Mingchen Zhuge","Xin Cheng","Sirui Hong","Jinlin Wang","Bingnan Zheng","Bang Liu","Yuyu Luo","Chenglin Wu"],"pdf_url":"https://arxiv.org/pdf/2410.10762v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.17848v1","updated":"2025-02-25T04:51:17Z","published":"2025-02-25T04:51:17Z","title":"LR${}^{2}$Bench: Evaluating Long-chain Reflective Reasoning Capabilities\n  of Large Language Models via Constraint Satisfaction Problems","summary":"  Recent progress in o1-like models has significantly enhanced the reasoning\nabilities of Large Language Models (LLMs), empowering them to tackle\nincreasingly complex tasks through reflection capabilities, such as making\nassumptions, backtracking, and self-refinement. However, effectively evaluating\nsuch reflection capabilities remains challenging due to the lack of appropriate\nbenchmarks. To bridge this gap, we introduce LR${}^{2}$Bench, a novel benchmark\ndesigned to evaluate the Long-chain Reflective Reasoning capabilities of LLMs.\nLR${}^{2}$Bench comprises 850 samples across six Constraint Satisfaction\nProblems (CSPs) where reflective reasoning is crucial for deriving solutions\nthat meet all given constraints. Each type of task focuses on distinct\nconstraint patterns, such as knowledge-based, logical, and spatial constraints,\nproviding a comprehensive evaluation of diverse problem-solving scenarios. We\nconduct extensive evaluation on both conventional models and o1-like models.\nOur experimental results reveal that even the most advanced reasoning-specific\nmodels, such as DeepSeek-R1 and OpenAI o1-preview, struggle with tasks in\nLR${}^{2}$Bench, achieving an average Exact Match score of only 20.0% and\n23.6%, respectively. These findings underscore the significant room for\nimprovement in the reflective reasoning capabilities of current LLMs. The\nleaderboard of our benchmark is available at\nhttps://huggingface.co/spaces/UltraRonin/LR2Bench\n","authors":["Jianghao Chen","Zhenlin Wei","Zhenjiang Ren","Ziyong Li","Jiajun Zhang"],"pdf_url":"https://arxiv.org/pdf/2502.17848v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.17839v1","updated":"2025-02-25T04:38:38Z","published":"2025-02-25T04:38:38Z","title":"Say Less, Mean More: Leveraging Pragmatics in Retrieval-Augmented\n  Generation","summary":"  We propose a simple, unsupervised method that injects pragmatic principles in\nretrieval-augmented generation (RAG) frameworks such as Dense Passage\nRetrieval~\\cite{karpukhin2020densepassageretrievalopendomain} to enhance the\nutility of retrieved contexts. Our approach first identifies which sentences in\na pool of documents retrieved by RAG are most relevant to the question at hand,\ncover all the topics addressed in the input question and no more, and then\nhighlights these sentences within their context, before they are provided to\nthe LLM, without truncating or altering the context in any other way. We show\nthat this simple idea brings consistent improvements in experiments on three\nquestion answering tasks (ARC-Challenge, PubHealth and PopQA) using five\ndifferent LLMs. It notably enhances relative accuracy by up to 19.7\\% on\nPubHealth and 10\\% on ARC-Challenge compared to a conventional RAG system.\n","authors":["Haris Riaz","Ellen Riloff","Mihai Surdeanu"],"pdf_url":"https://arxiv.org/pdf/2502.17839v1.pdf","comment":"16 pages, 2 figures"},{"id":"http://arxiv.org/abs/2409.11598v3","updated":"2025-02-25T04:13:34Z","published":"2024-09-17T23:10:04Z","title":"Towards Fair RAG: On the Impact of Fair Ranking in Retrieval-Augmented\n  Generation","summary":"  Modern language models frequently include retrieval components to improve\ntheir outputs, giving rise to a growing number of retrieval-augmented\ngeneration (RAG) systems. Yet, most existing work in RAG has underemphasized\nfair ranking techniques and neglected the diverse interests of all\nstakeholders. In this paper, we present the first comprehensive study of RAG\nsystems that incorporate fairness-aware rankings, focusing on both ranking\nfairness and attribution fairness - ensuring equitable exposure of sources\ncited in the final text. We specifically examine item-side fairness, i.e.,\nwhether retrieved documents receive balanced exposure, and assess how this\naffects both the system's overall performance and the eventual distribution of\ncited sources. Across twelve RAG models and seven tasks, we find that\nfairness-aware retrieval frequently retains or even improves ranking\neffectiveness and generation quality, countering the widespread belief that\nfairness compromises system performance. Moreover, we show that fair retrieval\nleads to more balanced attribution in the final responses, ensuring that the\ncited sources are credited more equitably. Our results underscore the\nimportance of item-side fairness throughout both retrieval and generation\nphases, offering key insights for building more responsible and equitable RAG\nsystems and illustrating promising avenues for future exploration in fair\nranking and source attribution.\n","authors":["To Eun Kim","Fernando Diaz"],"pdf_url":"https://arxiv.org/pdf/2409.11598v3.pdf","comment":"Top 5 Spotlight at AFME Workshop at NeurIPS 2024"},{"id":"http://arxiv.org/abs/2502.16600v2","updated":"2025-02-25T04:13:25Z","published":"2025-02-23T15:00:53Z","title":"Revealing the Pragmatic Dilemma for Moral Reasoning Acquisition in\n  Language Models","summary":"  Ensuring that Large Language Models (LLMs) return just responses which adhere\nto societal values is crucial for their broader application. Prior research has\nshown that LLMs often fail to perform satisfactorily on tasks requiring moral\ncognizance, such as ethics-based judgments. While current approaches have\nfocused on fine-tuning LLMs with curated datasets to improve their capabilities\non such tasks, choosing the optimal learning paradigm to enhance the ethical\nresponses of LLMs remains an open research debate. In this work, we aim to\naddress this fundamental question: can current learning paradigms enable LLMs\nto acquire sufficient moral reasoning capabilities? Drawing from distributional\nsemantics theory and the pragmatic nature of moral discourse, our analysis\nindicates that performance improvements follow a mechanism similar to that of\nsemantic-level tasks, and therefore remain affected by the pragmatic nature of\nmorals latent in discourse, a phenomenon we name the pragmatic dilemma. We\nconclude that this pragmatic dilemma imposes significant limitations on the\ngeneralization ability of current learning paradigms, making it the primary\nbottleneck for moral reasoning acquisition in LLMs.\n","authors":["Guangliang Liu","Lei Jiang","Xitong Zhang","Kristen Marie Johnson"],"pdf_url":"https://arxiv.org/pdf/2502.16600v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.17823v1","updated":"2025-02-25T04:03:04Z","published":"2025-02-25T04:03:04Z","title":"A General Framework to Enhance Fine-tuning-based LLM Unlearning","summary":"  Unlearning has been proposed to remove copyrighted and privacy-sensitive data\nfrom Large Language Models (LLMs). Existing approaches primarily rely on\nfine-tuning-based methods, which can be categorized into gradient ascent-based\n(GA-based) and suppression-based methods. However, they often degrade model\nutility (the ability to respond to normal prompts). In this work, we aim to\ndevelop a general framework that enhances the utility of fine-tuning-based\nunlearning methods. To achieve this goal, we first investigate the common\nproperty between GA-based and suppression-based methods. We unveil that\nGA-based methods unlearn by distinguishing the target data (i.e., the data to\nbe removed) and suppressing related generations, which is essentially the same\nstrategy employed by suppression-based methods. Inspired by this finding, we\nintroduce Gated Representation UNlearning (GRUN) which has two components: a\nsoft gate function for distinguishing target data and a suppression module\nusing Representation Fine-tuning (ReFT) to adjust representations rather than\nmodel parameters. Experiments show that GRUN significantly improves the\nunlearning and utility. Meanwhile, it is general for fine-tuning-based methods,\nefficient and promising for sequential unlearning.\n","authors":["Jie Ren","Zhenwei Dai","Xianfeng Tang","Hui Liu","Jingying Zeng","Zhen Li","Rahul Goutam","Suhang Wang","Yue Xing","Qi He","Hui Liu"],"pdf_url":"https://arxiv.org/pdf/2502.17823v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.11000v3","updated":"2025-02-25T03:55:35Z","published":"2024-02-16T17:03:05Z","title":"ASGEA: Exploiting Logic Rules from Align-Subgraphs for Entity Alignment","summary":"  Entity alignment (EA) aims to identify entities across different knowledge\ngraphs that represent the same real-world objects. Recent embedding-based EA\nmethods have achieved state-of-the-art performance in EA yet faced\ninterpretability challenges as they purely rely on the embedding distance and\nneglect the logic rules behind a pair of aligned entities. In this paper, we\npropose the Align-Subgraph Entity Alignment (ASGEA) framework to exploit logic\nrules from Align-Subgraphs. ASGEA uses anchor links as bridges to construct\nAlign-Subgraphs and spreads along the paths across KGs, which distinguishes it\nfrom the embedding-based methods. Furthermore, we design an interpretable\nPath-based Graph Neural Network, ASGNN, to effectively identify and integrate\nthe logic rules across KGs. We also introduce a node-level multi-modal\nattention mechanism coupled with multi-modal enriched anchors to augment the\nAlign-Subgraph. Our experimental results demonstrate the superior performance\nof ASGEA over the existing embedding-based methods in both EA and Multi-Modal\nEA (MMEA) tasks.\n","authors":["Yangyifei Luo","Zhuo Chen","Lingbing Guo","Qian Li","Wenxuan Zeng","Zhixin Cai","Jianxin Li"],"pdf_url":"https://arxiv.org/pdf/2402.11000v3.pdf","comment":"Ongoing work; 16 pages, 9 Tables, 8 Figures; Code:\n  https://github.com/lyyf2002/ASGEA"},{"id":"http://arxiv.org/abs/2502.17817v1","updated":"2025-02-25T03:48:19Z","published":"2025-02-25T03:48:19Z","title":"Predicting Through Generation: Why Generation Is Better for Prediction","summary":"  This paper argues that generating output tokens is more effective than using\npooled representations for prediction tasks because token-level generation\nretains more mutual information. Since LLMs are trained on massive text corpora\nusing next-token prediction, generation aligns naturally with their learned\nbehavior. Using the Data Processing Inequality (DPI), we provide both\ntheoretical and empirical evidence supporting this claim. However,\nautoregressive models face two key challenges when used for prediction: (1)\nexposure bias, where the model sees ground truth tokens during training but\nrelies on its own predictions during inference, leading to errors, and (2)\nformat mismatch, where discrete tokens do not always align with the tasks\nrequired output structure. To address these challenges, we introduce\nPredGen(Predicting Through Generating), an end to end framework that (i) uses\nscheduled sampling to reduce exposure bias, and (ii) introduces a task adapter\nto convert the generated tokens into structured outputs. Additionally, we\nintroduce Writer-Director Alignment Loss (WDAL), which ensures consistency\nbetween token generation and final task predictions, improving both text\ncoherence and numerical accuracy. We evaluate PredGen on multiple\nclassification and regression benchmarks. Our results show that PredGen\nconsistently outperforms standard baselines, demonstrating its effectiveness in\nstructured prediction tasks.\n","authors":["Md Kowsher","Nusrat Jahan Prottasha","Prakash Bhat","Chun-Nam Yu","Mojtaba Soltanalian","Ivan Garibay","Ozlem Garibay","Chen Chen","Niloofar Yousefi"],"pdf_url":"https://arxiv.org/pdf/2502.17817v1.pdf","comment":"Preprint paper"},{"id":"http://arxiv.org/abs/2502.04420v3","updated":"2025-02-25T03:42:15Z","published":"2025-02-06T15:26:26Z","title":"KVTuner: Sensitivity-Aware Layer-wise Mixed Precision KV Cache\n  Quantization for Efficient and Nearly Lossless LLM Inference","summary":"  KV cache quantization can improve Large Language Models (LLMs) inference\nthroughput and latency in long contexts and large batch-size scenarios while\npreserving LLMs effectiveness. However, current methods have three unsolved\nissues: overlooking layer-wise sensitivity to KV cache quantization, high\noverhead of online fine-grained decision-making, and low flexibility to\ndifferent LLMs and constraints. Therefore, we thoroughly analyze the inherent\ncorrelation of layer-wise transformer attention patterns to KV cache\nquantization errors and study why key cache is more important than value cache\nfor quantization error reduction. We further propose a simple yet effective\nframework KVTuner to adaptively search for the optimal hardware-friendly\nlayer-wise KV quantization precision pairs for coarse-grained KV cache with\nmulti-objective optimization and directly utilize the offline searched\nconfigurations during online inference. To reduce the computational cost of\noffline calibration, we utilize the intra-layer KV precision pair pruning and\ninter-layer clustering to reduce the search space. Experimental results show\nthat we can achieve nearly lossless 3.25-bit mixed precision KV cache\nquantization for LLMs like Llama-3.1-8B-Instruct and 4.0-bit for sensitive\nmodels like Qwen2.5-7B-Instruct on mathematical reasoning tasks. The maximum\ninference throughput can be improved by 38.3% compared with KV8 quantization\nover various context lengths. Our code and searched configurations are\navailable at https://github.com/cmd2001/KVTuner.\n","authors":["Xing Li","Zeyu Xing","Yiming Li","Linping Qu","Hui-Ling Zhen","Wulong Liu","Yiwu Yao","Sinno Jialin Pan","Mingxuan Yuan"],"pdf_url":"https://arxiv.org/pdf/2502.04420v3.pdf","comment":"36 pages. Code: https://github.com/cmd2001/KVTuner"},{"id":"http://arxiv.org/abs/2502.17814v1","updated":"2025-02-25T03:40:36Z","published":"2025-02-25T03:40:36Z","title":"An Overview of Large Language Models for Statisticians","summary":"  Large Language Models (LLMs) have emerged as transformative tools in\nartificial intelligence (AI), exhibiting remarkable capabilities across diverse\ntasks such as text generation, reasoning, and decision-making. While their\nsuccess has primarily been driven by advances in computational power and deep\nlearning architectures, emerging problems -- in areas such as uncertainty\nquantification, decision-making, causal inference, and distribution shift --\nrequire a deeper engagement with the field of statistics. This paper explores\npotential areas where statisticians can make important contributions to the\ndevelopment of LLMs, particularly those that aim to engender trustworthiness\nand transparency for human users. Thus, we focus on issues such as uncertainty\nquantification, interpretability, fairness, privacy, watermarking and model\nadaptation. We also consider possible roles for LLMs in statistical analysis.\nBy bridging AI and statistics, we aim to foster a deeper collaboration that\nadvances both the theoretical foundations and practical applications of LLMs,\nultimately shaping their role in addressing complex societal challenges.\n","authors":["Wenlong Ji","Weizhe Yuan","Emily Getzen","Kyunghyun Cho","Michael I. Jordan","Song Mei","Jason E Weston","Weijie J. Su","Jing Xu","Linjun Zhang"],"pdf_url":"https://arxiv.org/pdf/2502.17814v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.17812v1","updated":"2025-02-25T03:37:43Z","published":"2025-02-25T03:37:43Z","title":"Can Multimodal LLMs Perform Time Series Anomaly Detection?","summary":"  Large language models (LLMs) have been increasingly used in time series\nanalysis. However, the potential of multimodal LLMs (MLLMs), particularly\nvision-language models, for time series remains largely under-explored. One\nnatural way for humans to detect time series anomalies is through visualization\nand textual description. Motivated by this, we raise a critical and practical\nresearch question: Can multimodal LLMs perform time series anomaly detection?\nTo answer this, we propose VisualTimeAnomaly benchmark to evaluate MLLMs in\ntime series anomaly detection (TSAD). Our approach transforms time series\nnumerical data into the image format and feed these images into various MLLMs,\nincluding proprietary models (GPT-4o and Gemini-1.5) and open-source models\n(LLaVA-NeXT and Qwen2-VL), each with one larger and one smaller variant. In\ntotal, VisualTimeAnomaly contains 12.4k time series images spanning 3 scenarios\nand 3 anomaly granularities with 9 anomaly types across 8 MLLMs. Starting with\nthe univariate case (point- and range-wise anomalies), we extend our evaluation\nto more practical scenarios, including multivariate and irregular time series\nscenarios, and variate-wise anomalies. Our study reveals several key insights:\n  1) MLLMs detect range- and variate-wise anomalies more effectively than\npoint-wise anomalies.\n  2) MLLMs are highly robust to irregular time series, even with 25% of the\ndata missing.\n  3) Open-source MLLMs perform comparably to proprietary models in TSAD. While\nopen-source MLLMs excel on univariate time series, proprietary MLLMs\ndemonstrate superior effectiveness on multivariate time series.\n  To the best of our knowledge, this is the first work to comprehensively\ninvestigate MLLMs for TSAD, particularly for multivariate and irregular time\nseries scenarios. We release our dataset and code at\nhttps://github.com/mllm-ts/VisualTimeAnomaly to support future research.\n","authors":["Xiongxiao Xu","Haoran Wang","Yueqing Liang","Philip S. Yu","Yue Zhao","Kai Shu"],"pdf_url":"https://arxiv.org/pdf/2502.17812v1.pdf","comment":"9 pages for the main content; 32 pages for the full paper including\n  the appendix. More resources on the intersection of multimodal LLMs and time\n  series analysis are on the website https://mllm-ts.github.io"},{"id":"http://arxiv.org/abs/2405.15349v3","updated":"2025-02-25T03:33:47Z","published":"2024-05-24T08:42:40Z","title":"Everything is Editable: Extend Knowledge Editing to Unstructured Data in\n  Large Language Models","summary":"  Recent knowledge editing methods have primarily focused on modifying\nstructured knowledge in large language models. However, this task setting\noverlooks the fact that a significant portion of real-world knowledge is stored\nin an unstructured format, characterized by long-form content, noise, and a\ncomplex yet comprehensive nature. Techniques like \"local layer key-value\nstorage\" and \"term-driven optimization\", as used in previous methods like\nMEMIT, are not effective for handling unstructured knowledge. To address these\nchallenges, we propose a novel Unstructured Knowledge Editing method, namely\nUnKE, which extends previous assumptions in the layer dimension and token\ndimension. Firstly, in the layer dimension, we propose non-local block\nkey-value storage to replace local layer key-value storage, increasing the\nrepresentation ability of key-value pairs and incorporating attention layer\nknowledge. Secondly, in the token dimension, we replace \"term-driven\noptimization\" with \"cause-driven optimization\", which edits the last token\ndirectly while preserving context, avoiding the need to locate terms and\npreventing the loss of context information. Results on newly proposed\nunstructured knowledge editing dataset (UnKEBench) and traditional structured\ndatasets demonstrate that UnKE achieves remarkable performance, surpassing\nstrong baselines. In addition, UnKE has robust batch editing and sequential\nediting capabilities.\n","authors":["Jingcheng Deng","Zihao Wei","Liang Pang","Hanxing Ding","Huawei Shen","Xueqi Cheng"],"pdf_url":"https://arxiv.org/pdf/2405.15349v3.pdf","comment":"ICLR 2025"},{"id":"http://arxiv.org/abs/2502.17810v1","updated":"2025-02-25T03:31:48Z","published":"2025-02-25T03:31:48Z","title":"URO-Bench: A Comprehensive Benchmark for End-to-End Spoken Dialogue\n  Models","summary":"  In recent years, with advances in large language models (LLMs), end-to-end\nspoken dialogue models (SDMs) have made significant strides. Compared to\ntext-based LLMs, the evaluation of SDMs needs to take speech-related aspects\ninto account, such as paralinguistic information and speech quality. However,\nthere is still a lack of comprehensive evaluations for SDMs in speech-to-speech\n(S2S) scenarios. To address this gap, we propose URO-Bench, an extensive\nbenchmark for SDMs. Notably, URO-Bench is the first S2S benchmark that covers\nevaluations about multilingualism, multi-round dialogues, and paralinguistics.\nOur benchmark is divided into two difficulty levels: basic track and pro track,\nconsisting of 16 and 20 datasets respectively, evaluating the model's abilities\nin Understanding, Reasoning, and Oral conversation. Evaluations on our proposed\nbenchmark reveal that current open-source SDMs perform rather well in daily QA\ntasks, but lag behind their backbone LLMs in terms of instruction-following\nability and also suffer from catastrophic forgetting. Their performance in\nadvanced evaluations of paralinguistic information and audio understanding\nremains subpar, highlighting the need for further research in this direction.\nWe hope that URO-Bench can effectively facilitate the development of spoken\ndialogue models by providing a multifaceted evaluation of existing models and\nhelping to track progress in this area.\n","authors":["Ruiqi Yan","Xiquan Li","Wenxi Chen","Zhikang Niu","Chen Yang","Ziyang Ma","Kai Yu","Xie Chen"],"pdf_url":"https://arxiv.org/pdf/2502.17810v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.10573v2","updated":"2025-02-25T03:13:27Z","published":"2024-08-20T06:24:47Z","title":"Putting People in LLMs' Shoes: Generating Better Answers via Question\n  Rewriter","summary":"  Large Language Models (LLMs) have demonstrated significant capabilities,\nparticularly in the domain of question answering (QA). However, their\neffectiveness in QA is often undermined by the vagueness of user questions. To\naddress this issue, we introduce single-round instance-level prompt\noptimization, referred to as question rewriter. By enhancing the\nintelligibility of human questions for black-box LLMs, our question rewriter\nimproves the quality of generated answers. The rewriter is optimized using\ndirect preference optimization based on feedback collected from automatic\ncriteria for evaluating generated answers; therefore, its training does not\nrequire costly human annotations. The experiments across multiple black-box\nLLMs and long-form question answering (LFQA) datasets demonstrate the efficacy\nof our method. This paper provides a practical framework for training question\nrewriters and sets a precedent for future explorations in prompt optimization\nwithin LFQA tasks. Code is available at\nhttps://github.com/3244we/Question-Rewriter.\n","authors":["Junhao Chen","Bowen Wang","Zhouqiang Jiang","Yuta Nakashima"],"pdf_url":"https://arxiv.org/pdf/2408.10573v2.pdf","comment":"7 pages, 4 figures, 5 tables and accepted at AAAI 2025 Main\n  Conference"},{"id":"http://arxiv.org/abs/2502.17800v1","updated":"2025-02-25T03:03:35Z","published":"2025-02-25T03:03:35Z","title":"Your Language Model May Think Too Rigidly: Achieving Reasoning\n  Consistency with Symmetry-Enhanced Training","summary":"  Large Language Models (LLMs) have demonstrated strong reasoning capabilities\nacross various tasks. However, even minor variations in query phrasing, despite\npreserving the underlying semantic meaning, can significantly affect their\nperformance. To address this, we focus on enhancing LLMs' awareness of symmetry\nin query variations and propose syMmetry-ENhanceD (MEND) Data Augmentation, a\ndata-centric approach that improves the model's ability to extract useful\ninformation from context. Unlike existing methods that emphasize reasoning\nchain augmentation, our approach improves model robustness at the knowledge\nextraction stage through query augmentations, enabling more data-efficient\ntraining and stronger generalization to Out-of-Distribution (OOD) settings.\nExtensive experiments on both logical and arithmetic reasoning tasks show that\nMEND enhances reasoning performance across diverse query variations, providing\nnew insight into improving LLM robustness through structured dataset curation.\n","authors":["Yihang Yao","Zhepeng Cen","Miao Li","William Han","Yuyou Zhang","Emerson Liu","Zuxin Liu","Chuang Gan","Ding Zhao"],"pdf_url":"https://arxiv.org/pdf/2502.17800v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.17797v1","updated":"2025-02-25T03:02:24Z","published":"2025-02-25T03:02:24Z","title":"Enhancing Human Evaluation in Machine Translation with Comparative\n  Judgment","summary":"  Human evaluation is crucial for assessing rapidly evolving language models\nbut is influenced by annotator proficiency and task design. This study explores\nthe integration of comparative judgment into human annotation for machine\ntranslation (MT) and evaluates three annotation setups-point-wise\nMultidimensional Quality Metrics (MQM), side-by-side (SxS) MQM, and its\nsimplified version SxS relative ranking (RR). In MQM, annotators mark error\nspans with categories and severity levels. SxS MQM extends MQM to pairwise\nerror annotation for two translations of the same input, while SxS RR focuses\non selecting the better output without labeling errors.\n  Key findings are: (1) the SxS settings achieve higher inter-annotator\nagreement than MQM; (2) SxS MQM enhances inter-translation error marking\nconsistency compared to MQM by, on average, 38.5% for explicitly compared MT\nsystems and 19.5% for others; (3) all annotation settings return stable system\nrankings, with SxS RR offering a more efficient alternative to (SxS) MQM; (4)\nthe SxS settings highlight subtle errors overlooked in MQM without altering\nabsolute system evaluations.\n  To spur further research, we will release the triply annotated datasets\ncomprising 377 ZhEn and 104 EnDe annotation examples.\n","authors":["Yixiao Song","Parker Riley","Daniel Deutsch","Markus Freitag"],"pdf_url":"https://arxiv.org/pdf/2502.17797v1.pdf","comment":"Preprint, 15 pages"},{"id":"http://arxiv.org/abs/2502.16806v2","updated":"2025-02-25T02:52:29Z","published":"2025-02-24T03:30:29Z","title":"CoT2Align: Cross-Chain of Thought Distillation via Optimal Transport\n  Alignment for Language Models with Different Tokenizers","summary":"  Large Language Models (LLMs) achieve state-of-the-art performance across\nvarious NLP tasks but face deployment challenges due to high computational\ncosts and memory constraints. Knowledge distillation (KD) is a promising\nsolution, transferring knowledge from large teacher models to smaller student\nmodels. However, existing KD methods often assume shared vocabularies and\ntokenizers, limiting their flexibility. While approaches like Universal Logit\nDistillation (ULD) and Dual-Space Knowledge Distillation (DSKD) address\nvocabulary mismatches, they overlook the critical \\textbf{reasoning-aware\ndistillation} aspect. To bridge this gap, we propose CoT2Align a universal KD\nframework that integrates Chain-of-Thought (CoT) augmentation and introduces\nCross-CoT Alignment to enhance reasoning transfer. Additionally, we extend\nOptimal Transport beyond token-wise alignment to a sequence-level and\nlayer-wise alignment approach that adapts to varying sequence lengths while\npreserving contextual integrity. Comprehensive experiments demonstrate that\nCoT2Align outperforms existing KD methods across different vocabulary settings,\nimproving reasoning capabilities and robustness in domain-specific tasks.\n","authors":["Anh Duc Le","Tu Vu","Nam Le Hai","Nguyen Thi Ngoc Diep","Linh Ngo Van","Trung Le","Thien Huu Nguyen"],"pdf_url":"https://arxiv.org/pdf/2502.16806v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.13708v5","updated":"2025-02-25T02:40:44Z","published":"2023-11-22T21:59:46Z","title":"Dynamic Fault Analysis in Substations Based on Knowledge Graphs","summary":"  To address the challenge of identifying hidden danger in substations from\nunstructured text, a novel dynamic analysis method is proposed. We first\nextract relevant information from the unstructured text, and then leverages a\nflexible distributed search engine built on Elastic-Search to handle the data.\nFollowing this, the hidden Markov model is employed to train the data within\nthe engine. The Viterbi algorithm is integrated to decipher the hidden state\nsequences, facilitating the segmentation and labeling of entities related to\nhidden dangers. The final step involves using the Neo4j graph database to\ndynamically create a knowledge graph that visualizes hidden dangers in the\nsubstation. The effectiveness of the proposed method is demonstrated through a\ncase analysis from a specific substation with hidden dangers revealed in the\ntext records.\n","authors":["Weiwei Li","Xing Liu","Wei Wang","Lu Chen","Sizhe Li","Hui Fan"],"pdf_url":"https://arxiv.org/pdf/2311.13708v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.17787v1","updated":"2025-02-25T02:39:57Z","published":"2025-02-25T02:39:57Z","title":"AIR: Complex Instruction Generation via Automatic Iterative Refinement","summary":"  With the development of large language models, their ability to follow simple\ninstructions has significantly improved. However, adhering to complex\ninstructions remains a major challenge. Current approaches to generating\ncomplex instructions are often irrelevant to the current instruction\nrequirements or suffer from limited scalability and diversity. Moreover,\nmethods such as back-translation, while effective for simple instruction\ngeneration, fail to leverage the rich contents and structures in large web\ncorpora. In this paper, we propose a novel automatic iterative refinement\nframework to generate complex instructions with constraints, which not only\nbetter reflects the requirements of real scenarios but also significantly\nenhances LLMs' ability to follow complex instructions. The AIR framework\nconsists of two stages: (1)Generate an initial instruction from a document;\n(2)Iteratively refine instructions with LLM-as-judge guidance by comparing the\nmodel's output with the document to incorporate valuable constraints. Finally,\nwe construct the AIR-10K dataset with 10K complex instructions and demonstrate\nthat instructions generated with our approach significantly improve the model's\nability to follow complex instructions, outperforming existing methods for\ninstruction generation.\n","authors":["Wei Liu","Yancheng He","Hui Huang","Chengwei Hu","Jiaheng Liu","Shilong Li","Wenbo Su","Bo Zheng"],"pdf_url":"https://arxiv.org/pdf/2502.17787v1.pdf","comment":"The first three authors contributed equally, 20 pages"},{"id":"http://arxiv.org/abs/2502.17785v1","updated":"2025-02-25T02:28:48Z","published":"2025-02-25T02:28:48Z","title":"Exploring the Potential of Large Language Models for Estimating the\n  Reading Comprehension Question Difficulty","summary":"  Reading comprehension is a key for individual success, yet the assessment of\nquestion difficulty remains challenging due to the extensive human annotation\nand large-scale testing required by traditional methods such as linguistic\nanalysis and Item Response Theory (IRT). While these robust approaches provide\nvaluable insights, their scalability is limited. There is potential for Large\nLanguage Models (LLMs) to automate question difficulty estimation; however,\nthis area remains underexplored. Our study investigates the effectiveness of\nLLMs, specifically OpenAI's GPT-4o and o1, in estimating the difficulty of\nreading comprehension questions using the Study Aid and Reading Assessment\n(SARA) dataset. We evaluated both the accuracy of the models in answering\ncomprehension questions and their ability to classify difficulty levels as\ndefined by IRT. The results indicate that, while the models yield difficulty\nestimates that align meaningfully with derived IRT parameters, there are\nnotable differences in their sensitivity to extreme item characteristics. These\nfindings suggest that LLMs can serve as the scalable method for automated\ndifficulty assessment, particularly in dynamic interactions between learners\nand Adaptive Instructional Systems (AIS), bridging the gap between traditional\npsychometric techniques and modern AIS for reading comprehension and paving the\nway for more adaptive and personalized educational assessments.\n","authors":["Yoshee Jain","John Hollander","Amber He","Sunny Tang","Liang Zhang","John Sabatini"],"pdf_url":"https://arxiv.org/pdf/2502.17785v1.pdf","comment":"13 pages, 2 figures"},{"id":"http://arxiv.org/abs/2502.17784v1","updated":"2025-02-25T02:27:32Z","published":"2025-02-25T02:27:32Z","title":"MuCoS: Efficient Drug-Target Prediction through Multi-Context-Aware\n  Sampling","summary":"  Drug-target interactions are critical for understanding biological processes\nand advancing drug discovery. However, traditional methods such as ComplEx-SE,\nTransE, and DistMult struggle with unseen relationships and negative triplets,\nwhich limits their effectiveness in drug-target prediction. To address these\nchallenges, we propose Multi-Context-Aware Sampling (MuCoS), an efficient and\npositively accurate method for drug-target prediction. MuCoS reduces\ncomputational complexity by prioritizing neighbors of higher density to capture\ninformative structural patterns. These optimized neighborhood representations\nare integrated with BERT, enabling contextualized embeddings for accurate\nprediction of missing relationships or tail entities. MuCoS avoids the need for\nnegative triplet sampling, reducing computation while improving performance\nover unseen entities and relations. Experiments on the KEGG50k biomedical\ndataset show that MuCoS improved over existing models by 13\\% on MRR, 7\\% on\nHits@1, 4\\% on Hits@3, and 18\\% on Hits@10 for the general relationship, and by\n6\\% on MRR, 1\\% on Hits@1, 3\\% on Hits@3, and 12\\% on Hits@10 for prediction of\ndrug-target relationship.\n","authors":["Haji Gul","Abdul Gani Haji Naim","Ajaz A. Bhat"],"pdf_url":"https://arxiv.org/pdf/2502.17784v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.02486v2","updated":"2025-02-25T02:25:39Z","published":"2025-01-05T09:37:23Z","title":"LLMPC: Large Language Model Predictive Control","summary":"  Recent advancements in prompting techniques for Large Language Models (LLMs)\nhave improved their reasoning, planning, and action abilities. This paper\nexamines these prompting techniques through the lens of model predictive\ncontrol (MPC). We show that LLMs act as implicit planning cost function\nminimizers when planning prompts are used. We propose a unified MPC framework\nfor planning with LLMs and demonstrate improved performance over few shot\nprompting on several planning benchmarks.\n","authors":["Gabriel Maher"],"pdf_url":"https://arxiv.org/pdf/2501.02486v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.03521v2","updated":"2025-02-25T02:17:05Z","published":"2024-09-27T00:01:32Z","title":"Building a Chinese Medical Dialogue System: Integrating Large-scale\n  Corpora and Novel Models","summary":"  The global COVID-19 pandemic underscored major deficiencies in traditional\nhealthcare systems, hastening the advancement of online medical services,\nespecially in medical triage and consultation. However, existing studies face\ntwo main challenges. First, the scarcity of large-scale, publicly available,\ndomain-specific medical datasets due to privacy concerns, with current datasets\nbeing small and limited to a few diseases, limiting the effectiveness of triage\nmethods based on Pre-trained Language Models (PLMs). Second, existing methods\nlack medical knowledge and struggle to accurately understand professional terms\nand expressions in patient-doctor consultations. To overcome these obstacles,\nwe construct the Large-scale Chinese Medical Dialogue Corpora (LCMDC), thereby\naddressing the data shortage in this field. Moreover, we further propose a\nnovel triage system that combines BERT-based supervised learning with prompt\nlearning, as well as a GPT-based medical consultation model. To enhance domain\nknowledge acquisition, we pre-trained PLMs using our self-constructed\nbackground corpus. Experimental results on the LCMDC demonstrate the efficacy\nof our proposed systems.\n","authors":["Xinyuan Wang","Haozhou Li","Dingfang Zheng","Qinke Peng"],"pdf_url":"https://arxiv.org/pdf/2410.03521v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.14321v2","updated":"2025-02-25T02:12:44Z","published":"2023-08-28T06:05:18Z","title":"Leveraging Medical Knowledge Graphs Into Large Language Models for\n  Diagnosis Prediction: Design and Application Study","summary":"  Electronic Health Records (EHRs) and routine documentation practices play a\nvital role in patients' daily care, providing a holistic record of health,\ndiagnoses, and treatment. However, complex and verbose EHR narratives overload\nhealthcare providers, risking diagnostic inaccuracies. While Large Language\nModels (LLMs) have showcased their potential in diverse language tasks, their\napplication in the healthcare arena needs to ensure the minimization of\ndiagnostic errors and the prevention of patient harm. In this paper, we outline\nan innovative approach for augmenting the proficiency of LLMs in the realm of\nautomated diagnosis generation, achieved through the incorporation of a medical\nknowledge graph (KG) and a novel graph model: Dr.Knows, inspired by the\nclinical diagnostic reasoning process. We derive the KG from the National\nLibrary of Medicine's Unified Medical Language System (UMLS), a robust\nrepository of biomedical knowledge. Our method negates the need for\npre-training and instead leverages the KG as an auxiliary instrument aiding in\nthe interpretation and summarization of complex medical concepts. Using\nreal-world hospital datasets, our experimental results demonstrate that the\nproposed approach of combining LLMs with KG has the potential to improve the\naccuracy of automated diagnosis generation. More importantly, our approach\noffers an explainable diagnostic pathway, edging us closer to the realization\nof AI-augmented diagnostic decision support systems.\n","authors":["Yanjun Gao","Ruizhe Li","Emma Croxford","John Caskey","Brian W Patterson","Matthew Churpek","Timothy Miller","Dmitriy Dligach","Majid Afshar"],"pdf_url":"https://arxiv.org/pdf/2308.14321v2.pdf","comment":"Published in JMIR AI"},{"id":"http://arxiv.org/abs/2502.17776v1","updated":"2025-02-25T02:11:42Z","published":"2025-02-25T02:11:42Z","title":"Tip of the Tongue Query Elicitation for Simulated Evaluation","summary":"  Tip-of-the-tongue (TOT) search occurs when a user struggles to recall a\nspecific identifier, such as a document title. While common, existing search\nsystems often fail to effectively support TOT scenarios. Research on TOT\nretrieval is further constrained by the challenge of collecting queries, as\ncurrent approaches rely heavily on community question-answering (CQA) websites,\nleading to labor-intensive evaluation and domain bias. To overcome these\nlimitations, we introduce two methods for eliciting TOT queries - leveraging\nlarge language models (LLMs) and human participants - to facilitate simulated\nevaluations of TOT retrieval systems. Our LLM-based TOT user simulator\ngenerates synthetic TOT queries at scale, achieving high correlations with how\nCQA-based TOT queries rank TOT retrieval systems when tested in the Movie\ndomain. Additionally, these synthetic queries exhibit high linguistic\nsimilarity to CQA-derived queries. For human-elicited queries, we developed an\ninterface that uses visual stimuli to place participants in a TOT state,\nenabling the collection of natural queries. In the Movie domain, system rank\ncorrelation and linguistic similarity analyses confirm that human-elicited\nqueries are both effective and closely resemble CQA-based queries. These\napproaches reduce reliance on CQA-based data collection while expanding\ncoverage to underrepresented domains, such as Landmark and Person. LLM-elicited\nqueries for the Movie, Landmark, and Person domains have been released as test\nqueries in the TREC 2024 TOT track, with human-elicited queries scheduled for\ninclusion in the TREC 2025 TOT track. Additionally, we provide source code for\nsynthetic query generation and the human query collection interface, along with\ncurated visual stimuli used for eliciting TOT queries.\n","authors":["Yifan He","To Eun Kim","Fernando Diaz","Jaime Arguello","Bhaskar Mitra"],"pdf_url":"https://arxiv.org/pdf/2502.17776v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.17775v1","updated":"2025-02-25T02:10:30Z","published":"2025-02-25T02:10:30Z","title":"FoREST: Frame of Reference Evaluation in Spatial Reasoning Tasks","summary":"  Spatial reasoning is a fundamental aspect of human intelligence. One key\nconcept in spatial cognition is the Frame of Reference (FoR), which identifies\nthe perspective of spatial expressions. Despite its significance, FoR has\nreceived limited attention in AI models that need spatial intelligence. There\nis a lack of dedicated benchmarks and in-depth evaluation of large language\nmodels (LLMs) in this area. To address this issue, we introduce the Frame of\nReference Evaluation in Spatial Reasoning Tasks (FoREST) benchmark, designed to\nassess FoR comprehension in LLMs. We evaluate LLMs on answering questions that\nrequire FoR comprehension and layout generation in text-to-image models using\nFoREST. Our results reveal a notable performance gap across different FoR\nclasses in various LLMs, affecting their ability to generate accurate layouts\nfor text-to-image generation. This highlights critical shortcomings in FoR\ncomprehension. To improve FoR understanding, we propose Spatial-Guided\nprompting, which improves LLMs ability to extract essential spatial concepts.\nOur proposed method improves overall performance across spatial reasoning\ntasks.\n","authors":["Tanawan Premsri","Parisa Kordjamshidi"],"pdf_url":"https://arxiv.org/pdf/2502.17775v1.pdf","comment":"9 pages"}]},"2025-02-26T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2406.06608v6","updated":"2025-02-26T18:59:01Z","published":"2024-06-06T18:10:11Z","title":"The Prompt Report: A Systematic Survey of Prompt Engineering Techniques","summary":"  Generative Artificial Intelligence (GenAI) systems are increasingly being\ndeployed across diverse industries and research domains. Developers and\nend-users interact with these systems through the use of prompting and prompt\nengineering. Although prompt engineering is a widely adopted and extensively\nresearched area, it suffers from conflicting terminology and a fragmented\nontological understanding of what constitutes an effective prompt due to its\nrelatively recent emergence. We establish a structured understanding of prompt\nengineering by assembling a taxonomy of prompting techniques and analyzing\ntheir applications. We present a detailed vocabulary of 33 vocabulary terms, a\ntaxonomy of 58 LLM prompting techniques, and 40 techniques for other\nmodalities. Additionally, we provide best practices and guidelines for prompt\nengineering, including advice for prompting state-of-the-art (SOTA) LLMs such\nas ChatGPT. We further present a meta-analysis of the entire literature on\nnatural language prefix-prompting. As a culmination of these efforts, this\npaper presents the most comprehensive survey on prompt engineering to date.\n","authors":["Sander Schulhoff","Michael Ilie","Nishant Balepur","Konstantine Kahadze","Amanda Liu","Chenglei Si","Yinheng Li","Aayush Gupta","HyoJung Han","Sevien Schulhoff","Pranav Sandeep Dulepet","Saurav Vidyadhara","Dayeon Ki","Sweta Agrawal","Chau Pham","Gerson Kroiz","Feileen Li","Hudson Tao","Ashay Srivastava","Hevander Da Costa","Saloni Gupta","Megan L. Rogers","Inna Goncearenco","Giuseppe Sarli","Igor Galynker","Denis Peskoff","Marine Carpuat","Jules White","Shyamal Anadkat","Alexander Hoyle","Philip Resnik"],"pdf_url":"https://arxiv.org/pdf/2406.06608v6.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.18578v2","updated":"2025-02-26T18:58:53Z","published":"2025-01-30T18:50:25Z","title":"R.I.P.: Better Models by Survival of the Fittest Prompts","summary":"  Training data quality is one of the most important drivers of final model\nquality. In this work, we introduce a method for evaluating data integrity\nbased on the assumption that low-quality input prompts result in high variance\nand low quality responses. This is achieved by measuring the rejected response\nquality and the reward gap between the chosen and rejected preference pair. Our\nmethod, Rejecting Instruction Preferences (RIP) can be used to filter prompts\nfrom existing training sets, or to make high quality synthetic datasets,\nyielding large performance gains across various benchmarks compared to\nunfiltered data. Using Llama 3.1-8B-Instruct, RIP improves AlpacaEval2 LC Win\nRate by 9.4%, Arena-Hard by 8.7%, and WildBench by 9.9%. Using Llama\n3.3-70B-Instruct, RIP improves Arena-Hard from 67.5 to 82.9, which is from 18th\nplace to 6th overall in the leaderboard.\n","authors":["Ping Yu","Weizhe Yuan","Olga Golovneva","Tianhao Wu","Sainbayar Sukhbaatar","Jason Weston","Jing Xu"],"pdf_url":"https://arxiv.org/pdf/2501.18578v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.19416v1","updated":"2025-02-26T18:58:30Z","published":"2025-02-26T18:58:30Z","title":"Norm Growth and Stability Challenges in Localized Sequential Knowledge\n  Editing","summary":"  This study investigates the impact of localized updates to large language\nmodels (LLMs), specifically in the context of knowledge editing - a task aimed\nat incorporating or modifying specific facts without altering broader model\ncapabilities. We first show that across different post-training interventions\nlike continuous pre-training, full fine-tuning and LORA-based fine-tuning, the\nFrobenius norm of the updated matrices always increases. This increasing norm\nis especially detrimental for localized knowledge editing, where only a subset\nof matrices are updated in a model . We reveal a consistent phenomenon across\nvarious editing techniques, including fine-tuning, hypernetwork-based\napproaches, and locate-and-edit methods: the norm of the updated matrix\ninvariably increases with successive updates. Such growth disrupts model\nbalance, particularly when isolated matrices are updated while the rest of the\nmodel remains static, leading to potential instability and degradation of\ndownstream performance. Upon deeper investigations of the intermediate\nactivation vectors, we find that the norm of internal activations decreases and\nis accompanied by shifts in the subspaces occupied by these activations, which\nshows that these activation vectors now occupy completely different regions in\nthe representation space compared to the unedited model. With our paper, we\nhighlight the technical challenges with continuous and localized sequential\nknowledge editing and their implications for maintaining model stability and\nutility.\n","authors":["Akshat Gupta","Christine Fang","Atahan Ozdemir","Maochuan Lu","Ahmed Alaa","Thomas Hartvigsen","Gopala Anumanchipalli"],"pdf_url":"https://arxiv.org/pdf/2502.19416v1.pdf","comment":"Accepted for Oral Presentation at KnowFM @ AAAI 2025. arXiv admin\n  note: text overlap with arXiv:2502.01636"},{"id":"http://arxiv.org/abs/2502.19413v1","updated":"2025-02-26T18:56:52Z","published":"2025-02-26T18:56:52Z","title":"Project Alexandria: Towards Freeing Scientific Knowledge from Copyright\n  Burdens via LLMs","summary":"  Paywalls, licenses and copyright rules often restrict the broad dissemination\nand reuse of scientific knowledge. We take the position that it is both legally\nand technically feasible to extract the scientific knowledge in scholarly\ntexts. Current methods, like text embeddings, fail to reliably preserve factual\ncontent, and simple paraphrasing may not be legally sound. We urge the\ncommunity to adopt a new idea: convert scholarly documents into Knowledge Units\nusing LLMs. These units use structured data capturing entities, attributes and\nrelationships without stylistic content. We provide evidence that Knowledge\nUnits: (1) form a legally defensible framework for sharing knowledge from\ncopyrighted research texts, based on legal analyses of German copyright law and\nU.S. Fair Use doctrine, and (2) preserve most (~95%) factual knowledge from\noriginal text, measured by MCQ performance on facts from the original\ncopyrighted text across four research domains. Freeing scientific knowledge\nfrom copyright promises transformative benefits for scientific research and\neducation by allowing language models to reuse important facts from copyrighted\ntext. To support this, we share open-source tools for converting research\ndocuments into Knowledge Units. Overall, our work posits the feasibility of\ndemocratizing access to scientific knowledge while respecting copyright.\n","authors":["Christoph Schuhmann","Gollam Rabby","Ameya Prabhu","Tawsif Ahmed","Andreas Hochlehnert","Huu Nguyen","Nick Akinci Heidrich","Ludwig Schmidt","Robert Kaczmarczyk","Sören Auer","Jenia Jitsev","Matthias Bethge"],"pdf_url":"https://arxiv.org/pdf/2502.19413v1.pdf","comment":"Technical Report"},{"id":"http://arxiv.org/abs/2502.19412v1","updated":"2025-02-26T18:56:38Z","published":"2025-02-26T18:56:38Z","title":"The Mighty ToRR: A Benchmark for Table Reasoning and Robustness","summary":"  Despite its real-world significance, model performance on tabular data\nremains underexplored, leaving uncertainty about which model to rely on and\nwhich prompt configuration to adopt. To address this gap, we create ToRR, a\nbenchmark for Table Reasoning and Robustness, that measures model performance\nand robustness on table-related tasks. The benchmark includes 10 datasets that\ncover different types of table reasoning capabilities across varied domains.\nToRR goes beyond model performance rankings, and is designed to reflect whether\nmodels can handle tabular data consistently and robustly, across a variety of\ncommon table representation formats. We present a leaderboard as well as\ncomprehensive analyses of the results of leading models over ToRR. Our results\nreveal a striking pattern of brittle model behavior, where even strong models\nare unable to perform robustly on tabular data tasks. Although no specific\ntable format leads to consistently better performance, we show that testing\nover multiple formats is crucial for reliably estimating model capabilities.\nMoreover, we show that the reliability boost from testing multiple prompts can\nbe equivalent to adding more test examples. Overall, our findings show that\ntable understanding and reasoning tasks remain a significant challenge.\n","authors":["Shir Ashury-Tahan","Yifan Mai","Rajmohan C","Ariel Gera","Yotam Perlitz","Asaf Yehudai","Elron Bandel","Leshem Choshen","Eyal Shnarch","Percy Liang","Michal Shmueli-Scheuer"],"pdf_url":"https://arxiv.org/pdf/2502.19412v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.04697v2","updated":"2025-02-26T18:55:54Z","published":"2024-12-06T01:20:16Z","title":"Privacy-Preserving Retrieval-Augmented Generation with Differential\n  Privacy","summary":"  With the recent remarkable advancement of large language models (LLMs), there\nhas been a growing interest in utilizing them in the domains with highly\nsensitive data that lies outside their training data. For this purpose,\nretrieval-augmented generation (RAG) is particularly effective -- it assists\nLLMs by directly providing relevant information from the external knowledge\nsources. However, without extra privacy safeguards, RAG outputs risk leaking\nsensitive information from the external data source. In this work, we explore\nRAG under differential privacy (DP), a formal guarantee of data privacy. The\nmain challenge with differentially private RAG is how to generate long accurate\nanswers within a moderate privacy budget. We address this by proposing an\nalgorithm that smartly spends privacy budget only for the tokens that require\nthe sensitive information and uses the non-private LLM for other tokens. Our\nextensive empirical evaluations reveal that our algorithm outperforms the\nnon-RAG baseline under a reasonable privacy budget of $\\epsilon\\approx 10$\nacross different models and datasets.\n","authors":["Tatsuki Koga","Ruihan Wu","Kamalika Chaudhuri"],"pdf_url":"https://arxiv.org/pdf/2412.04697v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.19411v1","updated":"2025-02-26T18:55:42Z","published":"2025-02-26T18:55:42Z","title":"Code to Think, Think to Code: A Survey on Code-Enhanced Reasoning and\n  Reasoning-Driven Code Intelligence in LLMs","summary":"  In large language models (LLMs), code and reasoning reinforce each other:\ncode offers an abstract, modular, and logic-driven structure that supports\nreasoning, while reasoning translates high-level goals into smaller, executable\nsteps that drive more advanced code intelligence. In this study, we examine how\ncode serves as a structured medium for enhancing reasoning: it provides\nverifiable execution paths, enforces logical decomposition, and enables runtime\nvalidation. We also explore how improvements in reasoning have transformed code\nintelligence from basic completion to advanced capabilities, enabling models to\naddress complex software engineering tasks through planning and debugging.\nFinally, we identify key challenges and propose future research directions to\nstrengthen this synergy, ultimately improving LLM's performance in both areas.\n","authors":["Dayu Yang","Tianyang Liu","Daoan Zhang","Antoine Simoulin","Xiaoyi Liu","Yuwei Cao","Zhaopu Teng","Xin Qian","Grey Yang","Jiebo Luo","Julian McAuley"],"pdf_url":"https://arxiv.org/pdf/2502.19411v1.pdf","comment":"Project Repo: https://github.com/dayuyang1999/Awesome-Code-Reasoning"},{"id":"http://arxiv.org/abs/2502.19409v1","updated":"2025-02-26T18:55:06Z","published":"2025-02-26T18:55:06Z","title":"ImageChain: Advancing Sequential Image-to-Text Reasoning in Multimodal\n  Large Language Models","summary":"  Reasoning over sequences of images remains a challenge for multimodal large\nlanguage models (MLLMs). While recent models incorporate multi-image data\nduring pre-training, they still struggle to recognize sequential structures,\noften treating images independently. This work introduces ImageChain, a\nframework that enhances MLLMs with sequential reasoning capabilities over image\ndata by modeling visual sequences as a multi-turn conversation. In ImageChain,\nimages are interleaved with corresponding textual descriptions to form a\ncontrolled dialogue that explicitly captures temporal dependencies and\nnarrative progression. Our method optimizes for the task of next-scene\ndescription, where the model generates a context-aware description of an\nupcoming scene based on preceding visual and textual cues. We demonstrate that\nour approach improves performance on the next-scene description task --\nachieving an average improvement from 3.7% to 19% in SimRate, a metric that\nquantifies semantic similarity to human-annotated ground truths. Moreover,\nImageChain achieves robust zero-shot out-of-domain performance in applications\nranging from comics to robotics. Extensive experiments validate that\ninstruction-tuning in a multimodal, multi-turn conversation design is key to\nbridging the gap between static image understanding and temporally-aware\nreasoning.\n","authors":["Danae Sánchez Villegas","Ingo Ziegler","Desmond Elliott"],"pdf_url":"https://arxiv.org/pdf/2502.19409v1.pdf","comment":"Code, dataset, and checkpoints are publicly available at\n  https://github.com/danaesavi/ImageChain"},{"id":"http://arxiv.org/abs/2502.19407v1","updated":"2025-02-26T18:54:39Z","published":"2025-02-26T18:54:39Z","title":"Learning Code-Edit Embedding to Model Student Debugging Behavior","summary":"  Providing effective feedback for programming assignments in computer science\neducation can be challenging: students solve problems by iteratively submitting\ncode, executing it, and using limited feedback from the compiler or the\nauto-grader to debug. Analyzing student debugging behavior in this process may\nreveal important insights into their knowledge and inform better personalized\nsupport tools. In this work, we propose an encoder-decoder-based model that\nlearns meaningful code-edit embeddings between consecutive student code\nsubmissions, to capture their debugging behavior. Our model leverages\ninformation on whether a student code submission passes each test case to\nfine-tune large language models (LLMs) to learn code editing representations.\nIt enables personalized next-step code suggestions that maintain the student's\ncoding style while improving test case correctness. Our model also enables us\nto analyze student code-editing patterns to uncover common student errors and\ndebugging behaviors, using clustering techniques. Experimental results on a\nreal-world student code submission dataset demonstrate that our model excels at\ncode reconstruction and personalized code suggestion while revealing\ninteresting patterns in student debugging behavior.\n","authors":["Hasnain Heickal","Andrew Lan"],"pdf_url":"https://arxiv.org/pdf/2502.19407v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.19400v1","updated":"2025-02-26T18:50:09Z","published":"2025-02-26T18:50:09Z","title":"TheoremExplainAgent: Towards Multimodal Explanations for LLM Theorem\n  Understanding","summary":"  Understanding domain-specific theorems often requires more than just\ntext-based reasoning; effective communication through structured visual\nexplanations is crucial for deeper comprehension. While large language models\n(LLMs) demonstrate strong performance in text-based theorem reasoning, their\nability to generate coherent and pedagogically meaningful visual explanations\nremains an open challenge. In this work, we introduce TheoremExplainAgent, an\nagentic approach for generating long-form theorem explanation videos (over 5\nminutes) using Manim animations. To systematically evaluate multimodal theorem\nexplanations, we propose TheoremExplainBench, a benchmark covering 240 theorems\nacross multiple STEM disciplines, along with 5 automated evaluation metrics.\nOur results reveal that agentic planning is essential for generating detailed\nlong-form videos, and the o3-mini agent achieves a success rate of 93.8% and an\noverall score of 0.77. However, our quantitative and qualitative studies show\nthat most of the videos produced exhibit minor issues with visual element\nlayout. Furthermore, multimodal explanations expose deeper reasoning flaws that\ntext-based explanations fail to reveal, highlighting the importance of\nmultimodal explanations.\n","authors":["Max Ku","Thomas Chong","Jonathan Leung","Krish Shah","Alvin Yu","Wenhu Chen"],"pdf_url":"https://arxiv.org/pdf/2502.19400v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.11261v2","updated":"2025-02-26T18:44:29Z","published":"2024-10-15T04:35:56Z","title":"Beyond Linear Approximations: A Novel Pruning Approach for Attention\n  Matrix","summary":"  Large Language Models (LLMs) have shown immense potential in enhancing\nvarious aspects of our daily lives, from conversational AI to search and AI\nassistants. However, their growing capabilities come at the cost of extremely\nlarge model sizes, making deployment on edge devices challenging due to memory\nand computational constraints. This paper introduces a novel approach to LLM\nweight pruning that directly optimizes for approximating the attention matrix,\na core component of transformer architectures. Unlike existing methods that\nfocus on linear approximations, our approach accounts for the non-linear nature\nof the Softmax attention mechanism. We provide theoretical guarantees for the\nconvergence of our Gradient Descent-based optimization method to a near-optimal\npruning mask solution. Our empirical results demonstrate the effectiveness of\nour non-linear pruning approach in maintaining model performance while\nsignificantly reducing computational costs, which is beyond the current\nstate-of-the-art methods, i.e., SparseGPT and Wanda, by a large margin. This\nwork establishes a new theoretical foundation for pruning algorithm design in\nLLMs, potentially paving the way for more efficient LLM inference on\nresource-constrained devices.\n","authors":["Yingyu Liang","Jiangxuan Long","Zhenmei Shi","Zhao Song","Yufa Zhou"],"pdf_url":"https://arxiv.org/pdf/2410.11261v2.pdf","comment":"ICLR 2025"},{"id":"http://arxiv.org/abs/2502.19387v1","updated":"2025-02-26T18:32:15Z","published":"2025-02-26T18:32:15Z","title":"Residual Speech Embeddings for Tone Classification: Removing Linguistic\n  Content to Enhance Paralinguistic Analysis","summary":"  Self-supervised learning models for speech processing, such as wav2vec2,\nHuBERT, WavLM, and Whisper, generate embeddings that capture both linguistic\nand paralinguistic information, making it challenging to analyze tone\nindependently of spoken content. In this work, we introduce a method for\ndisentangling paralinguistic features from linguistic content by regressing\nspeech embeddings onto their corresponding text embeddings and using the\nresiduals as a representation of vocal tone. We evaluate this approach across\nmultiple self-supervised speech embeddings, demonstrating that residual\nembeddings significantly improve tone classification performance compared to\nraw speech embeddings. Our results show that this method enhances linear\nseparability, enabling improved classification even with simple models such as\nlogistic regression. Visualization of the residual embeddings further confirms\nthe successful removal of linguistic information while preserving tone-related\nfeatures. These findings highlight the potential of residual embeddings for\napplications in sentiment analysis, speaker characterization, and\nparalinguistic speech processing.\n","authors":["Hamdan Al Ahbabi","Gautier Marti","Saeed AlMarri","Ibrahim Elfadel"],"pdf_url":"https://arxiv.org/pdf/2502.19387v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.15823v2","updated":"2025-02-26T18:13:06Z","published":"2025-02-20T03:48:00Z","title":"InductionBench: LLMs Fail in the Simplest Complexity Class","summary":"  Large language models (LLMs) have shown remarkable improvements in reasoning\nand many existing benchmarks have been addressed by models such as o1 and o3\neither fully or partially. However, a majority of these benchmarks emphasize\ndeductive reasoning, including mathematical and coding tasks in which rules\nsuch as mathematical axioms or programming syntax are clearly defined, based on\nwhich LLMs can plan and apply these rules to arrive at a solution. In contrast,\ninductive reasoning, where one infers the underlying rules from observed data,\nremains less explored. Such inductive processes lie at the heart of scientific\ndiscovery, as they enable researchers to extract general principles from\nempirical observations. To assess whether LLMs possess this capacity, we\nintroduce InductionBench, a new benchmark designed to evaluate the inductive\nreasoning ability of LLMs. Our experimental findings reveal that even the most\nadvanced models available struggle to master the simplest complexity classes\nwithin the subregular hierarchy of functions, highlighting a notable deficiency\nin current LLMs' inductive reasoning capabilities. Coda and data are available\nhttps://github.com/Wenyueh/inductive_reasoning_benchmark.\n","authors":["Wenyue Hua","Tyler Wong","Sun Fei","Liangming Pan","Adam Jardine","William Yang Wang"],"pdf_url":"https://arxiv.org/pdf/2502.15823v2.pdf","comment":"24 pages, 7 figures"},{"id":"http://arxiv.org/abs/2411.07133v3","updated":"2025-02-26T18:10:05Z","published":"2024-11-11T17:06:48Z","title":"Stronger Models are NOT Stronger Teachers for Instruction Tuning","summary":"  Instruction tuning has been widely adopted to ensure large language models\n(LLMs) follow user instructions effectively. The resulting\ninstruction-following capabilities of LLMs heavily rely on the instruction\ndatasets used for tuning. Recently, synthetic instruction datasets have emerged\nas an economically viable solution to provide LLMs diverse and high-quality\ninstructions. However, existing approaches typically assume that larger or\nstronger models are stronger teachers for instruction tuning, and hence simply\nadopt these models as response generators to the synthetic instructions. In\nthis paper, we challenge this commonly-adopted assumption. Our extensive\nexperiments across five base models and twenty response generators reveal that\nlarger and stronger models are not necessarily stronger teachers of smaller\nmodels. We refer to this phenomenon as the Larger Models' Paradox. We observe\nthat existing metrics cannot precisely predict the effectiveness of response\ngenerators since they ignore the compatibility between teachers and base models\nbeing fine-tuned. We thus develop a novel metric, named as\nCompatibility-Adjusted Reward (CAR) to measure the effectiveness of response\ngenerators. Our experiments across five base models demonstrate that CAR\noutperforms almost all baselines.\n","authors":["Zhangchen Xu","Fengqing Jiang","Luyao Niu","Bill Yuchen Lin","Radha Poovendran"],"pdf_url":"https://arxiv.org/pdf/2411.07133v3.pdf","comment":"This is paper is accepted at NAACL 2025"},{"id":"http://arxiv.org/abs/2502.01619v2","updated":"2025-02-26T18:03:54Z","published":"2025-02-03T18:51:43Z","title":"Learning to Generate Unit Tests for Automated Debugging","summary":"  Unit tests (UTs) play an instrumental role in assessing code correctness as\nwell as providing feedback to large language models (LLMs), motivating\nautomated test generation. However, we uncover a trade-off between generating\nunit test inputs that reveal errors when given a faulty code and correctly\npredicting the unit test output without access to the gold solution. To address\nthis trade-off, we propose UTGen, which teaches LLMs to generate unit test\ninputs that reveal errors along with their correct expected outputs based on\ntask descriptions. Since model-generated tests can provide noisy signals (e.g.,\nfrom incorrectly predicted outputs), we propose UTDebug that (i) scales UTGen\nvia test-time compute to improve UT output prediction, and (ii) validates and\nbacktracks edits based on multiple generated UTs to avoid overfitting, and\nhelps LLMs debug effectively. We show that UTGen outperforms other LLM-based\nbaselines by 7.59% based on a metric measuring the presence of both\nerror-revealing UT inputs and correct UT outputs. When used with UTDebug, we\nfind that feedback from UTGen's unit tests improves pass@1 accuracy of Qwen2.5\n32B on HumanEvalFix and our own harder debugging split of MBPP+ by over 3.17%\nand 12.35% (respectively) over other LLM-based UT generation baselines. Lastly,\nwe demonstrate that UTGen is a better judge for code correctness, outperforming\na state-of-the-art trained 8B reward model by 4.43% on HumanEval+ with\nbest-of-10 sampling using Qwen2.5 7B.\n","authors":["Archiki Prasad","Elias Stengel-Eskin","Justin Chih-Yao Chen","Zaid Khan","Mohit Bansal"],"pdf_url":"https://arxiv.org/pdf/2502.01619v2.pdf","comment":"First two authors contributed equally. Dataset and Code:\n  https://github.com/archiki/UTGenDebug"},{"id":"http://arxiv.org/abs/2502.19363v1","updated":"2025-02-26T18:01:19Z","published":"2025-02-26T18:01:19Z","title":"DataMan: Data Manager for Pre-training Large Language Models","summary":"  The performance emergence of large language models (LLMs) driven by data\nscaling laws makes the selection of pre-training data increasingly important.\nHowever, existing methods rely on limited heuristics and human intuition,\nlacking comprehensive and clear guidelines. To address this, we are inspired by\n``reverse thinking'' -- prompting LLMs to self-identify which criteria benefit\nits performance. As its pre-training capabilities are related to perplexity\n(PPL), we derive 14 quality criteria from the causes of text perplexity\nanomalies and introduce 15 common application domains to support domain mixing.\nIn this paper, we train a Data Manager (DataMan) to learn quality ratings and\ndomain recognition from pointwise rating, and use it to annotate a 447B token\npre-training corpus with 14 quality ratings and domain type. Our experiments\nvalidate our approach, using DataMan to select 30B tokens to train a\n1.3B-parameter language model, demonstrating significant improvements in\nin-context learning (ICL), perplexity, and instruction-following ability over\nthe state-of-the-art baseline. The best-performing model, based on the Overall\nScore l=5 surpasses a model trained with 50% more data using uniform sampling.\nWe continue pre-training with high-rated, domain-specific data annotated by\nDataMan to enhance domain-specific ICL performance and thus verify DataMan's\ndomain mixing ability. Our findings emphasize the importance of quality\nranking, the complementary nature of quality criteria, and their low\ncorrelation with perplexity, analyzing misalignment between PPL and ICL\nperformance. We also thoroughly analyzed our pre-training dataset, examining\nits composition, the distribution of quality ratings, and the original document\nsources.\n","authors":["Ru Peng","Kexin Yang","Yawen Zeng","Junyang Lin","Dayiheng Liu","Junbo Zhao"],"pdf_url":"https://arxiv.org/pdf/2502.19363v1.pdf","comment":"ICLR2025 paper"},{"id":"http://arxiv.org/abs/2502.19361v1","updated":"2025-02-26T17:59:27Z","published":"2025-02-26T17:59:27Z","title":"Can Large Language Models Detect Errors in Long Chain-of-Thought\n  Reasoning?","summary":"  Recently, o1-like models have drawn significant attention, where these models\nproduce the long Chain-of-Thought (CoT) reasoning steps to improve the\nreasoning abilities of existing Large Language Models (LLMs). In this paper, to\nunderstand the qualities of these long CoTs and measure the critique abilities\nof existing LLMs on these long CoTs, we introduce the DeltaBench, including the\ngenerated long CoTs from different o1-like models (e.g., QwQ, DeepSeek-R1) for\ndifferent reasoning tasks (e.g., Math, Code, General Reasoning), to measure the\nability to detect errors in long CoT reasoning. Based on DeltaBench, we first\nperform fine-grained analysis of the generated long CoTs to discover the\neffectiveness and efficiency of different o1-like models. Then, we conduct\nextensive evaluations of existing process reward models (PRMs) and critic\nmodels to detect the errors of each annotated process, which aims to\ninvestigate the boundaries and limitations of existing PRMs and critic models.\nFinally, we hope that DeltaBench could guide developers to better understand\nthe long CoT reasoning abilities of their models.\n","authors":["Yancheng He","Shilong Li","Jiaheng Liu","Weixun Wang","Xingyuan Bu","Ge Zhang","Zhongyuan Peng","Zhaoxiang Zhang","Wenbo Su","Bo Zheng"],"pdf_url":"https://arxiv.org/pdf/2502.19361v1.pdf","comment":"The first three authors contributed equally, 27 pages"},{"id":"http://arxiv.org/abs/2412.04318v2","updated":"2025-02-26T17:51:31Z","published":"2024-12-05T16:34:20Z","title":"The Hyperfitting Phenomenon: Sharpening and Stabilizing LLMs for\n  Open-Ended Text Generation","summary":"  This paper introduces the counter-intuitive generalization results of\noverfitting pre-trained large language models (LLMs) on very small datasets. In\nthe setting of open-ended text generation, it is well-documented that LLMs tend\nto generate repetitive and dull sequences, a phenomenon that is especially\napparent when generating using greedy decoding. This issue persists even with\nstate-of-the-art LLMs containing billions of parameters, trained via next-token\nprediction on large datasets. We find that by further fine-tuning these models\nto achieve a near-zero training loss on a small set of samples -- a process we\nrefer to as hyperfitting -- the long-sequence generative capabilities are\ngreatly enhanced. Greedy decoding with these Hyperfitted models even outperform\nTop-P sampling over long-sequences, both in terms of diversity and human\npreferences. This phenomenon extends to LLMs of various sizes, different\ndomains, and even autoregressive image generation. We further find this\nphenomena to be distinctly different from that of Grokking and double descent.\nSurprisingly, our experiments indicate that hyperfitted models rarely fall into\nrepeating sequences they were trained on, and even explicitly blocking these\nsequences results in high-quality output. All hyperfitted models produce\nextremely low-entropy predictions, often allocating nearly all probability to a\nsingle token.\n","authors":["Fredrik Carlsson","Fangyu Liu","Daniel Ward","Murathan Kurfali","Joakim Nivre"],"pdf_url":"https://arxiv.org/pdf/2412.04318v2.pdf","comment":"Under review at ICLR"},{"id":"http://arxiv.org/abs/2410.17170v2","updated":"2025-02-26T17:40:00Z","published":"2024-10-22T16:50:00Z","title":"Self-calibration for Language Model Quantization and Pruning","summary":"  Quantization and pruning are fundamental approaches for model compression,\nenabling efficient inference for language models. In a post-training setting,\nstate-of-the-art quantization and pruning methods require calibration data, a\nsmall set of unlabeled examples. Conventionally, this is randomly sampled web\ntext, aiming to reflect the model training data. However, this poses two key\nproblems: (1) unrepresentative calibration examples can harm model performance,\nand (2) organizations increasingly avoid releasing model training data. In this\npaper, we propose self-calibration as a solution. Our approach requires no\nexternal data, instead leveraging the model itself to generate synthetic\ncalibration data, with a view to better approximating the pre-training data\ndistribution. We extensively compare the performance of self-calibration with\nseveral baselines, across a variety of models, compression methods, and tasks.\nOur approach proves consistently competitive in maximizing downstream task\nperformance, frequently outperforming even using real data.\n","authors":["Miles Williams","George Chrysostomou","Nikolaos Aletras"],"pdf_url":"https://arxiv.org/pdf/2410.17170v2.pdf","comment":"NAACL 2025"},{"id":"http://arxiv.org/abs/2502.19347v1","updated":"2025-02-26T17:38:58Z","published":"2025-02-26T17:38:58Z","title":"Controlled Diversity: Length-optimized Natural Language Generation","summary":"  LLMs are not generally able to adjust the length of their outputs based on\nstrict length requirements, a capability that would improve their usefulness in\napplications that require adherence to diverse user and system requirements. We\npresent an approach to train LLMs to acquire this capability by augmenting\nexisting data and applying existing fine-tuning techniques, which we compare\nbased on the trained models' adherence to the length requirement and overall\nresponse quality relative to the baseline model. Our results demonstrate that\nthese techniques can be successfully applied to train LLMs to adhere to length\nrequirements, with the trained models generating texts which better align to\nthe length requirements. Our results indicate that our method may change the\nresponse quality when using training data that was not generated by the\nbaseline model. This allows simultaneous alignment to another training\nobjective in certain scenarios, but is undesirable otherwise. Training on a\ndataset containing the model's own responses eliminates this issue.\n","authors":["Diana Marie Schenke","Timo Baumann"],"pdf_url":"https://arxiv.org/pdf/2502.19347v1.pdf","comment":"ISCA/ITG Workshop on Diversity in Large Speech and Language Models"},{"id":"http://arxiv.org/abs/2502.19339v1","updated":"2025-02-26T17:32:07Z","published":"2025-02-26T17:32:07Z","title":"Evaluating LLMs and Pre-trained Models for Text Summarization Across\n  Diverse Datasets","summary":"  Text summarization plays a crucial role in natural language processing by\ncondensing large volumes of text into concise and coherent summaries. As\ndigital content continues to grow rapidly and the demand for effective\ninformation retrieval increases, text summarization has become a focal point of\nresearch in recent years. This study offers a thorough evaluation of four\nleading pre-trained and open-source large language models: BART, FLAN-T5,\nLLaMA-3-8B, and Gemma-7B, across five diverse datasets CNN/DM, Gigaword, News\nSummary, XSum, and BBC News. The evaluation employs widely recognized automatic\nmetrics, including ROUGE-1, ROUGE-2, ROUGE-L, BERTScore, and METEOR, to assess\nthe models' capabilities in generating coherent and informative summaries. The\nresults reveal the comparative strengths and limitations of these models in\nprocessing various text types.\n","authors":["Tohida Rehman","Soumabha Ghosh","Kuntal Das","Souvik Bhattacharjee","Debarshi Kumar Sanyal","Samiran Chattopadhyay"],"pdf_url":"https://arxiv.org/pdf/2502.19339v1.pdf","comment":"5 pages, 2 figures, 6 tables"},{"id":"http://arxiv.org/abs/2501.12431v2","updated":"2025-02-26T17:20:53Z","published":"2025-01-21T16:49:00Z","title":"Modality Interactive Mixture-of-Experts for Fake News Detection","summary":"  The proliferation of fake news on social media platforms disproportionately\nimpacts vulnerable populations, eroding trust, exacerbating inequality, and\namplifying harmful narratives. Detecting fake news in multimodal contexts --\nwhere deceptive content combines text and images -- is particularly challenging\ndue to the nuanced interplay between modalities. Existing multimodal fake news\ndetection methods often emphasize cross-modal consistency but ignore the\ncomplex interactions between text and visual elements, which may complement,\ncontradict, or independently influence the predicted veracity of a post. To\naddress these challenges, we present Modality Interactive Mixture-of-Experts\nfor Fake News Detection (MIMoE-FND), a novel hierarchical Mixture-of-Experts\nframework designed to enhance multimodal fake news detection by explicitly\nmodeling modality interactions through an interaction gating mechanism. Our\napproach models modality interactions by evaluating two key aspects of modality\ninteractions: unimodal prediction agreement and semantic alignment. The\nhierarchical structure of MIMoE-FND allows for distinct learning pathways\ntailored to different fusion scenarios, adapting to the unique characteristics\nof each modality interaction. By tailoring fusion strategies to diverse\nmodality interaction scenarios, MIMoE-FND provides a more robust and nuanced\napproach to multimodal fake news detection. We evaluate our approach on three\nreal-world benchmarks spanning two languages, demonstrating its superior\nperformance compared to state-of-the-art methods. By enhancing the accuracy and\ninterpretability of fake news detection, MIMoE-FND offers a promising tool to\nmitigate the spread of misinformation, with the potential to better safeguard\nvulnerable communities against its harmful effects.\n","authors":["Yifan Liu","Yaokun Liu","Zelin Li","Ruichen Yao","Yang Zhang","Dong Wang"],"pdf_url":"https://arxiv.org/pdf/2501.12431v2.pdf","comment":"Accepted by the Proceedings of the ACM Web Conference 2025"},{"id":"http://arxiv.org/abs/2502.19328v1","updated":"2025-02-26T17:19:12Z","published":"2025-02-26T17:19:12Z","title":"Agentic Reward Modeling: Integrating Human Preferences with Verifiable\n  Correctness Signals for Reliable Reward Systems","summary":"  Reward models (RMs) are crucial for the training and inference-time scaling\nup of large language models (LLMs). However, existing reward models primarily\nfocus on human preferences, neglecting verifiable correctness signals which\nhave shown strong potential in training LLMs. In this paper, we propose agentic\nreward modeling, a reward system that combines reward models with verifiable\ncorrectness signals from different aspects to provide reliable rewards. We\nempirically implement a reward agent, named RewardAgent, that combines human\npreference rewards with two verifiable signals: factuality and instruction\nfollowing, to provide more reliable rewards. We conduct comprehensive\nexperiments on existing reward model benchmarks and inference time best-of-n\nsearches on real-world downstream tasks. RewardAgent significantly outperforms\nvanilla reward models, demonstrating its effectiveness. We further construct\ntraining preference pairs using RewardAgent and train an LLM with the DPO\nobjective, achieving superior performance on various NLP benchmarks compared to\nconventional reward models. Our codes are publicly released to facilitate\nfurther research (https://github.com/THU-KEG/Agentic-Reward-Modeling).\n","authors":["Hao Peng","Yunjia Qi","Xiaozhi Wang","Zijun Yao","Bin Xu","Lei Hou","Juanzi Li"],"pdf_url":"https://arxiv.org/pdf/2502.19328v1.pdf","comment":"16 pages, 5 figures"},{"id":"http://arxiv.org/abs/2502.19320v1","updated":"2025-02-26T17:13:19Z","published":"2025-02-26T17:13:19Z","title":"Shh, don't say that! Domain Certification in LLMs","summary":"  Large language models (LLMs) are often deployed to perform constrained tasks,\nwith narrow domains. For example, customer support bots can be built on top of\nLLMs, relying on their broad language understanding and capabilities to enhance\nperformance. However, these LLMs are adversarially susceptible, potentially\ngenerating outputs outside the intended domain. To formalize, assess, and\nmitigate this risk, we introduce domain certification; a guarantee that\naccurately characterizes the out-of-domain behavior of language models. We then\npropose a simple yet effective approach, which we call VALID that provides\nadversarial bounds as a certificate. Finally, we evaluate our method across a\ndiverse set of datasets, demonstrating that it yields meaningful certificates,\nwhich bound the probability of out-of-domain samples tightly with minimum\npenalty to refusal behavior.\n","authors":["Cornelius Emde","Alasdair Paren","Preetham Arvind","Maxime Kayser","Tom Rainforth","Thomas Lukasiewicz","Bernard Ghanem","Philip H. S. Torr","Adel Bibi"],"pdf_url":"https://arxiv.org/pdf/2502.19320v1.pdf","comment":"10 pages, includes appendix Published in International Conference on\n  Learning Representations (ICLR) 2025"},{"id":"http://arxiv.org/abs/2502.19312v1","updated":"2025-02-26T17:08:46Z","published":"2025-02-26T17:08:46Z","title":"FSPO: Few-Shot Preference Optimization of Synthetic Preference Data in\n  LLMs Elicits Effective Personalization to Real Users","summary":"  Effective personalization of LLMs is critical for a broad range of\nuser-interfacing applications such as virtual assistants and content curation.\nInspired by the strong in-context learning capabilities of LLMs, we propose\nFew-Shot Preference Optimization (FSPO), which reframes reward modeling as a\nmeta-learning problem. Under this framework, an LLM learns to quickly adapt to\na user via a few labeled preferences from that user, constructing a\npersonalized reward function for them. Additionally, since real-world\npreference data is scarce and challenging to collect at scale, we propose\ncareful design choices to construct synthetic preference datasets for\npersonalization, generating over 1M synthetic personalized preferences using\npublicly available LLMs. In particular, to successfully transfer from synthetic\ndata to real users, we find it crucial for the data to exhibit both high\ndiversity and coherent, self-consistent structure. We evaluate FSPO on\npersonalized open-ended generation for up to 1,500 synthetic users across\nacross three domains: movie reviews, pedagogical adaptation based on\neducational background, and general question answering, along with a controlled\nhuman study. Overall, FSPO achieves an 87% Alpaca Eval winrate on average in\ngenerating responses that are personalized to synthetic users and a 72% winrate\nwith real human users in open-ended question answering.\n","authors":["Anikait Singh","Sheryl Hsu","Kyle Hsu","Eric Mitchell","Stefano Ermon","Tatsunori Hashimoto","Archit Sharma","Chelsea Finn"],"pdf_url":"https://arxiv.org/pdf/2502.19312v1.pdf","comment":"Website: https://fewshot-preference-optimization.github.io/"},{"id":"http://arxiv.org/abs/2502.10973v2","updated":"2025-02-26T16:48:07Z","published":"2025-02-16T03:24:33Z","title":"Akan Cinematic Emotions (ACE): A Multimodal Multi-party Dataset for\n  Emotion Recognition in Movie Dialogues","summary":"  In this paper, we introduce the Akan Conversation Emotion (ACE) dataset, the\nfirst multimodal emotion dialogue dataset for an African language, addressing\nthe significant lack of resources for low-resource languages in emotion\nrecognition research. ACE, developed for the Akan language, contains 385\nemotion-labeled dialogues and 6,162 utterances across audio, visual, and\ntextual modalities, along with word-level prosodic prominence annotations. The\npresence of prosodic labels in this dataset also makes it the first\nprosodically annotated African language dataset. We demonstrate the quality and\nutility of ACE through experiments using state-of-the-art emotion recognition\nmethods, establishing solid baselines for future research. We hope ACE inspires\nfurther work on inclusive, linguistically and culturally diverse NLP resources.\n","authors":["David Sasu","Zehui Wu","Ziwei Gong","Run Chen","Pengyuan Shi","Lin Ai","Julia Hirschberg","Natalie Schluter"],"pdf_url":"https://arxiv.org/pdf/2502.10973v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.06689v3","updated":"2025-02-26T16:36:55Z","published":"2025-01-12T02:43:59Z","title":"TAPO: Task-Referenced Adaptation for Prompt Optimization","summary":"  Prompt engineering can significantly improve the performance of large\nlanguage models (LLMs), with automated prompt optimization (APO) gaining\nsignificant attention due to the time-consuming and laborious nature of manual\nprompt design. However, much of the existing work in APO overlooks\ntask-specific characteristics, resulting in prompts that lack domain\nspecificity and are not well-suited for task-specific optimization. In this\npaper, we introduce TAPO, a multitask-aware prompt optimization framework\ncomposed of three key modules. First, a task-aware metric selection module is\nproposed to enhance task-specific prompt generation capabilities. Second, we\npresent a multi-metrics evaluation module to jointly evaluate prompts from\nmultiple perspectives. Third, an evolution-based optimization framework is\nintroduced for automatic prompt refinement, which improves adaptability across\nvarious tasks. Extensive experiments on six datasets demonstrate the\neffectiveness of our approach, and our code is publicly available.\n","authors":["Wenxin Luo","Weirui Wang","Xiaopeng Li","Weibo Zhou","Pengyue Jia","Xiangyu Zhao"],"pdf_url":"https://arxiv.org/pdf/2501.06689v3.pdf","comment":"Accepted to ICASSP 2025"},{"id":"http://arxiv.org/abs/2502.19279v1","updated":"2025-02-26T16:33:41Z","published":"2025-02-26T16:33:41Z","title":"CritiQ: Mining Data Quality Criteria from Human Preferences","summary":"  Language model heavily depends on high-quality data for optimal performance.\nExisting approaches rely on manually designed heuristics, the perplexity of\nexisting models, training classifiers, or careful prompt engineering, which\nrequire significant expert experience and human annotation effort while\nintroduce biases. We introduce CritiQ, a novel data selection method that\nautomatically mines criteria from human preferences for data quality with only\n$\\sim$30 human-annotated pairs and performs efficient data selection. The main\ncomponent, CritiQ Flow, employs a manager agent to evolve quality criteria and\nworker agents to make pairwise judgments. We build a knowledge base that\nextracts quality criteria from previous work to boost CritiQ Flow. Compared to\nperplexity- and classifier- based methods, verbal criteria are more\ninterpretable and possess reusable value. After deriving the criteria, we train\nthe CritiQ Scorer to give quality scores and perform efficient data selection.\nWe demonstrate the effectiveness of our method in the code, math, and logic\ndomains, achieving high accuracy on human-annotated test sets. To validate the\nquality of the selected data, we continually train Llama 3.1 models and observe\nimproved performance on downstream tasks compared to uniform sampling. Ablation\nstudies validate the benefits of the knowledge base and the reflection process.\nWe analyze how criteria evolve and the effectiveness of majority voting.\n","authors":["Honglin Guo","Kai Lv","Qipeng Guo","Tianyi Liang","Zhiheng Xi","Demin Song","Qiuyinzhe Zhang","Yu Sun","Kai Chen","Xipeng Qiu","Tao Gui"],"pdf_url":"https://arxiv.org/pdf/2502.19279v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.19276v1","updated":"2025-02-26T16:31:48Z","published":"2025-02-26T16:31:48Z","title":"Disentangled VAD Representations via a Variational Framework for\n  Political Stance Detection","summary":"  The stance detection task aims to categorise the stance regarding specified\ntargets. Current methods face challenges in effectively integrating sentiment\ninformation for stance detection. Moreover, the role of highly granular\nsentiment labelling in stance detection has been largely overlooked. This study\npresents a novel stance detection framework utilizing a variational autoencoder\n(VAE) to disentangle latent emotional features-value, arousal, and dominance\n(VAD)-from political discourse on social media. This approach addresses\nlimitations in current methods, particularly in in-target and cross-target\nstance detection scenarios. This research uses an advanced emotional annotation\ntool to annotate seven-class sentiment labels for P-STANCE. Evaluations on\nbenchmark datasets, including P-STANCE and SemEval-2016, reveal that\nPoliStance-VAE achieves state-of-the-art performance, surpassing models like\nBERT, BERTweet, and GPT-4o. PoliStance-VAE offers a robust and interpretable\nsolution for stance detection, demonstrating the effectiveness of integrating\nnuanced emotional representations. This framework paves the way for\nadvancements in natural language processing tasks, particularly those requiring\ndetailed emotional understanding.\n","authors":["Beiyu Xu","Zhiwei Liu","Sophia Ananiadou"],"pdf_url":"https://arxiv.org/pdf/2502.19276v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.19261v1","updated":"2025-02-26T16:06:36Z","published":"2025-02-26T16:06:36Z","title":"Drop-Upcycling: Training Sparse Mixture of Experts with Partial\n  Re-initialization","summary":"  The Mixture of Experts (MoE) architecture reduces the training and inference\ncost significantly compared to a dense model of equivalent capacity. Upcycling\nis an approach that initializes and trains an MoE model using a pre-trained\ndense model. While upcycling leads to initial performance gains, the training\nprogresses slower than when trained from scratch, leading to suboptimal\nperformance in the long term. We propose Drop-Upcycling - a method that\neffectively addresses this problem. Drop-Upcycling combines two seemingly\ncontradictory approaches: utilizing the knowledge of pre-trained dense models\nwhile statistically re-initializing some parts of the weights. This approach\nstrategically promotes expert specialization, significantly enhancing the MoE\nmodel's efficiency in knowledge acquisition. Extensive large-scale experiments\ndemonstrate that Drop-Upcycling significantly outperforms previous MoE\nconstruction methods in the long term, specifically when training on hundreds\nof billions of tokens or more. As a result, our MoE model with 5.9B active\nparameters achieves comparable performance to a 13B dense model in the same\nmodel family, while requiring approximately 1/4 of the training FLOPs. All\nexperimental resources, including source code, training data, model checkpoints\nand logs, are publicly available to promote reproducibility and future research\non MoE.\n","authors":["Taishi Nakamura","Takuya Akiba","Kazuki Fujii","Yusuke Oda","Rio Yokota","Jun Suzuki"],"pdf_url":"https://arxiv.org/pdf/2502.19261v1.pdf","comment":"To appear at the 13th International Conference on Learning\n  Representations (ICLR 2025)"},{"id":"http://arxiv.org/abs/2412.16410v2","updated":"2025-02-26T16:01:49Z","published":"2024-12-21T00:09:52Z","title":"Application of Multimodal Large Language Models in Autonomous Driving","summary":"  In this era of technological advancements, several cutting-edge techniques\nare being implemented to enhance Autonomous Driving (AD) systems, focusing on\nimproving safety, efficiency, and adaptability in complex driving environments.\nHowever, AD still faces some problems including performance limitations. To\naddress this problem, we conducted an in-depth study on implementing the\nMulti-modal Large Language Model. We constructed a Virtual Question Answering\n(VQA) dataset to fine-tune the model and address problems with the poor\nperformance of MLLM on AD. We then break down the AD decision-making process by\nscene understanding, prediction, and decision-making. Chain of Thought has been\nused to make the decision more perfectly. Our experiments and detailed analysis\nof Autonomous Driving give an idea of how important MLLM is for AD.\n","authors":["Md Robiul Islam"],"pdf_url":"https://arxiv.org/pdf/2412.16410v2.pdf","comment":"9 pages, 9 figures"},{"id":"http://arxiv.org/abs/2502.19249v1","updated":"2025-02-26T15:55:55Z","published":"2025-02-26T15:55:55Z","title":"Between Circuits and Chomsky: Pre-pretraining on Formal Languages\n  Imparts Linguistic Biases","summary":"  Pretraining language models on formal languages can improve their acquisition\nof natural language, but it is unclear which features of the formal language\nimpart an inductive bias that leads to effective transfer. Drawing on insights\nfrom linguistics and complexity theory, we hypothesize that effective transfer\noccurs when the formal language both captures dependency structures in natural\nlanguage and remains within the computational limitations of the model\narchitecture. Focusing on transformers, we find that formal languages with both\nthese properties enable language models to achieve lower loss on natural\nlanguage and better linguistic generalization compared to other languages. In\nfact, pre-pretraining, or training on formal-then-natural language, reduces\nloss more efficiently than the same amount of natural language. For a\n1B-parameter language model trained on roughly 1.6B tokens of natural language,\npre-pretraining achieves the same loss and better linguistic generalization\nwith a 33% smaller token budget. We also give mechanistic evidence of\ncross-task transfer from formal to natural language: attention heads acquired\nduring formal language pretraining remain crucial for the model's performance\non syntactic evaluations.\n","authors":["Michael Y. Hu","Jackson Petty","Chuan Shi","William Merrill","Tal Linzen"],"pdf_url":"https://arxiv.org/pdf/2502.19249v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.02371v3","updated":"2025-02-26T15:42:41Z","published":"2024-03-04T16:17:39Z","title":"NeuroVoz: a Castillian Spanish corpus of parkinsonian speech","summary":"  The screening of Parkinson's Disease (PD) through speech is hindered by a\nnotable lack of publicly available datasets in different languages. This fact\nlimits the reproducibility and further exploration of existing research.\n  To address this gap, this manuscript presents the NeuroVoz corpus consisting\nof 112 native Castilian-Spanish speakers, including 58 healthy controls and 54\nindividuals with PD, all recorded in ON state. The corpus showcases a diverse\narray of speech tasks: sustained vowels; diadochokinetic tests; 16\nListen-and-Repeat utterances; and spontaneous monologues.\n  The dataset is also complemented with subjective assessments of voice quality\nperformed by an expert according to the GRBAS scale\n(Grade/Roughness/Breathiness/Asthenia/Strain), as well as annotations with a\nthorough examination of phonation quality, intensity, speed, resonance,\nintelligibility, and prosody.\n  The corpus offers a substantial resource for the exploration of the impact of\nPD on speech. This data set has already supported several studies, achieving a\nbenchmark accuracy of 89% for the screening of PD. Despite these advances, the\nbroader challenge of conducting a language-agnostic, cross-corpora analysis of\nParkinsonian speech patterns remains open.\n","authors":["Janaína Mendes-Laureano","Jorge A. Gómez-García","Alejandro Guerrero-López","Elisa Luque-Buzo","Julián D. Arias-Londoño","Francisco J. Grandas-Pérez","Juan I. Godino-Llorente"],"pdf_url":"https://arxiv.org/pdf/2403.02371v3.pdf","comment":"Paper accepted at Scientific Data"},{"id":"http://arxiv.org/abs/2502.19230v1","updated":"2025-02-26T15:41:41Z","published":"2025-02-26T15:41:41Z","title":"Two Heads Are Better Than One: Dual-Model Verbal Reflection at\n  Inference-Time","summary":"  Large Language Models (LLMs) often struggle with complex reasoning scenarios.\nWhile preference optimization methods enhance reasoning performance through\ntraining, they often lack transparency in why one reasoning outcome is\npreferred over another. Verbal reflection techniques improve explainability but\nare limited in LLMs' critique and refinement capacity. To address these\nchallenges, we introduce a contrastive reflection synthesis pipeline that\nenhances the accuracy and depth of LLM-generated reflections. We further\npropose a dual-model reasoning framework within a verbal reinforcement learning\nparadigm, decoupling inference-time self-reflection into specialized, trained\nmodels for reasoning critique and refinement. Extensive experiments show that\nour framework outperforms traditional preference optimization methods across\nall evaluation metrics. Our findings also show that \"two heads are better than\none\", demonstrating that a collaborative Reasoner-Critic model achieves\nsuperior reasoning performance and transparency, compared to single-model\napproaches.\n","authors":["Jiazheng Li","Yuxiang Zhou","Junru Lu","Gladys Tyen","Lin Gui","Cesare Aloisi","Yulan He"],"pdf_url":"https://arxiv.org/pdf/2502.19230v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.12490v3","updated":"2025-02-26T15:30:00Z","published":"2025-02-18T03:19:48Z","title":"UniGenCoder: Merging Seq2Seq and Seq2Tree Paradigms for Unified Code\n  Generation","summary":"  Deep learning-based code generation has completely transformed the way\ndevelopers write programs today. Existing approaches to code generation have\nfocused either on the Sequence-to-Sequence paradigm, which generates target\ncode as a sequence of tokens, or the Sequence-to-Tree paradigm, which outputs\ncode as a sequence of actions. While these two paradigms are intuitively\ncomplementary, their combination has not been previously explored. By comparing\nthe code generated under these two paradigms, we find that integrating them\nholds significant potential. In this paper, we propose UniGenCoder for\ncode-related generation tasks, which consists of a shared encoder, a shared\ndecoder with a minimal set of additional parameters to unify two paradigms, and\na selector that dynamically chooses optimal paradigm for each instance. Also,\nduring the model training, we first perform the multi-task learning and\ndistillation strategies to facilitate knowledge transfer between two paradigms,\nand then leverage contrastive learning to train the selector. Experimental\nresults on the text-to-code and code-to-code generation tasks demonstrate the\neffectiveness of our proposed model. We release our code at\nhttps://github.com/DeepLearnXMU/UniGenCoder.\n","authors":["Liangying Shao","Yanfu Yan","Denys Poshyvanyk","Jinsong Su"],"pdf_url":"https://arxiv.org/pdf/2502.12490v3.pdf","comment":"accepted to 47th International Conference on Software Engineering\n  (ICSE 2025), NIER track"},{"id":"http://arxiv.org/abs/2309.08454v2","updated":"2025-02-26T15:28:38Z","published":"2023-09-15T14:57:28Z","title":"Combining TF-GridNet and Mixture Encoder for Continuous Speech\n  Separation for Meeting Transcription","summary":"  Many real-life applications of automatic speech recognition (ASR) require\nprocessing of overlapped speech. A common method involves first separating the\nspeech into overlap-free streams on which ASR is performed. Recently,\nTF-GridNet has shown impressive performance in speech separation in real\nreverberant conditions. Furthermore, a mixture encoder was proposed that\nleverages the mixed speech to mitigate the effect of separation artifacts. In\nthis work, we extended the mixture encoder from a static two-speaker scenario\nto a natural meeting context featuring an arbitrary number of speakers and\nvarying degrees of overlap. We further demonstrate its limits by the\nintegration with separators of varying strength including TF-GridNet. Our\nexperiments result in a new state-of-the-art performance on LibriCSS using a\nsingle microphone. They show that TF-GridNet largely closes the gap between\nprevious methods and oracle separation independent of mixture encoding. We\nfurther investigate the remaining potential for improvement.\n","authors":["Peter Vieting","Simon Berger","Thilo von Neumann","Christoph Boeddeker","Ralf Schlüter","Reinhold Haeb-Umbach"],"pdf_url":"https://arxiv.org/pdf/2309.08454v2.pdf","comment":"Presented at SLT 2024"},{"id":"http://arxiv.org/abs/2311.07978v3","updated":"2025-02-26T15:16:47Z","published":"2023-11-14T08:10:14Z","title":"AfroBench: How Good are Large Language Models on African Languages?","summary":"  Large-scale multilingual evaluations, such as MEGA, often include only a\nhandful of African languages due to the scarcity of high-quality evaluation\ndata and the limited discoverability of existing African datasets. This lack of\nrepresentation hinders comprehensive LLM evaluation across a diverse range of\nlanguages and tasks. To address these challenges, we introduce AfroBench -- a\nmulti-task benchmark for evaluating the performance of LLMs across 64 African\nlanguages, 15 tasks and 22 datasets. AfroBench consists of nine natural\nlanguage understanding datasets, six text generation datasets, six knowledge\nand question answering tasks, and one mathematical reasoning task. We present\nresults comparing the performance of prompting LLMs to fine-tuned baselines\nbased on BERT and T5-style models. Our results suggest large gaps in\nperformance between high-resource languages, such as English, and African\nlanguages across most tasks; but performance also varies based on the\navailability of monolingual data resources. Our findings confirm that\nperformance on African languages continues to remain a hurdle for current LLMs,\nunderscoring the need for additional efforts to close this gap.\n  https://mcgill-nlp.github.io/AfroBench/\n","authors":["Jessica Ojo","Odunayo Ogundepo","Akintunde Oladipo","Kelechi Ogueji","Jimmy Lin","Pontus Stenetorp","David Ifeoluwa Adelani"],"pdf_url":"https://arxiv.org/pdf/2311.07978v3.pdf","comment":"Under review"},{"id":"http://arxiv.org/abs/2502.19211v1","updated":"2025-02-26T15:13:20Z","published":"2025-02-26T15:13:20Z","title":"Negation-Induced Forgetting in LLMs","summary":"  The study explores whether Large Language Models (LLMs) exhibit\nnegation-induced forgetting (NIF), a cognitive phenomenon observed in humans\nwhere negating incorrect attributes of an object or event leads to diminished\nrecall of this object or event compared to affirming correct attributes (Mayo\net al., 2014; Zang et al., 2023). We adapted Zang et al. (2023) experimental\nframework to test this effect in ChatGPT-3.5, GPT-4o mini and\nLlama3-70b-instruct. Our results show that ChatGPT-3.5 exhibits NIF, with\nnegated information being less likely to be recalled than affirmed information.\nGPT-4o-mini showed a marginally significant NIF effect, while LLaMA-3-70B did\nnot exhibit NIF. The findings provide initial evidence of negation-induced\nforgetting in some LLMs, suggesting that similar cognitive biases may emerge in\nthese models. This work is a preliminary step in understanding how\nmemory-related phenomena manifest in LLMs.\n","authors":["Francesca Capuano","Ellen Boschert","Barbara Kaup"],"pdf_url":"https://arxiv.org/pdf/2502.19211v1.pdf","comment":"ISCA/ITG Workshop on Diversity in Large Speech and Language Models"},{"id":"http://arxiv.org/abs/2502.19209v1","updated":"2025-02-26T15:12:59Z","published":"2025-02-26T15:12:59Z","title":"Bi'an: A Bilingual Benchmark and Model for Hallucination Detection in\n  Retrieval-Augmented Generation","summary":"  Retrieval-Augmented Generation (RAG) effectively reduces hallucinations in\nLarge Language Models (LLMs) but can still produce inconsistent or unsupported\ncontent. Although LLM-as-a-Judge is widely used for RAG hallucination detection\ndue to its implementation simplicity, it faces two main challenges: the absence\nof comprehensive evaluation benchmarks and the lack of domain-optimized judge\nmodels. To bridge these gaps, we introduce \\textbf{Bi'an}, a novel framework\nfeaturing a bilingual benchmark dataset and lightweight judge models. The\ndataset supports rigorous evaluation across multiple RAG scenarios, while the\njudge models are fine-tuned from compact open-source LLMs. Extensive\nexperimental evaluations on Bi'anBench show our 14B model outperforms baseline\nmodels with over five times larger parameter scales and rivals state-of-the-art\nclosed-source LLMs. We will release our data and models soon at\nhttps://github.com/OpenSPG/KAG.\n","authors":["Zhouyu Jiang","Mengshu Sun","Zhiqiang Zhang","Lei Liang"],"pdf_url":"https://arxiv.org/pdf/2502.19209v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.19208v1","updated":"2025-02-26T15:12:37Z","published":"2025-02-26T15:12:37Z","title":"MultiConAD: A Unified Multilingual Conversational Dataset for Early\n  Alzheimer's Detection","summary":"  Dementia is a progressive cognitive syndrome with Alzheimer's disease (AD) as\nthe leading cause. Conversation-based AD detection offers a cost-effective\nalternative to clinical methods, as language dysfunction is an early biomarker\nof AD. However, most prior research has framed AD detection as a binary\nclassification problem, limiting the ability to identify Mild Cognitive\nImpairment (MCI)-a crucial stage for early intervention. Also, studies\nprimarily rely on single-language datasets, mainly in English, restricting\ncross-language generalizability. To address this gap, we make three key\ncontributions. First, we introduce a novel, multilingual dataset for AD\ndetection by unifying 16 publicly available dementia-related conversational\ndatasets. This corpus spans English, Spanish, Chinese, and Greek and\nincorporates both audio and text data derived from a variety of cognitive\nassessment tasks. Second, we perform finer-grained classification, including\nMCI, and evaluate various classifiers using sparse and dense text\nrepresentations. Third, we conduct experiments in monolingual and multilingual\nsettings, finding that some languages benefit from multilingual training while\nothers perform better independently. This study highlights the challenges in\nmultilingual AD detection and enables future research on both language-specific\napproaches and techniques aimed at improving model generalization and\nrobustness.\n","authors":["Arezo Shakeri","Mina Farmanbar","Krisztian Balog"],"pdf_url":"https://arxiv.org/pdf/2502.19208v1.pdf","comment":"11 pages, 3 Figures"},{"id":"http://arxiv.org/abs/2502.19207v1","updated":"2025-02-26T15:11:03Z","published":"2025-02-26T15:11:03Z","title":"FaithUn: Toward Faithful Forgetting in Language Models by Investigating\n  the Interconnectedness of Knowledge","summary":"  Various studies have attempted to remove sensitive or private knowledge from\na language model to prevent its unauthorized exposure. However, prior studies\nhave overlooked the complex and interconnected nature of knowledge, where\nrelated knowledge must be carefully examined. Specifically, they have failed to\nevaluate whether an unlearning method faithfully erases interconnected\nknowledge that should be removed, retaining knowledge that appears relevant but\nexists in a completely different context. To resolve this problem, we first\ndefine a new concept called superficial unlearning, which refers to the\nphenomenon where an unlearning method either fails to erase the interconnected\nknowledge it should remove or unintentionally erases irrelevant knowledge.\nBased on the definition, we introduce a new benchmark, FaithUn, to analyze and\nevaluate the faithfulness of unlearning in real-world knowledge QA settings.\nFurthermore, we propose a novel unlearning method, KLUE, which updates only\nknowledge-related neurons to achieve faithful unlearning. KLUE identifies\nknowledge neurons using an explainability method and updates only those neurons\nusing selected unforgotten samples. Experimental results demonstrate that\nwidely-used unlearning methods fail to ensure faithful unlearning, while our\nmethod shows significant effectiveness in real-world QA unlearning.\n","authors":["Nakyeong Yang","Minsung Kim","Seunghyun Yoon","Joongbo Shin","Kyomin Jung"],"pdf_url":"https://arxiv.org/pdf/2502.19207v1.pdf","comment":"16 pages"},{"id":"http://arxiv.org/abs/2502.19202v1","updated":"2025-02-26T15:09:28Z","published":"2025-02-26T15:09:28Z","title":"LiGT: Layout-infused Generative Transformer for Visual Question\n  Answering on Vietnamese Receipts","summary":"  \\textbf{Purpose:} Document Visual Question Answering (document VQA)\nchallenges multimodal systems to holistically handle textual, layout, and\nvisual modalities to provide appropriate answers. Document VQA has gained\npopularity in recent years due to the increasing amount of documents and the\nhigh demand for digitization. Nonetheless, most of document VQA datasets are\ndeveloped in high-resource languages such as English.\n  \\textbf{Methods:} In this paper, we present ReceiptVQA (\\textbf{Receipt}\n\\textbf{V}isual \\textbf{Q}uestion \\textbf{A}nswering), the initial large-scale\ndocument VQA dataset in Vietnamese dedicated to receipts, a document kind with\nhigh commercial potentials. The dataset encompasses \\textbf{9,000+} receipt\nimages and \\textbf{60,000+} manually annotated question-answer pairs. In\naddition to our study, we introduce LiGT (\\textbf{L}ayout-\\textbf{i}nfused\n\\textbf{G}enerative \\textbf{T}ransformer), a layout-aware encoder-decoder\narchitecture designed to leverage embedding layers of language models to\noperate layout embeddings, minimizing the use of additional neural modules.\n  \\textbf{Results:} Experiments on ReceiptVQA show that our architecture\nyielded promising performance, achieving competitive results compared with\noutstanding baselines. Furthermore, throughout analyzing experimental results,\nwe found evident patterns that employing encoder-only model architectures has\nconsiderable disadvantages in comparison to architectures that can generate\nanswers. We also observed that it is necessary to combine multiple modalities\nto tackle our dataset, despite the critical role of semantic understanding from\nlanguage models.\n  \\textbf{Conclusion:} We hope that our work will encourage and facilitate\nfuture development in Vietnamese document VQA, contributing to a diverse\nmultimodal research community in the Vietnamese language.\n","authors":["Thanh-Phong Le","Trung Le Chi Phan","Nghia Hieu Nguyen","Kiet Van Nguyen"],"pdf_url":"https://arxiv.org/pdf/2502.19202v1.pdf","comment":"Accepted at IJDAR"},{"id":"http://arxiv.org/abs/2502.19187v1","updated":"2025-02-26T14:50:50Z","published":"2025-02-26T14:50:50Z","title":"BIG-Bench Extra Hard","summary":"  Large language models (LLMs) are increasingly deployed in everyday\napplications, demanding robust general reasoning capabilities and diverse\nreasoning skillset. However, current LLM reasoning benchmarks predominantly\nfocus on mathematical and coding abilities, leaving a gap in evaluating broader\nreasoning proficiencies. One particular exception is the BIG-Bench dataset,\nwhich has served as a crucial benchmark for evaluating the general reasoning\ncapabilities of LLMs, thanks to its diverse set of challenging tasks that\nallowed for a comprehensive assessment of general reasoning across various\nskills within a unified framework. However, recent advances in LLMs have led to\nsaturation on BIG-Bench, and its harder version BIG-Bench Hard (BBH).\nState-of-the-art models achieve near-perfect scores on many tasks in BBH, thus\ndiminishing its utility. To address this limitation, we introduce BIG-Bench\nExtra Hard (BBEH), a new benchmark designed to push the boundaries of LLM\nreasoning evaluation. BBEH replaces each task in BBH with a novel task that\nprobes a similar reasoning capability but exhibits significantly increased\ndifficulty. We evaluate various models on BBEH and observe a (harmonic) average\naccuracy of 9.8\\% for the best general-purpose model and 44.8\\% for the best\nreasoning-specialized model, indicating substantial room for improvement and\nhighlighting the ongoing challenge of achieving robust general reasoning in\nLLMs. We release BBEH publicly at: https://github.com/google-deepmind/bbeh.\n","authors":["Mehran Kazemi","Bahare Fatemi","Hritik Bansal","John Palowitch","Chrysovalantis Anastasiou","Sanket Vaibhav Mehta","Lalit K. Jain","Virginia Aglietti","Disha Jindal","Peter Chen","Nishanth Dikkala","Gladys Tyen","Xin Liu","Uri Shalit","Silvia Chiappa","Kate Olszewska","Yi Tay","Vinh Q. Tran","Quoc V. Le","Orhan Firat"],"pdf_url":"https://arxiv.org/pdf/2502.19187v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.19175v1","updated":"2025-02-26T14:31:43Z","published":"2025-02-26T14:31:43Z","title":"MEDDxAgent: A Unified Modular Agent Framework for Explainable Automatic\n  Differential Diagnosis","summary":"  Differential Diagnosis (DDx) is a fundamental yet complex aspect of clinical\ndecision-making, in which physicians iteratively refine a ranked list of\npossible diseases based on symptoms, antecedents, and medical knowledge. While\nrecent advances in large language models have shown promise in supporting DDx,\nexisting approaches face key limitations, including single-dataset evaluations,\nisolated optimization of components, unrealistic assumptions about complete\npatient profiles, and single-attempt diagnosis. We introduce a Modular\nExplainable DDx Agent (MEDDxAgent) framework designed for interactive DDx,\nwhere diagnostic reasoning evolves through iterative learning, rather than\nassuming a complete patient profile is accessible. MEDDxAgent integrates three\nmodular components: (1) an orchestrator (DDxDriver), (2) a history taking\nsimulator, and (3) two specialized agents for knowledge retrieval and diagnosis\nstrategy. To ensure robust evaluation, we introduce a comprehensive DDx\nbenchmark covering respiratory, skin, and rare diseases. We analyze single-turn\ndiagnostic approaches and demonstrate the importance of iterative refinement\nwhen patient profiles are not available at the outset. Our broad evaluation\ndemonstrates that MEDDxAgent achieves over 10% accuracy improvements in\ninteractive DDx across both large and small LLMs, while offering critical\nexplainability into its diagnostic reasoning process.\n","authors":["Daniel Rose","Chia-Chien Hung","Marco Lepri","Israa Alqassem","Kiril Gashteovski","Carolin Lawrence"],"pdf_url":"https://arxiv.org/pdf/2502.19175v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.18168v2","updated":"2025-02-26T14:27:33Z","published":"2025-02-25T13:00:05Z","title":"SECURA: Sigmoid-Enhanced CUR Decomposition with Uninterrupted Retention\n  and Low-Rank Adaptation in Large Language Models","summary":"  With the rapid development of large language models (LLMs), fully fine-tuning\n(FT) these models has become increasingly impractical due to the high\ncomputational demands. Additionally, FT can lead to catastrophic forgetting. As\nan alternative, Low-Rank Adaptation (LoRA) has been proposed, which fine-tunes\nonly a small subset of parameters, achieving similar performance to FT while\nsignificantly reducing resource requirements. However, since LoRA inherits FT's\ndesign, the issue of catastrophic forgetting remains.\n  To address these challenges, we propose SECURA: Sigmoid-Enhanced CUR\nDecomposition LoRA, a novel parameter-efficient fine-tuning (PEFT) variant that\nmitigates catastrophic forgetting while improving fine-tuning performance. Our\nmethod introduces a new normalization technique, SigNorm, to enhance parameter\nretention and overall performance.\n  SECURA has been evaluated on a variety of tasks, including mathematical\nproblem-solving (GSM8K), challenging question-answering (CNNDM), translation\n(NewsDE), and complex multiple-choice reasoning (LogiQA). Experimental results\nshow that SECURA achieves an average fine-tuning improvement of 3.59% across\nfour multiple-choice question (MCQ) tasks and a 2.51% improvement across five\nquestion-answering (QA) tasks on models such as Gemma2 2b, Qwen2 1.5b, Qwen 2\n7b, Llama3 8b, and Llama3.1 8b, compared to DoRA. Moreover, SECURA demonstrates\nsuperior knowledge retention capabilities, maintaining more than 70% accuracy\non basic LLM knowledge across 16 continual learning tests, outperforming\nExperience Replay (ER), Sequential Learning (SEQ), EWC, I-LoRA, and CUR-LoRA.\n","authors":["Yuxuan Zhang"],"pdf_url":"https://arxiv.org/pdf/2502.18168v2.pdf","comment":"New work on Parameter-Efficient Fine-Tuning (PEFT) for large language\n  models. Includes new techniques SigNorm and CABR-LoRA for optimizing\n  fine-tune performance and Knowledge retention"},{"id":"http://arxiv.org/abs/2502.10990v2","updated":"2025-02-26T14:26:14Z","published":"2025-02-16T04:23:52Z","title":"FinMTEB: Finance Massive Text Embedding Benchmark","summary":"  Embedding models play a crucial role in representing and retrieving\ninformation across various NLP applications. Recent advances in large language\nmodels (LLMs) have further enhanced the performance of embedding models. While\nthese models are often benchmarked on general-purpose datasets, real-world\napplications demand domain-specific evaluation. In this work, we introduce the\nFinance Massive Text Embedding Benchmark (FinMTEB), a specialized counterpart\nto MTEB designed for the financial domain. FinMTEB comprises 64 financial\ndomain-specific embedding datasets across 7 tasks that cover diverse textual\ntypes in both Chinese and English, such as financial news articles, corporate\nannual reports, ESG reports, regulatory filings, and earnings call transcripts.\nWe also develop a finance-adapted model, Fin-E5, using a persona-based data\nsynthetic method to cover diverse financial embedding tasks for training.\nThrough extensive evaluation of 15 embedding models, including Fin-E5, we show\nthree key findings: (1) performance on general-purpose benchmarks shows limited\ncorrelation with financial domain tasks; (2) domain-adapted models consistently\noutperform their general-purpose counterparts; and (3) surprisingly, a simple\nBag-of-Words (BoW) approach outperforms sophisticated dense embeddings in\nfinancial Semantic Textual Similarity (STS) tasks, underscoring current\nlimitations in dense embedding techniques. Our work establishes a robust\nevaluation framework for financial NLP applications and provides crucial\ninsights for developing domain-specific embedding models.\n","authors":["Yixuan Tang","Yi Yang"],"pdf_url":"https://arxiv.org/pdf/2502.10990v2.pdf","comment":"https://github.com/yixuantt/FinMTEB"},{"id":"http://arxiv.org/abs/2502.19163v1","updated":"2025-02-26T14:17:56Z","published":"2025-02-26T14:17:56Z","title":"TestNUC: Enhancing Test-Time Computing Approaches through Neighboring\n  Unlabeled Data Consistency","summary":"  Test-time computing approaches, which leverage additional computational\nresources during inference, have been proven effective in enhancing large\nlanguage model performance. This work introduces a novel, linearly scaling\napproach, TestNUC, that improves test-time predictions by leveraging the local\nconsistency of neighboring unlabeled data-it classifies an input instance by\nconsidering not only the model's prediction on that instance but also on\nneighboring unlabeled instances. We evaluate TestNUC across eight diverse\ndatasets, spanning intent classification, topic mining, domain discovery, and\nemotion detection, demonstrating its consistent superiority over baseline\nmethods such as standard prompting and self-consistency. Furthermore, TestNUC\ncan be seamlessly integrated with existing test-time computing approaches,\nsubstantially boosting their performance. Our analysis reveals that TestNUC\nscales effectively with increasing amounts of unlabeled data and performs\nrobustly across different embedding models, making it practical for real-world\napplications. Our code is available at https://github.com/HenryPengZou/TestNUC.\n","authors":["Henry Peng Zou","Zhengyao Gu","Yue Zhou","Yankai Chen","Weizhi Zhang","Liancheng Fang","Yibo Wang","Yangning Li","Kay Liu","Philip S. Yu"],"pdf_url":"https://arxiv.org/pdf/2502.19163v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.19160v1","updated":"2025-02-26T14:15:28Z","published":"2025-02-26T14:15:28Z","title":"Detecting Linguistic Indicators for Stereotype Assessment with Large\n  Language Models","summary":"  Social categories and stereotypes are embedded in language and can introduce\ndata bias into Large Language Models (LLMs). Despite safeguards, these biases\noften persist in model behavior, potentially leading to representational harm\nin outputs. While sociolinguistic research provides valuable insights into the\nformation of stereotypes, NLP approaches for stereotype detection rarely draw\non this foundation and often lack objectivity, precision, and interpretability.\nTo fill this gap, in this work we propose a new approach that detects and\nquantifies the linguistic indicators of stereotypes in a sentence. We derive\nlinguistic indicators from the Social Category and Stereotype Communication\n(SCSC) framework which indicate strong social category formulation and\nstereotyping in language, and use them to build a categorization scheme. To\nautomate this approach, we instruct different LLMs using in-context learning to\napply the approach to a sentence, where the LLM examines the linguistic\nproperties and provides a basis for a fine-grained assessment. Based on an\nempirical evaluation of the importance of different linguistic indicators, we\nlearn a scoring function that measures the linguistic indicators of a\nstereotype. Our annotations of stereotyped sentences show that these indicators\nare present in these sentences and explain the strength of a stereotype. In\nterms of model performance, our results show that the models generally perform\nwell in detecting and classifying linguistic indicators of category labels used\nto denote a category, but sometimes struggle to correctly evaluate the\nassociated behaviors and characteristics. Using more few-shot examples within\nthe prompts, significantly improves performance. Model performance increases\nwith size, as Llama-3.3-70B-Instruct and GPT-4 achieve comparable results that\nsurpass those of Mixtral-8x7B-Instruct, GPT-4-mini and Llama-3.1-8B-Instruct.\n","authors":["Rebekka Görge","Michael Mock","Héctor Allende-Cid"],"pdf_url":"https://arxiv.org/pdf/2502.19160v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.19158v1","updated":"2025-02-26T14:14:58Z","published":"2025-02-26T14:14:58Z","title":"When Personalization Meets Reality: A Multi-Faceted Analysis of\n  Personalized Preference Learning","summary":"  While Reinforcement Learning from Human Feedback (RLHF) is widely used to\nalign Large Language Models (LLMs) with human preferences, it typically assumes\nhomogeneous preferences across users, overlooking diverse human values and\nminority viewpoints. Although personalized preference learning addresses this\nby tailoring separate preferences for individual users, the field lacks\nstandardized methods to assess its effectiveness. We present a multi-faceted\nevaluation framework that measures not only performance but also fairness,\nunintended effects, and adaptability across varying levels of preference\ndivergence. Through extensive experiments comparing eight personalization\nmethods across three preference datasets, we demonstrate that performance\ndifferences between methods could reach 36% when users strongly disagree, and\npersonalization can introduce up to 20% safety misalignment. These findings\nhighlight the critical need for holistic evaluation approaches to advance the\ndevelopment of more effective and inclusive preference learning systems.\n","authors":["Yijiang River Dong","Tiancheng Hu","Yinhong Liu","Ahmet Üstün","Nigel Collier"],"pdf_url":"https://arxiv.org/pdf/2502.19158v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.19149v1","updated":"2025-02-26T14:08:17Z","published":"2025-02-26T14:08:17Z","title":"Isolating Language-Coding from Problem-Solving: Benchmarking LLMs with\n  PseudoEval","summary":"  Existing code generation benchmarks for Large Language Models (LLMs) such as\nHumanEval and MBPP are designed to study LLMs' end-to-end performance, where\nthe benchmarks feed a problem description in natural language as input and\nexamine the generated code in specific programming languages. However, the\nevaluation scores revealed in this way provide a little hint as to the\nbottleneck of the code generation -- whether LLMs are struggling with their\nproblem-solving capability or language-coding capability. To answer this\nquestion, we construct PseudoEval, a multilingual code generation benchmark\nthat provides a solution written in pseudocode as input. By doing so, the\nbottleneck of code generation in various programming languages could be\nisolated and identified. Our study yields several interesting findings. For\nexample, we identify that the bottleneck of LLMs in Python programming is\nproblem-solving, while Rust is struggling relatively more in language-coding.\nAlso, our study indicates that problem-solving capability may transfer across\nprogramming languages, while language-coding needs more language-specific\neffort, especially for undertrained programming languages. Finally, we release\nthe pipeline of constructing PseudoEval to facilitate the extension to existing\nbenchmarks. PseudoEval is available at:\nhttps://anonymous.4open.science/r/PseudocodeACL25-7B74.\n","authors":["Jiarong Wu","Songqiang Chen","Jialun Cao","Hau Ching Lo","Shing-Chi Cheung"],"pdf_url":"https://arxiv.org/pdf/2502.19149v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.19148v1","updated":"2025-02-26T14:07:37Z","published":"2025-02-26T14:07:37Z","title":"Amulet: ReAlignment During Test Time for Personalized Preference\n  Adaptation of LLMs","summary":"  How to align large language models (LLMs) with user preferences from a static\ngeneral dataset has been frequently studied. However, user preferences are\nusually personalized, changing, and diverse regarding culture, values, or time.\nThis leads to the problem that the actual user preferences often do not\ncoincide with those trained by the model developers in the practical use of\nLLMs. Since we cannot collect enough data and retrain for every demand,\nresearching efficient real-time preference adaptation methods based on the\nbackbone LLMs during test time is important. To this end, we introduce Amulet,\na novel, training-free framework that formulates the decoding process of every\ntoken as a separate online learning problem with the guidance of simple\nuser-provided prompts, thus enabling real-time optimization to satisfy users'\npersonalized preferences. To reduce the computational cost brought by this\noptimization process for each token, we additionally provide a closed-form\nsolution for each iteration step of the optimization process, thereby reducing\nthe computational time cost to a negligible level. The detailed experimental\nresults demonstrate that Amulet can achieve significant performance\nimprovements in rich settings with combinations of different LLMs, datasets,\nand user preferences, while maintaining acceptable computational efficiency.\n","authors":["Zhaowei Zhang","Fengshuo Bai","Qizhi Chen","Chengdong Ma","Mingzhi Wang","Haoran Sun","Zilong Zheng","Yaodong Yang"],"pdf_url":"https://arxiv.org/pdf/2502.19148v1.pdf","comment":"Accepted by ICLR 2025, Project page:\n  https://zowiezhang.github.io/projects/Amulet"},{"id":"http://arxiv.org/abs/2406.00034v2","updated":"2025-02-26T14:07:05Z","published":"2024-05-26T21:39:53Z","title":"Adaptive Activation Steering: A Tuning-Free LLM Truthfulness Improvement\n  Method for Diverse Hallucinations Categories","summary":"  Recent studies have indicated that Large Language Models (LLMs) harbor an\ninherent understanding of truthfulness, yet often fail to consistently express\nit and generate false statements. This gap between \"knowing\" and \"telling\"\nposes a challenge for ensuring the truthfulness of generated content. Inspired\nby recent work on the practice of encoding human-interpretable concepts\nlinearly within large language models, we treat truthfulness as a specially\nlinearly encoded concept within LLMs, and introduce Adaptive Activation\nSteering (ACT), a tuning-free method that adaptively shifts LLM's activations\nin the \"truthful\" direction during inference. ACT addresses diverse categories\nof hallucinations by utilizing diverse truthfulness-related steering vectors\nand adjusting the steering intensity adaptively. Applied as an add-on across\nvarious models, ACT significantly improves truthfulness in LLaMA ($\\uparrow$\n142%), LLaMA2 ($\\uparrow$ 24%), Alpaca ($\\uparrow$ 36%), Vicuna ($\\uparrow$\n28%), LLaMA2-Chat ($\\uparrow$ 19%), and LLaMA3($\\uparrow$ 34%). Furthermore, we\nverify ACT's scalability across larger models (13B, 33B, 65B), underscoring the\nadaptability of ACT to large-scale language models. Our code is available at\nhttps://github.com/tianlwang/ACT.\n","authors":["Tianlong Wang","Xianfeng Jiao","Yinghao Zhu","Zhongzhi Chen","Yifan He","Xu Chu","Junyi Gao","Yasha Wang","Liantao Ma"],"pdf_url":"https://arxiv.org/pdf/2406.00034v2.pdf","comment":"ACM TheWebConf 2025 Conference (WWW 2025) Research Track"},{"id":"http://arxiv.org/abs/2502.08279v3","updated":"2025-02-26T13:57:59Z","published":"2025-02-12T10:36:55Z","title":"What Is That Talk About? A Video-to-Text Summarization Dataset for\n  Scientific Presentations","summary":"  Transforming recorded videos into concise and accurate textual summaries is a\ngrowing challenge in multimodal learning. This paper introduces VISTA, a\ndataset specifically designed for video-to-text summarization in scientific\ndomains. VISTA contains 18,599 recorded AI conference presentations paired with\ntheir corresponding paper abstracts. We benchmark the performance of\nstate-of-the-art large models and apply a plan-based framework to better\ncapture the structured nature of abstracts. Both human and automated\nevaluations confirm that explicit planning enhances summary quality and factual\nconsistency. However, a considerable gap remains between models and human\nperformance, highlighting the challenges of scientific video summarization.\n","authors":["Dongqi Liu","Chenxi Whitehouse","Xi Yu","Louis Mahon","Rohit Saxena","Zheng Zhao","Yifu Qiu","Mirella Lapata","Vera Demberg"],"pdf_url":"https://arxiv.org/pdf/2502.08279v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.11718v3","updated":"2025-02-26T13:56:22Z","published":"2025-02-17T12:02:23Z","title":"ChineseSimpleVQA -- \"See the World, Discover Knowledge\": A Chinese\n  Factuality Evaluation for Large Vision Language Models","summary":"  The evaluation of factual accuracy in large vision language models (LVLMs)\nhas lagged behind their rapid development, making it challenging to fully\nreflect these models' knowledge capacity and reliability. In this paper, we\nintroduce the first factuality-based visual question-answering benchmark in\nChinese, named ChineseSimpleVQA, aimed at assessing the visual factuality of\nLVLMs across 8 major topics and 56 subtopics. The key features of this\nbenchmark include a focus on the Chinese language, diverse knowledge types, a\nmulti-hop question construction, high-quality data, static consistency, and\neasy-to-evaluate through short answers. Moreover, we contribute a rigorous data\nconstruction pipeline and decouple the visual factuality into two parts: seeing\nthe world (i.e., object recognition) and discovering knowledge. This decoupling\nallows us to analyze the capability boundaries and execution mechanisms of\nLVLMs. Subsequently, we evaluate 34 advanced open-source and closed-source\nmodels, revealing critical performance gaps within this field.\n","authors":["Jihao Gu","Yingyao Wang","Pi Bu","Chen Wang","Ziming Wang","Tengtao Song","Donglai Wei","Jiale Yuan","Yingxiu Zhao","Yancheng He","Shilong Li","Jiaheng Liu","Meng Cao","Jun Song","Yingshui Tan","Xiang Li","Wenbo Su","Zhicheng Zheng","Xiaoyong Zhu","Bo Zheng"],"pdf_url":"https://arxiv.org/pdf/2502.11718v3.pdf","comment":"24 pages, 21 figures"},{"id":"http://arxiv.org/abs/2406.05516v2","updated":"2025-02-26T13:56:16Z","published":"2024-06-08T16:35:31Z","title":"Verbalized Probabilistic Graphical Modeling","summary":"  Human cognition excels at transcending sensory input and forming latent\nrepresentations that structure our understanding of the world. Although Large\nLanguage Models (LLMs) can produce chain-of-thought reasoning, they lack a\nprincipled framework to capture latent structures and model uncertainty,\nespecially in compositional reasoning tasks. We propose Verbalized\nProbabilistic Graphical Modeling (vPGM), a Bayesian prompting framework that\nguides LLMs to simulate key principles of Probabilistic Graphical Models (PGMs)\nin natural language. Unlike many traditional probabilistic methods requiring\nsubstantial domain expertise or specialized training, vPGM bypasses\nexpert-driven model design, making it well-suited for scenarios with limited\nassumptions or scarce data. We evaluated our model on several compositional\nreasoning tasks, both close-ended and open-ended. Our results indicate that the\nmodel effectively enhances confidence calibration and text generation quality.\n","authors":["Hengguan Huang","Xing Shen","Songtao Wang","Lingfa Meng","Dianbo Liu","Hao Wang","Samir Bhatt"],"pdf_url":"https://arxiv.org/pdf/2406.05516v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.02551v2","updated":"2025-02-26T13:51:56Z","published":"2024-10-03T14:55:22Z","title":"ColaCare: Enhancing Electronic Health Record Modeling through Large\n  Language Model-Driven Multi-Agent Collaboration","summary":"  We introduce ColaCare, a framework that enhances Electronic Health Record\n(EHR) modeling through multi-agent collaboration driven by Large Language\nModels (LLMs). Our approach seamlessly integrates domain-specific expert models\nwith LLMs to bridge the gap between structured EHR data and text-based\nreasoning. Inspired by the Multidisciplinary Team (MDT) approach used in\nclinical settings, ColaCare employs two types of agents: DoctorAgents and a\nMetaAgent, which collaboratively analyze patient data. Expert models process\nand generate predictions from numerical EHR data, while LLM agents produce\nreasoning references and decision-making reports within the MDT-driven\ncollaborative consultation framework. The MetaAgent orchestrates the\ndiscussion, facilitating consultations and evidence-based debates among\nDoctorAgents, simulating diverse expertise in clinical decision-making. We\nadditionally incorporate the Merck Manual of Diagnosis and Therapy (MSD)\nmedical guideline within a retrieval-augmented generation (RAG) module for\nmedical evidence support, addressing the challenge of knowledge currency.\nExtensive experiments conducted on three EHR datasets demonstrate ColaCare's\nsuperior performance in clinical mortality outcome and readmission prediction\ntasks, underscoring its potential to revolutionize clinical decision support\nsystems and advance personalized precision medicine. All code, case studies and\na questionnaire are available at the project website:\nhttps://colacare.netlify.app.\n","authors":["Zixiang Wang","Yinghao Zhu","Huiya Zhao","Xiaochen Zheng","Dehao Sui","Tianlong Wang","Wen Tang","Yasha Wang","Ewen Harrison","Chengwei Pan","Junyi Gao","Liantao Ma"],"pdf_url":"https://arxiv.org/pdf/2410.02551v2.pdf","comment":"ACM TheWebConf 2025 Conference (WWW 2025) Research Track"},{"id":"http://arxiv.org/abs/2502.19127v1","updated":"2025-02-26T13:34:52Z","published":"2025-02-26T13:34:52Z","title":"Self-Memory Alignment: Mitigating Factual Hallucinations with\n  Generalized Improvement","summary":"  Large Language Models (LLMs) often struggle to align their responses with\nobjective facts, resulting in the issue of factual hallucinations, which can be\ndifficult to detect and mislead users without relevant knowledge. While\npost-training techniques have been employed to mitigate the issue, existing\nmethods usually suffer from poor generalization and trade-offs in different\ncapabilities. In this paper, we propose to address it by directly augmenting\nLLM's fundamental ability to precisely leverage its existing memory--the\nknowledge acquired from pre-training data. We introduce self-memory alignment\n(SMA), which fine-tunes the model on self-generated responses to precise and\nsimple factual questions through preference optimization. Furthermore, we\nconstruct FactualBench, a comprehensive and precise factual QA dataset\ncontaining 181k Chinese data spanning 21 domains, to facilitate both evaluation\nand training. Extensive experiments show that SMA significantly improves LLMs'\noverall performance, with consistent enhancement across various benchmarks\nconcerning factuality, as well as helpfulness and comprehensive skills.\n","authors":["Siyuan Zhang","Yichi Zhang","Yinpeng Dong","Hang Su"],"pdf_url":"https://arxiv.org/pdf/2502.19127v1.pdf","comment":"29 pages, 17 figures"},{"id":"http://arxiv.org/abs/2406.00036v2","updated":"2025-02-26T13:18:09Z","published":"2024-05-27T10:53:15Z","title":"EMERGE: Enhancing Multimodal Electronic Health Records Predictive\n  Modeling with Retrieval-Augmented Generation","summary":"  The integration of multimodal Electronic Health Records (EHR) data has\nsignificantly advanced clinical predictive capabilities. Existing models, which\nutilize clinical notes and multivariate time-series EHR data, often fall short\nof incorporating the necessary medical context for accurate clinical tasks,\nwhile previous approaches with knowledge graphs (KGs) primarily focus on\nstructured knowledge extraction. In response, we propose EMERGE, a\nRetrieval-Augmented Generation (RAG) driven framework to enhance multimodal EHR\npredictive modeling. We extract entities from both time-series data and\nclinical notes by prompting Large Language Models (LLMs) and align them with\nprofessional PrimeKG, ensuring consistency. In addition to triplet\nrelationships, we incorporate entities' definitions and descriptions for richer\nsemantics. The extracted knowledge is then used to generate task-relevant\nsummaries of patients' health statuses. Finally, we fuse the summary with other\nmodalities using an adaptive multimodal fusion network with cross-attention.\nExtensive experiments on the MIMIC-III and MIMIC-IV datasets' in-hospital\nmortality and 30-day readmission tasks demonstrate the superior performance of\nthe EMERGE framework over baseline models. Comprehensive ablation studies and\nanalysis highlight the efficacy of each designed module and robustness to data\nsparsity. EMERGE contributes to refining the utilization of multimodal EHR data\nin healthcare, bridging the gap with nuanced medical contexts essential for\ninformed clinical predictions. We have publicly released the code at\nhttps://github.com/yhzhu99/EMERGE.\n","authors":["Yinghao Zhu","Changyu Ren","Zixiang Wang","Xiaochen Zheng","Shiyun Xie","Junlan Feng","Xi Zhu","Zhoujun Li","Liantao Ma","Chengwei Pan"],"pdf_url":"https://arxiv.org/pdf/2406.00036v2.pdf","comment":"CIKM 2024 Full Research Paper; arXiv admin note: text overlap with\n  arXiv:2402.07016"},{"id":"http://arxiv.org/abs/2502.19115v1","updated":"2025-02-26T13:10:38Z","published":"2025-02-26T13:10:38Z","title":"Improving customer service with automatic topic detection in user emails","summary":"  This study introduces a novel Natural Language Processing pipeline that\nenhances customer service efficiency at Telekom Srbija, a leading Serbian\ntelecommunications company, through automated email topic detection and\nlabelling. Central to the pipeline is BERTopic, a modular architecture that\nallows unsupervised topic modelling. After a series of preprocessing and\npost-processing steps, we assign one of 12 topics and several additional labels\nto incoming emails, allowing customer service to filter and access them through\na custom-made application. The model's performance was evaluated by assessing\nthe speed and correctness of the automatically assigned topics across a test\ndataset of 100 customer emails. The pipeline shows broad applicability across\nlanguages, particularly for those that are low-resourced and morphologically\nrich. The system now operates in the company's production environment,\nstreamlining customer service operations through automated email\nclassification.\n","authors":["Bojana Bašaragin","Darija Medvecki","Gorana Gojić","Milena Oparnica","Dragiša Mišković"],"pdf_url":"https://arxiv.org/pdf/2502.19115v1.pdf","comment":"Paper submitted to the 15th International Conference on Information\n  Society and Technology (ICIST), Kopaonik, Serbia, 9-12 March 2025"},{"id":"http://arxiv.org/abs/2502.16894v2","updated":"2025-02-26T13:02:48Z","published":"2025-02-24T06:48:13Z","title":"Make LoRA Great Again: Boosting LoRA with Adaptive Singular Values and\n  Mixture-of-Experts Optimization Alignment","summary":"  While Low-Rank Adaptation (LoRA) enables parameter-efficient fine-tuning for\nLarge Language Models (LLMs), its performance often falls short of Full\nFine-Tuning (Full FT). Current methods optimize LoRA by initializing with\nstatic singular value decomposition (SVD) subsets, leading to suboptimal\nleveraging of pre-trained knowledge. Another path for improving LoRA is\nincorporating a Mixture-of-Experts (MoE) architecture. However, weight\nmisalignment and complex gradient dynamics make it challenging to adopt SVD\nprior to the LoRA MoE architecture. To mitigate these issues, we propose\n\\underline{G}reat L\\underline{o}R\\underline{A} Mixture-of-Exper\\underline{t}\n(GOAT), a framework that (1) adaptively integrates relevant priors using an\nSVD-structured MoE, and (2) aligns optimization with full fine-tuned MoE by\nderiving a theoretical scaling factor. We demonstrate that proper scaling,\nwithout modifying the architecture or training algorithms, boosts LoRA MoE's\nefficiency and performance. Experiments across 25 datasets, including natural\nlanguage understanding, commonsense reasoning, image classification, and\nnatural language generation, demonstrate GOAT's state-of-the-art performance,\nclosing the gap with Full FT.\n","authors":["Chenghao Fan","Zhenyi Lu","Sichen Liu","Xiaoye Qu","Wei Wei","Chengfeng Gu","Yu Cheng"],"pdf_url":"https://arxiv.org/pdf/2502.16894v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.19110v1","updated":"2025-02-26T13:01:49Z","published":"2025-02-26T13:01:49Z","title":"Conformal Linguistic Calibration: Trading-off between Factuality and\n  Specificity","summary":"  Language model outputs are not always reliable; this prompts research into\nmethods for adapting model responses based on uncertainty. Common approaches\ninclude: \\emph{abstention}, where models refrain from generating responses when\nuncertain; and \\emph{linguistic calibration}, where models hedge their\nstatements using uncertainty quantifiers. However, abstention can withhold\nvaluable information, while linguistically calibrated responses are often\nchallenging to leverage in downstream tasks. We propose a unifying view of both\napproaches, Conformal Linguistic Calibration (CLC), reinterpreting linguistic\ncalibration as answer set prediction. We begin by presenting a unified\nframework that connects abstention and linguistic calibration through the lens\nof linguistic pragmatics. We then describe an implementation that allows for\ncontrolling the level of imprecision in model responses. Experimental results\nshow that our method produces calibrated outputs with conformal guarantees on\nfactual accuracy. Furthermore, our approach enables fine-tuning models to\nperform uncertainty-aware adaptive claim rewriting, offering a controllable\nbalance between factuality and specificity.\n","authors":["Zhengping Jiang","Anqi Liu","Benjamin Van Durme"],"pdf_url":"https://arxiv.org/pdf/2502.19110v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.17475v2","updated":"2025-02-26T12:57:16Z","published":"2025-02-16T13:28:55Z","title":"ECG-Expert-QA: A Benchmark for Evaluating Medical Large Language Models\n  in Heart Disease Diagnosis","summary":"  We present ECG-Expert-QA, a comprehensive multimodal dataset designed for\nevaluating diagnostic capabilities in ECG interpretation, integrating real\nclinical data with systematically generated synthetic cases. The dataset\nencompasses six fundamental diagnostic tasks, comprising 47,211 meticulously\ncurated question-answer pairs that span a spectrum of clinical scenarios, from\nbasic rhythm analysis to complex case interpretation. By simulating challenging\nclinical cases through a rigorous medical knowledge-guided process,\nECG-Expert-QA not only enhances the availability of annotated diagnostic data\nbut also significantly increases the complexity and diversity of clinical\npresentations, including rare cardiac conditions and temporal progression\npatterns. This design enables comprehensive evaluation of medical language\nmodels across multiple dimensions, including diagnostic accuracy, clinical\nreasoning, and knowledge integration. To facilitate global research\ncollaboration, ECG-Expert-QA is available in both Chinese and English versions,\nwith rigorous quality control ensuring linguistic and clinical consistency. The\ndataset's challenging diagnostic tasks, which include interpretation of complex\narrhythmias, identification of subtle ischemic changes, and integration of\nclinical context, establish it as an effective benchmark for advancing\nAI-assisted ECG interpretation and pushing the boundaries of current diagnostic\nmodels. Our dataset is open-source and available at\nhttps://github.com/Zaozzz/ECG-Expert-QA\n","authors":["Xu Wang","Jiaju Kang","Puyu Han"],"pdf_url":"https://arxiv.org/pdf/2502.17475v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.19104v1","updated":"2025-02-26T12:46:59Z","published":"2025-02-26T12:46:59Z","title":"Evaluating Gender Bias in German Machine Translation","summary":"  We present WinoMTDE, a new gender bias evaluation test set designed to assess\noccupational stereotyping and underrepresentation in German machine translation\n(MT) systems. Building on the automatic evaluation method introduced by\narXiv:1906.00591v1 [cs.CL], we extend the approach to German, a language with\ngrammatical gender. The WinoMTDE dataset comprises 288 German sentences that\nare balanced in regard to gender, as well as stereotype, which was annotated\nusing German labor statistics. We conduct a large-scale evaluation of five\nwidely used MT systems and a large language model. Our results reveal\npersistent bias in most models, with the LLM outperforming traditional systems.\nThe dataset and evaluation code are publicly available under\nhttps://github.com/michellekappl/mt_gender_german.\n","authors":["Michelle Kappl"],"pdf_url":"https://arxiv.org/pdf/2502.19104v1.pdf","comment":"ISCA/ITG Workshop on Diversity in Large Speech and Language Models"},{"id":"http://arxiv.org/abs/2502.19103v1","updated":"2025-02-26T12:46:36Z","published":"2025-02-26T12:46:36Z","title":"LongEval: A Comprehensive Analysis of Long-Text Generation Through a\n  Plan-based Paradigm","summary":"  Large Language Models (LLMs) have achieved remarkable success in various\nnatural language processing tasks, yet their ability to generate long-form\ncontent remains poorly understood and evaluated. Our analysis reveals that\ncurrent LLMs struggle with length requirements and information density in\nlong-text generation, with performance deteriorating as text length increases.\nTo quantitively locate such a performance degradation and provide further\ninsights on model development, we present LongEval, a benchmark that evaluates\nlong-text generation through both direct and plan-based generation paradigms,\ninspired by cognitive and linguistic writing models. The comprehensive\nexperiments in this work reveal interesting findings such as that while model\nsize correlates with generation ability, the small-scale model (e.g.,\nLongWriter), well-trained on long texts, has comparable performance. All code\nand datasets are released in https://github.com/Wusiwei0410/LongEval.\n","authors":["Siwei Wu","Yizhi Li","Xingwei Qu","Rishi Ravikumar","Yucheng Li","Tyler Loakman Shanghaoran Quan Xiaoyong Wei","Riza Batista-Navarro","Chenghua Lin"],"pdf_url":"https://arxiv.org/pdf/2502.19103v1.pdf","comment":"Under review"},{"id":"http://arxiv.org/abs/2402.18945v3","updated":"2025-02-26T12:40:42Z","published":"2024-02-29T08:20:49Z","title":"SynGhost: Invisible and Universal Task-agnostic Backdoor Attack via\n  Syntactic Transfer","summary":"  Although pre-training achieves remarkable performance, it suffers from\ntask-agnostic backdoor attacks due to vulnerabilities in data and training\nmechanisms. These attacks can transfer backdoors to various downstream tasks.\nIn this paper, we introduce $\\mathtt{maxEntropy}$, an entropy-based poisoning\nfilter that mitigates such risks. To overcome the limitations of manual target\nsetting and explicit triggers, we propose $\\mathtt{SynGhost}$, an invisible and\nuniversal task-agnostic backdoor attack via syntactic transfer, further\nexposing vulnerabilities in pre-trained language models (PLMs). Specifically,\n$\\mathtt{SynGhost}$ injects multiple syntactic backdoors into the pre-training\nspace through corpus poisoning, while preserving the PLM's pre-training\ncapabilities. Second, $\\mathtt{SynGhost}$ adaptively selects optimal targets\nbased on contrastive learning, creating a uniform distribution in the\npre-training space. To identify syntactic differences, we also introduce an\nawareness module to minimize interference between backdoors. Experiments show\nthat $\\mathtt{SynGhost}$ poses significant threats and can transfer to various\ndownstream tasks. Furthermore, $\\mathtt{SynGhost}$ resists defenses based on\nperplexity, fine-pruning, and $\\mathtt{maxEntropy}$. The code is available at\nhttps://github.com/Zhou-CyberSecurity-AI/SynGhost.\n","authors":["Pengzhou Cheng","Wei Du","Zongru Wu","Fengwei Zhang","Libo Chen","Zhuosheng Zhang","Gongshen Liu"],"pdf_url":"https://arxiv.org/pdf/2402.18945v3.pdf","comment":"17 pages, 16 figures, 12 tables, accepted at NAACL 2025 Findings"},{"id":"http://arxiv.org/abs/2410.08964v3","updated":"2025-02-26T12:39:40Z","published":"2024-10-11T16:32:05Z","title":"Language Imbalance Driven Rewarding for Multilingual Self-improving","summary":"  Large Language Models (LLMs) have achieved state-of-the-art performance\nacross numerous tasks. However, these advancements have predominantly benefited\n\"first-class\" languages such as English and Chinese, leaving many other\nlanguages underrepresented. This imbalance, while limiting broader\napplications, generates a natural preference ranking between languages,\noffering an opportunity to bootstrap the multilingual capabilities of LLM in a\nself-improving manner. Thus, we propose $\\textit{Language Imbalance Driven\nRewarding}$, where the inherent imbalance between dominant and non-dominant\nlanguages within LLMs is leveraged as a reward signal. Iterative DPO training\ndemonstrates that this approach not only enhances LLM performance in\nnon-dominant languages but also improves the dominant language's capacity,\nthereby yielding an iterative reward signal. Fine-tuning\nMeta-Llama-3-8B-Instruct over two iterations of this approach results in\ncontinuous improvements in multilingual performance across\ninstruction-following and arithmetic reasoning tasks, evidenced by an average\nimprovement of 7.46% win rate on the X-AlpacaEval leaderboard and 13.9%\naccuracy on the MGSM benchmark. This work serves as an initial exploration,\npaving the way for multilingual self-improvement of LLMs. The code is available\nat https://github.com/ZNLP/Language-Imbalance-Driven-Rewarding\n","authors":["Wen Yang","Junhong Wu","Chen Wang","Chengqing Zong","Jiajun Zhang"],"pdf_url":"https://arxiv.org/pdf/2410.08964v3.pdf","comment":"Camera ready version for ICLR 2025"},{"id":"http://arxiv.org/abs/2502.19078v1","updated":"2025-02-26T12:11:16Z","published":"2025-02-26T12:11:16Z","title":"Sparse Brains are Also Adaptive Brains: Cognitive-Load-Aware Dynamic\n  Activation for LLMs","summary":"  Dense large language models(LLMs) face critical efficiency bottlenecks as\nthey rigidly activate all parameters regardless of input complexity. While\nexisting sparsity methods(static pruning or dynamic activation) address this\npartially, they either lack adaptivity to contextual or model structural\ndemands or incur prohibitive computational overhead. Inspired by human brain's\ndual-process mechanisms - predictive coding (N400) for backbone sparsity and\nstructural reanalysis (P600) for complex context - we propose CLADA, a\n\\textit{\\textbf{C}ognitive-\\textbf{L}oad-\\textbf{A}ware \\textbf{D}ynamic\n\\textbf{A}ctivation} framework that synergizes statistical sparsity with\nsemantic adaptability. Our key insight is that LLM activations exhibit two\ncomplementary patterns: 1) \\textit{Global statistical sparsity} driven by\nsequence-level prefix information, and 2) \\textit{Local semantic adaptability}\nmodulated by cognitive load metrics(e.g., surprisal and entropy). CLADA employs\na hierarchical thresholding strategy: a baseline from offline error-controlled\noptimization ensures 40\\%+ sparsity, dynamically adjusted by real-time\ncognitive signals. Evaluations across six mainstream LLMs and nine benchmarks\ndemonstrate that CLADA achieves \\textbf{~20\\% average speedup with <2\\%\naccuracy drop}, outperforming Griffin (5\\%+ degradation) and TT (negligible\nspeedup). Crucially, we establish the first formal connection between\nneurolinguistic event-related potential (ERP) components and LLM efficiency\nmechanisms through multi-level regression analysis ($R^2=0.17$ for\nsparsity-adaptation synergy). Requiring no retraining or architectural changes,\nCLADA offers a deployable solution for resource-aware LLM inference while\nadvancing biologically-inspired AI design. Our code is available at\n\\href{https://github.com/Oldify/CLADA}{CLADA}.\n","authors":["Yiheng Yang","Yujie Wang","Chi Ma","Lei Yu","Emmanuele Chersoni","Chu-Ren Huang"],"pdf_url":"https://arxiv.org/pdf/2502.19078v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.07095v6","updated":"2025-02-26T11:57:30Z","published":"2024-10-09T17:34:27Z","title":"MLE-bench: Evaluating Machine Learning Agents on Machine Learning\n  Engineering","summary":"  We introduce MLE-bench, a benchmark for measuring how well AI agents perform\nat machine learning engineering. To this end, we curate 75 ML\nengineering-related competitions from Kaggle, creating a diverse set of\nchallenging tasks that test real-world ML engineering skills such as training\nmodels, preparing datasets, and running experiments. We establish human\nbaselines for each competition using Kaggle's publicly available leaderboards.\nWe use open-source agent scaffolds to evaluate several frontier language models\non our benchmark, finding that the best-performing setup--OpenAI's o1-preview\nwith AIDE scaffolding--achieves at least the level of a Kaggle bronze medal in\n16.9% of competitions. In addition to our main results, we investigate various\nforms of resource scaling for AI agents and the impact of contamination from\npre-training. We open-source our benchmark code (github.com/openai/mle-bench/)\nto facilitate future research in understanding the ML engineering capabilities\nof AI agents.\n","authors":["Jun Shern Chan","Neil Chowdhury","Oliver Jaffe","James Aung","Dane Sherburn","Evan Mays","Giulio Starace","Kevin Liu","Leon Maksin","Tejal Patwardhan","Lilian Weng","Aleksander Mądry"],"pdf_url":"https://arxiv.org/pdf/2410.07095v6.pdf","comment":"10 pages, 17 pages appendix. Equal contribution by first seven\n  authors, authors randomized. ICLR version"},{"id":"http://arxiv.org/abs/2502.19074v1","updated":"2025-02-26T11:56:43Z","published":"2025-02-26T11:56:43Z","title":"Improving the quality of Web-mined Parallel Corpora of Low-Resource\n  Languages using Debiasing Heuristics","summary":"  Parallel Data Curation (PDC) techniques aim to filter out noisy parallel\nsentences from the web-mined corpora. Prior research has demonstrated that\nranking sentence pairs using similarity scores on sentence embeddings derived\nfrom Pre-trained Multilingual Language Models (multiPLMs) and training the NMT\nsystems with the top-ranked samples, produces superior NMT performance than\nwhen trained using the full dataset. However, previous research has shown that\nthe choice of multiPLM significantly impacts the ranking quality. This paper\ninvestigates the reasons behind this disparity across multiPLMs. Using the\nweb-mined corpora CCMatrix and CCAligned for En$\\rightarrow$Si,\nEn$\\rightarrow$Ta and Si$\\rightarrow$Ta, we show that different multiPLMs\n(LASER3, XLM-R, and LaBSE) are biased towards certain types of sentences, which\nallows noisy sentences to creep into the top-ranked samples. We show that by\nemploying a series of heuristics, this noise can be removed to a certain\nextent. This results in improving the results of NMT systems trained with\nweb-mined corpora and reduces the disparity across multiPLMs.\n","authors":["Aloka Fernando","Surangika Ranathunga","Nisansa de Silva"],"pdf_url":"https://arxiv.org/pdf/2502.19074v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.19067v1","updated":"2025-02-26T11:48:42Z","published":"2025-02-26T11:48:42Z","title":"IndicEval-XL: Bridging Linguistic Diversity in Code Generation Across\n  Indic Languages","summary":"  Large Language Models (LLMs) have demonstrated remarkable capabilities in\ncode generation from natural language prompts, revolutionizing software\ndevelopment workflows. As we advance towards agent-based development paradigms,\nthese models form the cornerstone of next-generation software development\nlifecycles. However, current benchmarks for evaluating multilingual code\ngeneration capabilities are predominantly English-centric, limiting their\napplicability across the global developer community. To address this\nlimitation, we present IndicEval-XL, a comprehensive benchmark for code\ngeneration that incorporates 6 major Indic languages, collectively spoken by\napproximately 14\\% of the world's population. Our benchmark bridges these\nlanguages with 12 programming languages, creating a robust evaluation\nframework. This work is particularly significant given India's representation\nof one-eighth of the global population and the crucial role Indic languages\nplay in Indian society. IndicEval-XL represents a significant step toward\nexpanding the linguistic diversity in code generation systems and evaluation\nframeworks. By developing resources that support multiple languages, we aim to\nmake AI-powered development tools more inclusive and accessible to developers\nof various linguistic backgrounds. To facilitate further research and\ndevelopment in this direction, we make our dataset and evaluation benchmark\npublicly available at https://github.com/telekom/IndicEval-XL\n","authors":["Ujjwal Singh","Aditi Sharma","Nikhil Gupta"," Deepakshi","Vivek Kumar Jha"],"pdf_url":"https://arxiv.org/pdf/2502.19067v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.15766v3","updated":"2025-02-26T11:47:58Z","published":"2024-08-28T12:59:12Z","title":"Learning Harmonized Representations for Speculative Sampling","summary":"  Speculative sampling is a promising approach to accelerate the decoding stage\nfor Large Language Models (LLMs). Recent advancements that leverage target\nLLM's contextual information, such as hidden states and KV cache, have shown\nsignificant practical improvements. However, these approaches suffer from\ninconsistent context between training and decoding. We also observe another\ndiscrepancy between the training and decoding objectives in existing\nspeculative sampling methods. In this work, we propose a solution named\nHArmonized Speculative Sampling (HASS) that learns harmonized representations\nto address these issues. HASS accelerates the decoding stage without adding\ninference overhead through harmonized objective distillation and harmonized\ncontext alignment. Experiments on four LLaMA models demonstrate that HASS\nachieves 2.81x-4.05x wall-clock time speedup ratio averaging across three\ndatasets, surpassing EAGLE-2 by 8%-20%. The code is available at\nhttps://github.com/HArmonizedSS/HASS.\n","authors":["Lefan Zhang","Xiaodan Wang","Yanhua Huang","Ruiwen Xu"],"pdf_url":"https://arxiv.org/pdf/2408.15766v3.pdf","comment":"Published as a conference paper at ICLR 2025"},{"id":"http://arxiv.org/abs/2502.19064v1","updated":"2025-02-26T11:43:25Z","published":"2025-02-26T11:43:25Z","title":"Can Large Language Models Outperform Non-Experts in Poetry Evaluation? A\n  Comparative Study Using the Consensual Assessment Technique","summary":"  The Consensual Assessment Technique (CAT) evaluates creativity through\nholistic expert judgments. We investigate the use of two advanced Large\nLanguage Models (LLMs), Claude-3-Opus and GPT-4o, to evaluate poetry by a\nmethodology inspired by the CAT. Using a dataset of 90 poems, we found that\nthese LLMs can surpass the results achieved by non-expert human judges at\nmatching a ground truth based on publication venue, particularly when assessing\nsmaller subsets of poems. Claude-3-Opus exhibited slightly superior performance\nthan GPT-4o. We show that LLMs are viable tools for accurately assessing\npoetry, paving the way for their broader application into other creative\ndomains.\n","authors":["Piotr Sawicki","Marek Grześ","Dan Brown","Fabrício Góes"],"pdf_url":"https://arxiv.org/pdf/2502.19064v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.19058v1","updated":"2025-02-26T11:17:50Z","published":"2025-02-26T11:17:50Z","title":"MathClean: A Benchmark for Synthetic Mathematical Data Cleaning","summary":"  With the rapid development of large language models (LLMs), the quality of\ntraining data has become crucial. Among the various types of training data,\nmathematical data plays a key role in enabling LLMs to acquire strong reasoning\nabilities. While high-quality open-source data is important, it is often\ninsufficient for pre-training, necessitating the addition of synthetic math\nproblems. However, synthetic math questions and answers can introduce\ninaccuracies, which may degrade both the training data and web data. Therefore,\nan effective method for cleaning synthetic math data is essential. In this\npaper, we propose the MathClean benchmark to evaluate the effectiveness of math\ndata cleaning models. The MathClean benchmark consists of 2,000 correct\nquestions and 2,000 erroneous questions with additional 2,000 correct and\nerroneous answers sourced from augmented data based on GSM8K and MATH.\nMoreover, we also annotate error types for each question or answer, since it\ncan assess whether models can correctly identify the error categories for\nfuture improvements. Finally, we present comprehensive evaluations using\nstate-of-the-art (SOTA) models. Our results demonstrate that even strong models\nlike GPT-o1 and DeepSeek-R1 perform poorly on this benchmark, highlighting the\nutility of MathClean. Our code and data is available at\nhttps://github.com/YuYingLi0/MathClean.\n","authors":["Hao Liang","Meiyi Qiang","Yuying Li","Zefeng He","Yongzhen Guo","Zhengzhou Zhu","Wentao Zhang","Bin Cui"],"pdf_url":"https://arxiv.org/pdf/2502.19058v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.03187v2","updated":"2025-02-26T11:10:48Z","published":"2024-12-04T10:15:12Z","title":"Weighted-Reward Preference Optimization for Implicit Model Fusion","summary":"  While fusing heterogeneous open-source LLMs with varying architectures and\nsizes can potentially integrate the strengths of different models, existing\nfusion methods face significant challenges, such as vocabulary alignment and\nmerging distribution matrices. These procedures are not only complex but also\nprone to introducing noise and errors. In this paper, we propose an implicit\nfusion method, Weighted-Reward Preference Optimization (WRPO), which leverages\npreference optimization between the source LLMs and the target LLM to transfer\ntheir capabilities effectively. WRPO eliminates the need for vocabulary\nalignment and matrix fusion and can be efficiently scaled to accommodate\nvarious LLMs. To address distributional deviations between the source and\ntarget LLMs, WRPO introduces a progressive adaptation strategy that gradually\nshifts reliance on preferred examples from the target LLM to the source LLMs.\nExtensive experiments on the MT-Bench, AlpacaEval-2, and Arena-Hard benchmarks\ndemonstrate that WRPO consistently outperforms existing knowledge fusion\nmethods and various fine-tuning baselines. When applied to LLaMA3-8B-Instruct\nas the target model, WRPO achieves a length-controlled win rate of 55.9%\nagainst GPT-4-Preview-1106 on AlpacaEval-2 and a win rate of 46.2% against\nGPT-4-0314 on Arena-Hard. Our code is available at\nhttps://github.com/SLIT-AI/WRPO.\n","authors":["Ziyi Yang","Fanqi Wan","Longguang Zhong","Tianyuan Shi","Xiaojun Quan"],"pdf_url":"https://arxiv.org/pdf/2412.03187v2.pdf","comment":"Accepted to ICLR 2025"},{"id":"http://arxiv.org/abs/2502.19024v1","updated":"2025-02-26T10:30:40Z","published":"2025-02-26T10:30:40Z","title":"Ground-level Viewpoint Vision-and-Language Navigation in Continuous\n  Environments","summary":"  Vision-and-Language Navigation (VLN) empowers agents to associate\ntime-sequenced visual observations with corresponding instructions to make\nsequential decisions. However, generalization remains a persistent challenge,\nparticularly when dealing with visually diverse scenes or transitioning from\nsimulated environments to real-world deployment. In this paper, we address the\nmismatch between human-centric instructions and quadruped robots with a\nlow-height field of view, proposing a Ground-level Viewpoint Navigation (GVNav)\napproach to mitigate this issue. This work represents the first attempt to\nhighlight the generalization gap in VLN across varying heights of visual\nobservation in realistic robot deployments. Our approach leverages weighted\nhistorical observations as enriched spatiotemporal contexts for instruction\nfollowing, effectively managing feature collisions within cells by assigning\nappropriate weights to identical features across different viewpoints. This\nenables low-height robots to overcome challenges such as visual obstructions\nand perceptual mismatches. Additionally, we transfer the connectivity graph\nfrom the HM3D and Gibson datasets as an extra resource to enhance spatial\npriors and a more comprehensive representation of real-world scenarios, leading\nto improved performance and generalizability of the waypoint predictor in\nreal-world environments. Extensive experiments demonstrate that our\nGround-level Viewpoint Navigation (GVnav) approach significantly improves\nperformance in both simulated environments and real-world deployments with\nquadruped robots.\n","authors":["Zerui Li","Gengze Zhou","Haodong Hong","Yanyan Shao","Wenqi Lyu","Yanyuan Qiao","Qi Wu"],"pdf_url":"https://arxiv.org/pdf/2502.19024v1.pdf","comment":"Accepted by ICRA 2025"},{"id":"http://arxiv.org/abs/2502.19008v1","updated":"2025-02-26T10:14:19Z","published":"2025-02-26T10:14:19Z","title":"Binary Neural Networks for Large Language Model: A Survey","summary":"  Large language models (LLMs) have wide applications in the field of natural\nlanguage processing(NLP), such as GPT-4 and Llama. However, with the\nexponential growth of model parameter sizes, LLMs bring significant resource\noverheads. Low-bit quantization, as a key technique, reduces memory usage and\ncomputational demands by decreasing the bit-width of model parameters,\nactivations, and gradients. Previous quantization methods for LLMs have largely\nemployed Post-Training Quantization (PTQ) and Quantization-Aware Training\n(QAT). PTQ does not require any retraining of the original model, while QAT\ninvolves optimizing precision during training to achieve the best quantization\nparameters. The BitNet team proposed a radically different approach, where\nquantization is performed from the start of model training, utilizing\nlow-precision binary weights during the training process. This approach has led\nto the emergence of many binary quantization techniques for large language\nmodels. This paper provides a comprehensive review of these binary quantization\ntechniques. Specifically, we will introduce binary quantization techniques in\ndeep neural networks and further explore their application to LLMs, reviewing\ntheir various contributions, implementations, and applications.\n","authors":["Liangdong Liu","Zhitong Zheng","Cong Wang","Tianhuang Su","Zhenyu Yang"],"pdf_url":"https://arxiv.org/pdf/2502.19008v1.pdf","comment":"23 pages, 7 figures"},{"id":"http://arxiv.org/abs/2306.04347v2","updated":"2025-02-26T10:11:39Z","published":"2023-06-07T11:25:20Z","title":"World Models for Math Story Problems","summary":"  Solving math story problems is a complex task for students and NLP models\nalike, requiring them to understand the world as described in the story and\nreason over it to compute an answer. Recent years have seen impressive\nperformance on automatically solving these problems with large pre-trained\nlanguage models and innovative techniques to prompt them. However, it remains\nunclear if these models possess accurate representations of mathematical\nconcepts. This leads to lack of interpretability and trustworthiness which\nimpedes their usefulness in various applications. In this paper, we consolidate\nprevious work on categorizing and representing math story problems and develop\nMathWorld, which is a graph-based semantic formalism specific for the domain of\nmath story problems. With MathWorld, we can assign world models to math story\nproblems which represent the situations and actions introduced in the text and\ntheir mathematical relationships. We combine math story problems from several\nexisting datasets and annotate a corpus of 1,019 problems and 3,204 logical\nforms with MathWorld. Using this data, we demonstrate the following use cases\nof MathWorld: (1) prompting language models with synthetically generated\nquestion-answer pairs to probe their reasoning and world modeling abilities,\nand (2) generating new problems by using the world models as a design space.\n","authors":["Andreas Opedal","Niklas Stoehr","Abulhair Saparov","Mrinmaya Sachan"],"pdf_url":"https://arxiv.org/pdf/2306.04347v2.pdf","comment":"ACL Findings 2023"},{"id":"http://arxiv.org/abs/2502.18993v1","updated":"2025-02-26T09:56:51Z","published":"2025-02-26T09:56:51Z","title":"MEBench: Benchmarking Large Language Models for Cross-Document\n  Multi-Entity Question Answering","summary":"  Multi-entity question answering (MEQA) represents significant challenges for\nlarge language models (LLM) and retrieval-augmented generation (RAG) systems,\nwhich frequently struggle to consolidate scattered information across diverse\ndocuments. While existing methods excel at single-document comprehension, they\noften struggle with cross-document aggregation, particularly when resolving\nentity-dense questions like \"What is the distribution of ACM Fellows among\nvarious fields of study?\", which require integrating entity-centric insights\nfrom heterogeneous sources (e.g., Wikipedia pages). To address this gap, we\nintroduce MEBench, a novel multi-document, multi-entity benchmark designed to\nsystematically evaluate LLMs' capacity to retrieve, consolidate, and reason\nover fragmented information. Our benchmark comprises 4,780 questions which are\nsystematically categorized into three primary categories, further divided into\neight distinct types, ensuring broad coverage of real-world multi-entity\nreasoning scenarios. Our experiments on state-of-the-art LLMs (e.g., GPT-4,\nLlama-3) and RAG pipelines reveal critical limitations: even advanced models\nachieve only 59% accuracy on MEBench. Our benchmark emphasizes the importance\nof completeness and factual precision of information extraction in MEQA tasks,\nusing Entity-Attributed F1 (EA-F1) metric for granular evaluation of\nentity-level correctness and attribution validity. MEBench not only highlights\nsystemic weaknesses in current LLM frameworks but also provides a foundation\nfor advancing robust, entity-aware QA architectures.\n","authors":["Teng Lin"],"pdf_url":"https://arxiv.org/pdf/2502.18993v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.18990v1","updated":"2025-02-26T09:54:33Z","published":"2025-02-26T09:54:33Z","title":"GenTool: Enhancing Tool Generalization in Language Models through\n  Zero-to-One and Weak-to-Strong Simulation","summary":"  Large Language Models (LLMs) can enhance their capabilities as AI assistants\nby integrating external tools, allowing them to access a wider range of\ninformation. While recent LLMs are typically fine-tuned with tool usage\nexamples during supervised fine-tuning (SFT), questions remain about their\nability to develop robust tool-usage skills and can effectively generalize to\nunseen queries and tools. In this work, we present GenTool, a novel training\nframework that prepares LLMs for diverse generalization challenges in tool\nutilization. Our approach addresses two fundamental dimensions critical for\nreal-world applications: Zero-to-One Generalization, enabling the model to\naddress queries initially lacking a suitable tool by adopting and utilizing one\nwhen it becomes available, and Weak-to-Strong Generalization, allowing models\nto leverage enhanced versions of existing tools to solve queries. To achieve\nthis, we develop synthetic training data simulating these two dimensions of\ntool usage and introduce a two-stage fine-tuning approach: optimizing tool\nranking, then refining tool selection. Through extensive experiments across\nfour generalization scenarios, we demonstrate that our method significantly\nenhances the tool-usage capabilities of LLMs ranging from 1B to 8B parameters,\nachieving performance that surpasses GPT-4o. Furthermore, our analysis also\nprovides valuable insights into the challenges LLMs encounter in tool\ngeneralization.\n","authors":["Jie He","Jennifer Neville","Mengting Wan","Longqi Yang","Hui Liu","Xiaofeng Xu","Xia Song","Jeff Z. Pan","Pei Zhou"],"pdf_url":"https://arxiv.org/pdf/2502.18990v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.12851v4","updated":"2025-02-26T09:54:28Z","published":"2025-01-22T12:59:08Z","title":"ACEBench: Who Wins the Match Point in Tool Usage?","summary":"  Large Language Models (LLMs) have demonstrated significant potential in\ndecision-making and reasoning, particularly when integrated with various tools\nto effectively solve complex problems. However, existing benchmarks for\nevaluating LLMs' tool usage face several limitations: (1) limited evaluation\nscenarios, often lacking assessments in real multi-turn dialogue contexts; (2)\nnarrow evaluation dimensions, with insufficient detailed assessments of how\nLLMs use tools; and (3) reliance on LLMs or real API executions for evaluation,\nwhich introduces significant overhead. To address these challenges, we\nintroduce ACEBench, a comprehensive benchmark for assessing tool usage in LLMs.\nACEBench categorizes data into three primary types based on evaluation\nmethodology: Normal, Special, and Agent. \"Normal\" evaluates tool usage in basic\nscenarios; \"Special\" evaluates tool usage in situations with ambiguous or\nincomplete instructions; \"Agent\" evaluates tool usage through multi-agent\ninteractions to simulate real-world, multi-turn dialogues. We conducted\nextensive experiments using ACEBench, analyzing various LLMs in-depth and\nproviding a more granular examination of error causes across different data\ntypes.\n","authors":["Chen Chen","Xinlong Hao","Weiwen Liu","Xu Huang","Xingshan Zeng","Shuai Yu","Dexun Li","Shuai Wang","Weinan Gan","Yuefeng Huang","Wulong Liu","Xinzhi Wang","Defu Lian","Baoqun Yin","Yasheng Wang","Wu Liu"],"pdf_url":"https://arxiv.org/pdf/2501.12851v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.18980v1","updated":"2025-02-26T09:43:08Z","published":"2025-02-26T09:43:08Z","title":"PEToolLLM: Towards Personalized Tool Learning in Large Language Models","summary":"  Tool learning has emerged as a promising direction by extending Large\nLanguage Models' (LLMs) capabilities with external tools. Existing tool\nlearning studies primarily focus on the general-purpose tool-use capability,\nwhich addresses explicit user requirements in instructions. However, they\noverlook the importance of personalized tool-use capability, leading to an\ninability to handle implicit user preferences. To address the limitation, we\nfirst formulate the task of personalized tool learning, which integrates user's\ninteraction history towards personalized tool usage. To fill the gap of missing\nbenchmarks, we construct PEToolBench, featuring diverse user preferences\nreflected in interaction history under three distinct personalized settings,\nand encompassing a wide range of tool-use scenarios. Moreover, we propose a\nframework PEToolLLaMA to adapt LLMs to the personalized tool learning task,\nwhich is trained through supervised fine-tuning and direct preference\noptimization. Extensive experiments on PEToolBench demonstrate the superiority\nof PEToolLLaMA over existing LLMs.\n","authors":["Qiancheng Xu","Yongqi Li","Heming Xia","Fan Liu","Min Yang","Wenjie Li"],"pdf_url":"https://arxiv.org/pdf/2502.18980v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.18978v1","updated":"2025-02-26T09:37:21Z","published":"2025-02-26T09:37:21Z","title":"Low-Confidence Gold: Refining Low-Confidence Samples for Efficient\n  Instruction Tuning","summary":"  The effectiveness of instruction fine-tuning for Large Language Models is\nfundamentally constrained by the quality and efficiency of training datasets.\nThis work introduces Low-Confidence Gold (LCG), a novel filtering framework\nthat employs centroid-based clustering and confidence-guided selection for\nidentifying valuable instruction pairs. Through a semi-supervised approach\nusing a lightweight classifier trained on representative samples, LCG curates\nhigh-quality subsets while preserving data diversity. Experimental evaluation\ndemonstrates that models fine-tuned on LCG-filtered subsets of 6K samples\nachieve superior performance compared to existing methods, with substantial\nimprovements on MT-bench and consistent gains across comprehensive evaluation\nmetrics. The framework's efficacy while maintaining model performance\nestablishes a promising direction for efficient instruction tuning.\n","authors":["Hongyi Cal","ie Li","Wenzhen Dong"],"pdf_url":"https://arxiv.org/pdf/2502.18978v1.pdf","comment":"8 pages"},{"id":"http://arxiv.org/abs/2502.03382v2","updated":"2025-02-26T09:31:58Z","published":"2025-02-05T17:18:55Z","title":"High-Fidelity Simultaneous Speech-To-Speech Translation","summary":"  We introduce Hibiki, a decoder-only model for simultaneous speech\ntranslation. Hibiki leverages a multistream language model to synchronously\nprocess source and target speech, and jointly produces text and audio tokens to\nperform speech-to-text and speech-to-speech translation. We furthermore address\nthe fundamental challenge of simultaneous interpretation, which unlike its\nconsecutive counterpart, where one waits for the end of the source utterance to\nstart translating, adapts its flow to accumulate just enough context to produce\na correct translation in real-time, chunk by chunk. To do so, we introduce a\nweakly-supervised method that leverages the perplexity of an off-the-shelf text\ntranslation system to identify optimal delays on a per-word basis and create\naligned synthetic data. After supervised training, Hibiki performs adaptive,\nsimultaneous speech translation with vanilla temperature sampling. On a\nFrench-English simultaneous speech translation task, Hibiki demonstrates\nstate-of-the-art performance in translation quality, speaker fidelity and\nnaturalness. Moreover, the simplicity of its inference process makes it\ncompatible with batched translation and even real-time on-device deployment. We\nprovide examples as well as models and inference code.\n","authors":["Tom Labiausse","Laurent Mazaré","Edouard Grave","Patrick Pérez","Alexandre Défossez","Neil Zeghidour"],"pdf_url":"https://arxiv.org/pdf/2502.03382v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.18969v1","updated":"2025-02-26T09:27:54Z","published":"2025-02-26T09:27:54Z","title":"(Mis)Fitting: A Survey of Scaling Laws","summary":"  Modern foundation models rely heavily on using scaling laws to guide crucial\ntraining decisions. Researchers often extrapolate the optimal architecture and\nhyper parameters settings from smaller training runs by describing the\nrelationship between, loss, or task performance, and scale. All components of\nthis process vary, from the specific equation being fit, to the training setup,\nto the optimization method. Each of these factors may affect the fitted law,\nand therefore, the conclusions of a given study. We discuss discrepancies in\nthe conclusions that several prior works reach, on questions such as the\noptimal token to parameter ratio. We augment this discussion with our own\nanalysis of the critical impact that changes in specific details may effect in\na scaling study, and the resulting altered conclusions. Additionally, we survey\nover 50 papers that study scaling trends: while 45 of these papers quantify\nthese trends using a power law, most under-report crucial details needed to\nreproduce their findings. To mitigate this, we we propose a checklist for\nauthors to consider while contributing to scaling law research.\n","authors":["Margaret Li","Sneha Kudugunta","Luke Zettlemoyer"],"pdf_url":"https://arxiv.org/pdf/2502.18969v1.pdf","comment":"41 pages, 3 figure, first two authors contributed equally. ICLR, 2025"},{"id":"http://arxiv.org/abs/2502.18968v1","updated":"2025-02-26T09:26:54Z","published":"2025-02-26T09:26:54Z","title":"Know You First and Be You Better: Modeling Human-Like User Simulators\n  via Implicit Profiles","summary":"  User simulators are crucial for replicating human interactions with dialogue\nsystems, supporting both collaborative training and automatic evaluation,\nespecially for large language models (LLMs). However, existing simulators often\nrely solely on text utterances, missing implicit user traits such as\npersonality, speaking style, and goals. In contrast, persona-based methods lack\ngeneralizability, as they depend on predefined profiles of famous individuals\nor archetypes. To address these challenges, we propose User Simulator with\nimplicit Profiles (USP), a framework that infers implicit user profiles from\nhuman-machine conversations and uses them to generate more personalized and\nrealistic dialogues. We first develop an LLM-driven extractor with a\ncomprehensive profile schema. Then, we refine the simulation through\nconditional supervised fine-tuning and reinforcement learning with cycle\nconsistency, optimizing it at both the utterance and conversation levels.\nFinally, we adopt a diverse profile sampler to capture the distribution of\nreal-world user profiles. Experimental results demonstrate that USP outperforms\nstrong baselines in terms of authenticity and diversity while achieving\ncomparable performance in consistency. Furthermore, dynamic multi-turn\nevaluations based on USP strongly align with mainstream benchmarks,\ndemonstrating its effectiveness in real-world applications.\n","authors":["Kuang Wang","Xianfei Li","Shenghao Yang","Li Zhou","Feng Jiang","Haizhou Li"],"pdf_url":"https://arxiv.org/pdf/2502.18968v1.pdf","comment":"9 pages"},{"id":"http://arxiv.org/abs/2407.02936v2","updated":"2025-02-26T09:17:32Z","published":"2024-07-03T09:12:38Z","title":"GraCoRe: Benchmarking Graph Comprehension and Complex Reasoning in Large\n  Language Models","summary":"  Evaluating the graph comprehension and reasoning abilities of Large Language\nModels (LLMs) is challenging and often incomplete. Existing benchmarks focus\nprimarily on pure graph understanding, lacking a comprehensive evaluation\nacross all graph types and detailed capability definitions. This paper presents\nGraCoRe, a benchmark for systematically assessing LLMs' graph comprehension and\nreasoning. GraCoRe uses a three-tier hierarchical taxonomy to categorize and\ntest models on pure graph and heterogeneous graphs, subdividing capabilities\ninto 10 distinct areas tested through 19 tasks. Our benchmark includes 11\ndatasets with 5,140 graphs of varying complexity. We evaluate four\nclosed-source and eight open-source LLMs, conducting thorough analyses from\nboth ability and task perspectives. Key findings reveal that OpenAI o1 model\nhas amazing comprehension and reasoning capabilities, semantic enrichment\nenhances reasoning performance, node ordering impacts task success, and the\nability to process longer texts does not necessarily improve graph\ncomprehension or reasoning.GraCoRe is open-sourced at\nhttps://github.com/ZIKEYUAN/GraCoRe\n","authors":["Zike Yuan","Ming Liu","Hui Wang","Bing Qin"],"pdf_url":"https://arxiv.org/pdf/2407.02936v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14182v2","updated":"2025-02-26T09:17:27Z","published":"2024-10-18T05:21:05Z","title":"LabSafety Bench: Benchmarking LLMs on Safety Issues in Scientific Labs","summary":"  Artificial Intelligence (AI) is revolutionizing scientific research, yet its\ngrowing integration into laboratory environments presents critical safety\nchallenges. While large language models (LLMs) increasingly assist in tasks\nranging from procedural guidance to autonomous experiment orchestration, an\n\"illusion of understanding\" may lead researchers to overestimate their\nreliability. Such overreliance is especially hazardous in high-stakes\nlaboratory settings, where failures in hazard identification or risk assessment\ncan result in severe accidents. To address these concerns, we propose the\nLaboratory Safety Benchmark (LabSafety Bench), a comprehensive framework that\nevaluates LLMs and vision language models (VLMs) on their ability to identify\npotential hazards, assess risks, and predict the consequences of unsafe actions\nin lab environments. LabSafety Bench comprises 765 multiple-choice questions\naligned with US Occupational Safety and Health Administration (OSHA) protocols,\nalong with 520 realistic laboratory scenarios featuring dual evaluation tasks:\nthe Hazards Identification Test and the Consequence Identification Test, with\n4090 open-ended questions in total. Evaluations across eight proprietary\nmodels, seven open-weight LLMs, and four VLMs reveal that, despite advanced\nperformance on structured assessments, no model achieves the safety threshold\nrequired for reliable operation. None scored above 75% on the Hazards\nIdentification Test. Moreover, while proprietary models tend to excel in\nmultiple-choice evaluations, their performance in open-ended, real-world\nscenario responses is comparable to that of open-source models. These findings\nunderscore the urgent need for specialized evaluation frameworks to ensure the\nsafe and responsible deployment of AI in laboratory settings.\n","authors":["Yujun Zhou","Jingdong Yang","Yue Huang","Kehan Guo","Zoe Emory","Bikram Ghosh","Amita Bedar","Sujay Shekar","Pin-Yu Chen","Tian Gao","Werner Geyer","Nuno Moniz","Nitesh V Chawla","Xiangliang Zhang"],"pdf_url":"https://arxiv.org/pdf/2410.14182v2.pdf","comment":"71 pages"},{"id":"http://arxiv.org/abs/2408.07413v3","updated":"2025-02-26T09:13:06Z","published":"2024-08-14T09:43:32Z","title":"Knowledge in Superposition: Unveiling the Failures of Lifelong Knowledge\n  Editing for Large Language Models","summary":"  Knowledge editing aims to update outdated or incorrect knowledge in large\nlanguage models (LLMs). However, current knowledge editing methods have limited\nscalability for lifelong editing. This study explores the fundamental reason\nwhy knowledge editing fails in lifelong editing. We begin with the closed-form\nsolution derived from linear associative memory, which underpins\nstate-of-the-art knowledge editing methods. We extend the solution from single\nediting to lifelong editing, and through rigorous mathematical derivation,\nidentify an interference term in the final solution, suggesting that editing\nknowledge may impact irrelevant knowledge. Further analysis of the interference\nterm reveals a close relationship with superposition between knowledge\nrepresentations. When knowledge superposition does not exist in language\nmodels, the interference term vanishes, allowing for lossless knowledge\nediting. Experiments across numerous language models reveal that knowledge\nsuperposition is universal, exhibiting high kurtosis, zero mean, and\nheavy-tailed distributions with clear scaling laws. Ultimately, by combining\ntheory and experiments, we demonstrate that knowledge superposition is the\nfundamental reason for the failure of lifelong editing. Moreover, this is the\nfirst study to investigate knowledge editing from the perspective of\nsuperposition and provides a comprehensive observation of superposition across\nnumerous real-world language models. Code available at\nhttps://github.com/ChenhuiHu/knowledge_in_superposition.\n","authors":["Chenhui Hu","Pengfei Cao","Yubo Chen","Kang Liu","Jun Zhao"],"pdf_url":"https://arxiv.org/pdf/2408.07413v3.pdf","comment":"To be published in AAAI 2025 (Oral)"},{"id":"http://arxiv.org/abs/2502.18943v1","updated":"2025-02-26T08:47:19Z","published":"2025-02-26T08:47:19Z","title":"Towards Label-Only Membership Inference Attack against Pre-trained Large\n  Language Models","summary":"  Membership Inference Attacks (MIAs) aim to predict whether a data sample\nbelongs to the model's training set or not. Although prior research has\nextensively explored MIAs in Large Language Models (LLMs), they typically\nrequire accessing to complete output logits (\\ie, \\textit{logits-based\nattacks}), which are usually not available in practice. In this paper, we study\nthe vulnerability of pre-trained LLMs to MIAs in the \\textit{label-only\nsetting}, where the adversary can only access generated tokens (text). We first\nreveal that existing label-only MIAs have minor effects in attacking\npre-trained LLMs, although they are highly effective in inferring fine-tuning\ndatasets used for personalized LLMs. We find that their failure stems from two\nmain reasons, including better generalization and overly coarse perturbation.\nSpecifically, due to the extensive pre-training corpora and exposing each\nsample only a few times, LLMs exhibit minimal robustness differences between\nmembers and non-members. This makes token-level perturbations too coarse to\ncapture such differences.\n  To alleviate these problems, we propose \\textbf{PETAL}: a label-only\nmembership inference attack based on \\textbf{PE}r-\\textbf{T}oken\nsem\\textbf{A}ntic simi\\textbf{L}arity. Specifically, PETAL leverages\ntoken-level semantic similarity to approximate output probabilities and\nsubsequently calculate the perplexity. It finally exposes membership based on\nthe common assumption that members are `better' memorized and have smaller\nperplexity. We conduct extensive experiments on the WikiMIA benchmark and the\nmore challenging MIMIR benchmark. Empirically, our PETAL performs better than\nthe extensions of existing label-only attacks against personalized LLMs and\neven on par with other advanced logit-based attacks across all metrics on five\nprevalent open-source LLMs.\n","authors":["Yu He","Boheng Li","Liu Liu","Zhongjie Ba","Wei Dong","Yiming Li","Zhan Qin","Kui Ren","Chun Chen"],"pdf_url":"https://arxiv.org/pdf/2502.18943v1.pdf","comment":"Accepted by USENIX Security 2025"},{"id":"http://arxiv.org/abs/2502.18940v1","updated":"2025-02-26T08:43:47Z","published":"2025-02-26T08:43:47Z","title":"MathTutorBench: A Benchmark for Measuring Open-ended Pedagogical\n  Capabilities of LLM Tutors","summary":"  Evaluating the pedagogical capabilities of AI-based tutoring models is\ncritical for making guided progress in the field. Yet, we lack a reliable,\neasy-to-use, and simple-to-run evaluation that reflects the pedagogical\nabilities of models. To fill this gap, we present MathTutorBench, an\nopen-source benchmark for holistic tutoring model evaluation. MathTutorBench\ncontains a collection of datasets and metrics that broadly cover tutor\nabilities as defined by learning sciences research in dialog-based teaching. To\nscore the pedagogical quality of open-ended teacher responses, we train a\nreward model and show it can discriminate expert from novice teacher responses\nwith high accuracy. We evaluate a wide set of closed- and open-weight models on\nMathTutorBench and find that subject expertise, indicated by solving ability,\ndoes not immediately translate to good teaching. Rather, pedagogy and subject\nexpertise appear to form a trade-off that is navigated by the degree of\ntutoring specialization of the model. Furthermore, tutoring appears to become\nmore challenging in longer dialogs, where simpler questioning strategies begin\nto fail. We release the benchmark, code, and leaderboard openly to enable rapid\nbenchmarking of future models.\n","authors":["Jakub Macina","Nico Daheim","Ido Hakimi","Manu Kapur","Iryna Gurevych","Mrinmaya Sachan"],"pdf_url":"https://arxiv.org/pdf/2502.18940v1.pdf","comment":"https://eth-lre.github.io/mathtutorbench"},{"id":"http://arxiv.org/abs/2502.18935v1","updated":"2025-02-26T08:36:42Z","published":"2025-02-26T08:36:42Z","title":"JailBench: A Comprehensive Chinese Security Assessment Benchmark for\n  Large Language Models","summary":"  Large language models (LLMs) have demonstrated remarkable capabilities across\nvarious applications, highlighting the urgent need for comprehensive safety\nevaluations. In particular, the enhanced Chinese language proficiency of LLMs,\ncombined with the unique characteristics and complexity of Chinese expressions,\nhas driven the emergence of Chinese-specific benchmarks for safety assessment.\nHowever, these benchmarks generally fall short in effectively exposing LLM\nsafety vulnerabilities. To address the gap, we introduce JailBench, the first\ncomprehensive Chinese benchmark for evaluating deep-seated vulnerabilities in\nLLMs, featuring a refined hierarchical safety taxonomy tailored to the Chinese\ncontext. To improve generation efficiency, we employ a novel Automatic\nJailbreak Prompt Engineer (AJPE) framework for JailBench construction, which\nincorporates jailbreak techniques to enhance assessing effectiveness and\nleverages LLMs to automatically scale up the dataset through context-learning.\nThe proposed JailBench is extensively evaluated over 13 mainstream LLMs and\nachieves the highest attack success rate against ChatGPT compared to existing\nChinese benchmarks, underscoring its efficacy in identifying latent\nvulnerabilities in LLMs, as well as illustrating the substantial room for\nimprovement in the security and trustworthiness of LLMs within the Chinese\ncontext. Our benchmark is publicly available at\nhttps://github.com/STAIR-BUPT/JailBench.\n","authors":["Shuyi Liu","Simiao Cui","Haoran Bu","Yuming Shang","Xi Zhang"],"pdf_url":"https://arxiv.org/pdf/2502.18935v1.pdf","comment":"12 pages, 5 figures, accepted at PAKDD 2025"},{"id":"http://arxiv.org/abs/2502.18934v1","updated":"2025-02-26T08:36:20Z","published":"2025-02-26T08:36:20Z","title":"Kanana: Compute-efficient Bilingual Language Models","summary":"  We introduce Kanana, a series of bilingual language models that demonstrate\nexceeding performance in Korean and competitive performance in English. The\ncomputational cost of Kanana is significantly lower than that of\nstate-of-the-art models of similar size. The report details the techniques\nemployed during pre-training to achieve compute-efficient yet competitive\nmodels, including high quality data filtering, staged pre-training, depth\nup-scaling, and pruning and distillation. Furthermore, the report outlines the\nmethodologies utilized during the post-training of the Kanana models,\nencompassing supervised fine-tuning and preference optimization, aimed at\nenhancing their capability for seamless interaction with users. Lastly, the\nreport elaborates on plausible approaches used for language model adaptation to\nspecific scenarios, such as embedding, retrieval augmented generation, and\nfunction calling. The Kanana model series spans from 2.1B to 32.5B parameters\nwith 2.1B models (base, instruct, embedding) publicly released to promote\nresearch on Korean language models.\n","authors":[" Kanana LLM Team","Yunju Bak","Hojin Lee","Minho Ryu","Jiyeon Ham","Seungjae Jung","Daniel Wontae Nam","Taegyeong Eo","Donghun Lee","Doohae Jung","Boseop Kim","Nayeon Kim","Jaesun Park","Hyunho Kim","Hyunwoong Ko","Changmin Lee","Kyoung-Woon On","Seulye Baeg","Junrae Cho","Sunghee Jung","Jieun Kang","EungGyun Kim","Eunhwa Kim","Byeongil Ko","Daniel Lee","Minchul Lee","Miok Lee","Shinbok Lee","Gaeun Seo"],"pdf_url":"https://arxiv.org/pdf/2502.18934v1.pdf","comment":"40 pages, 15 figures"},{"id":"http://arxiv.org/abs/2412.17242v2","updated":"2025-02-26T08:13:52Z","published":"2024-12-23T03:30:34Z","title":"On the Generalization and Adaptation Ability of Machine-Generated Text\n  Detectors in Academic Writing","summary":"  The rising popularity of large language models (LLMs) has raised concerns\nabout machine-generated text (MGT), particularly in academic settings, where\nissues like plagiarism and misinformation are prevalent. As a result,\ndeveloping a highly generalizable and adaptable MGT detection system has become\nan urgent priority. Given that LLMs are most commonly misused in academic\nwriting, this work investigates the generalization and adaptation capabilities\nof MGT detectors in three key aspects specific to academic writing: First, we\nconstruct MGT-Acedemic, a large-scale dataset comprising over 336M tokens and\n749K samples. MGT-Acedemic focuses on academic writing, featuring human-written\ntexts (HWTs) and MGTs across STEM, Humanities, and Social Sciences, paired with\nan extensible code framework for efficient benchmarking. Second, we benchmark\nthe performance of various detectors for binary classification and attribution\ntasks in both in-domain and cross-domain settings. This benchmark reveals the\noften-overlooked challenges of attribution tasks. Third, we introduce a novel\nattribution task where models have to adapt to new classes over time without\n(or with very limited) access to prior training data in both few-shot and\nmany-shot scenarios. We implement eight different adapting techniques to\nimprove the performance and highlight the inherent complexity of the task. Our\nfindings provide insights into the generalization and adaptation ability of MGT\ndetectors across diverse scenarios and lay the foundation for building robust,\nadaptive detection systems. The code framework is available at\nhttps://github.com/Y-L-LIU/MGTBench-2.0.\n","authors":["Yule Liu","Zhiyuan Zhong","Yifan Liao","Zhen Sun","Jingyi Zheng","Jiaheng Wei","Qingyuan Gong","Fenghua Tong","Yang Chen","Yang Zhang","Xinlei He"],"pdf_url":"https://arxiv.org/pdf/2412.17242v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.18915v1","updated":"2025-02-26T08:07:17Z","published":"2025-02-26T08:07:17Z","title":"END: Early Noise Dropping for Efficient and Effective Context Denoising","summary":"  Large Language Models (LLMs) have demonstrated remarkable performance across\na wide range of natural language processing tasks. However, they are often\ndistracted by irrelevant or noisy context in input sequences that degrades\noutput quality. This problem affects both long- and short-context scenarios,\nsuch as retrieval-augmented generation, table question-answering, and\nin-context learning. We reveal that LLMs can implicitly identify whether input\nsequences contain useful information at early layers, prior to token\ngeneration. Leveraging this insight, we introduce Early Noise Dropping\n(\\textsc{END}), a novel approach to mitigate this issue without requiring\nfine-tuning the LLMs. \\textsc{END} segments input sequences into chunks and\nemploys a linear prober on the early layers of LLMs to differentiate between\ninformative and noisy chunks. By discarding noisy chunks early in the process,\n\\textsc{END} preserves critical information, reduces distraction, and lowers\ncomputational overhead. Extensive experiments demonstrate that \\textsc{END}\nsignificantly improves both performance and efficiency across different LLMs on\nmultiple evaluation datasets. Furthermore, by investigating LLMs' implicit\nunderstanding to the input with the prober, this work also deepens\nunderstanding of how LLMs do reasoning with contexts internally.\n","authors":["Hongye Jin","Pei Chen","Jingfeng Yang","Zhengyang Wang","Meng Jiang","Yifan Gao","Binxuan Huang","Xinyang Zhang","Zheng Li","Tianyi Liu","Huasheng Li","Bing Yin"],"pdf_url":"https://arxiv.org/pdf/2502.18915v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09933v3","updated":"2025-02-26T08:04:47Z","published":"2025-02-14T06:05:12Z","title":"MIR-Bench: Benchmarking LLM's Long-Context Intelligence via Many-Shot\n  In-Context Inductive Reasoning","summary":"  Inductive Reasoning (IR), the ability to summarize rules from examples and\napply on new ones, has long been viewed as a primal ability for general\nintelligence and widely studied by cognitive science and AI researchers. Many\nbenchmarks have been proposed to measure such ability for Large Language Models\n(LLMs); however, they focus on few-shot (usually $<$10) setting and lack\nevaluation for aggregating many pieces of information from long contexts. On\nthe other hand, the ever-growing context length of LLMs have brought forth the\nnovel paradigm of many-shot In-Context Learning (ICL), which addresses new\ntasks with hundreds to thousands of examples without expensive and inefficient\nfine-tuning. However, many-shot evaluations are mostly focused on\nclassification (a very limited aspect of IR), and popular long-context LLM\ntasks such as Needle-In-A-Haystack (NIAH) seldom require complicated\nintelligence for integrating many pieces of information. To fix the issues from\nboth worlds, we propose MIR-Bench, the first many-shot in-context inductive\nreasoning benchmark that asks LLM to induce output via input-output examples\nfrom underlying functions with diverse data format. Based on MIR-Bench, we\nstudy many novel problems for inductive reasoning and many-shot ICL, including\nrobustness against erroneous shots and the effect of Chain-of-Thought (CoT),\nand acquired insightful findings.\n","authors":["Kai Yan","Zhan Ling","Kang Liu","Yifan Yang","Ting-Han Fan","Lingfeng Shen","Zhengyin Du","Jiecao Chen"],"pdf_url":"https://arxiv.org/pdf/2502.09933v3.pdf","comment":"32 pages, 11 figures. v3 slightly adjust the author institution"},{"id":"http://arxiv.org/abs/2502.18913v1","updated":"2025-02-26T07:59:55Z","published":"2025-02-26T07:59:55Z","title":"CS-Dialogue: A 104-Hour Dataset of Spontaneous Mandarin-English\n  Code-Switching Dialogues for Speech Recognition","summary":"  Code-switching (CS), the alternation between two or more languages within a\nsingle conversation, presents significant challenges for automatic speech\nrecognition (ASR) systems. Existing Mandarin-English code-switching datasets\noften suffer from limitations in size, spontaneity, and the lack of full-length\ndialogue recordings with transcriptions, hindering the development of robust\nASR models for real-world conversational scenarios. This paper introduces\nCS-Dialogue, a novel large-scale Mandarin-English code-switching speech dataset\ncomprising 104 hours of spontaneous conversations from 200 speakers. Unlike\nprevious datasets, CS-Dialogue provides full-length dialogue recordings with\ncomplete transcriptions, capturing naturalistic code-switching patterns in\ncontinuous speech. We describe the data collection and annotation processes,\npresent detailed statistics of the dataset, and establish benchmark ASR\nperformance using state-of-the-art models. Our experiments, using Transformer,\nConformer, and Branchformer, demonstrate the challenges of code-switching ASR,\nand show that existing pre-trained models such as Whisper still have the space\nto improve. The CS-Dialogue dataset will be made freely available for all\nacademic purposes.\n","authors":["Jiaming Zhou","Yujie Guo","Shiwan Zhao","Haoqin Sun","Hui Wang","Jiabei He","Aobo Kong","Shiyao Wang","Xi Yang","Yequan Wang","Yonghua Lin","Yong Qin"],"pdf_url":"https://arxiv.org/pdf/2502.18913v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.06004v2","updated":"2025-02-26T07:47:30Z","published":"2024-04-09T04:20:27Z","title":"AiSAQ: All-in-Storage ANNS with Product Quantization for DRAM-free\n  Information Retrieval","summary":"  Graph-based approximate nearest neighbor search (ANNS) algorithms work\neffectively against large-scale vector retrieval. Among such methods, DiskANN\nachieves good recall-speed tradeoffs using both DRAM and storage. DiskANN\nadopts product quantization (PQ) to reduce memory usage, which is still\nproportional to the scale of datasets. In this paper, we propose All-in-Storage\nANNS with Product Quantization (AiSAQ), which offloads compressed vectors to\nthe SSD index. Our method achieves $\\sim$10 MB memory usage in query search\nwith billion-scale datasets without critical latency degradation. AiSAQ also\nreduces the index load time for query search preparation, which enables fast\nswitch between muitiple billion-scale indices.This method can be applied to\nretrievers of retrieval-augmented generation (RAG) and be scaled out with\nmultiple-server systems for emerging datasets. Our DiskANN-based implementation\nis available on GitHub.\n","authors":["Kento Tatsuno","Daisuke Miyashita","Taiga Ikeda","Kiyoshi Ishiyama","Kazunari Sumiyoshi","Jun Deguchi"],"pdf_url":"https://arxiv.org/pdf/2404.06004v2.pdf","comment":"6 pages, 8 figures and 5 tables"},{"id":"http://arxiv.org/abs/2411.02335v2","updated":"2025-02-26T07:39:03Z","published":"2024-11-04T17:59:04Z","title":"Sparsing Law: Towards Large Language Models with Greater Activation\n  Sparsity","summary":"  Activation sparsity denotes the existence of substantial weakly-contributed\nelements within activation outputs that can be eliminated, benefiting many\nimportant applications concerned with large language models (LLMs). Although\npromoting greater activation sparsity within LLMs deserves deep studies,\nexisting works lack comprehensive and quantitative research on the correlation\nbetween activation sparsity and potentially influential factors. In this paper,\nwe present a comprehensive study on the quantitative scaling properties and\ninfluential factors of the activation sparsity within decoder-only\nTransformer-based LLMs. Specifically, we propose PPL-$p\\%$ sparsity, a precise\nand performance-aware activation sparsity metric that is applicable to any\nactivation function. Through extensive experiments, we find several important\nphenomena. Firstly, different activation functions exhibit comparable\nperformance but opposite training-time sparsity trends. The activation ratio\n(i.e., $1-\\mathrm{sparsity\\ ratio}$) evolves as a convergent increasing\npower-law and decreasing logspace power-law with the amount of training data\nfor SiLU-activated and ReLU-activated LLMs, respectively. These demonstrate\nthat ReLU is more efficient as the activation function than SiLU and can\nleverage more training data to improve activation sparsity. Secondly, the\nactivation ratio linearly increases with the width-depth ratio below a certain\nbottleneck point, indicating the potential advantage of a deeper architecture\nat a fixed parameter scale. Finally, at similar width-depth ratios, we\nsurprisingly find that the limit value of activation sparsity varies weakly\nwith the parameter scale, i.e., the activation patterns within LLMs are\ninsensitive to the parameter scale. These empirical laws towards LLMs with\ngreater activation sparsity have important implications for making LLMs more\nefficient and interpretable.\n","authors":["Yuqi Luo","Chenyang Song","Xu Han","Yingfa Chen","Chaojun Xiao","Zhiyuan Liu","Maosong Sun"],"pdf_url":"https://arxiv.org/pdf/2411.02335v2.pdf","comment":"23 pages, 13 figures, 6 tables"},{"id":"http://arxiv.org/abs/2502.15487v2","updated":"2025-02-26T07:15:45Z","published":"2025-02-21T14:23:14Z","title":"ExpliCa: Evaluating Explicit Causal Reasoning in Large Language Models","summary":"  Large Language Models (LLMs) are increasingly used in tasks requiring\ninterpretive and inferential accuracy. In this paper, we introduce ExpliCa, a\nnew dataset for evaluating LLMs in explicit causal reasoning. ExpliCa uniquely\nintegrates both causal and temporal relations presented in different linguistic\norders and explicitly expressed by linguistic connectives. The dataset is\nenriched with crowdsourced human acceptability ratings. We tested LLMs on\nExpliCa through prompting and perplexity-based metrics. We assessed seven\ncommercial and open-source LLMs, revealing that even top models struggle to\nreach 0.80 accuracy. Interestingly, models tend to confound temporal relations\nwith causal ones, and their performance is also strongly influenced by the\nlinguistic order of the events. Finally, perplexity-based scores and prompting\nperformance are differently affected by model size.\n","authors":["Martina Miliani","Serena Auriemma","Alessandro Bondielli","Emmanuele Chersoni","Lucia Passaro","Irene Sucameli","Alessandro Lenci"],"pdf_url":"https://arxiv.org/pdf/2502.15487v2.pdf","comment":"Submitted to ACL 2025"},{"id":"http://arxiv.org/abs/2501.10487v2","updated":"2025-02-26T07:10:17Z","published":"2025-01-17T08:42:49Z","title":"Theme-Explanation Structure for Table Summarization using Large Language\n  Models: A Case Study on Korean Tabular Data","summary":"  This paper proposes the Theme-Explanation Structure-based Table Summarization\n(Tabular-TX) pipeline designed to process tabular data efficiently. Tabular-TX\npreprocesses tabular data by focusing on highlighted cells. It then generates\nsummary sentences following a structured format, where the Theme Part appears\nas an adverbial phrase, and the Explanation Part follows as a predictive\nclause. This approach enables tailored analysis by considering the structural\ncharacteristics of tables and their comparability. Unlike conventional\nfine-tuning approaches that require extensive labeled data and computational\nresources, our method leverages In-Context Learning to dynamically adapt to\ndifferent table structures without additional training, ensuring efficient and\nscalable table interpretation. Experimental results demonstrate that Tabular-TX\nsignificantly outperforms conventional fine-tuning-based methods, particularly\nin low-resource scenarios, by leveraging table structures and metadata more\neffectively through structured prompts. The results confirm that Tabular-TX\nenables more effective processing of complex tabular data. Furthermore, it\nserves as a viable alternative for table-based question answering and\nsummarization tasks in resource-constrained environments.\n","authors":["TaeYoon Kwack","Jisoo Kim","Ki Yong Jung","DongGeon Lee","Heesun Park"],"pdf_url":"https://arxiv.org/pdf/2501.10487v2.pdf","comment":"6 pages"},{"id":"http://arxiv.org/abs/2502.18890v1","updated":"2025-02-26T07:10:08Z","published":"2025-02-26T07:10:08Z","title":"From Hours to Minutes: Lossless Acceleration of Ultra Long Sequence\n  Generation up to 100K Tokens","summary":"  Generating ultra-long sequences with large language models (LLMs) has become\nincreasingly crucial but remains a highly time-intensive task, particularly for\nsequences up to 100K tokens. While traditional speculative decoding methods\nexist, simply extending their generation limits fails to accelerate the process\nand can be detrimental. Through an in-depth analysis, we identify three major\nchallenges hindering efficient generation: frequent model reloading, dynamic\nkey-value (KV) management and repetitive generation. To address these issues,\nwe introduce TOKENSWIFT, a novel framework designed to substantially accelerate\nthe generation process of ultra-long sequences while maintaining the target\nmodel's inherent quality. Experimental results demonstrate that TOKENSWIFT\nachieves over 3 times speedup across models of varying scales (1.5B, 7B, 8B,\n14B) and architectures (MHA, GQA). This acceleration translates to hours of\ntime savings for ultra-long sequence generation, establishing TOKENSWIFT as a\nscalable and effective solution at unprecedented lengths. Code can be found at\nhttps://github.com/bigai-nlco/TokenSwift.\n","authors":["Tong Wu","Junzhe Shen","Zixia Jia","Yuxuan Wang","Zilong Zheng"],"pdf_url":"https://arxiv.org/pdf/2502.18890v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.18889v1","updated":"2025-02-26T07:09:33Z","published":"2025-02-26T07:09:33Z","title":"Clip-TTS: Contrastive Text-content and Mel-spectrogram, A High-Huality\n  Text-to-Speech Method based on Contextual Semantic Understanding","summary":"  Traditional text-to-speech (TTS) methods primarily focus on establishing a\nmapping between phonemes and mel-spectrograms. However, during the phoneme\nencoding stage, there is often a lack of real mel-spectrogram auxiliary\ninformation, which results in the encoding process lacking true semantic\nunderstanding. At the same time, traditional TTS systems often struggle to\nbalance the inference speed of the model with the quality of the synthesized\nspeech. Methods that generate high-quality synthesized speech tend to have\nslower inference speeds, while faster inference methods often sacrifice speech\nquality. In this paper, I propose Clip-TTS, a TTS method based on the Clip\narchitecture. This method uses the Clip framework to establish a connection\nbetween text content and real mel-spectrograms during the text encoding stage,\nenabling the text encoder to directly learn the true semantics of the global\ncontext, thereby ensuring the quality of the synthesized speech. In terms of\nmodel architecture, I adopt the basic structure of Transformer, which allows\nClip-TTS to achieve fast inference speeds. Experimental results show that on\nthe LJSpeech and Baker datasets, the speech generated by Clip-TTS achieves\nstate-of-the-art MOS scores, and it also performs excellently on multi-emotion\ndatasets.Audio samples are available at: https://ltydd1314.github.io/.\n","authors":["Tianyun Liu"],"pdf_url":"https://arxiv.org/pdf/2502.18889v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.18886v1","updated":"2025-02-26T07:04:20Z","published":"2025-02-26T07:04:20Z","title":"On Pruning State-Space LLMs","summary":"  Recent work proposed state-space models (SSMs) as an efficient alternative to\ntransformer-based LLMs. Can these models be pruned to further reduce their\ncomputation costs? We adapt several pruning methods to the SSM structure, and\napply them to four SSM-based LLMs across multiple tasks. We find that such\nmodels are quite robust to some pruning methods (e.g. WANDA), while using other\nmethods lead to fast performance degradation.\n","authors":["Tamer Ghattas","Michael Hassid","Roy Schwartz"],"pdf_url":"https://arxiv.org/pdf/2502.18886v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.06638v2","updated":"2025-02-26T06:53:40Z","published":"2024-10-09T07:43:38Z","title":"Subtle Errors Matter: Preference Learning via Error-injected\n  Self-editing","summary":"  Large Language Models (LLMs) have exhibited strong mathematical reasoning\nprowess, tackling tasks ranging from basic arithmetic to advanced\ncompetition-level problems. However, frequently occurring subtle yet critical\nerrors, such as miscalculations or incorrect substitutions, limit the LLMs'\nfull potential. Existing studies to improve mathematical ability typically\ninvolve applying preference learning to step-wise solution pairs. Although\nthese methods leverage samples of varying granularity to mitigate reasoning\nerrors, they overlook critical subtle errors. In this work, we propose a novel\npreference learning framework called eRror-Injected Self-Editing (RISE), which\ninjects predefined subtle errors into pivotal tokens in reasoning or\ncomputation steps to construct hard pairs for error mitigation. In detail, RISE\nuses the LLM itself to edit a small number of tokens in the solution, injecting\ndesigned subtle errors. Then, pairs composed of self-edited solutions and their\ncorresponding correct ones, along with pairs of correct and incorrect solutions\nobtained through sampling, are used together for subtle error-aware DPO\ntraining. Compared with other preference learning methods, RISE further refines\nthe training objective without requiring fine-grained sampling or preference\nannotation. Extensive experiments validate the effectiveness of RISE, with\npreference learning on Qwen2-7B-Instruct yielding notable improvements of 3.0%\non GSM8K and 7.9% on MATH with only 4.5K training samples. Moreover, the effect\nof error mitigation extends from mathematical reasoning to logical reasoning\nand code generation.\n","authors":["Kaishuai Xu","Tiezheng Yu","Wenjun Hou","Yi Cheng","Chak Tou Leong","Liangyou Li","Xin Jiang","Lifeng Shang","Qun Liu","Wenjie Li"],"pdf_url":"https://arxiv.org/pdf/2410.06638v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.18116v2","updated":"2025-02-26T06:53:39Z","published":"2025-02-25T11:41:33Z","title":"Bayesian Optimization for Controlled Image Editing via LLMs","summary":"  In the rapidly evolving field of image generation, achieving precise control\nover generated content and maintaining semantic consistency remain significant\nlimitations, particularly concerning grounding techniques and the necessity for\nmodel fine-tuning. To address these challenges, we propose BayesGenie, an\noff-the-shelf approach that integrates Large Language Models (LLMs) with\nBayesian Optimization to facilitate precise and user-friendly image editing.\nOur method enables users to modify images through natural language descriptions\nwithout manual area marking, while preserving the original image's semantic\nintegrity. Unlike existing techniques that require extensive pre-training or\nfine-tuning, our approach demonstrates remarkable adaptability across various\nLLMs through its model-agnostic design. BayesGenie employs an adapted Bayesian\noptimization strategy to automatically refine the inference process parameters,\nachieving high-precision image editing with minimal user intervention. Through\nextensive experiments across diverse scenarios, we demonstrate that our\nframework significantly outperforms existing methods in both editing accuracy\nand semantic preservation, as validated using different LLMs including Claude3\nand GPT-4.\n","authors":["Chengkun Cai","Haoliang Liu","Xu Zhao","Zhongyu Jiang","Tianfang Zhang","Zongkai Wu","Jenq-Neng Hwang","Serge Belongie","Lei Li"],"pdf_url":"https://arxiv.org/pdf/2502.18116v2.pdf","comment":"8 figures"},{"id":"http://arxiv.org/abs/2502.18878v1","updated":"2025-02-26T06:45:29Z","published":"2025-02-26T06:45:29Z","title":"Learning to Generate Structured Output with Schema Reinforcement\n  Learning","summary":"  This study investigates the structured generation capabilities of large\nlanguage models (LLMs), focusing on producing valid JSON outputs against a\ngiven schema. Despite the widespread use of JSON in integrating language models\nwith programs, there is a lack of comprehensive analysis and benchmarking of\nthese capabilities. We explore various aspects of JSON generation, such as\nstructure understanding, escaping, and natural language description, to\ndetermine how to assess and enable LLMs to generate valid responses. Building\nupon this, we propose SchemaBench features around 40K different JSON schemas to\nobtain and assess models' abilities in generating valid JSON. We find that the\nlatest LLMs are still struggling to generate a valid JSON string. Moreover, we\ndemonstrate that incorporating reinforcement learning with a Fine-grained\nSchema Validator can further enhance models' understanding of JSON schema,\nleading to improved performance. Our models demonstrate significant improvement\nin both generating JSON outputs and downstream tasks.\n","authors":["Yaxi Lu","Haolun Li","Xin Cong","Zhong Zhang","Yesai Wu","Yankai Lin","Zhiyuan Liu","Fangming Liu","Maosong Sun"],"pdf_url":"https://arxiv.org/pdf/2502.18878v1.pdf","comment":"8 pages, 4 figures"},{"id":"http://arxiv.org/abs/2410.09343v2","updated":"2025-02-26T06:41:43Z","published":"2024-10-12T03:19:06Z","title":"ELICIT: LLM Augmentation via External In-Context Capability","summary":"  Enhancing the adaptive capabilities of large language models is a critical\npursuit in both research and application. Traditional fine-tuning methods\nrequire substantial data and computational resources, especially for enhancing\nspecific capabilities, while in-context learning is limited by the need for\nappropriate demonstrations and efficient token usage. Inspired by the\nexpression of in-context learned capabilities through task vectors and the\nconcept of modularization, we propose \\alg, a framework consisting of two\nmodules designed to effectively store and reuse task vectors to elicit the\ndiverse capabilities of models without additional training or inference tokens.\nOur comprehensive experiments and analysis demonstrate that our pipeline is\nhighly transferable across different input formats, tasks, and model\narchitectures. ELICIT serves as a plug-and-play performance booster to enable\nadaptive elicitation of model capabilities. By externally storing and reusing\nvectors that represent in-context learned capabilities, \\alg not only\ndemonstrates the potential to operate modular capabilities but also\nsignificantly enhances the performance, versatility, adaptability, and\nscalability of large language models. Our code will be publicly available at\nhttps://github.com/LINs-lab/ELICIT.\n","authors":["Futing Wang","Jianhao Yan","Yue Zhang","Tao Lin"],"pdf_url":"https://arxiv.org/pdf/2410.09343v2.pdf","comment":"Published at ICLR 2025"},{"id":"http://arxiv.org/abs/2411.07130v2","updated":"2025-02-26T06:40:58Z","published":"2024-11-11T17:00:59Z","title":"On Many-Shot In-Context Learning for Long-Context Evaluation","summary":"  Many-shot in-context learning (ICL) has emerged as a unique setup to both\nutilize and test the ability of large language models to handle long context.\nThis paper delves into long-context language model (LCLM) evaluation through\nmany-shot ICL. We first ask: what types of ICL tasks benefit from additional\ndemonstrations, and how effective are they in evaluating LCLMs? We find that\nclassification and summarization tasks show performance improvements with\nadditional demonstrations, while translation and reasoning tasks do not exhibit\nclear trends. Next, we investigate the extent to which different tasks\nnecessitate retrieval versus global context understanding. We develop metrics\nto categorize ICL tasks into two groups: (i) similar-sample learning (SSL):\ntasks where retrieval of the most similar examples is sufficient for good\nperformance, and (ii) all-sample learning (ASL): tasks that necessitate a\ndeeper comprehension of all examples in the prompt. Lastly, we introduce a new\nmany-shot ICL benchmark, MANYICLBENCH, to characterize model's ability on both\nfronts and benchmark 12 LCLMs using MANYICLBENCH. We find that while\nstate-of-the-art models demonstrate good performance up to 64k tokens in SSL\ntasks, many models experience significant performance drops at only 16k tokens\nin ASL tasks.\n","authors":["Kaijian Zou","Muhammad Khalifa","Lu Wang"],"pdf_url":"https://arxiv.org/pdf/2411.07130v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.10762v3","updated":"2025-02-26T06:38:03Z","published":"2024-10-14T17:40:40Z","title":"AFlow: Automating Agentic Workflow Generation","summary":"  Large language models (LLMs) have demonstrated remarkable potential in\nsolving complex tasks across diverse domains, typically by employing agentic\nworkflows that follow detailed instructions and operational sequences. However,\nconstructing these workflows requires significant human effort, limiting\nscalability and generalizability. Recent research has sought to automate the\ngeneration and optimization of these workflows, but existing methods still rely\non initial manual setup and fall short of achieving fully automated and\neffective workflow generation. To address this challenge, we reformulate\nworkflow optimization as a search problem over code-represented workflows,\nwhere LLM-invoking nodes are connected by edges. We introduce AFlow, an\nautomated framework that efficiently explores this space using Monte Carlo Tree\nSearch, iteratively refining workflows through code modification,\ntree-structured experience, and execution feedback. Empirical evaluations\nacross six benchmark datasets demonstrate AFlow's efficacy, yielding a 5.7%\naverage improvement over state-of-the-art baselines. Furthermore, AFlow enables\nsmaller models to outperform GPT-4o on specific tasks at 4.55% of its inference\ncost in dollars. The code is available at https://github.com/geekan/MetaGPT.\n","authors":["Jiayi Zhang","Jinyu Xiang","Zhaoyang Yu","Fengwei Teng","Xionghui Chen","Jiaqi Chen","Mingchen Zhuge","Xin Cheng","Sirui Hong","Jinlin Wang","Bingnan Zheng","Bang Liu","Yuyu Luo","Chenglin Wu"],"pdf_url":"https://arxiv.org/pdf/2410.10762v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.15790v3","updated":"2025-02-26T06:34:55Z","published":"2024-09-24T06:36:56Z","title":"Small Language Models: Survey, Measurements, and Insights","summary":"  Small language models (SLMs), despite their widespread adoption in modern\nsmart devices, have received significantly less academic attention compared to\ntheir large language model (LLM) counterparts, which are predominantly deployed\nin data centers and cloud environments. While researchers continue to improve\nthe capabilities of LLMs in the pursuit of artificial general intelligence, SLM\nresearch aims to make machine intelligence more accessible, affordable, and\nefficient for everyday tasks. Focusing on transformer-based, decoder-only\nlanguage models with 100M-5B parameters, we survey 70 state-of-the-art\nopen-source SLMs, analyzing their technical innovations across three axes:\narchitectures, training datasets, and training algorithms. In addition, we\nevaluate their capabilities in various domains, including commonsense\nreasoning, mathematics, in-context learning, and long context. To gain further\ninsight into their on-device runtime costs, we benchmark their inference\nlatency and memory footprints. Through in-depth analysis of our benchmarking\ndata, we offer valuable insights to advance research in this field.\n","authors":["Zhenyan Lu","Xiang Li","Dongqi Cai","Rongjie Yi","Fangming Liu","Xiwen Zhang","Nicholas D. Lane","Mengwei Xu"],"pdf_url":"https://arxiv.org/pdf/2409.15790v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.09008v3","updated":"2025-02-26T06:32:31Z","published":"2024-10-11T17:25:52Z","title":"SuperCorrect: Advancing Small LLM Reasoning with Thought Template\n  Distillation and Self-Correction","summary":"  Large language models (LLMs) like GPT-4, DeepSeek-R1, and ReasonFlux have\nshown significant improvements in various reasoning tasks. However, smaller\nLLMs still struggle with complex mathematical reasoning because they fail to\neffectively identify and correct reasoning errors. Recent reflection-based\nmethods aim to address these issues by enabling self-reflection and\nself-correction, but they still face challenges in independently detecting\nerrors in their reasoning steps. To overcome these limitations, we propose\nSuperCorrect, a novel two-stage framework that uses a large teacher model to\nsupervise and correct both the reasoning and reflection processes of a smaller\nstudent model. In the first stage, we extract hierarchical high-level and\ndetailed thought templates from the teacher model to guide the student model in\neliciting more fine-grained reasoning thoughts. In the second stage, we\nintroduce cross-model collaborative direct preference optimization (DPO) to\nenhance the self-correction abilities of the student model by following the\nteacher's correction traces during training. This cross-model DPO approach\nteaches the student model to effectively locate and resolve erroneous thoughts\nwith error-driven insights from the teacher model, breaking the bottleneck of\nits thoughts and acquiring new skills and knowledge to tackle challenging\nproblems. Extensive experiments consistently demonstrate our superiority over\nprevious methods. Notably, our SuperCorrect-7B model significantly surpasses\npowerful DeepSeekMath-7B by 7.8%/5.3% and Qwen2.5-Math-7B by 15.1%/6.3% on\nMATH/GSM8K benchmarks, achieving new SOTA performance among all 7B models.\nCode: https://github.com/YangLing0818/SuperCorrect-llm\n","authors":["Ling Yang","Zhaochen Yu","Tianjun Zhang","Minkai Xu","Joseph E. Gonzalez","Bin Cui","Shuicheng Yan"],"pdf_url":"https://arxiv.org/pdf/2410.09008v3.pdf","comment":"ICLR 2025. Project: https://github.com/YangLing0818/SuperCorrect-llm"},{"id":"http://arxiv.org/abs/2502.18874v1","updated":"2025-02-26T06:31:45Z","published":"2025-02-26T06:31:45Z","title":"Learning to Align Multi-Faceted Evaluation: A Unified and Robust\n  Framework","summary":"  Large Language Models (LLMs) are being used more and more extensively for\nautomated evaluation in various scenarios. Previous studies have attempted to\nfine-tune open-source LLMs to replicate the evaluation explanations and\njudgments of powerful proprietary models, such as GPT-4. However, these methods\nare largely limited to text-based analyses under predefined general criteria,\nresulting in reduced adaptability for unseen instructions and demonstrating\ninstability in evaluating adherence to quantitative and structural constraints.\nTo address these limitations, we propose a novel evaluation framework, ARJudge,\nthat adaptively formulates evaluation criteria and synthesizes both text-based\nand code-driven analyses to evaluate LLM responses. ARJudge consists of two\ncomponents: a fine-tuned Analyzer that generates multi-faceted evaluation\nanalyses and a tuning-free Refiner that combines and refines all analyses to\nmake the final judgment. We construct a Composite Analysis Corpus that\nintegrates tasks for evaluation criteria generation alongside text-based and\ncode-driven analysis generation to train the Analyzer. Our results demonstrate\nthat ARJudge outperforms existing fine-tuned evaluators in effectiveness and\nrobustness. Furthermore, it demonstrates the importance of multi-faceted\nevaluation and code-driven analyses in enhancing evaluation capabilities.\n","authors":["Kaishuai Xu","Tiezheng Yu","Wenjun Hou","Yi Cheng","Liangyou Li","Xin Jiang","Lifeng Shang","Qun Liu","Wenjie Li"],"pdf_url":"https://arxiv.org/pdf/2502.18874v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.18873v1","updated":"2025-02-26T06:31:04Z","published":"2025-02-26T06:31:04Z","title":"Multi-LLM Collaborative Search for Complex Problem Solving","summary":"  Large language models (LLMs) often struggle with complex reasoning tasks due\nto their limitations in addressing the vast reasoning space and inherent\nambiguities of natural language. We propose the Mixture-of-Search-Agents (MoSA)\nparadigm, a novel approach leveraging the collective expertise of multiple LLMs\nto enhance search-based reasoning. MoSA integrates diverse reasoning pathways\nby combining independent exploration with iterative refinement among LLMs,\nmitigating the limitations of single-model approaches. Using Monte Carlo Tree\nSearch (MCTS) as a backbone, MoSA enables multiple agents to propose and\naggregate reasoning steps, resulting in improved accuracy. Our comprehensive\nevaluation across four reasoning benchmarks demonstrates MoSA's consistent\nperformance improvements over single-agent and other multi-agent baselines,\nparticularly in complex mathematical and commonsense reasoning tasks.\n","authors":["Sen Yang","Yafu Li","Wai Lam","Yu Cheng"],"pdf_url":"https://arxiv.org/pdf/2502.18873v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.18864v1","updated":"2025-02-26T06:17:13Z","published":"2025-02-26T06:17:13Z","title":"Towards an AI co-scientist","summary":"  Scientific discovery relies on scientists generating novel hypotheses that\nundergo rigorous experimental validation. To augment this process, we introduce\nan AI co-scientist, a multi-agent system built on Gemini 2.0. The AI\nco-scientist is intended to help uncover new, original knowledge and to\nformulate demonstrably novel research hypotheses and proposals, building upon\nprior evidence and aligned to scientist-provided research objectives and\nguidance. The system's design incorporates a generate, debate, and evolve\napproach to hypothesis generation, inspired by the scientific method and\naccelerated by scaling test-time compute. Key contributions include: (1) a\nmulti-agent architecture with an asynchronous task execution framework for\nflexible compute scaling; (2) a tournament evolution process for self-improving\nhypotheses generation. Automated evaluations show continued benefits of\ntest-time compute, improving hypothesis quality. While general purpose, we\nfocus development and validation in three biomedical areas: drug repurposing,\nnovel target discovery, and explaining mechanisms of bacterial evolution and\nanti-microbial resistance. For drug repurposing, the system proposes candidates\nwith promising validation findings, including candidates for acute myeloid\nleukemia that show tumor inhibition in vitro at clinically applicable\nconcentrations. For novel target discovery, the AI co-scientist proposed new\nepigenetic targets for liver fibrosis, validated by anti-fibrotic activity and\nliver cell regeneration in human hepatic organoids. Finally, the AI\nco-scientist recapitulated unpublished experimental results via a parallel in\nsilico discovery of a novel gene transfer mechanism in bacterial evolution.\nThese results, detailed in separate, co-timed reports, demonstrate the\npotential to augment biomedical and scientific discovery and usher an era of AI\nempowered scientists.\n","authors":["Juraj Gottweis","Wei-Hung Weng","Alexander Daryin","Tao Tu","Anil Palepu","Petar Sirkovic","Artiom Myaskovsky","Felix Weissenberger","Keran Rong","Ryutaro Tanno","Khaled Saab","Dan Popovici","Jacob Blum","Fan Zhang","Katherine Chou","Avinatan Hassidim","Burak Gokturk","Amin Vahdat","Pushmeet Kohli","Yossi Matias","Andrew Carroll","Kavita Kulkarni","Nenad Tomasev","Yuan Guan","Vikram Dhillon","Eeshit Dhaval Vaishnav","Byron Lee","Tiago R D Costa","José R Penadés","Gary Peltz","Yunhan Xu","Annalisa Pawlosky","Alan Karthikesalingam","Vivek Natarajan"],"pdf_url":"https://arxiv.org/pdf/2502.18864v1.pdf","comment":"81 pages in total (main 38 pages, appendix 43 pages), 13 main\n  figures, 40 appendix figures, 1 main table, 2 appendix tables, 143 main\n  references, 7 appendix references"},{"id":"http://arxiv.org/abs/2402.05864v2","updated":"2025-02-26T06:09:49Z","published":"2024-02-08T17:54:23Z","title":"Permute-and-Flip: An optimally stable and watermarkable decoder for LLMs","summary":"  In this paper, we propose a new decoding method called Permute-and-Flip (PF)\ndecoder. It enjoys stability properties similar to the standard sampling\ndecoder, but is provably up to 2x better in its quality-stability tradeoff than\nsampling and never worse than any other decoder. We also design a cryptographic\nwatermarking scheme analogous to Aaronson (2023)'s Gumbel watermark, but\nnaturally tailored for PF decoder. The watermarking scheme does not change the\ndistribution to sample, while allowing arbitrarily low false positive rate and\nhigh recall whenever the generated text has high entropy. Our experiments show\nthat the PF decoder (and its watermarked counterpart) significantly\noutperform(s) naive sampling (and its Gumbel watermarked counterpart) in terms\nof perplexity, while retaining the same stability (and detectability), hence\nmaking it a promising new approach for LLM decoding. The code is available at\nhttps://github.com/XuandongZhao/pf-decoding\n","authors":["Xuandong Zhao","Lei Li","Yu-Xiang Wang"],"pdf_url":"https://arxiv.org/pdf/2402.05864v2.pdf","comment":"ICLR 2025"},{"id":"http://arxiv.org/abs/2502.18860v1","updated":"2025-02-26T06:05:29Z","published":"2025-02-26T06:05:29Z","title":"Exploring Rewriting Approaches for Different Conversational Tasks","summary":"  Conversational assistants often require a question rewriting algorithm that\nleverages a subset of past interactions to provide a more meaningful (accurate)\nanswer to the user's question or request. However, the exact rewriting approach\nmay often depend on the use case and application-specific tasks supported by\nthe conversational assistant, among other constraints. In this paper, we\nsystematically investigate two different approaches, denoted as rewriting and\nfusion, on two fundamentally different generation tasks, including a\ntext-to-text generation task and a multimodal generative task that takes as\ninput text and generates a visualization or data table that answers the user's\nquestion. Our results indicate that the specific rewriting or fusion approach\nhighly depends on the underlying use case and generative task. In particular,\nwe find that for a conversational question-answering assistant, the query\nrewriting approach performs best, whereas for a data analysis assistant that\ngenerates visualizations and data tables based on the user's conversation with\nthe assistant, the fusion approach works best. Notably, we explore two datasets\nfor the data analysis assistant use case, for short and long conversations, and\nwe find that query fusion always performs better, whereas for the\nconversational text-based question-answering, the query rewrite approach\nperforms best.\n","authors":["Md Mehrab Tanjim","Ryan A. Rossi","Mike Rimer","Xiang Chen","Sungchul Kim","Vaishnavi Muppala","Tong Yu","Zhengmian Hu","Ritwik Sinha","Wei Zhang","Iftikhar Ahamath Burhanuddin","Franck Dernoncourt"],"pdf_url":"https://arxiv.org/pdf/2502.18860v1.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2502.16949v2","updated":"2025-02-26T06:00:02Z","published":"2025-02-24T08:21:48Z","title":"SparseTransX: Efficient Training of Translation-Based Knowledge Graph\n  Embeddings Using Sparse Matrix Operations","summary":"  Knowledge graph (KG) learning offers a powerful framework for generating new\nknowledge and making inferences. Training KG embedding can take a significantly\nlong time, especially for larger datasets. Our analysis shows that the gradient\ncomputation of embedding is one of the dominant functions in the\ntranslation-based KG embedding training loop. We address this issue by\nreplacing the core embedding computation with SpMM (Sparse-Dense Matrix\nMultiplication) kernels. This allows us to unify multiple scatter (and gather)\noperations as a single operation, reducing training time and memory usage. We\ncreate a general framework for training KG models using sparse kernels and\nimplement four models, namely TransE, TransR, TransH, and TorusE. Our sparse\nimplementations exhibit up to 5.3x speedup on the CPU and up to 4.2x speedup on\nthe GPU with a significantly low GPU memory footprint. The speedups are\nconsistent across large and small datasets for a given model. Our proposed\nsparse approach can be extended to accelerate other translation-based (such as\nTransC, TransM, etc.) and non-translational (such as DistMult, ComplEx, RotatE,\netc.) models as well.\n","authors":["Md Saidul Hoque Anik","Ariful Azad"],"pdf_url":"https://arxiv.org/pdf/2502.16949v2.pdf","comment":"15 pages. To appear in MLSys 2025"},{"id":"http://arxiv.org/abs/2311.11482v7","updated":"2025-02-26T05:39:39Z","published":"2023-11-20T01:51:13Z","title":"Meta Prompting for AI Systems","summary":"  We introduce Meta Prompting (MP), a prompting paradigm designed to enhance\nthe utilization of large language models (LLMs) and AI systems in complex\nproblem-solving and data interaction. Grounded in type theory and category\ntheory, Meta Prompting prioritizes structural and syntactical considerations\nover traditional content-centric methods. In this work, we formally define Meta\nPrompting, delineate its distinctions from few-shot prompting, and demonstrate\nits effectiveness across various AI applications. In particular, we show that\nMeta Prompting can decompose intricate reasoning tasks into simpler\nsub-problems, thereby improving token efficiency and enabling fairer\ncomparisons with conventional few-shot techniques. Furthermore, we extend this\nframework to prompting tasks, allowing LLMs to recursively self-generate\nrefined prompts in a metaprogramming-like manner. Empirical evaluations reveal\nthat a Qwen-72B base language model equipped with Meta Prompting-without\nadditional instruction tuning-achieves a PASS@1 accuracy of 46.3% on MATH\nproblems, surpassing a supervised fine-tuned counterpart, 83.5% accuracy on\nGSM8K, and a 100% success rate on Game of 24 tasks using GPT-4. The code is\navailable at https://github.com/meta-prompting/meta-prompting.\n","authors":["Yifan Zhang","Yang Yuan","Andrew Chi-Chih Yao"],"pdf_url":"https://arxiv.org/pdf/2311.11482v7.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.18848v1","updated":"2025-02-26T05:35:53Z","published":"2025-02-26T05:35:53Z","title":"A Causal Lens for Evaluating Faithfulness Metrics","summary":"  Large Language Models (LLMs) offer natural language explanations as an\nalternative to feature attribution methods for model interpretability. However,\ndespite their plausibility, they may not reflect the model's internal reasoning\nfaithfully, which is crucial for understanding the model's true decision-making\nprocesses. Although several faithfulness metrics have been proposed, a unified\nevaluation framework remains absent. To address this gap, we present Causal\nDiagnosticity, a framework to evaluate faithfulness metrics for natural\nlanguage explanations. Our framework employs the concept of causal\ndiagnosticity, and uses model-editing methods to generate faithful-unfaithful\nexplanation pairs. Our benchmark includes four tasks: fact-checking, analogy,\nobject counting, and multi-hop reasoning. We evaluate a variety of faithfulness\nmetrics, including post-hoc explanation and chain-of-thought-based methods. We\nfind that all tested faithfulness metrics often fail to surpass a random\nbaseline. Our work underscores the need for improved metrics and more reliable\ninterpretability methods in LLMs.\n","authors":["Kerem Zaman","Shashank Srivastava"],"pdf_url":"https://arxiv.org/pdf/2502.18848v1.pdf","comment":"18 pages, 18 figures, 6 tables"},{"id":"http://arxiv.org/abs/2502.18845v1","updated":"2025-02-26T05:31:44Z","published":"2025-02-26T05:31:44Z","title":"Sliding Window Attention Training for Efficient Large Language Models","summary":"  Recent advances in transformer-based Large Language Models (LLMs) have\ndemonstrated remarkable capabilities across various tasks. However, their\nquadratic computational complexity concerning sequence length remains a\nsignificant bottleneck for processing long documents. As a result, many efforts\nlike sparse attention and state space models have been proposed to improve the\nefficiency of LLMs over long sequences. Though effective, these approaches\ncompromise the performance or introduce structural complexity. This calls for a\nsimple yet efficient model that preserves the fundamental Transformer\narchitecture. To this end, we introduce SWAT, which enables efficient\nlong-context handling via Sliding Window Attention Training. This paper first\nattributes the inefficiency of Transformers to the attention sink phenomenon\nresulting from the high variance of softmax operation. Then, we replace softmax\nwith the sigmoid function and utilize a balanced ALiBi and Rotary Position\nEmbedding for efficient information compression and retention. Experiments\ndemonstrate that SWAT achieves SOTA performance compared with state-of-the-art\nlinear recurrent architectures on eight benchmarks. Code is available at\nhttps://anonymous.4open.science/r/SWAT-attention.\n","authors":["Zichuan Fu","Wentao Song","Yejing Wang","Xian Wu","Yefeng Zheng","Yingying Zhang","Derong Xu","Xuetao Wei","Tong Xu","Xiangyu Zhao"],"pdf_url":"https://arxiv.org/pdf/2502.18845v1.pdf","comment":"14 pages, 5 figures"},{"id":"http://arxiv.org/abs/2410.05165v3","updated":"2025-02-26T05:30:36Z","published":"2024-10-07T16:23:36Z","title":"Efficient Inference for Large Language Model-based Generative\n  Recommendation","summary":"  Large Language Model (LLM)-based generative recommendation has achieved\nnotable success, yet its practical deployment is costly particularly due to\nexcessive inference latency caused by autoregressive decoding. For lossless LLM\ndecoding acceleration, Speculative Decoding (SD) has emerged as a promising\nsolution. However, applying SD to generative recommendation presents unique\nchallenges due to the requirement of generating top-K items (i.e., K distinct\ntoken sequences) as a recommendation list by beam search. This leads to more\nstringent verification in SD, where all the top-K sequences from the target LLM\nmust be successfully drafted by the draft model at each decoding step. To\nalleviate this, we consider 1) boosting top-K sequence alignment between the\ndraft model and the target LLM, and 2) relaxing the verification strategy to\nreduce trivial LLM calls. To this end, we propose an alignment framework named\nAtSpeed, which presents the AtSpeed-S optimization objective for top-K\nalignment under the strict top-K verification. Moreover, we introduce a relaxed\nsampling verification strategy that allows high-probability non-top-K drafted\nsequences to be accepted, significantly reducing LLM calls. Correspondingly, we\npropose AtSpeed-R for top-K alignment under this relaxed sampling verification.\nEmpirical results on two real-world datasets demonstrate that AtSpeed\nsignificantly accelerates LLM-based generative recommendation, e.g., near 2x\nspeedup under strict top-K verification and up to 2.5x speedup under relaxed\nsampling verification. The codes and datasets are released at\nhttps://github.com/Linxyhaha/AtSpeed.\n","authors":["Xinyu Lin","Chaoqun Yang","Wenjie Wang","Yongqi Li","Cunxiao Du","Fuli Feng","See-Kiong Ng","Tat-Seng Chua"],"pdf_url":"https://arxiv.org/pdf/2410.05165v3.pdf","comment":"Accepted by ICLR 2025"},{"id":"http://arxiv.org/abs/2502.18841v1","updated":"2025-02-26T05:30:19Z","published":"2025-02-26T05:30:19Z","title":"Sentiment Analysis of Movie Reviews Using BERT","summary":"  Sentiment Analysis (SA) or opinion mining is analysis of emotions and\nopinions from any kind of text. SA helps in tracking peoples viewpoints and it\nis an important factor when it comes to social media monitoring product and\nbrand recognition customer satisfaction customer loyalty advertising and\npromotions success and product acceptance. That is why SA is one of the active\nresearch areas in Natural Language Processing (NLP). SA is applied on data\nsourced from various media platforms to mine sentiment knowledge from them.\nVarious approaches have been deployed in the literature to solve the problem.\nMost techniques devise complex and sophisticated frameworks in order to attain\noptimal accuracy. This work aims to finetune Bidirectional Encoder\nRepresentations from Transformers (BERT) with Bidirectional Long Short-Term\nMemory (BiLSTM) for movie reviews sentiment analysis and still provide better\naccuracy than the State-of-The-Art (SOTA) methods. The paper also shows how\nsentiment analysis can be applied if someone wants to recommend a certain movie\nfor example by computing overall polarity of its sentiments predicted by the\nmodel. That is our proposed method serves as an upper-bound baseline in\nprediction of a predominant reaction to a movie. To compute overall polarity a\nheuristic algorithm is applied to BERTBiLSTM output vector. Our model can be\nextended to three-class four-class or any fine-grained classification and apply\noverall polarity computation again. This is intended to be exploited in future\nwork.\n","authors":["Gibson Nkhata","Usman Anjum","Justin Zhan"],"pdf_url":"https://arxiv.org/pdf/2502.18841v1.pdf","comment":"7 pages, 3 figures, published in the proceedings The Fifteenth\n  International Conference on Information, Process, and Knowledge Management\n  (eKNOW 2023)"},{"id":"http://arxiv.org/abs/2408.09327v2","updated":"2025-02-26T05:16:14Z","published":"2024-08-18T01:59:41Z","title":"Threshold Filtering Packing for Supervised Fine-Tuning: Training Related\n  Samples within Packs","summary":"  Packing for Supervised Fine-Tuning (SFT) in autoregressive models involves\nconcatenating data points of varying lengths until reaching the designed\nmaximum length to facilitate GPU processing. However, randomly concatenating\ndata points can lead to cross-contamination of sequences due to the significant\ndifference in their subject matter. The mainstream approaches in SFT ensure\nthat each token in the attention calculation phase only focuses on tokens\nwithin its own short sequence, without providing additional learning signals\nfor the preceding context. To address these challenges, we introduce Threshold\nFiltering Packing (TFP), a method that selects samples with related context\nwhile maintaining sufficient diversity within the same pack. Our experiments\nshow that TFP offers a simple-to-implement and scalable approach that\nsignificantly enhances SFT performance, with observed improvements of up to 7\\%\non GSM8K, 4\\% on HumanEval. Furthermore, results from bias benchmark datasets\nhighlight TFP's promising performance in improving fairness while also boosting\nprediction accuracy by 15\\%.\n","authors":["Jiancheng Dong","Lei Jiang","Wei Jin","Lu Cheng"],"pdf_url":"https://arxiv.org/pdf/2408.09327v2.pdf","comment":"14 pages, 4 figures"},{"id":"http://arxiv.org/abs/2502.18823v1","updated":"2025-02-26T04:58:03Z","published":"2025-02-26T04:58:03Z","title":"Evidence-Driven Marker Extraction for Social Media Suicide Risk\n  Detection","summary":"  Early detection of suicide risk from social media text is crucial for timely\nintervention. While Large Language Models (LLMs) offer promising capabilities\nin this domain, challenges remain in terms of interpretability and\ncomputational efficiency. This paper introduces Evidence-Driven LLM (ED-LLM), a\nnovel approach for clinical marker extraction and suicide risk classification.\nED-LLM employs a multi-task learning framework, jointly training a Mistral-7B\nbased model to identify clinical marker spans and classify suicide risk levels.\nThis evidence-driven strategy enhances interpretability by explicitly\nhighlighting textual evidence supporting risk assessments. Evaluated on the\nCLPsych datasets, ED-LLM demonstrates competitive performance in risk\nclassification and superior capability in clinical marker span identification\ncompared to baselines including fine-tuned LLMs, traditional machine learning,\nand prompt-based methods. The results highlight the effectiveness of multi-task\nlearning for interpretable and efficient LLM-based suicide risk assessment,\npaving the way for clinically relevant applications.\n","authors":["Carter Adams","Caleb Carter","Jackson Simmons"],"pdf_url":"https://arxiv.org/pdf/2502.18823v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.18817v1","updated":"2025-02-26T04:50:43Z","published":"2025-02-26T04:50:43Z","title":"Judge as A Judge: Improving the Evaluation of Retrieval-Augmented\n  Generation through the Judge-Consistency of Large Language Models","summary":"  Retrieval-Augmented Generation (RAG) has proven its effectiveness in\nalleviating hallucinations for Large Language Models (LLMs). However, existing\nautomated evaluation metrics cannot fairly evaluate the outputs generated by\nRAG models during training and evaluation. LLM-based judgment models provide\nthe potential to produce high-quality judgments, but they are highly sensitive\nto evaluation prompts, leading to inconsistencies when judging the output of\nRAG models. This paper introduces the Judge-Consistency (ConsJudge) method,\nwhich aims to enhance LLMs to generate more accurate evaluations for RAG\nmodels. Specifically, ConsJudge prompts LLMs to generate different judgments\nbased on various combinations of judgment dimensions, utilize the\njudge-consistency to evaluate these judgments and select the accepted and\nrejected judgments for DPO training. Our experiments show that ConsJudge can\neffectively provide more accurate judgments for optimizing RAG models across\nvarious RAG models and datasets. Further analysis reveals that judgments\ngenerated by ConsJudge have a high agreement with the superior LLM. All codes\nare available at https://github.com/OpenBMB/ConsJudge.\n","authors":["Shuliang Liu","Xinze Li","Zhenghao Liu","Yukun Yan","Cheng Yang","Zheni Zeng","Zhiyuan Liu","Maosong Sun","Ge Yu"],"pdf_url":"https://arxiv.org/pdf/2502.18817v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.18810v1","updated":"2025-02-26T04:39:22Z","published":"2025-02-26T04:39:22Z","title":"Holistic Audit Dataset Generation for LLM Unlearning via Knowledge Graph\n  Traversal and Redundancy Removal","summary":"  In recent years, Large Language Models (LLMs) have faced increasing demands\nto selectively remove sensitive information, protect privacy, and comply with\ncopyright regulations through unlearning, by Machine Unlearning. While\nevaluating unlearning effectiveness is crucial, existing benchmarks are limited\nin scale and comprehensiveness, typically containing only a few hundred test\ncases. We identify two critical challenges in generating holistic audit\ndatasets: ensuring audit adequacy and handling knowledge redundancy between\nforget and retain dataset. To address these challenges, we propose HANKER, an\nautomated framework for holistic audit dataset generation leveraging knowledge\ngraphs to achieve fine-grained coverage and eliminate redundant knowledge.\nApplying HANKER to the popular MUSE benchmark, we successfully generated over\n69,000 and 111,000 audit cases for the News and Books datasets respectively,\nidentifying thousands of knowledge memorization instances that the previous\nbenchmark failed to detect. Our empirical analysis uncovers how knowledge\nredundancy significantly skews unlearning effectiveness metrics, with redundant\ninstances artificially inflating the observed memorization measurements ROUGE\nfrom 19.7% to 26.1% and Entailment Scores from 32.4% to 35.2%, highlighting the\nnecessity of systematic deduplication for accurate assessment.\n","authors":["Weipeng Jiang","Juan Zhai","Shiqing Ma","Ziyan Lei","Xiaofei Xie","Yige Wang","Chao Shen"],"pdf_url":"https://arxiv.org/pdf/2502.18810v1.pdf","comment":"11 pages, 4 figures"},{"id":"http://arxiv.org/abs/2502.12022v2","updated":"2025-02-26T04:38:54Z","published":"2025-02-17T16:56:23Z","title":"Teaching LLMs According to Their Aptitude: Adaptive Reasoning for\n  Mathematical Problem Solving","summary":"  Existing approaches to mathematical reasoning with large language models\n(LLMs) rely on Chain-of-Thought (CoT) for generalizability or Tool-Integrated\nReasoning (TIR) for precise computation. While efforts have been made to\ncombine these methods, they primarily rely on post-selection or predefined\nstrategies, leaving an open question: whether LLMs can autonomously adapt their\nreasoning strategy based on their inherent capabilities. In this work, we\npropose TATA (Teaching LLMs According to Their Aptitude), an adaptive framework\nthat enables LLMs to personalize their reasoning strategy spontaneously,\naligning it with their intrinsic aptitude. TATA incorporates base-LLM-aware\ndata selection during supervised fine-tuning (SFT) to tailor training data to\nthe model's unique abilities. This approach equips LLMs to autonomously\ndetermine and apply the appropriate reasoning strategy at test time. We\nevaluate TATA through extensive experiments on six mathematical reasoning\nbenchmarks, using both general-purpose and math-specialized LLMs. Empirical\nresults demonstrate that TATA effectively combines the complementary strengths\nof CoT and TIR, achieving superior or comparable performance with improved\ninference efficiency compared to TIR alone. Further analysis underscores the\ncritical role of aptitude-aware data selection in enabling LLMs to make\neffective and adaptive reasoning decisions and align reasoning strategies with\nmodel capabilities.\n","authors":["Xin Xu","Yan Xu","Tianhao Chen","Yuchen Yan","Chengwu Liu","Zaoyu Chen","Yufei Wang","Yichun Yin","Yasheng Wang","Lifeng Shang","Qun Liu"],"pdf_url":"https://arxiv.org/pdf/2502.12022v2.pdf","comment":"8 pages"},{"id":"http://arxiv.org/abs/2502.14916v2","updated":"2025-02-26T04:35:15Z","published":"2025-02-19T08:08:53Z","title":"MKE-Coder: Multi-Axial Knowledge with Evidence Verification in ICD\n  Coding for Chinese EMRs","summary":"  The task of automatically coding the International Classification of Diseases\n(ICD) in the medical field has been well-established and has received much\nattention. Automatic coding of the ICD in the medical field has been successful\nin English but faces challenges when dealing with Chinese electronic medical\nrecords (EMRs). The first issue lies in the difficulty of extracting disease\ncode-related information from Chinese EMRs, primarily due to the concise\nwriting style and specific internal structure of the EMRs. The second problem\nis that previous methods have failed to leverage the disease-based multi-axial\nknowledge and lack of association with the corresponding clinical evidence.\nThis paper introduces a novel framework called MKE-Coder: Multi-axial Knowledge\nwith Evidence verification in ICD coding for Chinese EMRs. Initially, we\nidentify candidate codes for the diagnosis and categorize each of them into\nknowledge under four coding axes.Subsequently, we retrieve corresponding\nclinical evidence from the comprehensive content of EMRs and filter credible\nevidence through a scoring model. Finally, to ensure the validity of the\ncandidate code, we propose an inference module based on the masked language\nmodeling strategy. This module verifies that all the axis knowledge associated\nwith the candidate code is supported by evidence and provides recommendations\naccordingly. To evaluate the performance of our framework, we conduct\nexperiments using a large-scale Chinese EMR dataset collected from various\nhospitals. The experimental results demonstrate that MKE-Coder exhibits\nsignificant superiority in the task of automatic ICD coding based on Chinese\nEMRs. In the practical evaluation of our method within simulated real coding\nscenarios, it has been demonstrated that our approach significantly aids coders\nin enhancing both their coding accuracy and speed.\n","authors":["Xinxin You","Xien Liu","Xue Yang","Ziyi Wang","Ji Wu"],"pdf_url":"https://arxiv.org/pdf/2502.14916v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.18802v1","updated":"2025-02-26T04:17:19Z","published":"2025-02-26T04:17:19Z","title":"Language Models Grow Less Humanlike beyond Phase Transition","summary":"  LMs' alignment with human reading behavior (i.e. psychometric predictive\npower; PPP) is known to improve during pretraining up to a tipping point,\nbeyond which it either plateaus or degrades. Various factors, such as word\nfrequency, recency bias in attention, and context size, have been theorized to\naffect PPP, yet there is no current account that explains why such a tipping\npoint exists, and how it interacts with LMs' pretraining dynamics more\ngenerally. We hypothesize that the underlying factor is a pretraining phase\ntransition, characterized by the rapid emergence of specialized attention\nheads. We conduct a series of correlational and causal experiments to show that\nsuch a phase transition is responsible for the tipping point in PPP. We then\nshow that, rather than producing attention patterns that contribute to the\ndegradation in PPP, phase transitions alter the subsequent learning dynamics of\nthe model, such that further training keeps damaging PPP.\n","authors":["Tatsuya Aoyama","Ethan Wilcox"],"pdf_url":"https://arxiv.org/pdf/2502.18802v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.18798v1","updated":"2025-02-26T04:10:18Z","published":"2025-02-26T04:10:18Z","title":"ANPMI: Assessing the True Comprehension Capabilities of LLMs for\n  Multiple Choice Questions","summary":"  Multiple-choice benchmarks, consisting of various prompts and choices, are\namong the most widely used methods to assess a language model's natural\nlanguage understanding capability. Given a specific prompt, we typically\ncompute $P(Choice|Prompt)$ to evaluate how likely a language model is to\ngenerate the correct choice compared to incorrect ones. However, we observe\nthat performance measured using this approach reflects not only the model's\ncomprehension of the prompt but also its inherent biases for certain choices\nregardless of the prompt. This issue makes it challenging to accurately measure\na model's natural language understanding, as models may select the answer\nwithout fully understanding the prompt. To address this limitation, we propose\na novel metric called ANPMI, which normalizes Pointwise Mutual Information\n(PMI) by $-\\log P(Choice)$. ANPMI provides a more accurate assessment of the\nmodel's natural language understanding by ensuring that it is challenging to\nanswer a question without properly understanding the prompt.\n","authors":["Gyeongje Cho","Yeonkyoung So","Jaejin Lee"],"pdf_url":"https://arxiv.org/pdf/2502.18798v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.18795v1","updated":"2025-02-26T04:01:36Z","published":"2025-02-26T04:01:36Z","title":"Anything Goes? A Crosslinguistic Study of (Im)possible Language Learning\n  in LMs","summary":"  Do LLMs offer insights into human language learning? A common argument\nagainst this idea is that because their architecture and training paradigm are\nso vastly different from humans, LLMs can learn arbitrary inputs as easily as\nnatural languages. In this paper, we test this claim by training LMs to model\nimpossible and typologically unattested languages. Unlike previous work, which\nhas focused exclusively on English, we conduct experiments on 12 natural\nlanguages from 4 language families. Our results show that while GPT-2 small can\nprimarily distinguish attested languages from their impossible counterparts, it\ndoes not achieve perfect separation between all the attested languages and all\nthe impossible ones. We further test whether GPT-2 small distinguishes\ntypologically attested from unattested languages with different NP orders by\nmanipulating word order based on Greenberg's Universal 20. We find that the\nmodel's perplexity scores do not distinguish attested vs. unattested word\norders, as long as the unattested variants maintain constituency structure.\nThese findings suggest that language models exhibit some human-like inductive\nbiases, though these biases are weaker than those found in human learners.\n","authors":["Xiulin Yang","Tatsuya Aoyama","Yuekun Yao","Ethan Wilcox"],"pdf_url":"https://arxiv.org/pdf/2502.18795v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.18791v1","updated":"2025-02-26T03:56:34Z","published":"2025-02-26T03:56:34Z","title":"Seeing the Forest for the Trees: A Large Scale, Continuously Updating\n  Meta-Analysis of Frontier LLMs","summary":"  The surge of LLM studies makes synthesizing their findings challenging.\nMeta-analysis can uncover important trends across studies, but its use is\nlimited by the time-consuming nature of manual data extraction. Our study\npresents a semi-automated approach for meta-analysis that accelerates data\nextraction using LLMs. It automatically identifies relevant arXiv papers,\nextracts experimental results and related attributes, and organizes them into a\nstructured dataset. We conduct a comprehensive meta-analysis of frontier LLMs\nusing an automatically extracted dataset, reducing the effort of paper\nsurveying and data extraction by more than 93\\% compared to manual approaches.\nWe validate our dataset by showing that it reproduces key findings from a\nrecent manual meta-analysis about Chain-of-Thought (CoT), and also uncovers new\ninsights that go beyond it, showing for example that in-context examples\nbenefit multimodal tasks but offer limited gains in mathematical tasks compared\nto CoT. Our automatically updatable dataset enables continuous tracking of\ntarget models by extracting evaluation studies as new data becomes available.\nThrough our scientific artifacts and empirical analysis, we provide novel\ninsights into LLMs while facilitating ongoing meta-analyses of their behavior.\n","authors":["Jungsoo Park","Junmo Kang","Gabriel Stanovsky","Alan Ritter"],"pdf_url":"https://arxiv.org/pdf/2502.18791v1.pdf","comment":"21 pages, 9 figures"},{"id":"http://arxiv.org/abs/2502.18782v1","updated":"2025-02-26T03:30:13Z","published":"2025-02-26T03:30:13Z","title":"Active Few-Shot Learning for Text Classification","summary":"  The rise of Large Language Models (LLMs) has boosted the use of Few-Shot\nLearning (FSL) methods in natural language processing, achieving acceptable\nperformance even when working with limited training data. The goal of FSL is to\neffectively utilize a small number of annotated samples in the learning\nprocess. However, the performance of FSL suffers when unsuitable support\nsamples are chosen. This problem arises due to the heavy reliance on a limited\nnumber of support samples, which hampers consistent performance improvement\neven when more support samples are added. To address this challenge, we propose\nan active learning-based instance selection mechanism that identifies effective\nsupport instances from the unlabeled pool and can work with different LLMs. Our\nexperiments on five tasks show that our method frequently improves the\nperformance of FSL. We make our implementation available on GitHub.\n","authors":["Saeed Ahmadnia","Arash Yousefi Jordehi","Mahsa Hosseini Khasheh Heyran","Seyed Abolghasem Mirroshandel","Owen Rambow","Cornelia Caragea"],"pdf_url":"https://arxiv.org/pdf/2502.18782v1.pdf","comment":"Accepted to NAACL 2025 Main Conference; 18 pages, 8 figures, 13\n  tables including Appendix"},{"id":"http://arxiv.org/abs/2406.00023v3","updated":"2025-02-26T03:28:51Z","published":"2024-05-24T02:50:44Z","title":"Expert-Token Resonance MoE: Bidirectional Routing with Efficiency\n  Affinity-Driven Active Selection","summary":"  Mixture-of-Experts (MoE) architectures have emerged as a paradigm-shifting\napproach for large language models (LLMs), offering unprecedented computational\nefficiency. However, these architectures grapple with challenges of token\ndistribution imbalance and expert homogenization, impeding optimal semantic\ngeneralization. We propose a novel expert routing framework that incorporates:\n(1) An efficient routing mechanism with lightweight computation. (2) An\nadaptive bidirectional selection mechanism leveraging resonance between experts\nand tokens. (3) A module that determines the lower bounds of expert capacity\nbased on dynamic token distribution analysis, specifically designed to address\ndrop-and-pad strategies. It is also integrated with orthogonal feature\nextraction module and an optimized loss function for expert localization. This\nframework effectively reduces expert homogeneity while enhancing the\nperformance of the expert selection module. Additionally, we introduce a local\nexpert strategy that simultaneously improves load balancing and reduces network\ncommunication overhead. It achieves a 40\\% reduction in token processed by each\nexpert without compromising model convergence or efficacy. When coupled with\ncommunication optimizations, the training efficiency improvements of 5.4\\% to\n46.6\\% can be observed. After supervised fine-tuning, it exhibits performance\ngains of 9.7\\% to 14.1\\% across GDAD, GPQA, and TeleQnA benchmarks.\n","authors":["Jing Li","Zhijie Sun","Dachao Lin","Xuan He","Binfan Zheng","Yi Lin","Rongqian Zhao","Xin Chen"],"pdf_url":"https://arxiv.org/pdf/2406.00023v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.08197v2","updated":"2025-02-26T03:24:58Z","published":"2024-10-10T17:58:44Z","title":"From Exploration to Mastery: Enabling LLMs to Master Tools via\n  Self-Driven Interactions","summary":"  Tool learning enables Large Language Models (LLMs) to interact with external\nenvironments by invoking tools, serving as an effective strategy to mitigate\nthe limitations inherent in their pre-training data. In this process, tool\ndocumentation plays a crucial role by providing usage instructions for LLMs,\nthereby facilitating effective tool utilization. This paper concentrates on the\ncritical challenge of bridging the comprehension gap between LLMs and external\ntools due to the inadequacies and inaccuracies inherent in existing\nhuman-centric tool documentation. We propose a novel framework, DRAFT, aimed at\nDynamically Refining tool documentation through the Analysis of Feedback and\nTrials emanating from LLMs' interactions with external tools. This methodology\npivots on an innovative trial-and-error approach, consisting of three distinct\nlearning phases: experience gathering, learning from experience, and\ndocumentation rewriting, to iteratively enhance the tool documentation. This\nprocess is further optimized by implementing a diversity-promoting exploration\nstrategy to ensure explorative diversity and a tool-adaptive termination\nmechanism to prevent overfitting while enhancing efficiency. Extensive\nexperiments on multiple datasets demonstrate that DRAFT's iterative,\nfeedback-based refinement significantly ameliorates documentation quality,\nfostering a deeper comprehension and more effective utilization of tools by\nLLMs. Notably, our analysis reveals that the tool documentation refined via our\napproach demonstrates robust cross-model generalization capabilities.\n","authors":["Changle Qu","Sunhao Dai","Xiaochi Wei","Hengyi Cai","Shuaiqiang Wang","Dawei Yin","Jun Xu","Ji-Rong Wen"],"pdf_url":"https://arxiv.org/pdf/2410.08197v2.pdf","comment":"ICLR 2025 Oral;GitHub:https://github.com/quchangle1/DRAFT"},{"id":"http://arxiv.org/abs/2409.19381v5","updated":"2025-02-26T03:24:57Z","published":"2024-09-28T15:12:55Z","title":"HYBRIDMIND: Meta Selection of Natural Language and Symbolic Language for\n  Enhanced LLM Reasoning","summary":"  LLMs approach logical and mathematical reasoning through natural or symbolic\nlanguages. While natural language offers human-accessible flexibility but\nsuffers from ambiguity, symbolic reasoning provides precise, machine-executable\ninferences at the cost of strict domain constraints. We introduce HYBRIDMIND,\nan adaptive strategy that selects the optimal reasoning approach for each\nreasoning problem. Through extensive experiments, we evaluate both\nprompting-based approaches with state-of-the-art LLMs and fine-tuned\nopen-source models. We find that fine-tuning LLaMA-3.1-8B-Instruct as a\nmeta-selector outperforms GPT-4o's natural language reasoning by 4.4\\% on FOLIO\nand 1.3\\% on MATH. More notably, using GPT-3.5-turbo as a prompted\nmeta-selector yields a 10\\% improvement on FOLIO's challenging subset compared\nto GPT-4o. We will release our code and data to support future research.\n","authors":["Simeng Han","Tianyu Liu","Chuhan Li","Xuyuan Xiong","Arman Cohan"],"pdf_url":"https://arxiv.org/pdf/2409.19381v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.14843v3","updated":"2025-02-26T03:24:32Z","published":"2024-12-19T13:36:18Z","title":"Mapping and Influencing the Political Ideology of Large Language Models\n  using Synthetic Personas","summary":"  The analysis of political biases in large language models (LLMs) has\nprimarily examined these systems as single entities with fixed viewpoints.\nWhile various methods exist for measuring such biases, the impact of\npersona-based prompting on LLMs' political orientation remains unexplored. In\nthis work we leverage PersonaHub, a collection of synthetic persona\ndescriptions, to map the political distribution of persona-based prompted LLMs\nusing the Political Compass Test (PCT). We then examine whether these initial\ncompass distributions can be manipulated through explicit ideological prompting\ntowards diametrically opposed political orientations: right-authoritarian and\nleft-libertarian. Our experiments reveal that synthetic personas predominantly\ncluster in the left-libertarian quadrant, with models demonstrating varying\ndegrees of responsiveness when prompted with explicit ideological descriptors.\nWhile all models demonstrate significant shifts towards right-authoritarian\npositions, they exhibit more limited shifts towards left-libertarian positions,\nsuggesting an asymmetric response to ideological manipulation that may reflect\ninherent biases in model training.\n","authors":["Pietro Bernardelle","Leon Fröhling","Stefano Civelli","Riccardo Lunardi","Kevin Roitero","Gianluca Demartini"],"pdf_url":"https://arxiv.org/pdf/2412.14843v3.pdf","comment":"Companion Proceedings of the ACM Web Conference 2025 (WWW\n  Companion'25)"},{"id":"http://arxiv.org/abs/2502.00385v2","updated":"2025-02-26T03:23:41Z","published":"2025-02-01T09:53:17Z","title":"The Impact of Persona-based Political Perspectives on Hateful Content\n  Detection","summary":"  While pretraining language models with politically diverse content has been\nshown to improve downstream task fairness, such approaches require significant\ncomputational resources often inaccessible to many researchers and\norganizations. Recent work has established that persona-based prompting can\nintroduce political diversity in model outputs without additional training.\nHowever, it remains unclear whether such prompting strategies can achieve\nresults comparable to political pretraining for downstream tasks. We\ninvestigate this question using persona-based prompting strategies in\nmultimodal hate-speech detection tasks, specifically focusing on hate speech in\nmemes. Our analysis reveals that when mapping personas onto a political compass\nand measuring persona agreement, inherent political positioning has\nsurprisingly little correlation with classification decisions. Notably, this\nlack of correlation persists even when personas are explicitly injected with\nstronger ideological descriptors. Our findings suggest that while LLMs can\nexhibit political biases in their responses to direct political questions,\nthese biases may have less impact on practical classification tasks than\npreviously assumed. This raises important questions about the necessity of\ncomputationally expensive political pretraining for achieving fair performance\nin downstream tasks.\n","authors":["Stefano Civelli","Pietro Bernardelle","Gianluca Demartini"],"pdf_url":"https://arxiv.org/pdf/2502.00385v2.pdf","comment":"Companion Proceedings of the ACM Web Conference 2025 (WWW\n  Companion'25)"},{"id":"http://arxiv.org/abs/2502.18779v1","updated":"2025-02-26T03:22:44Z","published":"2025-02-26T03:22:44Z","title":"Towards Optimal Multi-draft Speculative Decoding","summary":"  Large Language Models (LLMs) have become an indispensable part of natural\nlanguage processing tasks. However, autoregressive sampling has become an\nefficiency bottleneck. Multi-Draft Speculative Decoding (MDSD) is a recent\napproach where, when generating each token, a small draft model generates\nmultiple drafts, and the target LLM verifies them in parallel, ensuring that\nthe final output conforms to the target model distribution. The two main design\nchoices in MDSD are the draft sampling method and the verification algorithm.\nFor a fixed draft sampling method, the optimal acceptance rate is a solution to\nan optimal transport problem, but the complexity of this problem makes it\ndifficult to solve for the optimal acceptance rate and measure the gap between\nexisting verification algorithms and the theoretical upper bound. This paper\ndiscusses the dual of the optimal transport problem, providing a way to\nefficiently compute the optimal acceptance rate. For the first time, we measure\nthe theoretical upper bound of MDSD efficiency for vocabulary sizes in the\nthousands and quantify the gap between existing verification algorithms and\nthis bound. We also compare different draft sampling methods based on their\noptimal acceptance rates. Our results show that the draft sampling method\nstrongly influences the optimal acceptance rate, with sampling without\nreplacement outperforming sampling with replacement. Additionally, existing\nverification algorithms do not reach the theoretical upper bound for both\nwithout replacement and with replacement sampling. Our findings suggest that\ncarefully designed draft sampling methods can potentially improve the optimal\nacceptance rate and enable the development of verification algorithms that\nclosely match the theoretical upper bound.\n","authors":["Zhengmian Hu","Tong Zheng","Vignesh Viswanathan","Ziyi Chen","Ryan A. Rossi","Yihan Wu","Dinesh Manocha","Heng Huang"],"pdf_url":"https://arxiv.org/pdf/2502.18779v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.18778v1","updated":"2025-02-26T03:21:12Z","published":"2025-02-26T03:21:12Z","title":"M2-omni: Advancing Omni-MLLM for Comprehensive Modality Support with\n  Competitive Performance","summary":"  We present M2-omni, a cutting-edge, open-source omni-MLLM that achieves\ncompetitive performance to GPT-4o. M2-omni employs a unified multimodal\nsequence modeling framework, which empowers Large Language Models(LLMs) to\nacquire comprehensive cross-modal understanding and generation capabilities.\nSpecifically, M2-omni can process arbitrary combinations of audio, video,\nimage, and text modalities as input, generating multimodal sequences\ninterleaving with audio, image, or text outputs, thereby enabling an advanced\nand interactive real-time experience. The training of such an omni-MLLM is\nchallenged by significant disparities in data quantity and convergence rates\nacross modalities. To address these challenges, we propose a step balance\nstrategy during pre-training to handle the quantity disparities in\nmodality-specific data. Additionally, a dynamically adaptive balance strategy\nis introduced during the instruction tuning stage to synchronize the\nmodality-wise training progress, ensuring optimal convergence. Notably, we\nprioritize preserving strong performance on pure text tasks to maintain the\nrobustness of M2-omni's language understanding capability throughout the\ntraining process. To our best knowledge, M2-omni is currently a very\ncompetitive open-source model to GPT-4o, characterized by its comprehensive\nmodality and task support, as well as its exceptional performance. We expect\nM2-omni will advance the development of omni-MLLMs, thus facilitating future\nresearch in this domain.\n","authors":["Qingpei Guo","Kaiyou Song","Zipeng Feng","Ziping Ma","Qinglong Zhang","Sirui Gao","Xuzheng Yu","Yunxiao Sun"," Tai-WeiChang","Jingdong Chen","Ming Yang","Jun Zhou"],"pdf_url":"https://arxiv.org/pdf/2502.18778v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.18772v1","updated":"2025-02-26T03:04:01Z","published":"2025-02-26T03:04:01Z","title":"Plutus: Benchmarking Large Language Models in Low-Resource Greek Finance","summary":"  Despite Greece's pivotal role in the global economy, large language models\n(LLMs) remain underexplored for Greek financial context due to the linguistic\ncomplexity of Greek and the scarcity of domain-specific datasets. Previous\nefforts in multilingual financial natural language processing (NLP) have\nexposed considerable performance disparities, yet no dedicated Greek financial\nbenchmarks or Greek-specific financial LLMs have been developed until now. To\nbridge this gap, we introduce Plutus-ben, the first Greek Financial Evaluation\nBenchmark, and Plutus-8B, the pioneering Greek Financial LLM, fine-tuned with\nGreek domain-specific data. Plutus-ben addresses five core financial NLP tasks\nin Greek: numeric and textual named entity recognition, question answering,\nabstractive summarization, and topic classification, thereby facilitating\nsystematic and reproducible LLM assessments. To underpin these tasks, we\npresent three novel, high-quality Greek financial datasets, thoroughly\nannotated by expert native Greek speakers, augmented by two existing resources.\nOur comprehensive evaluation of 22 LLMs on Plutus-ben reveals that Greek\nfinancial NLP remains challenging due to linguistic complexity, domain-specific\nterminology, and financial reasoning gaps. These findings underscore the\nlimitations of cross-lingual transfer, the necessity for financial expertise in\nGreek-trained models, and the challenges of adapting financial LLMs to Greek\ntext. We release Plutus-ben, Plutus-8B, and all associated datasets publicly to\npromote reproducible research and advance Greek financial NLP, fostering\nbroader multilingual inclusivity in finance.\n","authors":["Xueqing Peng","Triantafillos Papadopoulos","Efstathia Soufleri","Polydoros Giannouris","Ruoyu Xiang","Yan Wang","Lingfei Qian","Jimin Huang","Qianqian Xie","Sophia Ananiadou"],"pdf_url":"https://arxiv.org/pdf/2502.18772v1.pdf","comment":"18 pages, 6 figures"},{"id":"http://arxiv.org/abs/2502.18770v1","updated":"2025-02-26T02:57:59Z","published":"2025-02-26T02:57:59Z","title":"Reward Shaping to Mitigate Reward Hacking in RLHF","summary":"  Reinforcement Learning from Human Feedback (RLHF) is essential for aligning\nlarge language models (LLMs) with human values. However, RLHF is susceptible to\nreward hacking, where the agent exploits flaws in the reward function rather\nthan learning the intended behavior, thus degrading alignment. While reward\nshaping helps stabilize RLHF and partially mitigate reward hacking, a\nsystematic investigation into shaping techniques and their underlying\nprinciples remains lacking. To bridge this gap, we present a comprehensive\nstudy of the prevalent reward shaping methods. Our analysis suggests three key\ndesign principles: (1) RL reward is ideally bounded, (2) RL benefits from rapid\ninitial growth followed by gradual convergence, and (3) RL reward is best\nformulated as a function of centered reward. Guided by these insights, we\npropose Preference As Reward (PAR), a novel approach that leverages the latent\npreferences embedded within the reward model itself as the signal for\nreinforcement learning. We evaluated PAR on two base models, Gemma2-2B and\nLlama3-8B, using two datasets, Ultrafeedback-Binarized and HH-RLHF.\nExperimental results demonstrate PAR's superior performance over other reward\nshaping methods. On the AlpacaEval 2.0 benchmark, PAR achieves a win rate at\nleast 5 percentage points higher than competing approaches. Furthermore, PAR\nexhibits remarkable data efficiency, requiring only a single reference reward\nfor optimal performance, and maintains robustness against reward hacking even\nafter two full epochs of training. Code is available at\nhttps://github.com/PorUna-byte/PAR.\n","authors":["Jiayi Fu","Xuandong Zhao","Chengyuan Yao","Heng Wang","Qi Han","Yanghua Xiao"],"pdf_url":"https://arxiv.org/pdf/2502.18770v1.pdf","comment":"19 pages"},{"id":"http://arxiv.org/abs/2502.18274v2","updated":"2025-02-26T02:50:52Z","published":"2025-02-25T15:05:12Z","title":"Citrus: Leveraging Expert Cognitive Pathways in a Medical Language Model\n  for Advanced Medical Decision Support","summary":"  Large language models (LLMs), particularly those with reasoning capabilities,\nhave rapidly advanced in recent years, demonstrating significant potential\nacross a wide range of applications. However, their deployment in healthcare,\nespecially in disease reasoning tasks, is hindered by the challenge of\nacquiring expert-level cognitive data. In this paper, we introduce Citrus, a\nmedical language model that bridges the gap between clinical expertise and AI\nreasoning by emulating the cognitive processes of medical experts. The model is\ntrained on a large corpus of simulated expert disease reasoning data,\nsynthesized using a novel approach that accurately captures the decision-making\npathways of clinicians. This approach enables Citrus to better simulate the\ncomplex reasoning processes involved in diagnosing and treating medical\nconditions. To further address the lack of publicly available datasets for\nmedical reasoning tasks, we release the last-stage training data, including a\ncustom-built medical diagnostic dialogue dataset. This open-source contribution\naims to support further research and development in the field. Evaluations\nusing authoritative benchmarks such as MedQA, covering tasks in medical\nreasoning and language understanding, show that Citrus achieves superior\nperformance compared to other models of similar size. These results highlight\nCitrus potential to significantly enhance medical decision support systems,\nproviding a more accurate and efficient tool for clinical decision-making.\n","authors":["Guoxin Wang","Minyu Gao","Shuai Yang","Ya Zhang","Lizhi He","Liang Huang","Hanlin Xiao","Yexuan Zhang","Wanyue Li","Lu Chen","Jintao Fei","Xin Li"],"pdf_url":"https://arxiv.org/pdf/2502.18274v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.04077v2","updated":"2025-02-26T02:48:22Z","published":"2025-02-06T13:41:46Z","title":"AttentionPredictor: Temporal Pattern Matters for Efficient LLM Inference","summary":"  With the development of large language models (LLMs), efficient inference\nthrough Key-Value (KV) cache compression has attracted considerable attention,\nespecially for long-context generation. To compress the KV cache, recent\nmethods identify critical KV tokens through heuristic ranking with attention\nscores. However, these methods often struggle to accurately determine critical\ntokens as they neglect the \\textit{temporal patterns} in attention scores,\nresulting in a noticeable degradation in LLM performance. To address this\nchallenge, we propose AttentionPredictor, which is the first learning-based\ncritical token identification approach. Specifically, AttentionPredictor learns\na lightweight convolution model to capture spatiotemporal patterns and predict\nthe next-token attention score. An appealing feature of AttentionPredictor is\nthat it accurately predicts the attention score while consuming negligible\nmemory. Moreover, we propose a cross-token critical cache prefetching framework\nthat hides the token estimation time overhead to accelerate the decoding stage.\nBy retaining most of the attention information, AttentionPredictor achieves\n16$\\times$ KV cache compression with comparable LLM performance, significantly\noutperforming the state-of-the-art.\n","authors":["Qingyue Yang","Jie Wang","Xing Li","Zhihai Wang","Chen Chen","Lei Chen","Xianzhi Yu","Wulong Liu","Jianye Hao","Mingxuan Yuan","Bin Li"],"pdf_url":"https://arxiv.org/pdf/2502.04077v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.17380v2","updated":"2025-02-26T02:45:56Z","published":"2025-02-24T18:06:57Z","title":"Low-Rank and Sparse Model Merging for Multi-Lingual Speech Recognition\n  and Translation","summary":"  Language diversity presents a significant challenge in speech-to-text (S2T)\ntasks, such as automatic speech recognition and translation. Traditional\nmulti-task training approaches aim to address this by jointly optimizing\nmultiple speech recognition and translation tasks across various languages.\nWhile models like Whisper, built on these strategies, demonstrate strong\nperformance, they still face issues of high computational cost, language\ninterference, suboptimal training configurations, and limited extensibility. To\novercome these challenges, we introduce LoRS-Merging (low-rank and sparse model\nmerging), a novel technique designed to efficiently integrate models trained on\ndifferent languages or tasks while preserving performance and reducing\ncomputational overhead. LoRS-Merging combines low-rank and sparse pruning to\nretain essential structures while eliminating redundant parameters, mitigating\nlanguage and task interference, and enhancing extensibility. Experimental\nresults across a range of languages demonstrate that LoRS-Merging reduces the\nword error rate by 10% and improves BLEU scores by 4% compared to conventional\nmulti-lingual multi-task training baselines. Our findings suggest that model\nmerging, particularly LoRS-Merging, is a scalable and effective complement to\ntraditional multi-lingual training strategies for S2T applications.\n","authors":["Qiuming Zhao","Guangzhi Sun","Chao Zhang","Mingxing Xu","Thomas Fang Zheng"],"pdf_url":"https://arxiv.org/pdf/2502.17380v2.pdf","comment":"13 pages, submitted to ACL 2025"},{"id":"http://arxiv.org/abs/2405.14804v4","updated":"2025-02-26T02:21:40Z","published":"2024-05-23T17:13:50Z","title":"Can LLMs Solve longer Math Word Problems Better?","summary":"  Math Word Problems (MWPs) play a vital role in assessing the capabilities of\nLarge Language Models (LLMs), yet current research primarily focuses on\nquestions with concise contexts. The impact of longer contexts on mathematical\nreasoning remains under-explored. This study pioneers the investigation of\nContext Length Generalizability (CoLeG), which refers to the ability of LLMs to\nsolve MWPs with extended narratives. We introduce Extended Grade-School Math\n(E-GSM), a collection of MWPs featuring lengthy narratives, and propose two\nnovel metrics to evaluate the efficacy and resilience of LLMs in tackling these\nproblems. Our analysis of existing zero-shot prompting techniques with\nproprietary LLMs along with open-source LLMs reveals a general deficiency in\nCoLeG. To alleviate these issues, we propose tailored approaches for different\ncategories of LLMs. For proprietary LLMs, we introduce a new instructional\nprompt designed to mitigate the impact of long contexts. For open-source LLMs,\nwe develop a novel auxiliary task for fine-tuning to enhance CoLeG. Our\ncomprehensive results demonstrate the effectiveness of our proposed methods,\nshowing improved performance on E-GSM. Additionally, we conduct an in-depth\nanalysis to differentiate the effects of semantic understanding and reasoning\nefficacy, showing that our methods improves the latter. We also establish the\ngeneralizability of our methods across several other MWP benchmarks. Our\nfindings highlight the limitations of current LLMs and offer practical\nsolutions correspondingly, paving the way for further exploration of model\ngeneralizability and training methodologies.\n","authors":["Xin Xu","Tong Xiao","Zitong Chao","Zhenya Huang","Can Yang","Yang Wang"],"pdf_url":"https://arxiv.org/pdf/2405.14804v4.pdf","comment":"Accepted to ICLR 2025"},{"id":"http://arxiv.org/abs/2502.06415v2","updated":"2025-02-26T01:59:40Z","published":"2025-02-10T12:54:17Z","title":"Systematic Outliers in Large Language Models","summary":"  Outliers have been widely observed in Large Language Models (LLMs),\nsignificantly impacting model performance and posing challenges for model\ncompression. Understanding the functionality and formation mechanisms of these\noutliers is critically important. Existing works, however, largely focus on\nreducing the impact of outliers from an algorithmic perspective, lacking an\nin-depth investigation into their causes and roles. In this work, we provide a\ndetailed analysis of the formation process, underlying causes, and functions of\noutliers in LLMs. We define and categorize three types of outliers-activation\noutliers, weight outliers, and attention outliers-and analyze their\ndistributions across different dimensions, uncovering inherent connections\nbetween their occurrences and their ultimate influence on the attention\nmechanism. Based on these observations, we hypothesize and explore the\nmechanisms by which these outliers arise and function, demonstrating through\ntheoretical derivations and experiments that they emerge due to the\nself-attention mechanism's softmax operation. These outliers act as implicit\ncontext-aware scaling factors within the attention mechanism. As these outliers\nstem from systematic influences, we term them systematic outliers. Our study\nnot only enhances the understanding of Transformer-based LLMs but also shows\nthat structurally eliminating outliers can accelerate convergence and improve\nmodel compression. The code is avilable at\nhttps://github.com/an-yongqi/systematic-outliers.\n","authors":["Yongqi An","Xu Zhao","Tao Yu","Ming Tang","Jinqiao Wang"],"pdf_url":"https://arxiv.org/pdf/2502.06415v2.pdf","comment":"Accepted at ICLR 2025. Project Page:\n  https://github.com/an-yongqi/systematic-outliers"},{"id":"http://arxiv.org/abs/2502.11806v2","updated":"2025-02-26T01:52:03Z","published":"2025-02-17T13:50:29Z","title":"Exploring Translation Mechanism of Large Language Models","summary":"  Large language models (LLMs) have succeeded remarkably in multilingual\ntranslation tasks. However, the inherent translation mechanisms of LLMs remain\npoorly understood, largely due to sophisticated architectures and vast\nparameter scales. In response to this issue, this study explores the\ntranslation mechanism of LLM from the perspective of computational components\n(e.g., attention heads and MLPs). Path patching is utilized to explore causal\nrelationships between components, detecting those crucial for translation tasks\nand subsequently analyzing their behavioral patterns in human-interpretable\nterms. Comprehensive analysis reveals that translation is predominantly\nfacilitated by a sparse subset of specialized attention heads (less than 5\\%),\nwhich extract source language, indicator, and positional features. MLPs\nsubsequently integrate and process these features by transiting towards\nEnglish-centric latent representations. Notably, building on the above\nfindings, targeted fine-tuning of only 64 heads achieves translation\nimprovement comparable to full-parameter tuning while preserving general\ncapabilities.\n","authors":["Hongbin Zhang","Kehai Chen","Xuefeng Bai","Xiucheng Li","Yang Xiang","Min Zhang"],"pdf_url":"https://arxiv.org/pdf/2502.11806v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.03688v2","updated":"2025-02-26T01:49:17Z","published":"2025-02-06T00:38:25Z","title":"A Comparison of DeepSeek and Other LLMs","summary":"  Recently, DeepSeek has been the focus of attention in and beyond the AI\ncommunity. An interesting problem is how DeepSeek compares to other large\nlanguage models (LLMs). There are many tasks an LLM can do, and in this paper,\nwe use the task of predicting an outcome using a short text for comparison. We\nconsider two settings, an authorship classification setting and a citation\nclassification setting. In the first one, the goal is to determine whether a\nshort text is written by human or AI. In the second one, the goal is to\nclassify a citation to one of four types using the textual content. For each\nexperiment, we compare DeepSeek with $4$ popular LLMs: Claude, Gemini, GPT, and\nLlama.\n  We find that, in terms of classification accuracy, DeepSeek outperforms\nGemini, GPT, and Llama in most cases, but underperforms Claude. We also find\nthat DeepSeek is comparably slower than others but with a low cost to use,\nwhile Claude is much more expensive than all the others. Finally, we find that\nin terms of similarity, the output of DeepSeek is most similar to those of\nGemini and Claude (and among all $5$ LLMs, Claude and Gemini have the most\nsimilar outputs).\n  In this paper, we also present a fully-labeled dataset collected by\nourselves, and propose a recipe where we can use the LLMs and a recent data\nset, MADStat, to generate new data sets. The datasets in our paper can be used\nas benchmarks for future study on LLMs.\n","authors":["Tianchen Gao","Jiashun Jin","Zheng Tracy Ke","Gabriel Moryoussef"],"pdf_url":"https://arxiv.org/pdf/2502.03688v2.pdf","comment":"21 pages, 5 figures, 6 tables"},{"id":"http://arxiv.org/abs/2502.18746v1","updated":"2025-02-26T01:42:08Z","published":"2025-02-26T01:42:08Z","title":"Automatic Prompt Optimization via Heuristic Search: A Survey","summary":"  Recent advances in Large Language Models have led to remarkable achievements\nacross a variety of Natural Language Processing tasks, making prompt\nengineering increasingly central to guiding model outputs. While manual methods\ncan be effective, they typically rely on intuition and do not automatically\nrefine prompts over time. In contrast, automatic prompt optimization employing\nheuristic-based search algorithms can systematically explore and improve\nprompts with minimal human oversight. This survey proposes a comprehensive\ntaxonomy of these methods, categorizing them by where optimization occurs, what\nis optimized, what criteria drive the optimization, which operators generate\nnew prompts, and which iterative search algorithms are applied. We further\nhighlight specialized datasets and tools that support and accelerate automated\nprompt refinement. We conclude by discussing key open challenges pointing\ntoward future opportunities for more robust and versatile LLM applications.\n","authors":["Wendi Cui","Jiaxin Zhang","Zhuohang Li","Hao Sun","Damien Lopez","Kamalika Das","Bradley A. Malin","Sricharan Kumar"],"pdf_url":"https://arxiv.org/pdf/2502.18746v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.19622v1","updated":"2025-02-26T23:22:02Z","published":"2025-02-26T23:22:02Z","title":"Weaker LLMs' Opinions Also Matter: Mixture of Opinions Enhances LLM's\n  Mathematical Reasoning","summary":"  Recent advances in Large Language Models (LLMs) have raised interest in their\nformal reasoning capabilities, particularly in mathematics. While closed LLMs\nlike GPT-4 perform well on mathematical benchmarks, e.g., GSM8K, it remains\nunclear whether small to medium-sized open LLMs can achieve similar\nperformance, questioning their reliability. To close this gap, we propose a\npost-training approach leveraging a mixture of opinions (MoO) from weaker\nancillary LLMs to enhance a (relatively) stronger LLM's reasoning. For that,\neach post-training sample is augmented with Chain-of-Thought (CoT) reasoning\nsteps and answers from ancillary LLMs, enabling the main LLM to learn from\ndiverse perspectives. We compare MoO with standard supervised fine-tuning\n(SFT), few-shot prompting, and the Mixture of Agents (MoA) method on\nmathematical reasoning benchmarks. Our results show that incorporating weaker\nLLMs' opinions improves mathematical reasoning by an average of 5%,\nhighlighting the value of diverse perspectives in reasoning tasks.\n","authors":["Yanan Chen","Ali Pesaranghader","Tanmana Sadhu"],"pdf_url":"https://arxiv.org/pdf/2502.19622v1.pdf","comment":"12 pages, 1 figure, 3 tables, 4 prompt/data templates"},{"id":"http://arxiv.org/abs/2502.19614v1","updated":"2025-02-26T23:04:05Z","published":"2025-02-26T23:04:05Z","title":"Is Your Paper Being Reviewed by an LLM? A New Benchmark Dataset and\n  Approach for Detecting AI Text in Peer Review","summary":"  Peer review is a critical process for ensuring the integrity of published\nscientific research. Confidence in this process is predicated on the assumption\nthat experts in the relevant domain give careful consideration to the merits of\nmanuscripts which are submitted for publication. With the recent rapid\nadvancements in large language models (LLMs), a new risk to the peer review\nprocess is that negligent reviewers will rely on LLMs to perform the often time\nconsuming process of reviewing a paper. However, there is a lack of existing\nresources for benchmarking the detectability of AI text in the domain of peer\nreview.\n  To address this deficiency, we introduce a comprehensive dataset containing a\ntotal of 788,984 AI-written peer reviews paired with corresponding human\nreviews, covering 8 years of papers submitted to each of two leading AI\nresearch conferences (ICLR and NeurIPS). We use this new resource to evaluate\nthe ability of 18 existing AI text detection algorithms to distinguish between\npeer reviews written by humans and different state-of-the-art LLMs. Motivated\nby the shortcomings of existing methods, we propose a new detection approach\nwhich surpasses existing methods in the identification of AI written peer\nreviews. Our work reveals the difficulty of identifying AI-generated text at\nthe individual peer review level, highlighting the urgent need for new tools\nand methods to detect this unethical use of generative AI.\n","authors":["Sungduk Yu","Man Luo","Avinash Madusu","Vasudev Lal","Phillip Howard"],"pdf_url":"https://arxiv.org/pdf/2502.19614v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.06205v2","updated":"2025-02-26T23:03:04Z","published":"2024-10-08T17:07:01Z","title":"Round and Round We Go! What makes Rotary Positional Encodings useful?","summary":"  Positional Encodings (PEs) are a critical component of Transformer-based\nLarge Language Models (LLMs), providing the attention mechanism with important\nsequence-position information. One of the most popular types of encoding used\ntoday in LLMs are Rotary Positional Encodings (RoPE), that rotate the queries\nand keys based on their relative distance. A common belief is that RoPE is\nuseful because it helps to decay token dependency as relative distance\nincreases. In this work, we argue that this is unlikely to be the core reason.\nWe study the internals of a trained Gemma 7B model to understand how RoPE is\nbeing used at a mechanical level. We find that Gemma learns to use RoPE to\nconstruct robust \"positional\" attention patterns by exploiting the highest\nfrequencies. We also find that, in general, Gemma greatly prefers to use the\nlowest frequencies of RoPE, which we suspect are used to carry semantic\ninformation. We mathematically prove interesting behaviours of RoPE and conduct\nexperiments to verify our findings, proposing a modification of RoPE that fixes\nsome highlighted issues and improves performance. We believe that this work\nrepresents an interesting step in better understanding PEs in LLMs, which we\nbelieve holds crucial value for scaling LLMs to large sizes and context\nlengths.\n","authors":["Federico Barbero","Alex Vitvitskyi","Christos Perivolaropoulos","Razvan Pascanu","Petar Veličković"],"pdf_url":"https://arxiv.org/pdf/2410.06205v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.19612v1","updated":"2025-02-26T22:59:36Z","published":"2025-02-26T22:59:36Z","title":"Evaluation of Hate Speech Detection Using Large Language Models and\n  Geographical Contextualization","summary":"  The proliferation of hate speech on social media is one of the serious issues\nthat is bringing huge impacts to society: an escalation of violence,\ndiscrimination, and social fragmentation. The problem of detecting hate speech\nis intrinsically multifaceted due to cultural, linguistic, and contextual\ncomplexities and adversarial manipulations. In this study, we systematically\ninvestigate the performance of LLMs on detecting hate speech across\nmultilingual datasets and diverse geographic contexts. Our work presents a new\nevaluation framework in three dimensions: binary classification of hate speech,\ngeography-aware contextual detection, and robustness to adversarially generated\ntext. Using a dataset of 1,000 comments from five diverse regions, we evaluate\nthree state-of-the-art LLMs: Llama2 (13b), Codellama (7b), and DeepSeekCoder\n(6.7b). Codellama had the best binary classification recall with 70.6% and an\nF1-score of 52.18%, whereas DeepSeekCoder had the best performance in\ngeographic sensitivity, correctly detecting 63 out of 265 locations. The tests\nfor adversarial robustness also showed significant weaknesses; Llama2\nmisclassified 62.5% of manipulated samples. These results bring to light the\ntrade-offs between accuracy, contextual understanding, and robustness in the\ncurrent versions of LLMs. This work has thus set the stage for developing\ncontextually aware, multilingual hate speech detection systems by underlining\nkey strengths and limitations, therefore offering actionable insights for\nfuture research and real-world applications.\n","authors":["Anwar Hossain Zahid","Monoshi Kumar Roy","Swarna Das"],"pdf_url":"https://arxiv.org/pdf/2502.19612v1.pdf","comment":"6 pages, 2 figures"},{"id":"http://arxiv.org/abs/2502.19607v1","updated":"2025-02-26T22:45:08Z","published":"2025-02-26T22:45:08Z","title":"Revisiting Word Embeddings in the LLM Era","summary":"  Large Language Models (LLMs) have recently shown remarkable advancement in\nvarious NLP tasks. As such, a popular trend has emerged lately where NLP\nresearchers extract word/sentence/document embeddings from these large\ndecoder-only models and use them for various inference tasks with promising\nresults. However, it is still unclear whether the performance improvement of\nLLM-induced embeddings is merely because of scale or whether underlying\nembeddings they produce significantly differ from classical encoding models\nlike Word2Vec, GloVe, Sentence-BERT (SBERT) or Universal Sentence Encoder\n(USE). This is the central question we investigate in the paper by\nsystematically comparing classical decontextualized and contextualized word\nembeddings with the same for LLM-induced embeddings. Our results show that LLMs\ncluster semantically related words more tightly and perform better on analogy\ntasks in decontextualized settings. However, in contextualized settings,\nclassical models like SimCSE often outperform LLMs in sentence-level similarity\nassessment tasks, highlighting their continued relevance for fine-grained\nsemantics.\n","authors":["Yash Mahajan","Matthew Freestone","Sathyanarayanan Aakur","Santu Karmaker"],"pdf_url":"https://arxiv.org/pdf/2502.19607v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.10227v2","updated":"2025-02-26T22:21:27Z","published":"2024-11-15T14:40:59Z","title":"Entropy and type-token ratio in gigaword corpora","summary":"  There are different ways of measuring diversity in complex systems. In\nparticular, in language, lexical diversity is characterized in terms of the\ntype-token ratio and the word entropy. We here investigate both diversity\nmetrics in six massive linguistic datasets in English, Spanish, and Turkish,\nconsisting of books, news articles, and tweets. These gigaword corpora\ncorrespond to languages with distinct morphological features and differ in\nregisters and genres, thus constituting a varied testbed for a quantitative\napproach to lexical diversity. We unveil an empirical functional relation\nbetween entropy and type-token ratio of texts of a given corpus and language,\nwhich is a consequence of the statistical laws observed in natural language.\nFurther, in the limit of large text lengths we find an analytical expression\nfor this relation relying on both Zipf and Heaps laws that agrees with our\nempirical findings.\n","authors":["Pablo Rosillo-Rodes","Maxi San Miguel","David Sanchez"],"pdf_url":"https://arxiv.org/pdf/2411.10227v2.pdf","comment":"15 pages, 10 figures, 8 tables"},{"id":"http://arxiv.org/abs/2502.19590v1","updated":"2025-02-26T22:11:47Z","published":"2025-02-26T22:11:47Z","title":"A City of Millions: Mapping Literary Social Networks At Scale","summary":"  We release 70,509 high-quality social networks extracted from multilingual\nfiction and nonfiction narratives. We additionally provide metadata for ~30,000\nof these texts (73% nonfiction and 27% fiction) written between 1800 and 1999\nin 58 languages. This dataset provides information on historical social worlds\nat an unprecedented scale, including data for 1,192,855 individuals in\n2,805,482 pair-wise relationships annotated for affinity and relationship type.\nWe achieve this scale by automating previously manual methods of extracting\nsocial networks; specifically, we adapt an existing annotation task as a\nlanguage model prompt, ensuring consistency at scale with the use of structured\noutput. This dataset provides an unprecedented resource for the humanities and\nsocial sciences by providing data on cognitive models of social realities.\n","authors":["Sil Hamilton","Rebecca M. M. Hicke","David Mimno","Matthew Wilkens"],"pdf_url":"https://arxiv.org/pdf/2502.19590v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.14497v2","updated":"2025-02-26T22:10:36Z","published":"2024-06-20T16:59:52Z","title":"CodeRAG-Bench: Can Retrieval Augment Code Generation?","summary":"  While language models (LMs) have proven remarkably adept at generating code,\nmany programs are challenging for LMs to generate using their parametric\nknowledge alone. Providing external contexts such as library documentation can\nfacilitate generating accurate and functional code. Despite the success of\nretrieval-augmented generation (RAG) in various text-oriented tasks, its\npotential for improving code generation remains under-explored. In this work,\nwe conduct a systematic, large-scale analysis by asking: in what scenarios can\nretrieval benefit code generation models? and what challenges remain? We first\ncurate a comprehensive evaluation benchmark, CodeRAG-Bench, encompassing three\ncategories of code generation tasks, including basic programming, open-domain,\nand repository-level problems. We aggregate documents from five sources for\nmodels to retrieve contexts: competition solutions, online tutorials, library\ndocumentation, StackOverflow posts, and GitHub repositories. We examine\ntop-performing models on CodeRAG-Bench by providing contexts retrieved from one\nor multiple sources. While notable gains are made in final code generation by\nretrieving high-quality contexts across various settings, our analysis reveals\nroom for improvement -- current retrievers still struggle to fetch useful\ncontexts especially with limited lexical overlap, and generators fail to\nimprove with limited context lengths or abilities to integrate additional\ncontexts. We hope CodeRAG-Bench serves as an effective testbed to encourage\nfurther development of advanced code-oriented RAG methods.\n","authors":["Zora Zhiruo Wang","Akari Asai","Xinyan Velocity Yu","Frank F. Xu","Yiqing Xie","Graham Neubig","Daniel Fried"],"pdf_url":"https://arxiv.org/pdf/2406.14497v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.19587v1","updated":"2025-02-26T22:00:22Z","published":"2025-02-26T22:00:22Z","title":"NeoBERT: A Next-Generation BERT","summary":"  Recent innovations in architecture, pre-training, and fine-tuning have led to\nthe remarkable in-context learning and reasoning abilities of large\nauto-regressive language models such as LLaMA and DeepSeek. In contrast,\nencoders like BERT and RoBERTa have not seen the same level of progress despite\nbeing foundational for many downstream NLP applications. To bridge this gap, we\nintroduce NeoBERT, a next-generation encoder that redefines the capabilities of\nbidirectional models by integrating state-of-the-art advancements in\narchitecture, modern data, and optimized pre-training methodologies. NeoBERT is\ndesigned for seamless adoption: it serves as a plug-and-play replacement for\nexisting base models, relies on an optimal depth-to-width ratio, and leverages\nan extended context length of 4,096 tokens. Despite its compact 250M parameter\nfootprint, it achieves state-of-the-art results on the massive MTEB benchmark,\noutperforming BERT large, RoBERTa large, NomicBERT, and ModernBERT under\nidentical fine-tuning conditions. In addition, we rigorously evaluate the\nimpact of each modification on GLUE and design a uniform fine-tuning and\nevaluation framework for MTEB. We release all code, data, checkpoints, and\ntraining scripts to accelerate research and real-world adoption.\n","authors":["Lola Le Breton","Quentin Fournier","Mariam El Mezouar","Sarath Chandar"],"pdf_url":"https://arxiv.org/pdf/2502.19587v1.pdf","comment":"19 pages, 5 figures, 9 tables. Submitted to TMLR"},{"id":"http://arxiv.org/abs/2407.06172v3","updated":"2025-02-26T21:53:59Z","published":"2024-07-08T17:48:42Z","title":"On Speeding Up Language Model Evaluation","summary":"  Developing prompt-based methods with Large Language Models (LLMs) requires\nmaking numerous decisions, which give rise to a combinatorial search problem\nover hyper-parameters. This exhaustive evaluation can be time-consuming and\ncostly. In this paper, we propose an $\\textit{adaptive}$ approach to explore\nthis space. We are exploiting the fact that often only few samples are needed\nto identify clearly superior or inferior settings, and that many evaluation\ntests are highly correlated. We lean on multi-armed bandits to sequentially\nidentify the next (method, validation sample)-pair to evaluate and utilize\nlow-rank matrix factorization to fill in missing evaluations. We carefully\nassess the efficacy of our approach on several competitive benchmark problems\nand show that it can identify the top-performing method using only 5-15% of the\ntypical resources -- resulting in 85-95% LLM cost savings. Our code is\navailable at https://github.com/kilian-group/banditeval.\n","authors":["Jin Peng Zhou","Christian K. Belardi","Ruihan Wu","Travis Zhang","Carla P. Gomes","Wen Sun","Kilian Q. Weinberger"],"pdf_url":"https://arxiv.org/pdf/2407.06172v3.pdf","comment":"ICLR 2025"},{"id":"http://arxiv.org/abs/2502.19582v1","updated":"2025-02-26T21:49:54Z","published":"2025-02-26T21:49:54Z","title":"Where Are We? Evaluating LLM Performance on African Languages","summary":"  Africa's rich linguistic heritage remains underrepresented in NLP, largely\ndue to historical policies that favor foreign languages and create significant\ndata inequities. In this paper, we integrate theoretical insights on Africa's\nlanguage landscape with an empirical evaluation using Sahara - a comprehensive\nbenchmark curated from large-scale, publicly accessible datasets capturing the\ncontinent's linguistic diversity. By systematically assessing the performance\nof leading large language models (LLMs) on Sahara, we demonstrate how\npolicy-induced data variations directly impact model effectiveness across\nAfrican languages. Our findings reveal that while a few languages perform\nreasonably well, many Indigenous languages remain marginalized due to sparse\ndata. Leveraging these insights, we offer actionable recommendations for policy\nreforms and inclusive data practices. Overall, our work underscores the urgent\nneed for a dual approach - combining theoretical understanding with empirical\nevaluation - to foster linguistic diversity in AI for African communities.\n","authors":["Ife Adebara","Hawau Olamide Toyin","Nahom Tesfu Ghebremichael","AbdelRahim Elmadany","Muhammad Abdul-Mageed"],"pdf_url":"https://arxiv.org/pdf/2502.19582v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.22446v2","updated":"2025-02-26T21:37:16Z","published":"2024-10-29T18:27:11Z","title":"Do Large Language Models Align with Core Mental Health Counseling\n  Competencies?","summary":"  The rapid evolution of Large Language Models (LLMs) presents a promising\nsolution to the global shortage of mental health professionals. However, their\nalignment with essential counseling competencies remains underexplored. We\nintroduce CounselingBench, a novel NCMHCE-based benchmark evaluating 22\ngeneral-purpose and medical-finetuned LLMs across five key competencies. While\nfrontier models surpass minimum aptitude thresholds, they fall short of\nexpert-level performance, excelling in Intake, Assessment & Diagnosis but\nstruggling with Core Counseling Attributes and Professional Practice & Ethics.\nSurprisingly, medical LLMs do not outperform generalist models in accuracy,\nthough they provide slightly better justifications while making more\ncontext-related errors. These findings highlight the challenges of developing\nAI for mental health counseling, particularly in competencies requiring empathy\nand nuanced reasoning. Our results underscore the need for specialized,\nfine-tuned models aligned with core mental health counseling competencies and\nsupported by human oversight before real-world deployment. Code and data\nassociated with this manuscript can be found at:\nhttps://github.com/cuongnguyenx/CounselingBench\n","authors":["Viet Cuong Nguyen","Mohammad Taher","Dongwan Hong","Vinicius Konkolics Possobom","Vibha Thirunellayi Gopalakrishnan","Ekta Raj","Zihang Li","Heather J. Soled","Michael L. Birnbaum","Srijan Kumar","Munmun De Choudhury"],"pdf_url":"https://arxiv.org/pdf/2410.22446v2.pdf","comment":"10 Pages, Accepted to Findings of NAACL 2025"},{"id":"http://arxiv.org/abs/2502.19573v1","updated":"2025-02-26T21:33:06Z","published":"2025-02-26T21:33:06Z","title":"Do Large Language Models Know How Much They Know?","summary":"  Large Language Models (LLMs) have emerged as highly capable systems and are\nincreasingly being integrated into various uses. However, the rapid pace of\ntheir deployment has outpaced a comprehensive understanding of their internal\nmechanisms and a delineation of their capabilities and limitations. A desired\nattribute of an intelligent system is its ability to recognize the scope of its\nown knowledge. To investigate whether LLMs embody this characteristic, we\ndevelop a benchmark designed to challenge these models to enumerate all\ninformation they possess on specific topics. This benchmark evaluates whether\nthe models recall excessive, insufficient, or the precise amount of\ninformation, thereby indicating their awareness of their own knowledge. Our\nfindings reveal that all tested LLMs, given sufficient scale, demonstrate an\nunderstanding of how much they know about specific topics. While different\narchitectures exhibit varying rates of this capability's emergence, the results\nsuggest that awareness of knowledge may be a generalizable attribute of LLMs.\nFurther research is needed to confirm this potential and fully elucidate the\nunderlying mechanisms.\n","authors":["Gabriele Prato","Jerry Huang","Prasannna Parthasarathi","Shagun Sodhani","Sarath Chandar"],"pdf_url":"https://arxiv.org/pdf/2502.19573v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.19559v1","updated":"2025-02-26T20:54:51Z","published":"2025-02-26T20:54:51Z","title":"Stay Focused: Problem Drift in Multi-Agent Debate","summary":"  Multi-agent debate - multiple instances of large language models discussing\nproblems in turn-based interaction - has shown promise for solving knowledge\nand reasoning tasks. However, these methods show limitations, particularly when\nscaling them to longer reasoning chains. In this study, we unveil a new issue\nof multi-agent debate: discussions drift away from the initial problem over\nmultiple turns. We define this phenomenon as problem drift and quantify its\npresence across ten tasks (i.e., three generative, three knowledge, three\nreasoning, and one instruction-following task). To identify the reasons for\nthis issue, we perform a human study with eight experts on discussions\nsuffering from problem drift, who find the most common issues are a lack of\nprogress (35% of cases), low-quality feedback (26% of cases), and a lack of\nclarity (25% of cases). To systematically address the issue of problem drift,\nwe propose DRIFTJudge, a method based on LLM-as-a-judge, to detect problem\ndrift at test-time. We further propose DRIFTPolicy, a method to mitigate 31% of\nproblem drift cases. Our study can be seen as a first step to understanding a\nkey limitation of multi-agent debate, highlighting pathways for improving their\neffectiveness in the future.\n","authors":["Jonas Becker","Lars Benedikt Kaesberg","Andreas Stephan","Jan Philip Wahle","Terry Ruas","Bela Gipp"],"pdf_url":"https://arxiv.org/pdf/2502.19559v1.pdf","comment":"34 pages, 21 figures, 4 tables, under review"}]},"2025-02-27T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2502.20383v1","updated":"2025-02-27T18:56:26Z","published":"2025-02-27T18:56:26Z","title":"Why Are Web AI Agents More Vulnerable Than Standalone LLMs? A Security\n  Analysis","summary":"  Recent advancements in Web AI agents have demonstrated remarkable\ncapabilities in addressing complex web navigation tasks. However, emerging\nresearch shows that these agents exhibit greater vulnerability compared to\nstandalone Large Language Models (LLMs), despite both being built upon the same\nsafety-aligned models. This discrepancy is particularly concerning given the\ngreater flexibility of Web AI Agent compared to standalone LLMs, which may\nexpose them to a wider range of adversarial user inputs. To build a scaffold\nthat addresses these concerns, this study investigates the underlying factors\nthat contribute to the increased vulnerability of Web AI agents. Notably, this\ndisparity stems from the multifaceted differences between Web AI agents and\nstandalone LLMs, as well as the complex signals - nuances that simple\nevaluation metrics, such as success rate, often fail to capture. To tackle\nthese challenges, we propose a component-level analysis and a more granular,\nsystematic evaluation framework. Through this fine-grained investigation, we\nidentify three critical factors that amplify the vulnerability of Web AI\nagents; (1) embedding user goals into the system prompt, (2) multi-step action\ngeneration, and (3) observational capabilities. Our findings highlights the\npressing need to enhance security and robustness in AI agent design and provide\nactionable insights for targeted defense strategies.\n","authors":["Jeffrey Yang Fan Chiang","Seungjae Lee","Jia-Bin Huang","Furong Huang","Yizheng Chen"],"pdf_url":"https://arxiv.org/pdf/2502.20383v1.pdf","comment":"Project website: http://vulnerable-ai-agents.github.io"},{"id":"http://arxiv.org/abs/2502.20380v1","updated":"2025-02-27T18:55:05Z","published":"2025-02-27T18:55:05Z","title":"Multi-Turn Code Generation Through Single-Step Rewards","summary":"  We address the problem of code generation from multi-turn execution feedback.\nExisting methods either generate code without feedback or use complex,\nhierarchical reinforcement learning to optimize multi-turn rewards. We propose\na simple yet scalable approach, $\\mu$Code, that solves multi-turn code\ngeneration using only single-step rewards. Our key insight is that code\ngeneration is a one-step recoverable MDP, where the correct code can be\nrecovered from any intermediate code state in a single turn. $\\mu$Code\niteratively trains both a generator to provide code solutions conditioned on\nmulti-turn execution feedback and a verifier to score the newly generated code.\nExperimental evaluations show that our approach achieves significant\nimprovements over the state-of-the-art baselines. We provide analysis of the\ndesign choices of the reward models and policy, and show the efficacy of\n$\\mu$Code at utilizing the execution feedback. Our code is available at\nhttps://github.com/portal-cornell/muCode.\n","authors":["Arnav Kumar Jain","Gonzalo Gonzalez-Pumariega","Wayne Chen","Alexander M Rush","Wenting Zhao","Sanjiban Choudhury"],"pdf_url":"https://arxiv.org/pdf/2502.20380v1.pdf","comment":"9 pages (not including references or appendix); 6 figures (in main\n  paper); (v1) preprint"},{"id":"http://arxiv.org/abs/2502.20377v1","updated":"2025-02-27T18:51:22Z","published":"2025-02-27T18:51:22Z","title":"PhantomWiki: On-Demand Datasets for Reasoning and Retrieval Evaluation","summary":"  High-quality benchmarks are essential for evaluating reasoning and retrieval\ncapabilities of large language models (LLMs). However, curating datasets for\nthis purpose is not a permanent solution as they are prone to data leakage and\ninflated performance results. To address these challenges, we propose\nPhantomWiki: a pipeline to generate unique, factually consistent document\ncorpora with diverse question-answer pairs. Unlike prior work, PhantomWiki is\nneither a fixed dataset, nor is it based on any existing data. Instead, a new\nPhantomWiki instance is generated on demand for each evaluation. We vary the\nquestion difficulty and corpus size to disentangle reasoning and retrieval\ncapabilities respectively, and find that PhantomWiki datasets are surprisingly\nchallenging for frontier LLMs. Thus, we contribute a scalable and data\nleakage-resistant framework for disentangled evaluation of reasoning,\nretrieval, and tool-use abilities. Our code is available at\nhttps://github.com/kilian-group/phantom-wiki.\n","authors":["Albert Gong","Kamilė Stankevičiūtė","Chao Wan","Anmol Kabra","Raphael Thesmar","Johann Lee","Julius Klenke","Carla P. Gomes","Kilian Q. Weinberger"],"pdf_url":"https://arxiv.org/pdf/2502.20377v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.20364v1","updated":"2025-02-27T18:35:39Z","published":"2025-02-27T18:35:39Z","title":"Bridging Legal Knowledge and AI: Retrieval-Augmented Generation with\n  Vector Stores, Knowledge Graphs, and Hierarchical Non-negative Matrix\n  Factorization","summary":"  Agentic Generative AI, powered by Large Language Models (LLMs) with\nRetrieval-Augmented Generation (RAG), Knowledge Graphs (KGs), and Vector Stores\n(VSs), represents a transformative technology applicable to specialized domains\nsuch as legal systems, research, recommender systems, cybersecurity, and global\nsecurity, including proliferation research. This technology excels at inferring\nrelationships within vast unstructured or semi-structured datasets. The legal\ndomain here comprises complex data characterized by extensive, interrelated,\nand semi-structured knowledge systems with complex relations. It comprises\nconstitutions, statutes, regulations, and case law. Extracting insights and\nnavigating the intricate networks of legal documents and their relations is\ncrucial for effective legal research. Here, we introduce a generative AI system\nthat integrates RAG, VS, and KG, constructed via Non-Negative Matrix\nFactorization (NMF), to enhance legal information retrieval and AI reasoning\nand minimize hallucinations. In the legal system, these technologies empower AI\nagents to identify and analyze complex connections among cases, statutes, and\nlegal precedents, uncovering hidden relationships and predicting legal\ntrends-challenging tasks that are essential for ensuring justice and improving\noperational efficiency. Our system employs web scraping techniques to\nsystematically collect legal texts, such as statutes, constitutional\nprovisions, and case law, from publicly accessible platforms like Justia. It\nbridges the gap between traditional keyword-based searches and contextual\nunderstanding by leveraging advanced semantic representations, hierarchical\nrelationships, and latent topic discovery. This framework supports legal\ndocument clustering, summarization, and cross-referencing, for scalable,\ninterpretable, and accurate retrieval for semi-structured data while advancing\ncomputational law and AI.\n","authors":["Ryan C. Barron","Maksim E. Eren","Olga M. Serafimova","Cynthia Matuszek","Boian S. Alexandrov"],"pdf_url":"https://arxiv.org/pdf/2502.20364v1.pdf","comment":"10 pages, 6 figures, 5 tables"},{"id":"http://arxiv.org/abs/2502.20356v1","updated":"2025-02-27T18:29:09Z","published":"2025-02-27T18:29:09Z","title":"Bridging the Creativity Understanding Gap: Small-Scale Human Alignment\n  Enables Expert-Level Humor Ranking in LLMs","summary":"  Large Language Models (LLMs) have shown significant limitations in\nunderstanding creative content, as demonstrated by Hessel et al. (2023)'s\ninfluential work on the New Yorker Cartoon Caption Contest (NYCCC). Their study\nexposed a substantial gap between LLMs and humans in humor comprehension,\nestablishing that understanding and evaluating creative content is key\nchallenge in AI development. We revisit this challenge by decomposing humor\nunderstanding into three components and systematically improve each: enhancing\nvisual understanding through improved annotation, utilizing LLM-generated humor\nreasoning and explanations, and implementing targeted alignment with human\npreference data. Our refined approach achieves 82.4% accuracy in caption\nranking, singificantly improving upon the previous 67% benchmark and matching\nthe performance of world-renowned human experts in this domain. Notably, while\nattempts to mimic subgroup preferences through various persona prompts showed\nminimal impact, model finetuning with crowd preferences proved remarkably\neffective. These findings reveal that LLM limitations in creative judgment can\nbe effectively addressed through focused alignment to specific subgroups and\nindividuals. Lastly, we propose the position that achieving artificial general\nintelligence necessitates systematic collection of human preference data across\ncreative domains. We advocate that just as human creativity is deeply\ninfluenced by individual and cultural preferences, training LLMs with diverse\nhuman preference data may be essential for developing true creative\nunderstanding.\n","authors":["Kuan Lok Zhou","Jiayi Chen","Siddharth Suresh","Reuben Narad","Timothy T. Rogers","Lalit K Jain","Robert D Nowak","Bob Mankoff","Jifan Zhang"],"pdf_url":"https://arxiv.org/pdf/2502.20356v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.20354v1","updated":"2025-02-27T18:27:30Z","published":"2025-02-27T18:27:30Z","title":"Towards Responsible AI in Education: Hybrid Recommendation System for\n  K-12 Students Case Study","summary":"  The growth of Educational Technology (EdTech) has enabled highly personalized\nlearning experiences through Artificial Intelligence (AI)-based recommendation\nsystems tailored to each student needs. However, these systems can\nunintentionally introduce biases, potentially limiting fair access to learning\nresources. This study presents a recommendation system for K-12 students,\ncombining graph-based modeling and matrix factorization to provide personalized\nsuggestions for extracurricular activities, learning resources, and\nvolunteering opportunities. To address fairness concerns, the system includes a\nframework to detect and reduce biases by analyzing feedback across protected\nstudent groups. This work highlights the need for continuous monitoring in\neducational recommendation systems to support equitable, transparent, and\neffective learning opportunities for all students.\n","authors":["Nazarii Drushchak","Vladyslava Tyshchenko","Nataliya Polyakovska"],"pdf_url":"https://arxiv.org/pdf/2502.20354v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.20350v1","updated":"2025-02-27T18:22:33Z","published":"2025-02-27T18:22:33Z","title":"KEDRec-LM: A Knowledge-distilled Explainable Drug Recommendation Large\n  Language Model","summary":"  Drug discovery is a critical task in biomedical natural language processing\n(NLP), yet explainable drug discovery remains underexplored. Meanwhile, large\nlanguage models (LLMs) have shown remarkable abilities in natural language\nunderstanding and generation. Leveraging LLMs for explainable drug discovery\nhas the potential to improve downstream tasks and real-world applications. In\nthis study, we utilize open-source drug knowledge graphs, clinical trial data,\nand PubMed publications to construct a comprehensive dataset for the\nexplainable drug discovery task, named \\textbf{expRxRec}. Furthermore, we\nintroduce \\textbf{KEDRec-LM}, an instruction-tuned LLM which distills knowledge\nfrom rich medical knowledge corpus for drug recommendation and rationale\ngeneration. To encourage further research in this area, we will publicly\nrelease\\footnote{A copy is attached with this submission} both the dataset and\nKEDRec-LM.\n","authors":["Kai Zhang","Rui Zhu","Shutian Ma","Jingwei Xiong","Yejin Kim","Fabricio Murai","Xiaozhong Liu"],"pdf_url":"https://arxiv.org/pdf/2502.20350v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.20344v1","updated":"2025-02-27T18:16:47Z","published":"2025-02-27T18:16:47Z","title":"Sparse Auto-Encoder Interprets Linguistic Features in Large Language\n  Models","summary":"  Large language models (LLMs) excel in tasks that require complex linguistic\nabilities, such as reference disambiguation and metaphor\nrecognition/generation. Although LLMs possess impressive capabilities, their\ninternal mechanisms for processing and representing linguistic knowledge remain\nlargely opaque. Previous work on linguistic mechanisms has been limited by\ncoarse granularity, insufficient causal analysis, and a narrow focus. In this\nstudy, we present a systematic and comprehensive causal investigation using\nsparse auto-encoders (SAEs). We extract a wide range of linguistic features\nfrom six dimensions: phonetics, phonology, morphology, syntax, semantics, and\npragmatics. We extract, evaluate, and intervene on these features by\nconstructing minimal contrast datasets and counterfactual sentence datasets. We\nintroduce two indices-Feature Representation Confidence (FRC) and Feature\nIntervention Confidence (FIC)-to measure the ability of linguistic features to\ncapture and control linguistic phenomena. Our results reveal inherent\nrepresentations of linguistic knowledge in LLMs and demonstrate the potential\nfor controlling model outputs. This work provides strong evidence that LLMs\npossess genuine linguistic knowledge and lays the foundation for more\ninterpretable and controllable language modeling in future research.\n","authors":["Yi Jing","Zijun Yao","Lingxu Ran","Hongzhu Guo","Xiaozhi Wang","Lei Hou","Juanzi Li"],"pdf_url":"https://arxiv.org/pdf/2502.20344v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.20339v1","updated":"2025-02-27T18:08:16Z","published":"2025-02-27T18:08:16Z","title":"Thinking Slow, Fast: Scaling Inference Compute with Distilled Reasoners","summary":"  Recent advancements have demonstrated that the performance of large language\nmodels (LLMs) can be significantly enhanced by scaling computational resources\nat test time. A common strategy involves generating multiple Chain-of-Thought\n(CoT) trajectories and aggregating their outputs through various selection\nmechanisms. This raises a fundamental question: can models with lower\ncomplexity leverage their superior generation throughput to outperform\nsimilarly sized Transformers for a fixed computational budget? To address this\nquestion and overcome the lack of strong subquadratic reasoners, we distill\npure and hybrid Mamba models from pretrained Transformers. Trained on only 8\nbillion tokens, our distilled models show strong performance and scaling on\nmathematical reasoning datasets while being much faster at inference for large\nbatches and long sequences. Despite the zero-shot performance hit due to\ndistillation, both pure and hybrid Mamba models can scale their coverage and\naccuracy performance past their Transformer teacher models under fixed time\nbudgets, opening a new direction for scaling inference compute.\n","authors":["Daniele Paliotta","Junxiong Wang","Matteo Pagliardini","Kevin Y. Li","Aviv Bick","J. Zico Kolter","Albert Gu","François Fleuret","Tri Dao"],"pdf_url":"https://arxiv.org/pdf/2502.20339v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.20335v1","updated":"2025-02-27T18:05:15Z","published":"2025-02-27T18:05:15Z","title":"Expertise Is What We Want","summary":"  Clinical decision-making depends on expert reasoning, which is guided by\nstandardized, evidence-based guidelines. However, translating these guidelines\ninto automated clinical decision support systems risks inaccuracy and\nimportantly, loss of nuance. We share an application architecture, the Large\nLanguage Expert (LLE), that combines the flexibility and power of Large\nLanguage Models (LLMs) with the interpretability, explainability, and\nreliability of Expert Systems. LLMs help address key challenges of Expert\nSystems, such as integrating and codifying knowledge, and data normalization.\nConversely, an Expert System-like approach helps overcome challenges with LLMs,\nincluding hallucinations, atomic and inexpensive updates, and testability.\n  To highlight the power of the Large Language Expert (LLE) system, we built an\nLLE to assist with the workup of patients newly diagnosed with cancer. Timely\ninitiation of cancer treatment is critical for optimal patient outcomes.\nHowever, increasing complexity in diagnostic recommendations has made it\ndifficult for primary care physicians to ensure their patients have completed\nthe necessary workup before their first visit with an oncologist. As with many\nreal-world clinical tasks, these workups require the analysis of unstructured\nhealth records and the application of nuanced clinical decision logic. In this\nstudy, we describe the design & evaluation of an LLE system built to rapidly\nidentify and suggest the correct diagnostic workup. The system demonstrated a\nhigh degree of clinical-level accuracy (>95%) and effectively addressed gaps\nidentified in real-world data from breast and colon cancer patients at a large\nacademic center.\n","authors":["Alan Ashworth","Munir Al-Dajani","Keegan Duchicela","Kiril Kafadarov","Allison Kurian","Othman Laraki","Amina Lazrak","Divneet Mandair","Wendy McKennon","Rebecca Miksad","Jayodita Sanghvi","Travis Zack"],"pdf_url":"https://arxiv.org/pdf/2502.20335v1.pdf","comment":"18 pages, 7 figures, 5 tables"},{"id":"http://arxiv.org/abs/2502.20332v1","updated":"2025-02-27T18:02:15Z","published":"2025-02-27T18:02:15Z","title":"Emergent Symbolic Mechanisms Support Abstract Reasoning in Large\n  Language Models","summary":"  Many recent studies have found evidence for emergent reasoning capabilities\nin large language models, but debate persists concerning the robustness of\nthese capabilities, and the extent to which they depend on structured reasoning\nmechanisms. To shed light on these issues, we perform a comprehensive study of\nthe internal mechanisms that support abstract rule induction in an open-source\nlanguage model (Llama3-70B). We identify an emergent symbolic architecture that\nimplements abstract reasoning via a series of three computations. In early\nlayers, symbol abstraction heads convert input tokens to abstract variables\nbased on the relations between those tokens. In intermediate layers, symbolic\ninduction heads perform sequence induction over these abstract variables.\nFinally, in later layers, retrieval heads predict the next token by retrieving\nthe value associated with the predicted abstract variable. These results point\ntoward a resolution of the longstanding debate between symbolic and neural\nnetwork approaches, suggesting that emergent reasoning in neural networks\ndepends on the emergence of symbolic mechanisms.\n","authors":["Yukang Yang","Declan Campbell","Kaixuan Huang","Mengdi Wang","Jonathan Cohen","Taylor Webb"],"pdf_url":"https://arxiv.org/pdf/2502.20332v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.20330v1","updated":"2025-02-27T17:59:36Z","published":"2025-02-27T17:59:36Z","title":"Long-Context Inference with Retrieval-Augmented Speculative Decoding","summary":"  The emergence of long-context large language models (LLMs) offers a promising\nalternative to traditional retrieval-augmented generation (RAG) for processing\nextensive documents. However, the computational overhead of long-context\ninference, particularly in managing key-value (KV) caches, presents significant\nefficiency challenges. While Speculative Decoding (SD) traditionally\naccelerates inference using smaller draft models, its effectiveness diminishes\nsubstantially in long-context scenarios due to memory-bound KV cache\noperations. We present Retrieval-Augmented Speculative Decoding (RAPID), which\nleverages RAG for both accelerating and enhancing generation quality in\nlong-context inference. RAPID introduces the RAG drafter-a draft LLM operating\non shortened retrieval contexts-to speculate on the generation of long-context\ntarget LLMs. Our approach enables a new paradigm where same-scale or even\nlarger LLMs can serve as RAG drafters while maintaining computational\nefficiency. To fully leverage the potentially superior capabilities from\nstronger RAG drafters, we develop an inference-time knowledge transfer dynamic\nthat enriches the target distribution by RAG. Extensive experiments on the\nLLaMA-3.1 and Qwen2.5 backbones demonstrate that RAPID effectively integrates\nthe strengths of both approaches, achieving significant performance\nimprovements (e.g., from 39.33 to 42.83 on InfiniteBench for LLaMA-3.1-8B) with\nmore than 2x speedups. Our analyses reveal that RAPID achieves robust\nacceleration beyond 32K context length and demonstrates superior generation\nquality in real-world applications.\n","authors":["Guanzheng Chen","Qilong Feng","Jinjie Ni","Xin Li","Michael Qizhe Shieh"],"pdf_url":"https://arxiv.org/pdf/2502.20330v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.00075v4","updated":"2025-02-27T17:49:33Z","published":"2024-06-21T19:18:16Z","title":"Logicbreaks: A Framework for Understanding Subversion of Rule-based\n  Inference","summary":"  We study how to subvert large language models (LLMs) from following\nprompt-specified rules. We first formalize rule-following as inference in\npropositional Horn logic, a mathematical system in which rules have the form\n\"if $P$ and $Q$, then $R$\" for some propositions $P$, $Q$, and $R$. Next, we\nprove that although small transformers can faithfully follow such rules,\nmaliciously crafted prompts can still mislead both theoretical constructions\nand models learned from data. Furthermore, we demonstrate that popular attack\nalgorithms on LLMs find adversarial prompts and induce attention patterns that\nalign with our theory. Our novel logic-based framework provides a foundation\nfor studying LLMs in rule-based settings, enabling a formal analysis of tasks\nlike logical reasoning and jailbreak attacks.\n","authors":["Anton Xue","Avishree Khare","Rajeev Alur","Surbhi Goel","Eric Wong"],"pdf_url":"https://arxiv.org/pdf/2407.00075v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.20315v1","updated":"2025-02-27T17:41:49Z","published":"2025-02-27T17:41:49Z","title":"LangProBe: a Language Programs Benchmark","summary":"  Composing language models (LMs) into multi-step language programs and\nautomatically optimizing their modular prompts is now a mainstream paradigm for\nbuilding AI systems, but the tradeoffs in this space have only scarcely been\nstudied before. We introduce LangProBe, the first large-scale benchmark for\nevaluating the architectures and optimization strategies for language programs,\nwith over 2000 combinations of tasks, architectures, optimizers, and choices of\nLMs. Using LangProBe, we are the first to study the impact of program\narchitectures and optimizers (and their compositions together and with\ndifferent models) on tradeoffs of quality and cost. We find that optimized\nlanguage programs offer strong cost--quality Pareto improvement over raw calls\nto models, but simultaneously demonstrate that human judgment (or empirical\ndecisions) about which compositions to pursue is still necessary for best\nperformance. We will open source the code and evaluation data for LangProBe.\n","authors":["Shangyin Tan","Lakshya A Agrawal","Arnav Singhvi","Liheng Lai","Michael J Ryan","Dan Klein","Omar Khattab","Koushik Sen","Matei Zaharia"],"pdf_url":"https://arxiv.org/pdf/2502.20315v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.20301v1","updated":"2025-02-27T17:29:46Z","published":"2025-02-27T17:29:46Z","title":"M^3Builder: A Multi-Agent System for Automated Machine Learning in\n  Medical Imaging","summary":"  Agentic AI systems have gained significant attention for their ability to\nautonomously perform complex tasks. However, their reliance on well-prepared\ntools limits their applicability in the medical domain, which requires to train\nspecialized models. In this paper, we make three contributions: (i) We present\nM3Builder, a novel multi-agent system designed to automate machine learning\n(ML) in medical imaging. At its core, M3Builder employs four specialized agents\nthat collaborate to tackle complex, multi-step medical ML workflows, from\nautomated data processing and environment configuration to self-contained auto\ndebugging and model training. These agents operate within a medical imaging ML\nworkspace, a structured environment designed to provide agents with free-text\ndescriptions of datasets, training codes, and interaction tools, enabling\nseamless communication and task execution. (ii) To evaluate progress in\nautomated medical imaging ML, we propose M3Bench, a benchmark comprising four\ngeneral tasks on 14 training datasets, across five anatomies and three imaging\nmodalities, covering both 2D and 3D data. (iii) We experiment with seven\nstate-of-the-art large language models serving as agent cores for our system,\nsuch as Claude series, GPT-4o, and DeepSeek-V3. Compared to existing ML agentic\ndesigns, M3Builder shows superior performance on completing ML tasks in medical\nimaging, achieving a 94.29% success rate using Claude-3.7-Sonnet as the agent\ncore, showing huge potential towards fully automated machine learning in\nmedical imaging.\n","authors":["Jinghao Feng","Qiaoyu Zheng","Chaoyi Wu","Ziheng Zhao","Ya Zhang","Yanfeng Wang","Weidi Xie"],"pdf_url":"https://arxiv.org/pdf/2502.20301v1.pdf","comment":"38 pages, 7 figures"},{"id":"http://arxiv.org/abs/2502.20299v1","updated":"2025-02-27T17:26:56Z","published":"2025-02-27T17:26:56Z","title":"An exploration of features to improve the generalisability of fake news\n  detection models","summary":"  Fake news poses global risks by influencing elections and spreading\nmisinformation, making detection critical. Existing NLP and supervised Machine\nLearning methods perform well under cross-validation but struggle to generalise\nacross datasets, even within the same domain. This issue stems from coarsely\nlabelled training data, where articles are labelled based on their publisher,\nintroducing biases that token-based models like TF-IDF and BERT are sensitive\nto. While Large Language Models (LLMs) offer promise, their application in fake\nnews detection remains limited. This study demonstrates that meaningful\nfeatures can still be extracted from coarsely labelled data to improve\nreal-world robustness. Stylistic features-lexical, syntactic, and semantic-are\nexplored due to their reduced sensitivity to dataset biases. Additionally,\nnovel social-monetisation features are introduced, capturing economic\nincentives behind fake news, such as advertisements, external links, and social\nmedia elements. The study trains on the coarsely labelled NELA 2020-21 dataset\nand evaluates using the manually labelled Facebook URLs dataset, a gold\nstandard for generalisability. Results highlight the limitations of token-based\nmodels trained on biased data and contribute to the scarce evidence on LLMs\nlike LLaMa in this field. Findings indicate that stylistic and\nsocial-monetisation features offer more generalisable predictions than\ntoken-based methods and LLMs. Statistical and permutation feature importance\nanalyses further reveal their potential to enhance performance and mitigate\ndataset biases, providing a path forward for improving fake news detection.\n","authors":["Nathaniel Hoy","Theodora Koulouri"],"pdf_url":"https://arxiv.org/pdf/2502.20299v1.pdf","comment":"Accepted at Expert Systems with Applications (Elsevier)"},{"id":"http://arxiv.org/abs/2502.20273v1","updated":"2025-02-27T17:01:23Z","published":"2025-02-27T17:01:23Z","title":"How Much is Enough? The Diminishing Returns of Tokenization Training\n  Data","summary":"  Tokenization, a crucial initial step in natural language processing, is often\nassumed to benefit from larger training datasets. This paper investigates the\nimpact of tokenizer training data sizes ranging from 1GB to 900GB. Our findings\nreveal diminishing returns as the data size increases, highlighting a practical\nlimit on how much further scaling the training data can improve tokenization\nquality. We analyze this phenomenon and attribute the saturation effect to the\nconstraints imposed by the pre-tokenization stage of tokenization. These\nresults offer valuable insights for optimizing the tokenization process and\nhighlight potential avenues for future research in tokenization algorithms.\n","authors":["Varshini Reddy","Craig W. Schmidt","Yuval Pinter","Chris Tanner"],"pdf_url":"https://arxiv.org/pdf/2502.20273v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.14739v2","updated":"2025-02-27T17:01:09Z","published":"2025-02-20T17:05:58Z","title":"SuperGPQA: Scaling LLM Evaluation across 285 Graduate Disciplines","summary":"  Large language models (LLMs) have demonstrated remarkable proficiency in\nmainstream academic disciplines such as mathematics, physics, and computer\nscience. However, human knowledge encompasses over 200 specialized disciplines,\nfar exceeding the scope of existing benchmarks. The capabilities of LLMs in\nmany of these specialized fields-particularly in light industry, agriculture,\nand service-oriented disciplines-remain inadequately evaluated. To address this\ngap, we present SuperGPQA, a comprehensive benchmark that evaluates\ngraduate-level knowledge and reasoning capabilities across 285 disciplines. Our\nbenchmark employs a novel Human-LLM collaborative filtering mechanism to\neliminate trivial or ambiguous questions through iterative refinement based on\nboth LLM responses and expert feedback. Our experimental results reveal\nsignificant room for improvement in the performance of current state-of-the-art\nLLMs across diverse knowledge domains (e.g., the reasoning-focused model\nDeepSeek-R1 achieved the highest accuracy of 61.82% on SuperGPQA), highlighting\nthe considerable gap between current model capabilities and artificial general\nintelligence. Additionally, we present comprehensive insights from our\nmanagement of a large-scale annotation process, involving over 80 expert\nannotators and an interactive Human-LLM collaborative system, offering valuable\nmethodological guidance for future research initiatives of comparable scope.\n","authors":["M-A-P Team","Xinrun Du","Yifan Yao","Kaijing Ma","Bingli Wang","Tianyu Zheng","Kang Zhu","Minghao Liu","Yiming Liang","Xiaolong Jin","Zhenlin Wei","Chujie Zheng","Kaixin Deng","Shian Jia","Sichao Jiang","Yiyan Liao","Rui Li","Qinrui Li","Sirun Li","Yizhi Li","Yunwen Li","Dehua Ma","Yuansheng Ni","Haoran Que","Qiyao Wang","Zhoufutu Wen","Siwei Wu","Tianshun Xing","Ming Xu","Zhenzhu Yang","Zekun Moore Wang","Junting Zhou","Yuelin Bai","Xingyuan Bu","Chenglin Cai","Liang Chen","Yifan Chen","Chengtuo Cheng","Tianhao Cheng","Keyi Ding","Siming Huang","Yun Huang","Yaoru Li","Yizhe Li","Zhaoqun Li","Tianhao Liang","Chengdong Lin","Hongquan Lin","Yinghao Ma","Tianyang Pang","Zhongyuan Peng","Zifan Peng","Qige Qi","Shi Qiu","Xingwei Qu","Shanghaoran Quan","Yizhou Tan","Zili Wang","Chenqing Wang","Hao Wang","Yiya Wang","Yubo Wang","Jiajun Xu","Kexin Yang","Ruibin Yuan","Yuanhao Yue","Tianyang Zhan","Chun Zhang","Jinyang Zhang","Xiyue Zhang","Xingjian Zhang","Yue Zhang","Yongchi Zhao","Xiangyu Zheng","Chenghua Zhong","Yang Gao","Zhoujun Li","Dayiheng Liu","Qian Liu","Tianyu Liu","Shiwen Ni","Junran Peng","Yujia Qin","Wenbo Su","Guoyin Wang","Shi Wang","Jian Yang","Min Yang","Meng Cao","Xiang Yue","Zhaoxiang Zhang","Wangchunshu Zhou","Jiaheng Liu","Qunshu Lin","Wenhao Huang","Ge Zhang"],"pdf_url":"https://arxiv.org/pdf/2502.14739v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.02408v2","updated":"2025-02-27T16:53:40Z","published":"2024-10-18T15:22:07Z","title":"AI on My Shoulder: Supporting Emotional Labor in Front-Office Roles with\n  an LLM-based Empathetic Coworker","summary":"  Client-Service Representatives (CSRs) are vital to organizations. Frequent\ninteractions with disgruntled clients, however, disrupt their mental\nwell-being. To help CSRs regulate their emotions while interacting with uncivil\nclients, we designed Care-Pilot, an LLM-powered assistant, and evaluated its\nefficacy, perception, and use. Our comparative analyses between 665 human and\nCare-Pilot-generated support messages highlight Care-Pilot's ability to adapt\nto and demonstrate empathy in various incivility incidents. Additionally, 143\nCSRs assessed Care-Pilot's empathy as more sincere and actionable than human\nmessages. Finally, we interviewed 20 CSRs who interacted with Care-Pilot in a\nsimulation exercise. They reported that Care-Pilot helped them avoid negative\nthinking, recenter thoughts, and humanize clients; showing potential for\nbridging gaps in coworker support. Yet, they also noted deployment challenges\nand emphasized the indispensability of shared experiences. We discuss future\ndesigns and societal implications of AI-mediated emotional labor, underscoring\nempathy as a critical function for AI assistants for worker mental health.\n","authors":["Vedant Das Swain","Qiuyue \"Joy\" Zhong","Jash Rajesh Parekh","Yechan Jeon","Roy Zimmermann","Mary Czerwinski","Jina Suh","Varun Mishra","Koustuv Saha","Javier Hernandez"],"pdf_url":"https://arxiv.org/pdf/2411.02408v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.15291v2","updated":"2025-02-27T16:47:40Z","published":"2024-12-19T07:10:51Z","title":"A Large-Scale Simulation on Large Language Models for Decision-Making in\n  Political Science","summary":"  While LLMs have demonstrated remarkable capabilities in text generation and\nreasoning, their ability to simulate human decision-making -- particularly in\npolitical contexts -- remains an open question. However, modeling voter\nbehavior presents unique challenges due to limited voter-level data, evolving\npolitical landscapes, and the complexity of human reasoning. In this study, we\ndevelop a theory-driven, multi-step reasoning framework that integrates\ndemographic, temporal and ideological factors to simulate voter decision-making\nat scale. Using synthetic personas calibrated to real-world voter data, we\nconduct large-scale simulations of recent U.S. presidential elections. Our\nmethod significantly improves simulation accuracy while mitigating model\nbiases. We examine its robustness by comparing performance across different\nLLMs. We further investigate the challenges and constraints that arise from\nLLM-based political simulations. Our work provides both a scalable framework\nfor modeling political decision-making behavior and insights into the promise\nand limitations of using LLMs in political science research.\n","authors":["Chenxiao Yu","Jinyi Ye","Yuangang Li","Zhaotian Weng","Zheng Li","Emilio Ferrara","Xiyang Hu","Yue Zhao"],"pdf_url":"https://arxiv.org/pdf/2412.15291v2.pdf","comment":"arXiv admin note: substantial text overlap with arXiv:2411.03321 This\n  version adds a new model to our experimental setup, modifies the paper's main\n  discussion, and updates the authorship list"},{"id":"http://arxiv.org/abs/2502.20258v1","updated":"2025-02-27T16:46:23Z","published":"2025-02-27T16:46:23Z","title":"LLM as a Broken Telephone: Iterative Generation Distorts Information","summary":"  As large language models are increasingly responsible for online content,\nconcerns arise about the impact of repeatedly processing their own outputs.\nInspired by the \"broken telephone\" effect in chained human communication, this\nstudy investigates whether LLMs similarly distort information through iterative\ngeneration. Through translation-based experiments, we find that distortion\naccumulates over time, influenced by language choice and chain complexity.\nWhile degradation is inevitable, it can be mitigated through strategic\nprompting techniques. These findings contribute to discussions on the long-term\neffects of AI-mediated information propagation, raising important questions\nabout the reliability of LLM-generated content in iterative workflows.\n","authors":["Amr Mohamed","Mingmeng Geng","Michalis Vazirgiannis","Guokan Shang"],"pdf_url":"https://arxiv.org/pdf/2502.20258v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.17787v2","updated":"2025-02-27T16:42:10Z","published":"2025-02-25T02:39:57Z","title":"AIR: Complex Instruction Generation via Automatic Iterative Refinement","summary":"  With the development of large language models, their ability to follow simple\ninstructions has significantly improved. However, adhering to complex\ninstructions remains a major challenge. Current approaches to generating\ncomplex instructions are often irrelevant to the current instruction\nrequirements or suffer from limited scalability and diversity. Moreover,\nmethods such as back-translation, while effective for simple instruction\ngeneration, fail to leverage the rich contents and structures in large web\ncorpora. In this paper, we propose a novel automatic iterative refinement\nframework to generate complex instructions with constraints, which not only\nbetter reflects the requirements of real scenarios but also significantly\nenhances LLMs' ability to follow complex instructions. The AIR framework\nconsists of two stages: (1)Generate an initial instruction from a document;\n(2)Iteratively refine instructions with LLM-as-judge guidance by comparing the\nmodel's output with the document to incorporate valuable constraints. Finally,\nwe construct the AIR-10K dataset with 10K complex instructions and demonstrate\nthat instructions generated with our approach significantly improve the model's\nability to follow complex instructions, outperforming existing methods for\ninstruction generation.\n","authors":["Wei Liu","Yancheng He","Hui Huang","Chengwei Hu","Jiaheng Liu","Shilong Li","Wenbo Su","Bo Zheng"],"pdf_url":"https://arxiv.org/pdf/2502.17787v2.pdf","comment":"The first three authors contributed equally, 20 pages"},{"id":"http://arxiv.org/abs/2502.19361v2","updated":"2025-02-27T16:34:04Z","published":"2025-02-26T17:59:27Z","title":"Can Large Language Models Detect Errors in Long Chain-of-Thought\n  Reasoning?","summary":"  Recently, o1-like models have drawn significant attention, where these models\nproduce the long Chain-of-Thought (CoT) reasoning steps to improve the\nreasoning abilities of existing Large Language Models (LLMs). In this paper, to\nunderstand the qualities of these long CoTs and measure the critique abilities\nof existing LLMs on these long CoTs, we introduce the DeltaBench, including the\ngenerated long CoTs from different o1-like models (e.g., QwQ, DeepSeek-R1) for\ndifferent reasoning tasks (e.g., Math, Code, General Reasoning), to measure the\nability to detect errors in long CoT reasoning. Based on DeltaBench, we first\nperform fine-grained analysis of the generated long CoTs to discover the\neffectiveness and efficiency of different o1-like models. Then, we conduct\nextensive evaluations of existing process reward models (PRMs) and critic\nmodels to detect the errors of each annotated process, which aims to\ninvestigate the boundaries and limitations of existing PRMs and critic models.\nFinally, we hope that DeltaBench could guide developers to better understand\nthe long CoT reasoning abilities of their models.\n","authors":["Yancheng He","Shilong Li","Jiaheng Liu","Weixun Wang","Xingyuan Bu","Ge Zhang","Zhongyuan Peng","Zhaoxiang Zhang","Zhicheng Zheng","Wenbo Su","Bo Zheng"],"pdf_url":"https://arxiv.org/pdf/2502.19361v2.pdf","comment":"The first four authors contributed equally, 27 pages"},{"id":"http://arxiv.org/abs/2406.18696v2","updated":"2025-02-27T16:33:30Z","published":"2024-06-26T18:58:23Z","title":"Sequence Graph Network for Online Debate Analysis","summary":"  Online debates involve a dynamic exchange of ideas over time, where\nparticipants need to actively consider their opponents' arguments, respond with\ncounterarguments, reinforce their own points, and introduce more compelling\narguments as the discussion unfolds. Modeling such a complex process is not a\nsimple task, as it necessitates the incorporation of both sequential\ncharacteristics and the capability to capture interactions effectively. To\naddress this challenge, we employ a sequence-graph approach. Building the\nconversation as a graph allows us to effectively model interactions between\nparticipants through directed edges. Simultaneously, the propagation of\ninformation along these edges in a sequential manner enables us to capture a\nmore comprehensive representation of context. We also introduce a Sequence\nGraph Attention layer to illustrate the proposed information update scheme. The\nexperimental results show that sequence graph networks achieve superior results\nto existing methods in online debates.\n","authors":["Quan Mai","Susan Gauch","Douglas Adams","Miaoqing Huang"],"pdf_url":"https://arxiv.org/pdf/2406.18696v2.pdf","comment":"8 pages, 4 figures"},{"id":"http://arxiv.org/abs/2409.13156v2","updated":"2025-02-27T16:30:42Z","published":"2024-09-20T01:46:07Z","title":"RRM: Robust Reward Model Training Mitigates Reward Hacking","summary":"  Reward models (RMs) play a pivotal role in aligning large language models\n(LLMs) with human preferences. However, traditional RM training, which relies\non response pairs tied to specific prompts, struggles to disentangle\nprompt-driven preferences from prompt-independent artifacts, such as response\nlength and format. In this work, we expose a fundamental limitation of current\nRM training methods, where RMs fail to effectively distinguish between\ncontextual signals and irrelevant artifacts when determining preferences. To\naddress this, we introduce a causal framework that learns preferences\nindependent of these artifacts and propose a novel data augmentation technique\ndesigned to eliminate them. Extensive experiments show that our approach\nsuccessfully filters out undesirable artifacts, yielding a more robust reward\nmodel (RRM). Our RRM improves the performance of a pairwise reward model\ntrained on Gemma-2-9b-it, on RewardBench, increasing accuracy from 80.61% to\n84.15%. Additionally, we train two DPO policies using both the RM and RRM,\ndemonstrating that the RRM significantly enhances DPO-aligned policies,\nimproving MT-Bench scores from 7.27 to 8.31 and length-controlled win-rates in\nAlpacaEval-2 from 33.46% to 52.49%.\n","authors":["Tianqi Liu","Wei Xiong","Jie Ren","Lichang Chen","Junru Wu","Rishabh Joshi","Yang Gao","Jiaming Shen","Zhen Qin","Tianhe Yu","Daniel Sohn","Anastasiia Makarova","Jeremiah Liu","Yuan Liu","Bilal Piot","Abe Ittycheriah","Aviral Kumar","Mohammad Saleh"],"pdf_url":"https://arxiv.org/pdf/2409.13156v2.pdf","comment":"Accepted in ICLR 2025"},{"id":"http://arxiv.org/abs/2502.20246v1","updated":"2025-02-27T16:30:00Z","published":"2025-02-27T16:30:00Z","title":"Beyond Natural Language Perplexity: Detecting Dead Code Poisoning in\n  Code Generation Datasets","summary":"  The increasing adoption of large language models (LLMs) for code-related\ntasks has raised concerns about the security of their training datasets. One\ncritical threat is dead code poisoning, where syntactically valid but\nfunctionally redundant code is injected into training data to manipulate model\nbehavior. Such attacks can degrade the performance of neural code search\nsystems, leading to biased or insecure code suggestions. Existing detection\nmethods, such as token-level perplexity analysis, fail to effectively identify\ndead code due to the structural and contextual characteristics of programming\nlanguages. In this paper, we propose DePA (Dead Code Perplexity Analysis), a\nnovel line-level detection and cleansing method tailored to the structural\nproperties of code. DePA computes line-level perplexity by leveraging the\ncontextual relationships between code lines and identifies anomalous lines by\ncomparing their perplexity to the overall distribution within the file. Our\nexperiments on benchmark datasets demonstrate that DePA significantly\noutperforms existing methods, achieving 0.14-0.19 improvement in detection\nF1-score and a 44-65% increase in poisoned segment localization precision.\nFurthermore, DePA enhances detection speed by 0.62-23x, making it practical for\nlarge-scale dataset cleansing. Overall, by addressing the unique challenges of\ndead code poisoning, DePA provides a robust and efficient solution for\nsafeguarding the integrity of code generation model training datasets.\n","authors":["Chichien Tsai","Chiamu Yu","Yingdar Lin","Yusung Wu","Weibin Lee"],"pdf_url":"https://arxiv.org/pdf/2502.20246v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.20245v1","updated":"2025-02-27T16:29:14Z","published":"2025-02-27T16:29:14Z","title":"From Retrieval to Generation: Comparing Different Approaches","summary":"  Knowledge-intensive tasks, particularly open-domain question answering\n(ODQA), document reranking, and retrieval-augmented language modeling, require\na balance between retrieval accuracy and generative flexibility. Traditional\nretrieval models such as BM25 and Dense Passage Retrieval (DPR), efficiently\nretrieve from large corpora but often lack semantic depth. Generative models\nlike GPT-4-o provide richer contextual understanding but face challenges in\nmaintaining factual consistency. In this work, we conduct a systematic\nevaluation of retrieval-based, generation-based, and hybrid models, with a\nprimary focus on their performance in ODQA and related retrieval-augmented\ntasks. Our results show that dense retrievers, particularly DPR, achieve strong\nperformance in ODQA with a top-1 accuracy of 50.17\\% on NQ, while hybrid models\nimprove nDCG@10 scores on BEIR from 43.42 (BM25) to 52.59, demonstrating their\nstrength in document reranking. Additionally, we analyze language modeling\ntasks using WikiText-103, showing that retrieval-based approaches like BM25\nachieve lower perplexity compared to generative and hybrid methods,\nhighlighting their utility in retrieval-augmented generation. By providing\ndetailed comparisons and practical insights into the conditions where each\napproach excels, we aim to facilitate future optimizations in retrieval,\nreranking, and generative models for ODQA and related knowledge-intensive\napplications.\n","authors":["Abdelrahman Abdallah","Jamshid Mozafari","Bhawna Piryani","Mohammed Ali","Adam Jatowt"],"pdf_url":"https://arxiv.org/pdf/2502.20245v1.pdf","comment":"work on progress"},{"id":"http://arxiv.org/abs/2502.20238v1","updated":"2025-02-27T16:23:25Z","published":"2025-02-27T16:23:25Z","title":"FINEREASON: Evaluating and Improving LLMs' Deliberate Reasoning through\n  Reflective Puzzle Solving","summary":"  Many challenging reasoning tasks require not just rapid, intuitive responses,\nbut a more deliberate, multi-step approach. Recent progress in large language\nmodels (LLMs) highlights an important shift from the \"System 1\" way of quick\nreactions to the \"System 2\" style of reflection-and-correction problem solving.\nHowever, current benchmarks heavily rely on the final-answer accuracy, leaving\nmuch of a model's intermediate reasoning steps unexamined. This fails to assess\nthe model's ability to reflect and rectify mistakes within the reasoning\nprocess. To bridge this gap, we introduce FINEREASON, a logic-puzzle benchmark\nfor fine-grained evaluation of LLMs' reasoning capabilities. Each puzzle can be\ndecomposed into atomic steps, making it ideal for rigorous validation of\nintermediate correctness. Building on this, we introduce two tasks: state\nchecking, and state transition, for a comprehensive evaluation of how models\nassess the current situation and plan the next move. To support broader\nresearch, we also provide a puzzle training set aimed at enhancing performance\non general mathematical tasks. We show that models trained on our state\nchecking and transition data demonstrate gains in math reasoning by up to 5.1%\non GSM8K.\n","authors":["Guizhen Chen","Weiwen Xu","Hao Zhang","Hou Pong Chan","Chaoqun Liu","Lidong Bing","Deli Zhao","Anh Tuan Luu","Yu Rong"],"pdf_url":"https://arxiv.org/pdf/2502.20238v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.13339v2","updated":"2025-02-27T15:49:02Z","published":"2024-10-17T08:48:54Z","title":"Probing-RAG: Self-Probing to Guide Language Models in Selective Document\n  Retrieval","summary":"  Retrieval-Augmented Generation (RAG) enhances language models by retrieving\nand incorporating relevant external knowledge. However, traditional\nretrieve-and-generate processes may not be optimized for real-world scenarios,\nwhere queries might require multiple retrieval steps or none at all. In this\npaper, we propose a Probing-RAG, which utilizes the hidden state\nrepresentations from the intermediate layers of language models to adaptively\ndetermine the necessity of additional retrievals for a given query. By\nemploying a pre-trained prober, Probing-RAG effectively captures the model's\ninternal cognition, enabling reliable decision-making about retrieving external\ndocuments. Experimental results across five open-domain QA datasets demonstrate\nthat Probing-RAG outperforms previous methods while reducing the number of\nredundant retrieval steps.\n","authors":["Ingeol Baek","Hwan Chang","Byeongjeong Kim","Jimin Lee","Hwanhee Lee"],"pdf_url":"https://arxiv.org/pdf/2410.13339v2.pdf","comment":"NAACL 2025 Findings"},{"id":"http://arxiv.org/abs/2306.09830v2","updated":"2025-02-27T15:47:57Z","published":"2023-06-16T13:15:26Z","title":"Sheffield's Submission to the AmericasNLP Shared Task on Machine\n  Translation into Indigenous Languages","summary":"  In this paper we describe the University of Sheffield's submission to the\nAmericasNLP 2023 Shared Task on Machine Translation into Indigenous Languages\nwhich comprises the translation from Spanish to eleven indigenous languages.\nOur approach consists of extending, training, and ensembling different\nvariations of NLLB-200. We use data provided by the organizers and data from\nvarious other sources such as constitutions, handbooks, news articles, and\nbacktranslations generated from monolingual data. On the dev set, our best\nsubmission outperforms the baseline by 11% average chrF across all languages,\nwith substantial improvements particularly for Aymara, Guarani and Quechua. On\nthe test set, we achieve the highest average chrF of all the submissions, we\nrank first in four of the eleven languages, and at least one of our submissions\nranks in the top 3 for all languages.\n","authors":["Edward Gow-Smith","Danae Sánchez Villegas"],"pdf_url":"https://arxiv.org/pdf/2306.09830v2.pdf","comment":"Best-performing submission overall to the AmericasNLP 2023 Shared\n  Task. Code and models available here:\n  https://github.com/edwardgowsmith/americasnlp-2023-sheffield"},{"id":"http://arxiv.org/abs/2502.20204v1","updated":"2025-02-27T15:45:16Z","published":"2025-02-27T15:45:16Z","title":"Granite Embedding Models","summary":"  We introduce the Granite Embedding models, a family of encoder-based\nembedding models designed for retrieval tasks, spanning dense-retrieval and\nsparse retrieval architectures, with both English and Multilingual\ncapabilities. This report provides the technical details of training these\nhighly effective 12 layer embedding models, along with their efficient 6 layer\ndistilled counterparts. Extensive evaluations show that the models, developed\nwith techniques like retrieval oriented pretraining, contrastive finetuning,\nknowledge distillation, and model merging significantly outperform publicly\navailable models of similar sizes on both internal IBM retrieval and search\ntasks, and have equivalent performance on widely used information retrieval\nbenchmarks, while being trained on high-quality data suitable for enterprise\nuse. We publicly release all our Granite Embedding models under the Apache 2.0\nlicense, allowing both research and commercial use at\nhttps://huggingface.co/collections/ibm-granite.\n","authors":["Parul Awasthy","Aashka Trivedi","Yulong Li","Mihaela Bornea","David Cox","Abraham Daniels","Martin Franz","Gabe Goodhart","Bhavani Iyer","Vishwajeet Kumar","Luis Lastras","Scott McCarley","Rudra Murthy","Vignesh P","Sara Rosenthal","Salim Roukos","Jaydeep Sen","Sukriti Sharma","Avirup Sil","Kate Soule","Arafat Sultan","Radu Florian"],"pdf_url":"https://arxiv.org/pdf/2502.20204v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.10868v3","updated":"2025-02-27T15:37:21Z","published":"2025-01-18T20:26:00Z","title":"JSONSchemaBench: A Rigorous Benchmark of Structured Outputs for Language\n  Models","summary":"  Reliably generating structured outputs has become a critical capability for\nmodern language model (LM) applications. Constrained decoding has emerged as\nthe dominant technology across sectors for enforcing structured outputs during\ngeneration. Despite its growing adoption, little has been done with the\nsystematic evaluation of the behaviors and performance of constrained decoding.\nConstrained decoding frameworks have standardized around JSON Schema as a\nstructured data format, with most uses guaranteeing constraint compliance given\na schema. However, there is poor understanding of the effectiveness of the\nmethods in practice. We present an evaluation framework to assess constrained\ndecoding approaches across three critical dimensions: efficiency in generating\nconstraint-compliant outputs, coverage of diverse constraint types, and quality\nof the generated outputs. To facilitate this evaluation, we introduce\nJSONSchemaBench, a benchmark for constrained decoding comprising 10K real-world\nJSON schemas that encompass a wide range of constraints with varying\ncomplexity. We pair the benchmark with the existing official JSON Schema Test\nSuite and evaluate six state-of-the-art constrained decoding frameworks,\nincluding Guidance, Outlines, Llamacpp, XGrammar, OpenAI, and Gemini. Through\nextensive experiments, we gain insights into the capabilities and limitations\nof constrained decoding on structured generation with real-world JSON schemas.\nOur work provides actionable insights for improving constrained decoding\nframeworks and structured generation tasks, setting a new standard for\nevaluating constrained decoding and structured generation. We release\nJSONSchemaBench at https://github.com/guidance-ai/jsonschemabench\n","authors":["Saibo Geng","Hudson Cooper","Michał Moskal","Samuel Jenkins","Julian Berman","Nathan Ranchin","Robert West","Eric Horvitz","Harsha Nori"],"pdf_url":"https://arxiv.org/pdf/2501.10868v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.20196v1","updated":"2025-02-27T15:36:00Z","published":"2025-02-27T15:36:00Z","title":"ChineseEcomQA: A Scalable E-commerce Concept Evaluation Benchmark for\n  Large Language Models","summary":"  With the increasing use of Large Language Models (LLMs) in fields such as\ne-commerce, domain-specific concept evaluation benchmarks are crucial for\nassessing their domain capabilities. Existing LLMs may generate factually\nincorrect information within the complex e-commerce applications. Therefore, it\nis necessary to build an e-commerce concept benchmark. Existing benchmarks\nencounter two primary challenges: (1) handle the heterogeneous and diverse\nnature of tasks, (2) distinguish between generality and specificity within the\ne-commerce field. To address these problems, we propose \\textbf{ChineseEcomQA},\na scalable question-answering benchmark focused on fundamental e-commerce\nconcepts. ChineseEcomQA is built on three core characteristics: \\textbf{Focus\non Fundamental Concept}, \\textbf{E-commerce Generality} and \\textbf{E-commerce\nExpertise}. Fundamental concepts are designed to be applicable across a diverse\narray of e-commerce tasks, thus addressing the challenge of heterogeneity and\ndiversity. Additionally, by carefully balancing generality and specificity,\nChineseEcomQA effectively differentiates between broad e-commerce concepts,\nallowing for precise validation of domain capabilities. We achieve this through\na scalable benchmark construction process that combines LLM validation,\nRetrieval-Augmented Generation (RAG) validation, and rigorous manual\nannotation. Based on ChineseEcomQA, we conduct extensive evaluations on\nmainstream LLMs and provide some valuable insights. We hope that ChineseEcomQA\ncould guide future domain-specific evaluations, and facilitate broader LLM\nadoption in e-commerce applications.\n","authors":["Haibin Chen","Kangtao Lv","Chengwei Hu","Yanshi Li","Yujin Yuan","Yancheng He","Xingyao Zhang","Langming Liu","Shilei Liu","Wenbo Su","Bo Zheng"],"pdf_url":"https://arxiv.org/pdf/2502.20196v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.08521v2","updated":"2025-02-27T15:29:03Z","published":"2024-12-11T16:35:13Z","title":"EMS: Adaptive Evict-then-Merge Strategy for Head-wise KV Cache\n  Compression Based on Global-Local Importance","summary":"  As large language models (LLMs) continue to advance, the demand for higher\nquality and faster processing of long contexts across various applications is\ngrowing. KV cache is widely adopted as it stores previously generated key and\nvalue tokens, effectively reducing redundant computations during inference.\nHowever, as memory overhead becomes a significant concern, efficient\ncompression of KV cache has gained increasing attention. Most existing methods\nperform compression from two perspectives: identifying important tokens and\ndesigning compression strategies. However, these approaches often produce\nbiased distributions of important tokens due to the influence of accumulated\nattention scores or positional encoding. Furthermore, they overlook the\nsparsity and redundancy across different heads, which leads to difficulties in\npreserving the most effective information at the head level. To this end, we\npropose EMS to overcome these limitations, while achieving better KV cache\ncompression under extreme compression ratios. Specifically, we introduce a\nGlobal-Local score that combines accumulated attention scores from both global\nand local KV tokens to better identify the token importance. For the\ncompression strategy, we design an adaptive and unified Evict-then-Merge\nframework that accounts for the sparsity and redundancy of KV tokens across\ndifferent heads. Additionally, we implement the head-wise parallel compression\nthrough a zero-class mechanism to enhance efficiency. Extensive experiments\ndemonstrate our SOTA performance even under extreme compression ratios. EMS\nconsistently achieves the lowest perplexity, improves scores by over 1.28\npoints across four LLMs on LongBench under a 256 cache budget, and preserves\n95% retrieval accuracy with a cache budget less than 2% of the context length\nin the Needle-in-a-Haystack task.\n","authors":["Yingxin Li","Ye Li","Yuan Meng","Xinzhu Ma","Zihan Geng","Shutao Xia","Zhi Wang"],"pdf_url":"https://arxiv.org/pdf/2412.08521v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.20186v1","updated":"2025-02-27T15:22:14Z","published":"2025-02-27T15:22:14Z","title":"Layer-Aware Task Arithmetic: Disentangling Task-Specific and\n  Instruction-Following Knowledge","summary":"  Large language models (LLMs) demonstrate strong task-specific capabilities\nthrough fine-tuning, but merging multiple fine-tuned models often leads to\ndegraded performance due to overlapping instruction-following components. Task\nArithmetic (TA), which combines task vectors derived from fine-tuning, enables\nmulti-task learning and task forgetting but struggles to isolate task-specific\nknowledge from general instruction-following behavior. To address this, we\npropose Layer-Aware Task Arithmetic (LATA), a novel approach that assigns\nlayer-specific weights to task vectors based on their alignment with\ninstruction-following or task-specific components. By amplifying task-relevant\nlayers and attenuating instruction-following layers, LATA improves task\nlearning and forgetting performance while preserving overall model utility.\nExperiments on multiple benchmarks, including WikiText-2, GSM8K, and HumanEval,\ndemonstrate that LATA outperforms existing methods in both multi-task learning\nand selective task forgetting, achieving higher task accuracy and alignment\nwith minimal degradation in output quality. Our findings highlight the\nimportance of layer-wise analysis in disentangling task-specific and\ngeneral-purpose knowledge, offering a robust framework for efficient model\nmerging and editing.\n","authors":["Yan-Lun Chen","Yi-Ru Wei","Chia-Yi Hsu","Chia-Mu Yu","Chun-Ying Huang","Ying-Dar Lin","Yu-Sung Wu","Wei-Bin Lee"],"pdf_url":"https://arxiv.org/pdf/2502.20186v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16443v4","updated":"2025-02-27T15:22:03Z","published":"2024-10-21T19:12:33Z","title":"Improving Neuron-level Interpretability with White-box Language Models","summary":"  Neurons in auto-regressive language models like GPT-2 can be interpreted by\nanalyzing their activation patterns. Recent studies have shown that techniques\nsuch as dictionary learning, a form of post-hoc sparse coding, enhance this\nneuron-level interpretability. In our research, we are driven by the goal to\nfundamentally improve neural network interpretability by embedding sparse\ncoding directly within the model architecture, rather than applying it as an\nafterthought. In our study, we introduce a white-box transformer-like\narchitecture named Coding RAte TransformEr (CRATE), explicitly engineered to\ncapture sparse, low-dimensional structures within data distributions. Our\ncomprehensive experiments showcase significant improvements (up to 103%\nrelative improvement) in neuron-level interpretability across a variety of\nevaluation metrics. Detailed investigations confirm that this enhanced\ninterpretability is steady across different layers irrespective of the model\nsize, underlining CRATE's robust performance in enhancing neural network\ninterpretability. Further analysis shows that CRATE's increased\ninterpretability comes from its enhanced ability to consistently and\ndistinctively activate on relevant tokens. These findings point towards a\npromising direction for creating white-box foundation models that excel in\nneuron-level interpretation.\n","authors":["Hao Bai","Yi Ma"],"pdf_url":"https://arxiv.org/pdf/2410.16443v4.pdf","comment":"CPAL 2025 camera-ready version. Selected as Oral"},{"id":"http://arxiv.org/abs/2502.20175v1","updated":"2025-02-27T15:13:07Z","published":"2025-02-27T15:13:07Z","title":"An Extensive Evaluation of PDDL Capabilities in off-the-shelf LLMs","summary":"  In recent advancements, large language models (LLMs) have exhibited\nproficiency in code generation and chain-of-thought reasoning, laying the\ngroundwork for tackling automatic formal planning tasks. This study evaluates\nthe potential of LLMs to understand and generate Planning Domain Definition\nLanguage (PDDL), an essential representation in artificial intelligence\nplanning. We conduct an extensive analysis across 20 distinct models spanning 7\nmajor LLM families, both commercial and open-source. Our comprehensive\nevaluation sheds light on the zero-shot LLM capabilities of parsing,\ngenerating, and reasoning with PDDL. Our findings indicate that while some\nmodels demonstrate notable effectiveness in handling PDDL, others pose\nlimitations in more complex scenarios requiring nuanced planning knowledge.\nThese results highlight the promise and current limitations of LLMs in formal\nplanning tasks, offering insights into their application and guiding future\nefforts in AI-driven planning paradigms.\n","authors":["Kaustubh Vyas","Damien Graux","Sébastien Montella","Pavlos Vougiouklis","Ruofei Lai","Keshuang Li","Yang Ren","Jeff Z. Pan"],"pdf_url":"https://arxiv.org/pdf/2502.20175v1.pdf","comment":"Under review"},{"id":"http://arxiv.org/abs/2404.03471v4","updated":"2025-02-27T15:11:54Z","published":"2024-04-04T14:24:06Z","title":"The Impact of Unstated Norms in Bias Analysis of Language Models","summary":"  Bias in large language models (LLMs) has many forms, from overt\ndiscrimination to implicit stereotypes. Counterfactual bias evaluation is a\nwidely used approach to quantifying bias and often relies on template-based\nprobes that explicitly state group membership. It measures whether the outcome\nof a task performed by an LLM is invariant to a change in group membership. In\nthis work, we find that template-based probes can lead to unrealistic bias\nmeasurements. For example, LLMs appear to mistakenly cast text associated with\nWhite race as negative at higher rates than other groups. We hypothesize that\nthis arises artificially via a mismatch between commonly unstated norms, in the\nform of markedness, in the pretraining text of LLMs (e.g., Black president vs.\npresident) and templates used for bias measurement (e.g., Black president vs.\nWhite president). The findings highlight the potential misleading impact of\nvarying group membership through explicit mention in counterfactual bias\nquantification.\n","authors":["Farnaz Kohankhaki","D. B. Emerson","Jacob-Junqi Tian","Laleh Seyyed-Kalantari","Faiza Khan Khattak"],"pdf_url":"https://arxiv.org/pdf/2404.03471v4.pdf","comment":"15 Pages, 4 Figures, 4 Tables"},{"id":"http://arxiv.org/abs/2407.00468v2","updated":"2025-02-27T15:10:56Z","published":"2024-06-29T15:28:45Z","title":"MMEvalPro: Calibrating Multimodal Benchmarks Towards Trustworthy and\n  Efficient Evaluation","summary":"  Large Multimodal Models (LMMs) exhibit impressive cross-modal understanding\nand reasoning abilities, often assessed through multiple-choice questions\n(MCQs) that include an image, a question, and several options. However, many\nbenchmarks used for such evaluations suffer from systematic biases. Remarkably,\nLarge Language Models (LLMs) without any visual perception capabilities achieve\nnon-trivial performance, undermining the credibility of these evaluations. To\naddress this issue while maintaining the efficiency of MCQ evaluations, we\npropose MMEvalPro, a benchmark designed to avoid Type-I errors through a\ntrilogy evaluation pipeline and more rigorous metrics. For each original\nquestion from existing benchmarks, human annotators augment it by creating one\nperception question and one knowledge anchor question through a meticulous\nannotation process. MMEvalPro comprises $2,138$ question triplets, totaling\n$6,414$ distinct questions. Two-thirds of these questions are manually labeled\nby human experts, while the rest are sourced from existing benchmarks (MMMU,\nScienceQA, and MathVista). Compared with the existing benchmarks, our\nexperiments with the latest LLMs and LMMs demonstrate that MMEvalPro is more\nchallenging (the best LMM lags behind human performance by $31.73\\%$, compared\nto an average gap of $8.03\\%$ in previous benchmarks) and more trustworthy (the\nbest LLM trails the best LMM by $23.09\\%$, whereas the gap for previous\nbenchmarks is just $14.64\\%$). Our in-depth analysis explains the reason for\nthe large performance gap and justifies the trustworthiness of evaluation,\nunderscoring its significant potential for advancing future research.\n","authors":["Jinsheng Huang","Liang Chen","Taian Guo","Fu Zeng","Yusheng Zhao","Bohan Wu","Ye Yuan","Haozhe Zhao","Zhihui Guo","Yichi Zhang","Jingyang Yuan","Wei Ju","Luchen Liu","Tianyu Liu","Baobao Chang","Ming Zhang"],"pdf_url":"https://arxiv.org/pdf/2407.00468v2.pdf","comment":"18 pages, code released at https://github.com/chenllliang/MMEvalPro,\n  Homepage at https://mmevalpro.github.io/"},{"id":"http://arxiv.org/abs/2502.20172v1","updated":"2025-02-27T15:08:39Z","published":"2025-02-27T15:08:39Z","title":"Multimodal Representation Alignment for Image Generation: Text-Image\n  Interleaved Control Is Easier Than You Think","summary":"  The field of advanced text-to-image generation is witnessing the emergence of\nunified frameworks that integrate powerful text encoders, such as CLIP and T5,\nwith Diffusion Transformer backbones. Although there have been efforts to\ncontrol output images with additional conditions, like canny and depth map, a\ncomprehensive framework for arbitrary text-image interleaved control is still\nlacking. This gap is especially evident when attempting to merge concepts or\nvisual elements from multiple images in the generation process. To mitigate the\ngap, we conducted preliminary experiments showing that large multimodal models\n(LMMs) offer an effective shared representation space, where image and text can\nbe well-aligned to serve as a condition for external diffusion models. Based on\nthis discovery, we propose Dream Engine, an efficient and unified framework\ndesigned for arbitrary text-image interleaved control in image generation\nmodels. Building on powerful text-to-image models like SD3.5, we replace the\noriginal text-only encoders by incorporating versatile multimodal information\nencoders such as QwenVL. Our approach utilizes a two-stage training paradigm,\nconsisting of joint text-image alignment and multimodal interleaved instruction\ntuning. Our experiments demonstrate that this training method is effective,\nachieving a 0.69 overall score on the GenEval benchmark, and matching the\nperformance of state-of-the-art text-to-image models like SD3.5 and FLUX.\n","authors":["Liang Chen","Shuai Bai","Wenhao Chai","Weichu Xie","Haozhe Zhao","Leon Vinci","Junyang Lin","Baobao Chang"],"pdf_url":"https://arxiv.org/pdf/2502.20172v1.pdf","comment":"13 pages, 9 figures, codebase in\n  https://github.com/chenllliang/DreamEngine"},{"id":"http://arxiv.org/abs/2411.07175v2","updated":"2025-02-27T15:08:33Z","published":"2024-11-11T17:56:15Z","title":"Continual Memorization of Factoids in Language Models","summary":"  As new knowledge rapidly accumulates, language models (LMs) with pretrained\nknowledge quickly become obsolete. A common approach to updating LMs is\nfine-tuning them directly on new knowledge. However, recent studies have shown\nthat fine-tuning for memorization may be ineffective in storing knowledge or\nmay exacerbate hallucinations. In this work, we introduce a setting we call\ncontinual memorization, where a model must memorize and retain a set of\nfactoids through multiple stages of fine-tuning on subsequent datasets. We\ncharacterized the forgetting patterns through extensive experiments and show\nthat LMs widely suffer from forgetting, especially when needing to memorize\nfactoids in the second stage. We posit that forgetting can be alleviated by\nmodifying training dynamics: (1) protecting the memorization process when\nlearning factoids or (2) reducing interference from subsequent training stages.\nIntriguingly, we find that mixing randomly generated word sequences or generic\ndata sampled from pretraining corpora at different training stages effectively\nmitigates forgetting REMIX: Random and Generic Data Mixing). REMIX can recover\nperformance from severe forgetting, outperforming replay methods and other\ncontinual learning baselines. We analyze how REMIX influences the learning\nprocess and find that robust memorization follows a distinct pattern: the model\nstores factoids in earlier layers than usual and diversifies the layers that\nretain them, which results in easier recall and manipulate of the learned\nfactoids.\n","authors":["Howard Chen","Jiayi Geng","Adithya Bhaskar","Dan Friedman","Danqi Chen"],"pdf_url":"https://arxiv.org/pdf/2411.07175v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.20171v1","updated":"2025-02-27T15:07:51Z","published":"2025-02-27T15:07:51Z","title":"Representing Signs as Signs: One-Shot ISLR to Facilitate Functional Sign\n  Language Technologies","summary":"  Isolated Sign Language Recognition (ISLR) is crucial for scalable sign\nlanguage technology, yet language-specific approaches limit current models. To\naddress this, we propose a one-shot learning approach that generalises across\nlanguages and evolving vocabularies. Our method involves pretraining a model to\nembed signs based on essential features and using a dense vector search for\nrapid, accurate recognition of unseen signs. We achieve state-of-the-art\nresults, including 50.8% one-shot MRR on a large dictionary containing 10,235\nunique signs from a different language than the training set. Our approach is\nrobust across languages and support sets, offering a scalable, adaptable\nsolution for ISLR. Co-created with the Deaf and Hard of Hearing (DHH)\ncommunity, this method aligns with real-world needs, and advances scalable sign\nlanguage recognition.\n","authors":["Toon Vandendriessche","Mathieu De Coster","Annelies Lejon","Joni Dambre"],"pdf_url":"https://arxiv.org/pdf/2502.20171v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.20170v1","updated":"2025-02-27T15:07:47Z","published":"2025-02-27T15:07:47Z","title":"Re-evaluating Open-ended Evaluation of Large Language Models","summary":"  Evaluation has traditionally focused on ranking candidates for a specific\nskill. Modern generalist models, such as Large Language Models (LLMs),\ndecidedly outpace this paradigm. Open-ended evaluation systems, where candidate\nmodels are compared on user-submitted prompts, have emerged as a popular\nsolution. Despite their many advantages, we show that the current Elo-based\nrating systems can be susceptible to and even reinforce biases in data,\nintentional or accidental, due to their sensitivity to redundancies. To address\nthis issue, we propose evaluation as a 3-player game, and introduce novel\ngame-theoretic solution concepts to ensure robustness to redundancy. We show\nthat our method leads to intuitive ratings and provide insights into the\ncompetitive landscape of LLM development.\n","authors":["Siqi Liu","Ian Gemp","Luke Marris","Georgios Piliouras","Nicolas Heess","Marc Lanctot"],"pdf_url":"https://arxiv.org/pdf/2502.20170v1.pdf","comment":"Published at ICLR 2025"},{"id":"http://arxiv.org/abs/2502.20167v1","updated":"2025-02-27T15:05:00Z","published":"2025-02-27T15:05:00Z","title":"Similarity-Distance-Magnitude Universal Verification","summary":"  We solve the neural network robustness problem by adding Similarity (i.e.,\ncorrectly predicted depth-matches into training)-awareness and\nDistance-to-training-distribution-awareness to the existing output Magnitude\n(i.e., decision-boundary)-awareness of the softmax function. The resulting sdm\nactivation function provides strong signals of the relative epistemic\n(reducible) predictive uncertainty. We use this novel behavior to further\naddress the complementary HCI problem of mapping the output to\nhuman-interpretable summary statistics over relevant partitions of a held-out\ncalibration set. Estimates of prediction-conditional uncertainty are obtained\nvia a parsimonious learned transform over the class-conditional empirical CDFs\nof the output of a final-layer sdm activation function. For decision-making and\nas an intrinsic model check, estimates of class-conditional accuracy are\nobtained by further partitioning the high-probability regions of this\ncalibrated output into class-conditional, region-specific CDFs. The uncertainty\nestimates from sdm calibration are remarkably robust to test-time distribution\nshifts and out-of-distribution inputs; incorporate awareness of the effective\nsample size; provide estimates of uncertainty from the learning and data\nsplitting processes; and are well-suited for selective classification and\nconditional branching for additional test-time compute based on the predictive\nuncertainty, as for selective LLM generation, routing, and composition over\nmultiple models and retrieval. Finally, we construct sdm networks, LLMs with\nuncertainty-aware verification and interpretability-by-exemplar as intrinsic\nproperties. We provide open-source software implementing these results.\n","authors":["Allen Schmaltz"],"pdf_url":"https://arxiv.org/pdf/2502.20167v1.pdf","comment":"35 pages (8 Tables, 4 Algorithms, 5 Listings)"},{"id":"http://arxiv.org/abs/2502.16860v2","updated":"2025-02-27T14:50:10Z","published":"2025-02-24T05:51:53Z","title":"LongAttn: Selecting Long-context Training Data via Token-level Attention","summary":"  With the development of large language models (LLMs), there has been an\nincreasing need for significant advancements in handling long contexts. To\nenhance long-context capabilities, constructing high-quality training data with\nlong-range dependencies is crucial. Existing methods to select long-context\ndata often rely on sentence-level analysis, which can be greatly optimized in\nboth performance and efficiency. In this paper, we propose a novel token-level\nframework, LongAttn, which leverages the self-attention mechanism of LLMs to\nmeasure the long-range dependencies for the data. By calculating token-level\ndependency strength and distribution uniformity of token scores, LongAttn\neffectively quantifies long-range dependencies, enabling more accurate and\nefficient data selection. We filter LongABC-32K from open-source long-context\ndatasets (ArXiv, Book, and Code). Through our comprehensive experiments,\nLongAttn has demonstrated its excellent effectiveness, scalability, and\nefficiency. To facilitate future research in long-context data, we released our\ncode and the high-quality long-context training data LongABC-32K.\n","authors":["Longyun Wu","Dawei Zhu","Guangxiang Zhao","Zhuocheng Yu","Junfeng Ran","Xiangyu Wong","Lin Sun","Sujian Li"],"pdf_url":"https://arxiv.org/pdf/2502.16860v2.pdf","comment":"17 pages, 5 figures"},{"id":"http://arxiv.org/abs/2502.20140v1","updated":"2025-02-27T14:31:42Z","published":"2025-02-27T14:31:42Z","title":"Telephone Surveys Meet Conversational AI: Evaluating a LLM-Based\n  Telephone Survey System at Scale","summary":"  Telephone surveys remain a valuable tool for gathering insights but typically\nrequire substantial resources in training and coordinating human interviewers.\nThis work presents an AI-driven telephone survey system integrating\ntext-to-speech (TTS), a large language model (LLM), and speech-to-text (STT)\nthat mimics the versatility of human-led interviews on scale.\n  We tested the system across two populations, a pilot study in the United\nStates (n = 75) and a large-scale deployment in Peru (n = 2,739), inviting\nparticipants via web-based links and contacting them via direct phone calls.\nThe AI agent successfully administered open-ended and closed-ended questions,\nhandled basic clarifications, and dynamically navigated branching logic,\nallowing fast large-scale survey deployment without interviewer recruitment or\ntraining.\n  Our findings demonstrate that while the AI system's probing for qualitative\ndepth was more limited than human interviewers, overall data quality approached\nhuman-led standards for structured items. This study represents one of the\nfirst successful large-scale deployments of an LLM-based telephone interviewer\nin a real-world survey context. The AI-powered telephone survey system has the\npotential for expanding scalable, consistent data collecting across market\nresearch, social science, and public opinion studies, thus improving\noperational efficiency while maintaining appropriate data quality for research.\n","authors":["Max M. Lang","Sol Eskenazi"],"pdf_url":"https://arxiv.org/pdf/2502.20140v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.20135v1","updated":"2025-02-27T14:30:18Z","published":"2025-02-27T14:30:18Z","title":"Educator Attention: How computational tools can systematically identify\n  the distribution of a key resource for students","summary":"  Educator attention is critical for student success, yet how educators\ndistribute their attention across students remains poorly understood due to\ndata and methodological constraints. This study presents the first large-scale\ncomputational analysis of educator attention patterns, leveraging over 1\nmillion educator utterances from virtual group tutoring sessions linked to\ndetailed student demographic and academic achievement data. Using natural\nlanguage processing techniques, we systematically examine the recipient and\nnature of educator attention. Our findings reveal that educators often provide\nmore attention to lower-achieving students. However, disparities emerge across\ndemographic lines, particularly by gender. Girls tend to receive less attention\nwhen paired with boys, even when they are the lower achieving student in the\ngroup. Lower-achieving female students in mixed-gender pairs receive\nsignificantly less attention than their higher-achieving male peers, while\nlower-achieving male students receive significantly and substantially more\nattention than their higher-achieving female peers. We also find some\ndifferences by race and English learner (EL) status, with low-achieving Black\nstudents receiving additional attention only when paired with another Black\nstudent but not when paired with a non-Black peer. In contrast,\nhigher-achieving EL students receive disproportionately more attention than\ntheir lower-achieving EL peers. This work highlights how large-scale\ninteraction data and computational methods can uncover subtle but meaningful\ndisparities in teaching practices, providing empirical insights to inform more\nequitable and effective educational strategies.\n","authors":["Qingyang Zhang","Rose E. Wang","Ana T. Ribeiro","Dora Demszky","Susanna Loeb"],"pdf_url":"https://arxiv.org/pdf/2502.20135v1.pdf","comment":"The first two authors QZ and REW contributed equally. The last two\n  authors DD and SL advised equally"},{"id":"http://arxiv.org/abs/2502.20129v1","updated":"2025-02-27T14:24:51Z","published":"2025-02-27T14:24:51Z","title":"Finite State Automata Inside Transformers with Chain-of-Thought: A\n  Mechanistic Study on State Tracking","summary":"  Chain-of-Thought (CoT) significantly enhances the performance of large\nlanguage models (LLMs) across a wide range of tasks, and prior research shows\nthat CoT can theoretically increase expressiveness. However, there is limited\nmechanistic understanding of the algorithms that Transformer+CoT can learn. In\nthis work, we (1) evaluate the state tracking capabilities of Transformer+CoT\nand its variants, confirming the effectiveness of CoT. (2) Next, we identify\nthe circuit, a subset of model components, responsible for tracking the world\nstate, finding that late-layer MLP neurons play a key role. We propose two\nmetrics, compression and distinction, and show that the neuron sets for each\nstate achieve nearly 100% accuracy, providing evidence of an implicit finite\nstate automaton (FSA) embedded within the model. (3) Additionally, we explore\nthree realistic settings: skipping intermediate steps, introducing data noise,\nand testing length generalization. Our results demonstrate that Transformer+CoT\nlearns robust algorithms (FSA), highlighting its resilience in challenging\nscenarios.\n","authors":["Yifan Zhang","Wenyu Du","Dongming Jin","Jie Fu","Zhi Jin"],"pdf_url":"https://arxiv.org/pdf/2502.20129v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.15091v3","updated":"2025-02-27T14:21:43Z","published":"2024-08-27T14:22:02Z","title":"Relation Also Knows: Rethinking the Recall and Editing of Factual\n  Associations in Auto-Regressive Transformer Language Models","summary":"  The storage and recall of factual associations in auto-regressive transformer\nlanguage models (LMs) have drawn a great deal of attention, inspiring knowledge\nediting by directly modifying the located model weights. Most editing works\nachieve knowledge editing under the guidance of existing interpretations of\nknowledge recall that mainly focus on subject knowledge. However, these\ninterpretations are seriously flawed, neglecting relation information and\nleading to the over-generalizing problem for editing. In this work, we discover\na novel relation-focused perspective to interpret the knowledge recall of\ntransformer LMs during inference and apply it on single knowledge editing to\navoid over-generalizing. Experimental results on the dataset supplemented with\na new R-Specificity criterion demonstrate that our editing approach\nsignificantly alleviates over-generalizing while remaining competitive on other\ncriteria, breaking the domination of subject-focused editing for future\nresearch.\n","authors":["Xiyu Liu","Zhengxiao Liu","Naibin Gu","Zheng Lin","Wanli Ma","Ji Xiang","Weiping Wang"],"pdf_url":"https://arxiv.org/pdf/2408.15091v3.pdf","comment":"Accepted by AAAI25"},{"id":"http://arxiv.org/abs/2502.20127v1","updated":"2025-02-27T14:19:45Z","published":"2025-02-27T14:19:45Z","title":"SoRFT: Issue Resolving with Subtask-oriented Reinforced Fine-Tuning","summary":"  Mainstream issue-resolving frameworks predominantly rely on commercial\nmodels, leading to high costs and privacy concerns. Existing training\napproaches for issue resolving struggle with poor generalization and fail to\nfully leverage open-source development resources. We propose Subtask-oriented\nReinforced Fine-Tuning (SoRFT), a novel training approach to enhance the issue\nresolving capability of LLMs. We decomposes issue resolving into structured\nsubtasks: file localization, function localization, line localization, and code\nedit generation. SoRFT consists of two training stages: (1) rejection-sampled\nsupervised fine-tuning, Chain of Thought (CoT) data is filtered using\nground-truth before fine-tuning the LLM, and (2) rule-based reinforcement\nlearning, which leverages PPO with ground-truth based rewards. We evaluate the\nSoRFT-trained model on SWE-Bench Verified and SWE-Bench Lite, achieving\nstate-of-the-art (SOTA) performance among open-source models (e.g., resolve\n21.4% issues on SWE-Bench Verified with SoRFT-Qwen-7B). The experimental\nresults demonstrate that SoRFT significantly enhances issue-resolving\nperformance, improves model generalization, and provides a cost-efficient\nalternative to commercial models.\n","authors":["Zexiong Ma","Chao Peng","Pengfei Gao","Xiangxin Meng","Yanzhen Zou","Bing Xie"],"pdf_url":"https://arxiv.org/pdf/2502.20127v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.20122v1","updated":"2025-02-27T14:14:50Z","published":"2025-02-27T14:14:50Z","title":"Self-Training Elicits Concise Reasoning in Large Language Models","summary":"  Chain-of-thought (CoT) reasoning has enabled large language models (LLMs) to\nutilize additional computation through intermediate tokens to solve complex\ntasks. However, we posit that typical reasoning traces contain many redundant\ntokens, incurring extraneous inference costs. Upon examination of the output\ndistribution of current LLMs, we find evidence on their latent ability to\nreason more concisely, relative to their default behavior. To elicit this\ncapability, we propose simple fine-tuning methods which leverage self-generated\nconcise reasoning paths obtained by best-of-N sampling and few-shot\nconditioning, in task-specific settings. Our combined method achieves a 30%\nreduction in output tokens on average, across five model families on GSM8K and\nMATH, while maintaining average accuracy. By exploiting the fundamental\nstochasticity and in-context learning capabilities of LLMs, our self-training\napproach robustly elicits concise reasoning on a wide range of models,\nincluding those with extensive post-training. Code is available at\nhttps://github.com/TergelMunkhbat/concise-reasoning\n","authors":["Tergel Munkhbat","Namgyu Ho","Seohyun Kim","Yongjin Yang","Yujin Kim","Se-Young Yun"],"pdf_url":"https://arxiv.org/pdf/2502.20122v1.pdf","comment":"23 pages, 10 figures, 18 tables"},{"id":"http://arxiv.org/abs/2401.07923v2","updated":"2025-02-27T13:58:21Z","published":"2024-01-15T19:21:08Z","title":"Word Boundary Information Isn't Useful for Encoder Language Models","summary":"  All existing transformer-based approaches to NLP using subword tokenisation\nalgorithms encode whitespace (word boundary information) through the use of\nspecial space symbols (such as \\#\\# or \\_) forming part of tokens. These\nsymbols have been shown to a) lead to reduced morphological validity of\ntokenisations, and b) give substantial vocabulary redundancy. As such, removing\nthese symbols has been shown to have a beneficial effect on the processing of\nmorphologically complex words for transformer encoders in the pretrain-finetune\nparadigm. In this work, we explore whether word boundary information is at all\nuseful to such models. In particular, we train transformer encoders across four\ndifferent training scales, and investigate several alternative approaches to\nincluding word boundary information, evaluating on a range of tasks across\ndifferent domains and problem set-ups: GLUE (for sentence-level\nclassification), NER (for token-level classification), and two classification\ndatasets involving complex words (Superbizarre and FLOTA). Overall, through an\nextensive experimental setup that includes the pre-training of 29 models, we\nfind no substantial improvements from our alternative approaches, suggesting\nthat modifying tokenisers to remove word boundary information isn't leading to\na loss of useful information.\n","authors":["Edward Gow-Smith","Dylan Phelps","Harish Tayyar Madabushi","Carolina Scarton","Aline Villavicencio"],"pdf_url":"https://arxiv.org/pdf/2401.07923v2.pdf","comment":"9th Workshop on Representation Learning for NLP"},{"id":"http://arxiv.org/abs/2403.11807v6","updated":"2025-02-27T13:57:52Z","published":"2024-03-18T14:04:47Z","title":"How Far Are We on the Decision-Making of LLMs? Evaluating LLMs' Gaming\n  Ability in Multi-Agent Environments","summary":"  Decision-making is a complex process requiring diverse abilities, making it\nan excellent framework for evaluating Large Language Models (LLMs). Researchers\nhave examined LLMs' decision-making through the lens of Game Theory. However,\nexisting evaluation mainly focus on two-player scenarios where an LLM competes\nagainst another. Additionally, previous benchmarks suffer from test set leakage\ndue to their static design. We introduce GAMA($\\gamma$)-Bench, a new framework\nfor evaluating LLMs' Gaming Ability in Multi-Agent environments. It includes\neight classical game theory scenarios and a dynamic scoring scheme specially\ndesigned to quantitatively assess LLMs' performance. $\\gamma$-Bench allows\nflexible game settings and adapts the scoring system to different game\nparameters, enabling comprehensive evaluation of robustness, generalizability,\nand strategies for improvement. Our results indicate that GPT-3.5 demonstrates\nstrong robustness but limited generalizability, which can be enhanced using\nmethods like Chain-of-Thought. We also evaluate 13 LLMs from 6 model families,\nincluding GPT-3.5, GPT-4, Gemini, LLaMA-3.1, Mixtral, and Qwen-2.\nGemini-1.5-Pro outperforms others, scoring of $69.8$ out of $100$, followed by\nLLaMA-3.1-70B ($65.9$) and Mixtral-8x22B ($62.4$). Our code and experimental\nresults are publicly available at https://github.com/CUHK-ARISE/GAMABench.\n","authors":["Jen-tse Huang","Eric John Li","Man Ho Lam","Tian Liang","Wenxuan Wang","Youliang Yuan","Wenxiang Jiao","Xing Wang","Zhaopeng Tu","Michael R. Lyu"],"pdf_url":"https://arxiv.org/pdf/2403.11807v6.pdf","comment":"Accepted to ICLR 2025; 11 pages of main text; 26 pages of appendices;\n  Included models: GPT-3.5-{0613, 1106, 0125}, GPT-4-0125, GPT-4o-0806,\n  Gemini-{1.0, 1.5)-Pro, LLaMA-3.1-{7, 70, 405}B, Mixtral-8x{7, 22}B,\n  Qwen-2-72B"},{"id":"http://arxiv.org/abs/2502.20082v1","updated":"2025-02-27T13:41:07Z","published":"2025-02-27T13:41:07Z","title":"LongRoPE2: Near-Lossless LLM Context Window Scaling","summary":"  LongRoPE2 is a novel approach that extends the effective context window of\npre-trained large language models (LLMs) to the target length, while preserving\nthe performance on the original shorter context window. This is achieved by\nthree contributions: (1) a hypothesis that insufficient training in higher RoPE\ndimensions contributes to the persistent out-of-distribution (OOD) issues\nobserved in existing methods; (2) an effective RoPE rescaling algorithm that\nadopts evolutionary search guided by \"needle-driven\" perplexity to address the\ninsufficient training problem; (3) a mixed context window training approach\nthat fine-tunes model weights to adopt rescaled RoPE for long-context sequences\nwhile preserving the short-context performance with the original RoPE.\nExtensive experiments on LLaMA3-8B and Phi3-mini-3.8B across various benchmarks\nvalidate the hypothesis and demonstrate the effectiveness of LongRoPE2.\nRemarkably, LongRoPE2 extends LLaMA3-8B to achieve a 128K effective context\nlength while retaining over 98.5% of short-context performance, using only 10B\ntokens -- 80x fewer than Meta's approach, which fails to reach the target\neffective context length. Code will be available at\nhttps://github.com/microsoft/LongRoPE.\n","authors":["Ning Shang","Li Lyna Zhang","Siyuan Wang","Gaokai Zhang","Gilsinia Lopez","Fan Yang","Weizhu Chen","Mao Yang"],"pdf_url":"https://arxiv.org/pdf/2502.20082v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.06153v3","updated":"2025-02-27T13:33:55Z","published":"2024-10-08T15:52:42Z","title":"AgentSquare: Automatic LLM Agent Search in Modular Design Space","summary":"  Recent advancements in Large Language Models (LLMs) have led to a rapid\ngrowth of agentic systems capable of handling a wide range of complex tasks.\nHowever, current research largely relies on manual, task-specific design,\nlimiting their adaptability to novel tasks. In this paper, we introduce a new\nresearch problem: Modularized LLM Agent Search (MoLAS). We propose a modular\ndesign space that abstracts existing LLM agent designs into four fundamental\nmodules with uniform IO interface: Planning, Reasoning, Tool Use, and Memory.\nBuilding on this design space, we present a novel LLM agent search framework\ncalled AgentSquare, which introduces two core mechanisms, i.e., module\nevolution and recombination, to efficiently search for optimized LLM agents. To\nfurther accelerate the process, we design a performance predictor that uses\nin-context surrogate models to skip unpromising agent designs. Extensive\nexperiments across six benchmarks, covering the diverse scenarios of web,\nembodied, tool use and game applications, show that AgentSquare substantially\noutperforms hand-crafted agents, achieving an average performance gain of 17.2%\nagainst best-known human designs. Moreover, AgentSquare can generate\ninterpretable design insights, enabling a deeper understanding of agentic\narchitecture and its impact on task performance. We believe that the modular\ndesign space and AgentSquare search framework offer a platform for fully\nexploiting the potential of prior successful designs and consolidating the\ncollective efforts of research community. Code repo is available at\nhttps://github.com/tsinghua-fib-lab/AgentSquare.\n","authors":["Yu Shang","Yu Li","Keyu Zhao","Likai Ma","Jiahe Liu","Fengli Xu","Yong Li"],"pdf_url":"https://arxiv.org/pdf/2410.06153v3.pdf","comment":"25 pages"},{"id":"http://arxiv.org/abs/2502.20073v1","updated":"2025-02-27T13:31:13Z","published":"2025-02-27T13:31:13Z","title":"Collab-Overcooked: Benchmarking and Evaluating Large Language Models as\n  Collaborative Agents","summary":"  Large language models (LLMs) based agent systems have made great strides in\nreal-world applications beyond traditional NLP tasks. This paper proposes a new\nLLM-powered Multi-Agent System (LLM-MAS) benchmark, Collab-Overcooked, built on\nthe popular Overcooked-AI game with more applicable and challenging tasks in\ninteractive environments. Collab-Overcooked extends existing benchmarks from\ntwo novel perspectives. First, it provides a multi-agent framework supporting\ndiverse tasks and objectives and encourages collaboration through natural\nlanguage communication. Second, it introduces a spectrum of process-oriented\nevaluation metrics to assess the fine-grained collaboration capabilities of\ndifferent LLM agents, a dimension often overlooked in prior work. We conduct\nextensive experiments over 10 popular LLMs and show that, while the LLMs\npresent a strong ability in goal interpretation, there is a significant\ndiscrepancy in active collaboration and continuous adaption that are critical\nfor efficiently fulfilling complicated tasks. Notably, we highlight the\nstrengths and weaknesses in LLM-MAS and provide insights for improving and\nevaluating LLM-MAS on a unified and open-sourced benchmark. Environments, 30\nopen-ended tasks, and an integrated evaluation package are now publicly\navailable at https://github.com/YusaeMeow/Collab-Overcooked.\n","authors":["Haochen Sun","Shuwen Zhang","Lei Ren","Hao Xu","Hao Fu","Caixia Yuan","Xiaojie Wang"],"pdf_url":"https://arxiv.org/pdf/2502.20073v1.pdf","comment":"25 pages, 14 figures"},{"id":"http://arxiv.org/abs/2502.18934v2","updated":"2025-02-27T13:20:53Z","published":"2025-02-26T08:36:20Z","title":"Kanana: Compute-efficient Bilingual Language Models","summary":"  We introduce Kanana, a series of bilingual language models that demonstrate\nexceeding performance in Korean and competitive performance in English. The\ncomputational cost of Kanana is significantly lower than that of\nstate-of-the-art models of similar size. The report details the techniques\nemployed during pre-training to achieve compute-efficient yet competitive\nmodels, including high quality data filtering, staged pre-training, depth\nup-scaling, and pruning and distillation. Furthermore, the report outlines the\nmethodologies utilized during the post-training of the Kanana models,\nencompassing supervised fine-tuning and preference optimization, aimed at\nenhancing their capability for seamless interaction with users. Lastly, the\nreport elaborates on plausible approaches used for language model adaptation to\nspecific scenarios, such as embedding, retrieval augmented generation, and\nfunction calling. The Kanana model series spans from 2.1B to 32.5B parameters\nwith 2.1B models (base, instruct, embedding) publicly released to promote\nresearch on Korean language models.\n","authors":[" Kanana LLM Team","Yunju Bak","Hojin Lee","Minho Ryu","Jiyeon Ham","Seungjae Jung","Daniel Wontae Nam","Taegyeong Eo","Donghun Lee","Doohae Jung","Boseop Kim","Nayeon Kim","Jaesun Park","Hyunho Kim","Hyunwoong Ko","Changmin Lee","Kyoung-Woon On","Seulye Baeg","Junrae Cho","Sunghee Jung","Jieun Kang","EungGyun Kim","Eunhwa Kim","Byeongil Ko","Daniel Lee","Minchul Lee","Miok Lee","Shinbok Lee","Gaeun Seo"],"pdf_url":"https://arxiv.org/pdf/2502.18934v2.pdf","comment":"40 pages, 15 figures"},{"id":"http://arxiv.org/abs/2411.06899v2","updated":"2025-02-27T13:08:46Z","published":"2024-11-11T11:57:37Z","title":"LongSafety: Enhance Safety for Long-Context LLMs","summary":"  Recent advancements in model architectures and length extrapolation\ntechniques have significantly extended the context length of large language\nmodels (LLMs), paving the way for their application in increasingly complex\ntasks. However, despite the growing capabilities of long-context LLMs, the\nsafety issues in long-context scenarios remain underexplored. While safety\nalignment in short context has been widely studied, the safety concerns of\nlong-context LLMs have not been adequately addressed. In this work, we\nintroduce \\textbf{LongSafety}, a comprehensive safety alignment dataset for\nlong-context LLMs, containing 10 tasks and 17k samples, with an average length\nof 40.9k tokens. Our experiments demonstrate that training with LongSafety can\nenhance long-context safety performance while enhancing short-context safety\nand preserving general capabilities. Furthermore, we demonstrate that\nlong-context safety does not equal long-context alignment with short-context\nsafety data and LongSafety has generalizing capabilities in context length and\nlong-context safety scenarios.\n","authors":["Mianqiu Huang","Xiaoran Liu","Shaojun Zhou","Mozhi Zhang","Qipeng Guo","Linyang Li","Chenkun Tan","Yang Gao","Pengyu Wang","Linlin Li","Qun Liu","Yaqian Zhou","Xipeng Qiu","Xuanjing Huang"],"pdf_url":"https://arxiv.org/pdf/2411.06899v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.17184v3","updated":"2025-02-27T12:59:58Z","published":"2025-02-24T14:20:22Z","title":"Measuring Data Diversity for Instruction Tuning: A Systematic Analysis\n  and A Reliable Metric","summary":"  Data diversity is crucial for the instruction tuning of large language\nmodels. Existing studies have explored various diversity-aware data selection\nmethods to construct high-quality datasets and enhance model performance.\nHowever, the fundamental problem of precisely defining and measuring data\ndiversity remains underexplored, limiting clear guidance for data engineering.\nTo address this, we systematically analyze 11 existing diversity measurement\nmethods by evaluating their correlation with model performance through\nextensive fine-tuning experiments. Our results indicate that a reliable\ndiversity measure should properly account for both inter-sample differences and\nthe information distribution in the sample space. Building on this, we propose\nNovelSum, a new diversity metric based on sample-level \"novelty.\" Experiments\non both simulated and real-world data show that NovelSum accurately captures\ndiversity variations and achieves a 0.97 correlation with instruction-tuned\nmodel performance, highlighting its value in guiding data engineering\npractices. With NovelSum as an optimization objective, we further develop a\ngreedy, diversity-oriented data selection strategy that outperforms existing\napproaches, validating both the effectiveness and practical significance of our\nmetric.\n","authors":["Yuming Yang","Yang Nan","Junjie Ye","Shihan Dou","Xiao Wang","Shuo Li","Huijie Lv","Mingqi Wu","Tao Gui","Qi Zhang","Xuanjing Huang"],"pdf_url":"https://arxiv.org/pdf/2502.17184v3.pdf","comment":"16 pages. The related codes and resources will be released later.\n  Project page: https://github.com/UmeanNever/NovelSum"},{"id":"http://arxiv.org/abs/2502.17839v2","updated":"2025-02-27T12:55:08Z","published":"2025-02-25T04:38:38Z","title":"Say Less, Mean More: Leveraging Pragmatics in Retrieval-Augmented\n  Generation","summary":"  We propose a simple, unsupervised method that injects pragmatic principles in\nretrieval-augmented generation (RAG) frameworks such as Dense Passage Retrieval\nto enhance the utility of retrieved contexts. Our approach first identifies\nwhich sentences in a pool of documents retrieved by RAG are most relevant to\nthe question at hand, cover all the topics addressed in the input question and\nno more, and then highlights these sentences within their context, before they\nare provided to the LLM, without truncating or altering the context in any\nother way. We show that this simple idea brings consistent improvements in\nexperiments on three question answering tasks (ARC-Challenge, PubHealth and\nPopQA) using five different LLMs. It notably enhances relative accuracy by up\nto 19.7% on PubHealth and 10% on ARC-Challenge compared to a conventional RAG\nsystem.\n","authors":["Haris Riaz","Ellen Riloff","Mihai Surdeanu"],"pdf_url":"https://arxiv.org/pdf/2502.17839v2.pdf","comment":"16 pages, 2 figures, 8 tables. Preprint"},{"id":"http://arxiv.org/abs/2502.20047v1","updated":"2025-02-27T12:38:36Z","published":"2025-02-27T12:38:36Z","title":"Connecting the Persian-speaking World through Transliteration","summary":"  Despite speaking mutually intelligible varieties of the same language,\nspeakers of Tajik Persian, written in a modified Cyrillic alphabet, cannot read\nIranian and Afghan texts written in the Perso-Arabic script. As the vast\nmajority of Persian text on the Internet is written in Perso-Arabic,\nmonolingual Tajik speakers are unable to interface with the Internet in any\nmeaningful way. Due to overwhelming similarity between the formal registers of\nthese dialects and the scarcity of Tajik-Farsi parallel data, machine\ntransliteration has been proposed as more a practical and appropriate solution\nthan machine translation. This paper presents a transformer-based G2P approach\nto Tajik-Farsi transliteration, achieving chrF++ scores of 58.70 (Farsi to\nTajik) and 74.20 (Tajik to Farsi) on novel digraphic datasets, setting a\ncomparable baseline metric for future work. Our results also demonstrate the\nnon-trivial difficulty of this task in both directions. We also provide an\noverview of the differences between the two scripts and the challenges they\npresent, so as to aid future efforts in Tajik-Farsi transliteration.\n","authors":["Rayyan Merchant","Akhilesh Kakolu Ramarao","Kevin Tang"],"pdf_url":"https://arxiv.org/pdf/2502.20047v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.20046v1","updated":"2025-02-27T12:38:04Z","published":"2025-02-27T12:38:04Z","title":"Polish-ASTE: Aspect-Sentiment Triplet Extraction Datasets for Polish","summary":"  Aspect-Sentiment Triplet Extraction (ASTE) is one of the most challenging and\ncomplex tasks in sentiment analysis. It concerns the construction of triplets\nthat contain an aspect, its associated sentiment polarity, and an opinion\nphrase that serves as a rationale for the assigned polarity. Despite the\ngrowing popularity of the task and the many machine learning methods being\nproposed to address it, the number of datasets for ASTE is very limited. In\nparticular, no dataset is available for any of the Slavic languages. In this\npaper, we present two new datasets for ASTE containing customer opinions about\nhotels and purchased products expressed in Polish. We also perform experiments\nwith two ASTE techniques combined with two large language models for Polish to\ninvestigate their performance and the difficulty of the assembled datasets. The\nnew datasets are available under a permissive licence and have the same file\nformat as the English datasets, facilitating their use in future research.\n","authors":["Marta Lango","Borys Naglik","Mateusz Lango","Iwo Naglik"],"pdf_url":"https://arxiv.org/pdf/2502.20046v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.21018v3","updated":"2025-02-27T12:30:43Z","published":"2024-07-30T17:59:08Z","title":"ThinK: Thinner Key Cache by Query-Driven Pruning","summary":"  Large Language Models (LLMs) have revolutionized the field of natural\nlanguage processing, achieving unprecedented performance across a variety of\napplications. However, their increased computational and memory demands present\nsignificant challenges, especially when handling long sequences. This paper\nfocuses on the long-context scenario, addressing the inefficiencies in KV cache\nmemory consumption during inference. Unlike existing approaches that optimize\nthe memory based on the sequence length, we identify substantial redundancy in\nthe channel dimension of the KV cache, as indicated by an uneven magnitude\ndistribution and a low-rank structure in the attention weights. In response, we\npropose ThinK, a novel query-dependent KV cache pruning method designed to\nminimize attention weight loss while selectively pruning the least significant\nchannels. Our approach not only maintains or enhances model accuracy but also\nachieves a reduction in KV cache memory costs by over 20% compared with vanilla\nKV cache eviction and quantization methods. For instance, ThinK integrated with\nKIVI can achieve a 2.8x reduction in peak memory usage while maintaining nearly\nthe same quality, enabling up to a 5x increase in batch size when using a\nsingle GPU. Extensive evaluations on the LLaMA and Mistral models across\nvarious long-sequence datasets verified the efficiency of ThinK, establishing a\nnew baseline algorithm for efficient LLM deployment without compromising\nperformance. Our code has been made available at\nhttps://github.com/SalesforceAIResearch/ThinK.\n","authors":["Yuhui Xu","Zhanming Jie","Hanze Dong","Lei Wang","Xudong Lu","Aojun Zhou","Amrita Saha","Caiming Xiong","Doyen Sahoo"],"pdf_url":"https://arxiv.org/pdf/2407.21018v3.pdf","comment":"ICLR 2025 (Spotlight)"},{"id":"http://arxiv.org/abs/2405.14117v2","updated":"2025-02-27T12:29:11Z","published":"2024-05-23T02:44:12Z","title":"Knowledge Localization: Mission Not Accomplished? Enter Query\n  Localization!","summary":"  Large language models (LLMs) store extensive factual knowledge, but the\nmechanisms behind how they store and express this knowledge remain unclear. The\nKnowledge Neuron (KN) thesis is a prominent theory for explaining these\nmechanisms. This theory is based on the Knowledge Localization (KL) assumption,\nwhich suggests that a fact can be localized to a few knowledge storage units,\nnamely knowledge neurons.\n  However, this assumption has two limitations: first, it may be too rigid\nregarding knowledge storage, and second, it neglects the role of the attention\nmodule in knowledge expression.\n  In this paper, we first re-examine the KL assumption and demonstrate that its\nlimitations do indeed exist. To address these, we then present two new\nfindings, each targeting one of the limitations: one focusing on knowledge\nstorage and the other on knowledge expression. We summarize these findings as\n\\textbf{Query Localization} (QL) assumption and argue that the KL assumption\ncan be viewed as a simplification of the QL assumption. Based on QL assumption,\nwe further propose the Consistency-Aware KN modification method, which improves\nthe performance of knowledge modification, further validating our new\nassumption. We conduct 39 sets of experiments, along with additional\nvisualization experiments, to rigorously confirm our conclusions. Code is\navailable at https://github.com/heng840/KnowledgeLocalization.\n","authors":["Yuheng Chen","Pengfei Cao","Yubo Chen","Kang Liu","Jun Zhao"],"pdf_url":"https://arxiv.org/pdf/2405.14117v2.pdf","comment":"ICLR 2025 Spotlight"},{"id":"http://arxiv.org/abs/2502.20034v1","updated":"2025-02-27T12:20:02Z","published":"2025-02-27T12:20:02Z","title":"Vision-Encoders (Already) Know What They See: Mitigating Object\n  Hallucination via Simple Fine-Grained CLIPScore","summary":"  Recently, Large Vision-Language Models (LVLMs) show remarkable performance\nacross various domains. However, these models suffer from object hallucination.\nThis study revisits the previous claim that the primary cause of such\nhallucination lies in the limited representational capacity of the vision\nencoder. Our analysis reveals that the capacity of the vision encoder itself is\nalready enough for detecting object hallucination. Based on this insight, we\npropose a Fine-grained CLIPScore (F-CLIPScore), a simple yet effective\nevaluation metric that enhances object-level granularity by incorporating text\nembeddings at the noun phrase level. Evaluations on the OHD-Caps benchmark show\nthat F-CLIPScore significantly outperforms conventional CLIPScore in accuracy\nby a large margin of 39.6% without additional training. We further validate\nF-CLIPScore by showing that LVLM trained with the data filtered using\nF-CLIPScore exhibits reduced hallucination.\n","authors":["Hongseok Oh","Wonseok Hwang"],"pdf_url":"https://arxiv.org/pdf/2502.20034v1.pdf","comment":"4 pages"},{"id":"http://arxiv.org/abs/2502.12483v2","updated":"2025-02-27T11:45:30Z","published":"2025-02-18T03:09:55Z","title":"The Knowledge Microscope: Features as Better Analytical Lenses than\n  Neurons","summary":"  Previous studies primarily utilize MLP neurons as units of analysis for\nunderstanding the mechanisms of factual knowledge in Language Models (LMs);\nhowever, neurons suffer from polysemanticity, leading to limited knowledge\nexpression and poor interpretability. In this paper, we first conduct\npreliminary experiments to validate that Sparse Autoencoders (SAE) can\neffectively decompose neurons into features, which serve as alternative\nanalytical units. With this established, our core findings reveal three key\nadvantages of features over neurons: (1) Features exhibit stronger influence on\nknowledge expression and superior interpretability. (2) Features demonstrate\nenhanced monosemanticity, showing distinct activation patterns between related\nand unrelated facts. (3) Features achieve better privacy protection than\nneurons, demonstrated through our proposed FeatureEdit method, which\nsignificantly outperforms existing neuron-based approaches in erasing\nprivacy-sensitive information from LMs.Code and dataset will be available.\n","authors":["Yuheng Chen","Pengfei Cao","Kang Liu","Jun Zhao"],"pdf_url":"https://arxiv.org/pdf/2502.12483v2.pdf","comment":"ARR February UnderReview"},{"id":"http://arxiv.org/abs/2410.17247v2","updated":"2025-02-27T11:16:33Z","published":"2024-10-22T17:59:53Z","title":"PyramidDrop: Accelerating Your Large Vision-Language Models via Pyramid\n  Visual Redundancy Reduction","summary":"  In large vision-language models (LVLMs), images serve as inputs that carry a\nwealth of information. As the idiom \"A picture is worth a thousand words\"\nimplies, representing a single image in current LVLMs can require hundreds or\neven thousands of tokens. This results in significant computational costs,\nwhich grow quadratically as input image resolution increases, thereby severely\nimpacting the efficiency of both training and inference. Previous approaches\nhave attempted to reduce the number of image tokens either before or within the\nearly layers of LVLMs. However, these strategies inevitably result in the loss\nof crucial image information, ultimately diminishing model performance. To\naddress this challenge, we conduct an empirical study revealing that all visual\ntokens are necessary for LVLMs in the shallow layers, and token redundancy\nprogressively increases in the deeper layers of the model. To this end, we\npropose PyramidDrop, a visual redundancy reduction strategy for LVLMs to boost\ntheir efficiency in both training and inference with neglectable performance\nloss. Specifically, we partition the LVLM into several stages and drop part of\nthe image tokens at the end of each stage with a pre-defined ratio, creating\npyramid-like visual tokens across model layers. The dropping is based on a\nlightweight similarity calculation with a negligible time overhead. Extensive\nexperiments demonstrate that PyramidDrop can achieve a 40% training time and\n55% inference FLOPs acceleration of LLaVA-NeXT with comparable performance.\nBesides, the PyramidDrop could also serve as a plug-and-play strategy for\ninference acceleration without training, with better performance and lower\ninference cost than counterparts. Code is available at\nhttps://github.com/Cooperx521/PyramidDrop.\n","authors":["Long Xing","Qidong Huang","Xiaoyi Dong","Jiajie Lu","Pan Zhang","Yuhang Zang","Yuhang Cao","Conghui He","Jiaqi Wang","Feng Wu","Dahua Lin"],"pdf_url":"https://arxiv.org/pdf/2410.17247v2.pdf","comment":"CVPR 2025, code is available at\n  https://github.com/Cooperx521/PyramidDrop"},{"id":"http://arxiv.org/abs/2410.13461v2","updated":"2025-02-27T11:13:19Z","published":"2024-10-17T11:46:33Z","title":"Progressive Mixed-Precision Decoding for Efficient LLM Inference","summary":"  In spite of the great potential of large language models (LLMs) across\nvarious tasks, their deployment on resource-constrained devices remains\nchallenging due to their excessive computational and memory demands.\nQuantization has emerged as an effective solution by storing weights in reduced\nprecision. However, utilizing low precisions (i.e.~2/3-bit) to substantially\nalleviate the memory-boundedness of LLM decoding, still suffers from\nprohibitive performance drop. In this work, we argue that existing approaches\nfail to explore the diversity in computational patterns, redundancy, and\nsensitivity to approximations of the different phases of LLM inference,\nresorting to a uniform quantization policy throughout. Instead, we propose a\nnovel phase-aware method that selectively allocates precision during different\nphases of LLM inference, achieving both strong context extraction during\nprefill and efficient memory bandwidth utilization during decoding. To further\naddress the memory-boundedness of the decoding phase, we introduce Progressive\nMixed-Precision Decoding (PMPD), a technique that enables the gradual lowering\nof precision deeper in the generated sequence, together with a spectrum of\nprecision-switching schedulers that dynamically drive the precision-lowering\ndecisions in either task-adaptive or prompt-adaptive manner. Extensive\nevaluation across diverse language tasks shows that when targeting Nvidia GPUs,\nPMPD achieves 1.4$-$12.2$\\times$ speedup in matrix-vector multiplications over\nfp16 models, while when targeting an LLM-optimized NPU, our approach delivers a\nthroughput gain of 3.8$-$8.0$\\times$ over fp16 models and up to 1.54$\\times$\nover uniform quantization approaches while preserving the output quality.\n","authors":["Hao Mark Chen","Fuwen Tan","Alexandros Kouris","Royson Lee","Hongxiang Fan","Stylianos I. Venieris"],"pdf_url":"https://arxiv.org/pdf/2410.13461v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.05087v2","updated":"2025-02-27T11:04:13Z","published":"2025-02-07T17:04:39Z","title":"Mitigating Unintended Memorization with LoRA in Federated Learning for\n  LLMs","summary":"  Federated learning (FL) is a popular paradigm for collaborative training\nwhich avoids direct data exposure between clients. However, data privacy issues\nstill remain: FL-trained large language models are capable of memorizing and\ncompleting phrases and sentences contained in training data when given with\ntheir prefixes. Thus, it is possible for adversarial and honest-but-curious\nclients to recover training data of other participants simply through targeted\nprompting. In this work, we demonstrate that a popular and simple fine-tuning\nstrategy, low-rank adaptation (LoRA), reduces memorization during FL up to a\nfactor of 10. We study this effect by performing a medical question-answering\nfine-tuning task and injecting multiple replicas of out-of-distribution\nsensitive sequences drawn from an external clinical dataset. We observe a\nreduction in memorization for a wide variety of Llama 2 and 3 models, and find\nthat LoRA can reduce memorization in centralized learning as well. Furthermore,\nwe show that LoRA can be combined with other privacy-preserving techniques such\nas gradient clipping and Gaussian noising, secure aggregation, and Goldfish\nloss to further improve record-level privacy while maintaining performance.\n","authors":["Thierry Bossy","Julien Vignoud","Tahseen Rabbani","Juan R. Troncoso Pastoriza","Martin Jaggi"],"pdf_url":"https://arxiv.org/pdf/2502.05087v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.19982v1","updated":"2025-02-27T11:03:33Z","published":"2025-02-27T11:03:33Z","title":"Erasing Without Remembering: Safeguarding Knowledge Forgetting in Large\n  Language Models","summary":"  In this paper, we explore machine unlearning from a novel dimension, by\nstudying how to safeguard model unlearning in large language models (LLMs). Our\ngoal is to prevent unlearned models from recalling any related memory of the\ntargeted knowledge.We begin by uncovering a surprisingly simple yet overlooked\nfact: existing methods typically erase only the exact expressions of the\ntargeted knowledge, leaving paraphrased or related information intact. To\nrigorously measure such oversights, we introduce UGBench, the first benchmark\ntailored for evaluating the generalisation performance across 13\nstate-of-the-art methods.UGBench reveals that unlearned models can still recall\nparaphrased answers and retain target facts in intermediate layers. To address\nthis, we propose PERMU, a perturbation-based method that significantly enhances\nthe generalisation capabilities for safeguarding LLM unlearning.Experiments\ndemonstrate that PERMU delivers up to a 50.13% improvement in unlearning while\nmaintaining a 43.53% boost in robust generalisation. Our code can be found in\nhttps://github.com/MaybeLizzy/UGBench.\n","authors":["Huazheng Wang","Yongcheng Jing","Haifeng Sun","Yingjie Wang","Jingyu Wang","Jianxin Liao","Dacheng Tao"],"pdf_url":"https://arxiv.org/pdf/2502.19982v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.19981v1","updated":"2025-02-27T11:03:27Z","published":"2025-02-27T11:03:27Z","title":"The Lookahead Limitation: Why Multi-Operand Addition is Hard for LLMs","summary":"  Autoregressive large language models (LLMs) exhibit impressive performance\nacross various tasks but struggle with simple arithmetic, such as addition of\ntwo or more operands. We show that this struggle arises from LLMs' use of a\nsimple one-digit lookahead heuristic, which works fairly well (but not perfect)\nfor two-operand addition but fails in multi-operand cases, where the carry-over\nlogic is more complex. Our probing experiments and digit-wise accuracy\nevaluation show that LLMs fail precisely where a one-digit lookahead is\ninsufficient to account for cascading carries. We analyze the impact of\ntokenization strategies on arithmetic performance and show that all\ninvestigated models, regardless of tokenization, are inherently limited in the\naddition of multiple operands due to their reliance on a one-digit lookahead\nheuristic. Our findings reveal fundamental limitations that prevent LLMs from\ngeneralizing to more complex numerical reasoning.\n","authors":["Tanja Baeumel","Josef van Genabith","Simon Ostermann"],"pdf_url":"https://arxiv.org/pdf/2502.19981v1.pdf","comment":"Pre-print"},{"id":"http://arxiv.org/abs/2405.12910v3","updated":"2025-02-27T10:56:25Z","published":"2024-05-21T16:30:25Z","title":"Topic Classification of Case Law Using a Large Language Model and a New\n  Taxonomy for UK Law: AI Insights into Summary Judgment","summary":"  This paper addresses a critical gap in legal analytics by developing and\napplying a novel taxonomy for topic classification of summary judgment cases in\nthe United Kingdom. Using a curated dataset of summary judgment cases, we use\nthe Large Language Model Claude 3 Opus to explore functional topics and trends.\nWe find that Claude 3 Opus correctly classified the topic with an accuracy of\n87.13% and an F1 score of 0.87. The analysis reveals distinct patterns in the\napplication of summary judgments across various legal domains. As case law in\nthe United Kingdom is not originally labelled with keywords or a topic\nfiltering option, the findings not only refine our understanding of the\nthematic underpinnings of summary judgments but also illustrate the potential\nof combining traditional and AI-driven approaches in legal classification.\nTherefore, this paper provides a new and general taxonomy for UK law. The\nimplications of this work serve as a foundation for further research and policy\ndiscussions in the field of judicial administration and computational legal\nresearch methodologies.\n","authors":["Holli Sargeant","Ahmed Izzidien","Felix Steffek"],"pdf_url":"https://arxiv.org/pdf/2405.12910v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.19965v1","updated":"2025-02-27T10:45:27Z","published":"2025-02-27T10:45:27Z","title":"Deterministic or probabilistic? The psychology of LLMs as random number\n  generators","summary":"  Large Language Models (LLMs) have transformed text generation through\ninherently probabilistic context-aware mechanisms, mimicking human natural\nlanguage. In this paper, we systematically investigate the performance of\nvarious LLMs when generating random numbers, considering diverse configurations\nsuch as different model architectures, numerical ranges, temperature, and\nprompt languages. Our results reveal that, despite their stochastic\ntransformers-based architecture, these models often exhibit deterministic\nresponses when prompted for random numerical outputs. In particular, we find\nsignificant differences when changing the model, as well as the prompt\nlanguage, attributing this phenomenon to biases deeply embedded within the\ntraining data. Models such as DeepSeek-R1 can shed some light on the internal\nreasoning process of LLMs, despite arriving to similar results. These biases\ninduce predictable patterns that undermine genuine randomness, as LLMs are\nnothing but reproducing our own human cognitive biases.\n","authors":["Javier Coronado-Blázquez"],"pdf_url":"https://arxiv.org/pdf/2502.19965v1.pdf","comment":"31 pages, 12 figures"},{"id":"http://arxiv.org/abs/2410.13305v3","updated":"2025-02-27T10:38:51Z","published":"2024-10-17T08:05:02Z","title":"Reference-Based Post-OCR Processing with LLM for Precise Diacritic Text\n  in Historical Document Recognition","summary":"  Extracting fine-grained OCR text from aged documents in diacritic languages\nremains challenging due to unexpected artifacts, time-induced degradation, and\nlack of datasets. While standalone spell correction approaches have been\nproposed, they show limited performance for historical documents due to\nnumerous possible OCR error combinations and differences between modern and\nclassical corpus distributions. We propose a method utilizing available\ncontent-focused ebooks as a reference base to correct imperfect OCR-generated\ntext, supported by large language models. This technique generates\nhigh-precision pseudo-page-to-page labels for diacritic languages, where small\nstrokes pose significant challenges in historical conditions. The pipeline\neliminates various types of noise from aged documents and addresses issues such\nas missing characters, words, and disordered sequences. Our post-processing\nmethod, which generated a large OCR dataset of classical Vietnamese books,\nachieved a mean grading score of 8.72 on a 10-point scale. This outperformed\nthe state-of-the-art transformer-based Vietnamese spell correction model, which\nscored 7.03 when evaluated on a sampled subset of the dataset. We also trained\na baseline OCR model to assess and compare it with well-known engines.\nExperimental results demonstrate the strength of our baseline model compared to\nwidely used open-source solutions. The resulting dataset will be released\npublicly to support future studies.\n","authors":["Thao Do","Dinh Phu Tran","An Vo","Daeyoung Kim"],"pdf_url":"https://arxiv.org/pdf/2410.13305v3.pdf","comment":"Accepted in the AAAI 2025 (39th) AISI track. Dataset and repo are in\n  the paper"},{"id":"http://arxiv.org/abs/2502.19954v1","updated":"2025-02-27T10:30:50Z","published":"2025-02-27T10:30:50Z","title":"Collaborative Stance Detection via Small-Large Language Model\n  Consistency Verification","summary":"  Stance detection on social media aims to identify attitudes expressed in\ntweets towards specific targets. Current studies prioritize Large Language\nModels (LLMs) over Small Language Models (SLMs) due to the overwhelming\nperformance improving provided by LLMs. However, heavily relying on LLMs for\nstance detection, regardless of the cost, is impractical for real-world social\nmedia monitoring systems that require vast data analysis. To this end, we\npropose \\textbf{\\underline{Co}}llaborative Stance Detection via Small-Large\nLanguage Model Consistency \\textbf{\\underline{Ver}}ification (\\textbf{CoVer})\nframework, which enhances LLM utilization via context-shared batch reasoning\nand logical verification between LLM and SLM. Specifically, instead of\nprocessing each text individually, CoVer processes texts batch-by-batch,\nobtaining stance predictions and corresponding explanations via LLM reasoning\nin a shared context. Then, to exclude the bias caused by context noises, CoVer\nintroduces the SLM for logical consistency verification. Finally, texts that\nrepeatedly exhibit low logical consistency are classified using\nconsistency-weighted aggregation of prior LLM stance predictions. Our\nexperiments show that CoVer outperforms state-of-the-art methods across\nmultiple benchmarks in the zero-shot setting, achieving 0.54 LLM queries per\ntweet while significantly enhancing performance. Our CoVer offers a more\npractical solution for LLM deploying for social media stance detection.\n","authors":["Yu Yan","Sheng Sun","Zixiang Tang","Teli Liu","Min Liu"],"pdf_url":"https://arxiv.org/pdf/2502.19954v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.19953v1","updated":"2025-02-27T10:27:48Z","published":"2025-02-27T10:27:48Z","title":"GeoEdit: Geometric Knowledge Editing for Large Language Models","summary":"  Regular updates are essential for maintaining up-to-date knowledge in large\nlanguage models (LLMs). Consequently, various model editing methods have been\ndeveloped to update specific knowledge within LLMs. However, training-based\napproaches often struggle to effectively incorporate new knowledge while\npreserving unrelated general knowledge. To address this challenge, we propose a\nnovel framework called Geometric Knowledge Editing (GeoEdit). GeoEdit utilizes\nthe geometric relationships of parameter updates from fine-tuning to\ndifferentiate between neurons associated with new knowledge updates and those\nrelated to general knowledge perturbations. By employing a direction-aware\nknowledge identification method, we avoid updating neurons with directions\napproximately orthogonal to existing knowledge, thus preserving the model's\ngeneralization ability. For the remaining neurons, we integrate both old and\nnew knowledge for aligned directions and apply a \"forget-then-learn\" editing\nstrategy for opposite directions. Additionally, we introduce an\nimportance-guided task vector fusion technique that filters out redundant\ninformation and provides adaptive neuron-level weighting, further enhancing\nmodel editing performance. Extensive experiments on two publicly available\ndatasets demonstrate the superiority of GeoEdit over existing state-of-the-art\nmethods.\n","authors":["Yujie Feng","Liming Zhan","Zexin Lu","Yongxin Xu","Xu Chu","Yasha Wang","Jiannong Cao","Philip S. Yu","Xiao-Ming Wu"],"pdf_url":"https://arxiv.org/pdf/2502.19953v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.11401v2","updated":"2025-02-27T10:26:26Z","published":"2025-02-17T03:36:25Z","title":"Following the Autoregressive Nature of LLM Embeddings via Compression\n  and Alignment","summary":"  A new trend uses LLMs as dense text encoders via contrastive learning.\nHowever, since LLM embeddings predict the probability distribution of the next\ntoken, they are inherently generative and distributive, conflicting with\ncontrastive learning, which requires embeddings to capture full-text semantics\nand align via cosine similarity. This discrepancy hinders the full utilization\nof LLMs' pre-training capabilities, resulting in inefficient learning. In\nresponse to this issue, we propose AutoRegEmbed, a new contrastive learning\nmethod built on embedding conditional probability distributions, which\nintegrates two core tasks: information compression and conditional distribution\nalignment. The information compression task encodes text into the embedding\nspace, ensuring that the embedding vectors capture global semantics. The\nconditional distribution alignment task focuses on aligning text embeddings\nwith positive samples embeddings by leveraging the conditional distribution of\nembeddings while simultaneously reducing the likelihood of generating negative\nsamples from text embeddings, thereby achieving embedding alignment and\nuniformity. Experimental results demonstrate that our method significantly\noutperforms traditional contrastive learning approaches and achieves\nperformance comparable to state-of-the-art models when using the same amount of\ndata.\n","authors":["Jingcheng Deng","Zhongtao Jiang","Liang Pang","Liwei Chen","Kun Xu","Zihao Wei","Huawei Shen","Xueqi Cheng"],"pdf_url":"https://arxiv.org/pdf/2502.11401v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.19941v1","updated":"2025-02-27T10:11:53Z","published":"2025-02-27T10:11:53Z","title":"Alleviating Distribution Shift in Synthetic Data for Machine Translation\n  Quality Estimation","summary":"  Quality Estimation (QE) models evaluate the quality of machine translations\nwithout reference translations, serving as the reward models for the\ntranslation task. Due to the data scarcity, synthetic data generation has\nemerged as a promising solution. However, synthetic QE data often suffers from\ndistribution shift, which can manifest as discrepancies between pseudo and real\ntranslations, or in pseudo labels that do not align with human preferences. To\ntackle this issue, we introduce ADSQE, a novel framework for alleviating\ndistribution shift in synthetic QE data. To reduce the difference between\npseudo and real translations, we employ the constrained beam search algorithm\nand enhance translation diversity through the use of distinct generation\nmodels. ADSQE uses references, i.e., translation supervision signals, to guide\nboth the generation and annotation processes, enhancing the quality of\nword-level labels. ADSE further identifies the shortest phrase covering\nconsecutive error tokens, mimicking human annotation behavior, to assign the\nfinal phrase-level labels. Specially, we underscore that the translation model\ncan not annotate translations of itself accurately. Extensive experiments\ndemonstrate that ADSQE outperforms SOTA baselines like COMET in both supervised\nand unsupervised settings. Further analysis offers insights into synthetic data\ngeneration that could benefit reward models for other tasks.\n","authors":["Xiang Geng","Zhejian Lai","Jiajun Chen","Hao Yang","Shujian Huang"],"pdf_url":"https://arxiv.org/pdf/2502.19941v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.11843v2","updated":"2025-02-27T10:11:06Z","published":"2024-08-07T17:14:58Z","title":"Identifying and Mitigating Social Bias Knowledge in Language Models","summary":"  Generating fair and accurate predictions plays a pivotal role in deploying\nlarge language models (LLMs) in the real world. However, existing debiasing\nmethods inevitably generate unfair or incorrect predictions as they are\ndesigned and evaluated to achieve parity across different social groups but\nleave aside individual commonsense facts, resulting in modified knowledge that\nelicits unreasonable or undesired predictions. In this paper, we first\nestablish a new bias mitigation benchmark, BiaScope, which systematically\nassesses performance by leveraging newly constructed datasets and metrics on\nknowledge retention and generalization. Then, we propose a novel debiasing\napproach, Fairness Stamp (FAST), which enables fine-grained calibration of\nindividual social biases. FAST identifies the decisive layer responsible for\nstoring social biases and then calibrates its outputs by integrating a small\nmodular network, considering both bias mitigation and knowledge-preserving\ndemands. Comprehensive experiments demonstrate that FAST surpasses\nstate-of-the-art baselines with superior debiasing performance while not\ncompromising the overall model capability for knowledge retention and\ndownstream predictions. This highlights the potential of fine-grained debiasing\nstrategies to achieve fairness in LLMs.\n","authors":["Ruizhe Chen","Yichen Li","Jianfei Yang","Joey Tianyi Zhou","Jian Wu","Zuozhu Liu"],"pdf_url":"https://arxiv.org/pdf/2408.11843v2.pdf","comment":"NAACL 2025 Findings. arXiv admin note: substantial text overlap with\n  arXiv:2405.09341"},{"id":"http://arxiv.org/abs/2502.19917v1","updated":"2025-02-27T09:37:30Z","published":"2025-02-27T09:37:30Z","title":"Picking the Cream of the Crop: Visual-Centric Data Selection with\n  Collaborative Agents","summary":"  To improve Multimodal Large Language Models' (MLLMs) ability to process\nimages and complex instructions, researchers predominantly curate large-scale\nvisual instruction tuning datasets, which are either sourced from existing\nvision tasks or synthetically generated using LLMs and image descriptions.\nHowever, they often suffer from critical flaws, including misaligned\ninstruction-image pairs and low-quality images. Such issues hinder training\nefficiency and limit performance improvements, as models waste resources on\nnoisy or irrelevant data with minimal benefit to overall capability. To address\nthis issue, we propose a \\textbf{Vi}sual-Centric \\textbf{S}election approach\nvia \\textbf{A}gents Collaboration (ViSA), which centers on image quality\nassessment and image-instruction relevance evaluation. Specifically, our\napproach consists of 1) an image information quantification method via visual\nagents collaboration to select images with rich visual information, and 2) a\nvisual-centric instruction quality assessment method to select high-quality\ninstruction data related to high-quality images. Finally, we reorganize 80K\ninstruction data from large open-source datasets. Extensive experiments\ndemonstrate that ViSA outperforms or is comparable to current state-of-the-art\nmodels on seven benchmarks, using only 2.5\\% of the original data, highlighting\nthe efficiency of our data selection approach. Moreover, we conduct ablation\nstudies to validate the effectiveness of each component of our method. The code\nis available at https://github.com/HITsz-TMG/ViSA.\n","authors":["Zhenyu Liu","Yunxin Li","Baotian Hu","Wenhan Luo","Yaowei Wang","Min Zhang"],"pdf_url":"https://arxiv.org/pdf/2502.19917v1.pdf","comment":"15 pages, 7 figures"},{"id":"http://arxiv.org/abs/2402.13758v2","updated":"2025-02-27T09:26:10Z","published":"2024-02-21T12:35:19Z","title":"Factual consistency evaluation of summarization in the Era of large\n  language models","summary":"  Factual inconsistency with source documents in automatically generated\nsummaries can lead to misinformation or pose risks. Existing factual\nconsistency (FC) metrics are constrained by their performance, efficiency, and\nexplainability. Recent advances in Large language models (LLMs) have\ndemonstrated remarkable potential in text evaluation but their effectiveness in\nassessing FC in summarization remains underexplored. Prior research has mostly\nfocused on proprietary LLMs, leaving essential factors that affect their\nassessment capabilities unexplored. Additionally, current FC evaluation\nbenchmarks are restricted to news articles, casting doubt on the generality of\nthe FC methods tested on them. In this paper, we first address the gap by\nintroducing TreatFact a dataset of LLM-generated summaries of clinical texts,\nannotated for FC by domain experts. Moreover, we benchmark 11 LLMs for FC\nevaluation across news and clinical domains and analyse the impact of model\nsize, prompts, pre-training and fine-tuning data. Our findings reveal that\ndespite proprietary models prevailing on the task, open-source LLMs lag behind.\nNevertheless, there is potential for enhancing the performance of open-source\nLLMs through increasing model size, expanding pre-training data, and developing\nwell-curated fine-tuning data. Experiments on TreatFact suggest that both\nprevious methods and LLM-based evaluators are unable to capture factual\ninconsistencies in clinical summaries, posing a new challenge for FC\nevaluation.\n","authors":["Zheheng Luo","Qianqian Xie","Sophia Ananiadou"],"pdf_url":"https://arxiv.org/pdf/2402.13758v2.pdf","comment":"published on ESWA"},{"id":"http://arxiv.org/abs/2502.19907v1","updated":"2025-02-27T09:25:50Z","published":"2025-02-27T09:25:50Z","title":"Order Doesn't Matter, But Reasoning Does: Training LLMs with\n  Order-Centric Augmentation","summary":"  Logical reasoning is essential for large language models (LLMs) to ensure\naccurate and coherent inference. However, LLMs struggle with reasoning order\nvariations and fail to generalize across logically equivalent transformations.\nLLMs often rely on fixed sequential patterns rather than true logical\nunderstanding. To address this issue, we introduce an order-centric data\naugmentation framework based on commutativity in logical reasoning. We first\nrandomly shuffle independent premises to introduce condition order\naugmentation. For reasoning steps, we construct a directed acyclic graph (DAG)\nto model dependencies between steps, which allows us to identify valid\nreorderings of steps while preserving logical correctness. By leveraging\norder-centric augmentations, models can develop a more flexible and generalized\nreasoning process. Finally, we conduct extensive experiments across multiple\nlogical reasoning benchmarks, demonstrating that our method significantly\nenhances LLMs' reasoning performance and adaptability to diverse logical\nstructures. We release our codes and augmented data in\nhttps://anonymous.4open.science/r/Order-Centric-Data-Augmentation-822C/.\n","authors":["Qianxi He","Qianyu He","Jiaqing Liang","Yanghua Xiao","Weikang Zhou","Zeye Sun","Fei Yu"],"pdf_url":"https://arxiv.org/pdf/2502.19907v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2210.05715v2","updated":"2025-02-27T09:17:32Z","published":"2022-10-11T18:13:43Z","title":"Language Independent Stance Detection: Social Interaction-based\n  Embeddings and Large Language Models","summary":"  The large majority of the research performed on stance detection has been\nfocused on developing more or less sophisticated text classification systems,\neven when many benchmarks are based on social network data such as Twitter.\nThis paper aims to take on the stance detection task by placing the emphasis\nnot so much on the text itself but on the interaction data available on social\nnetworks. More specifically, we propose a new method to leverage social\ninformation such as friends and retweets by generating Relational Embeddings,\nnamely, dense vector representations of interaction pairs. Our experiments on\nseven publicly available datasets and four different languages (Basque,\nCatalan, Italian, and Spanish) show that combining our relational embeddings\nwith discriminative textual methods helps to substantially improve performance,\nobtaining state-of-the-art results for six out of seven evaluation settings,\noutperforming strong baselines based on Large Language Models, or other popular\ninteraction-based approaches such as DeepWalk or node2vec.\n","authors":["Joseba Fernandez de Landa","Rodrigo Agerri"],"pdf_url":"https://arxiv.org/pdf/2210.05715v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.06160v2","updated":"2025-02-27T09:06:45Z","published":"2024-11-09T12:09:26Z","title":"Expansion Quantization Network: An Efficient Micro-emotion Annotation\n  and Detection Framework","summary":"  Text emotion detection constitutes a crucial foundation for advancing\nartificial intelligence from basic comprehension to the exploration of\nemotional reasoning. Most existing emotion detection datasets rely on manual\nannotations, which are associated with high costs, substantial subjectivity,\nand severe label imbalances. This is particularly evident in the inadequate\nannotation of micro-emotions and the absence of emotional intensity\nrepresentation, which fail to capture the rich emotions embedded in sentences\nand adversely affect the quality of downstream task completion. By proposing an\nall-labels and training-set label regression method, we map label values to\nenergy intensity levels, thereby fully leveraging the learning capabilities of\nmachine models and the interdependencies among labels to uncover multiple\nemotions within samples. This led to the establishment of the Emotion\nQuantization Network (EQN) framework for micro-emotion detection and\nannotation. Using five commonly employed sentiment datasets, we conducted\ncomparative experiments with various models, validating the broad applicability\nof our framework within NLP machine learning models. Based on the EQN\nframework, emotion detection and annotation are conducted on the GoEmotions\ndataset. A comprehensive comparison with the results from Google literature\ndemonstrates that the EQN framework possesses a high capability for automatic\ndetection and annotation of micro-emotions. The EQN framework is the first to\nachieve automatic micro-emotion annotation with energy-level scores, providing\nstrong support for further emotion detection analysis and the quantitative\nresearch of emotion computing.\n","authors":["Jingyi Zhou","Senlin Luo","Haofan Chen"],"pdf_url":"https://arxiv.org/pdf/2411.06160v2.pdf","comment":"3.1 There is a misstatement in the EQN Framework section"},{"id":"http://arxiv.org/abs/2502.11089v2","updated":"2025-02-27T09:01:21Z","published":"2025-02-16T11:53:44Z","title":"Native Sparse Attention: Hardware-Aligned and Natively Trainable Sparse\n  Attention","summary":"  Long-context modeling is crucial for next-generation language models, yet the\nhigh computational cost of standard attention mechanisms poses significant\ncomputational challenges. Sparse attention offers a promising direction for\nimproving efficiency while maintaining model capabilities. We present NSA, a\nNatively trainable Sparse Attention mechanism that integrates algorithmic\ninnovations with hardware-aligned optimizations to achieve efficient\nlong-context modeling. NSA employs a dynamic hierarchical sparse strategy,\ncombining coarse-grained token compression with fine-grained token selection to\npreserve both global context awareness and local precision. Our approach\nadvances sparse attention design with two key innovations: (1) We achieve\nsubstantial speedups through arithmetic intensity-balanced algorithm design,\nwith implementation optimizations for modern hardware. (2) We enable end-to-end\ntraining, reducing pretraining computation without sacrificing model\nperformance. As shown in Figure 1, experiments show the model pretrained with\nNSA maintains or exceeds Full Attention models across general benchmarks,\nlong-context tasks, and instruction-based reasoning. Meanwhile, NSA achieves\nsubstantial speedups over Full Attention on 64k-length sequences across\ndecoding, forward propagation, and backward propagation, validating its\nefficiency throughout the model lifecycle.\n","authors":["Jingyang Yuan","Huazuo Gao","Damai Dai","Junyu Luo","Liang Zhao","Zhengyan Zhang","Zhenda Xie","Y. X. Wei","Lean Wang","Zhiping Xiao","Yuqing Wang","Chong Ruan","Ming Zhang","Wenfeng Liang","Wangding Zeng"],"pdf_url":"https://arxiv.org/pdf/2502.11089v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.19883v1","updated":"2025-02-27T08:44:04Z","published":"2025-02-27T08:44:04Z","title":"Beyond the Tip of Efficiency: Uncovering the Submerged Threats of\n  Jailbreak Attacks in Small Language Models","summary":"  Small language models (SLMs) have become increasingly prominent in the\ndeployment on edge devices due to their high efficiency and low computational\ncost. While researchers continue to advance the capabilities of SLMs through\ninnovative training strategies and model compression techniques, the security\nrisks of SLMs have received considerably less attention compared to large\nlanguage models (LLMs).To fill this gap, we provide a comprehensive empirical\nstudy to evaluate the security performance of 13 state-of-the-art SLMs under\nvarious jailbreak attacks. Our experiments demonstrate that most SLMs are quite\nsusceptible to existing jailbreak attacks, while some of them are even\nvulnerable to direct harmful prompts.To address the safety concerns, we\nevaluate several representative defense methods and demonstrate their\neffectiveness in enhancing the security of SLMs. We further analyze the\npotential security degradation caused by different SLM techniques including\narchitecture compression, quantization, knowledge distillation, and so on. We\nexpect that our research can highlight the security challenges of SLMs and\nprovide valuable insights to future work in developing more robust and secure\nSLMs.\n","authors":["Sibo Yi","Tianshuo Cong","Xinlei He","Qi Li","Jiaxing Song"],"pdf_url":"https://arxiv.org/pdf/2502.19883v1.pdf","comment":"12 pages. 6 figures"},{"id":"http://arxiv.org/abs/2502.19870v1","updated":"2025-02-27T08:21:28Z","published":"2025-02-27T08:21:28Z","title":"MMKE-Bench: A Multimodal Editing Benchmark for Diverse Visual Knowledge","summary":"  Knowledge editing techniques have emerged as essential tools for updating the\nfactual knowledge of large language models (LLMs) and multimodal models (LMMs),\nallowing them to correct outdated or inaccurate information without retraining\nfrom scratch. However, existing benchmarks for multimodal knowledge editing\nprimarily focus on entity-level knowledge represented as simple triplets, which\nfail to capture the complexity of real-world multimodal information. To address\nthis issue, we introduce MMKE-Bench, a comprehensive MultiModal Knowledge\nEditing Benchmark, designed to evaluate the ability of LMMs to edit diverse\nvisual knowledge in real-world scenarios. MMKE-Bench addresses these\nlimitations by incorporating three types of editing tasks: visual entity\nediting, visual semantic editing, and user-specific editing. Besides,\nMMKE-Bench uses free-form natural language to represent and edit knowledge,\noffering a more flexible and effective format. The benchmark consists of 2,940\npieces of knowledge and 8,363 images across 33 broad categories, with\nevaluation questions automatically generated and human-verified. We assess five\nstate-of-the-art knowledge editing methods on three prominent LMMs, revealing\nthat no method excels across all criteria, and that visual and user-specific\nedits are particularly challenging. MMKE-Bench sets a new standard for\nevaluating the robustness of multimodal knowledge editing techniques, driving\nprogress in this rapidly evolving field.\n","authors":["Yuntao Du","Kailin Jiang","Zhi Gao","Chenrui Shi","Zilong Zheng","Siyuan Qi","Qing Li"],"pdf_url":"https://arxiv.org/pdf/2502.19870v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16848v2","updated":"2025-02-27T08:15:49Z","published":"2024-10-22T09:35:42Z","title":"ETHIC: Evaluating Large Language Models on Long-Context Tasks with High\n  Information Coverage","summary":"  Recent advancements in large language models (LLM) capable of processing\nextremely long texts highlight the need for a dedicated evaluation benchmark to\nassess their long-context capabilities. However, existing methods, like the\nneedle-in-a-haystack test, do not effectively assess whether these models fully\nutilize contextual information, raising concerns about the reliability of\ncurrent evaluation techniques. To thoroughly examine the effectiveness of\nexisting benchmarks, we introduce a new metric called information coverage\n(IC), which quantifies the proportion of the input context necessary for\nanswering queries. Our findings indicate that current benchmarks exhibit low\nIC; although the input context may be extensive, the actual usable context is\noften limited. To address this, we present ETHIC, a novel benchmark designed to\nassess LLMs' ability to leverage the entire context. Our benchmark comprises\n1,986 test instances spanning four long-context tasks with high IC scores in\nthe domains of books, debates, medicine, and law. Our evaluations reveal\nsignificant performance drops in contemporary LLMs, highlighting a critical\nchallenge in managing long contexts. Our benchmark is available at\nhttps://github.com/dmis-lab/ETHIC.\n","authors":["Taewhoo Lee","Chanwoong Yoon","Kyochul Jang","Donghyeon Lee","Minju Song","Hyunjae Kim","Jaewoo Kang"],"pdf_url":"https://arxiv.org/pdf/2410.16848v2.pdf","comment":"NAACL 2025"},{"id":"http://arxiv.org/abs/2502.18798v2","updated":"2025-02-27T08:11:40Z","published":"2025-02-26T04:10:18Z","title":"ANPMI: Assessing the True Comprehension Capabilities of LLMs for\n  Multiple Choice Questions","summary":"  Multiple-choice benchmarks, consisting of various prompts and choices, are\namong the most widely used methods to assess a language model's natural\nlanguage understanding capability. Given a specific prompt, we typically\ncompute $P(Choice|Prompt)$ to evaluate how likely a language model is to\ngenerate the correct choice compared to incorrect ones. However, we observe\nthat performance measured using this approach reflects not only the model's\ncomprehension of the prompt but also its inherent biases for certain choices\nregardless of the prompt. This issue makes it challenging to accurately measure\na model's natural language understanding, as models may select the answer\nwithout fully understanding the prompt. To address this limitation, we propose\na novel metric called ANPMI, which normalizes Pointwise Mutual Information\n(PMI) by $-\\log P(Choice)$. ANPMI provides a more accurate assessment of the\nmodel's natural language understanding by ensuring that it is challenging to\nanswer a question without properly understanding the prompt.\n","authors":["Gyeongje Cho","Yeonkyoung So","Jaejin Lee"],"pdf_url":"https://arxiv.org/pdf/2502.18798v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.19860v1","updated":"2025-02-27T08:04:27Z","published":"2025-02-27T08:04:27Z","title":"MIND: Towards Immersive Psychological Healing with Multi-agent Inner\n  Dialogue","summary":"  Mental health issues are worsening in today's competitive society, such as\ndepression and anxiety. Traditional healings like counseling and chatbots fail\nto engage effectively, they often provide generic responses lacking emotional\ndepth. Although large language models (LLMs) have the potential to create more\nhuman-like interactions, they still struggle to capture subtle emotions. This\nrequires LLMs to be equipped with human-like adaptability and warmth. To fill\nthis gap, we propose the MIND (Multi-agent INner Dialogue), a novel paradigm\nthat provides more immersive psychological healing environments. Considering\nthe strong generative and role-playing ability of LLM agents, we predefine an\ninteractive healing framework and assign LLM agents different roles within the\nframework to engage in interactive inner dialogues with users, thereby\nproviding an immersive healing experience. We conduct extensive human\nexperiments in various real-world healing dimensions, and find that MIND\nprovides a more user-friendly experience than traditional paradigms. This\ndemonstrates that MIND effectively leverages the significant potential of LLMs\nin psychological healing.\n","authors":["Yujia Chen","Changsong Li","Yiming Wang","Qingqing Xiao","Nan Zhang","Zifan Kong","Peng Wang","Binyu Yan"],"pdf_url":"https://arxiv.org/pdf/2502.19860v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.19856v1","updated":"2025-02-27T07:59:01Z","published":"2025-02-27T07:59:01Z","title":"Team A at SemEval-2025 Task 11: Breaking Language Barriers in Emotion\n  Detection with Multilingual Models","summary":"  This paper describes the system submitted by Team A to SemEval 2025 Task 11,\n``Bridging the Gap in Text-Based Emotion Detection.'' The task involved\nidentifying the perceived emotion of a speaker from text snippets, with each\ninstance annotated with one of six emotions: joy, sadness, fear, anger,\nsurprise, or disgust. A dataset provided by the task organizers served as the\nfoundation for training and evaluating our models. Among the various approaches\nexplored, the best performance was achieved using multilingual embeddings\ncombined with a fully connected layer. This paper details the system\narchitecture, discusses experimental results, and highlights the advantages of\nleveraging multilingual representations for robust emotion detection in text.\n","authors":["P Sam Sahil","Anupam Jamatia"],"pdf_url":"https://arxiv.org/pdf/2502.19856v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.19852v1","updated":"2025-02-27T07:54:32Z","published":"2025-02-27T07:54:32Z","title":"ConvCodeWorld: Benchmarking Conversational Code Generation in\n  Reproducible Feedback Environments","summary":"  Large language models (LLMs) have proven invaluable for code generation,\nparticularly in interactive settings. However, existing code generation\nbenchmarks fail to capture the diverse feedback encountered in multi-turn\ninteractions, limiting our ability to evaluate LLMs in these contexts. To\naddress this gap, we present a set of novel benchmarks that explicitly model\nthe quality of feedback provided to code generation LLMs. Our contributions are\nthreefold: First, we introduce CONVCODEWORLD, a novel and reproducible\nenvironment for benchmarking interactive code generation. CONVCODEWORLD\nsimulates 9 distinct interactive code generation scenarios while systematically\ncombining three types of feedback: (a) compilation feedback; (b) execution\nfeedback with varying test coverage; (c) verbal feedback generated by GPT-4o\nwith different levels of expertise. Second, we introduce CONVCODEBENCH, a fast,\nstatic version of benchmark that uses pre-generated feedback logs, eliminating\nthe need for costly dynamic verbal feedback generation while maintaining strong\nSpearman's rank correlations (0.82 to 0.99) with CONVCODEWORLD. Third,\nextensive evaluations of both closed-source and open-source LLMs including\nR1-Distill on CONVCODEWORLD reveal key insights: (a) LLM performance varies\nsignificantly based on the feedback provided; (b) Weaker LLMs, with sufficient\nfeedback, can outperform single-turn results of state-of-the-art LLMs without\nfeedback; (c) Training on a specific feedback combination can limit an LLM's\nability to utilize unseen combinations; (d) LLMs solve problems in fewer turns\n(high MRR) may not solve as many problems overall (high Recall), and vice\nversa. All implementations and benchmarks will be made publicly available at\nhttps://huggingface.co/spaces/ConvCodeWorld/ConvCodeWorld\n","authors":["Hojae Han","Seung-won Hwang","Rajhans Samdani","Yuxiong He"],"pdf_url":"https://arxiv.org/pdf/2502.19852v1.pdf","comment":"ICLR 2025"},{"id":"http://arxiv.org/abs/2502.15132v2","updated":"2025-02-27T07:52:22Z","published":"2025-02-21T01:24:54Z","title":"CoT-ICL Lab: A Petri Dish for Studying Chain-of-Thought Learning from\n  In-Context Demonstrations","summary":"  We introduce CoT-ICL Lab, a framework and methodology to generate synthetic\ntokenized datasets and systematically study chain-of-thought (CoT) in-context\nlearning (ICL) in language models. CoT-ICL Lab allows fine grained control over\nthe complexity of in-context examples by decoupling (1) the causal structure\ninvolved in chain token generation from (2) the underlying token processing\nfunctions. We train decoder-only transformers (up to 700M parameters) on these\ndatasets and show that CoT accelerates the accuracy transition to higher values\nacross model sizes. In particular, we find that model depth is crucial for\nleveraging CoT with limited in-context examples, while more examples help\nshallow models match deeper model performance. Additionally, limiting the\ndiversity of token processing functions throughout training improves causal\nstructure learning via ICL. We also interpret these transitions by analyzing\ntransformer embeddings and attention maps. Overall, CoT-ICL Lab serves as a\nsimple yet powerful testbed for theoretical and empirical insights into ICL and\nCoT in language models.\n","authors":["Vignesh Kothapalli","Hamed Firooz","Maziar Sanjabi"],"pdf_url":"https://arxiv.org/pdf/2502.15132v2.pdf","comment":"23 pages, 27 figures, 3 tables, code at\n  https://github.com/kvignesh1420/cot-icl-lab"},{"id":"http://arxiv.org/abs/2501.13952v2","updated":"2025-02-27T07:51:29Z","published":"2025-01-20T06:35:01Z","title":"The Dual-use Dilemma in LLMs: Do Empowering Ethical Capacities Make a\n  Degraded Utility?","summary":"  Recent years have witnessed extensive efforts to enhance Large Language\nModels (LLMs) across various domains, alongside growing attention to their\nethical implications. However, a critical challenge remains largely overlooked:\nLLMs must balance between rejecting harmful requests for safety and\naccommodating legitimate ones for utility. This paper presents a Direct\nPreference Optimization (DPO) based alignment framework that achieves better\noverall performance by addressing this ethical-utility trade-off, using\nchemical domain applications as a proof-of-concept. Our alignment pipeline\nstarts with a GPT-assisted three-phase data generation scheme, in which we\ncreate LibraChemQA, a chemical question-answering dataset comprising 31.6k\ntriplet instances. By incorporating an innovative balanced seed in the data\ngeneration process, our framework systematically considers both legitimate and\nillegitimate requests. The framework also introduces a rephrasing mechanism for\nefficient data augmentation that enhances the model's chemical comprehension.\nWe further develop a novel hybrid evaluation scheme with LLM judges for precise\nassessment of both safety and utility. Experimental results demonstrate our\nmodel's substantial improvements in overall performance where both safety and\nutility are considered - the resulting model outperforms leading LLMs including\nClaude-3, GPT-4o, and LLaMA-3 by margins of 13.44%, 7.16%, and 7.10%\nrespectively on our released benchmark. At the end of this paper, we analyze\nexperimental results obtained from testing DeepSeek-R1 on our benchmark and\nreveal the critical ethical concerns raised by this highly acclaimed model. We\nhighlight that the long Chain-of-Thought (CoT) reasoning process employed by\nDeepSeek-R1, as well as other LLMs distilled from it, introduces significant\nethical vulnerabilities when exposed to users.\n","authors":["Yiyi Zhang","Xingyu Chen","Kexin Chen","Yuyang Du","Xilin Dang","Pheng-Ann Heng"],"pdf_url":"https://arxiv.org/pdf/2501.13952v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.23769v2","updated":"2025-02-27T07:47:40Z","published":"2024-10-31T09:33:37Z","title":"The Potential of LLMs in Medical Education: Generating Questions and\n  Answers for Qualification Exams","summary":"  In this work, we leverage LLMs to produce medical qualification exam\nquestions and the corresponding answers through few-shot prompts, investigating\nin-depth how LLMs meet the requirements in terms of coherence, evidence of\nstatement, factual consistency, and professionalism etc. Utilizing a\nmulticenter bidirectional anonymized database with respect to comorbid chronic\ndiseases, named Elderly Comorbidity Medical Database (CECMed), we tasked LLMs\nwith generating open-ended questions and answers based on a subset of sampled\nadmission reports. For CECMed, the retrospective cohort includes patients\nenrolled from January 2010 to January 2022 while the prospective cohort from\nJanuary 2023 to November 2023, with participants sourced from selected tertiary\nand community hospitals across the southern, northern, and central regions of\nChina. A total of 8 widely used LLMs were used, including ERNIE 4, ChatGLM 4,\nDoubao, Hunyuan, Spark 4, Qwen,\n  Conventional medical education requires sophisticated clinicians to formulate\nquestions and answers based on prototypes from EHRs, which is heuristic and\ntime-consuming. We found that mainstream LLMs could generate questions and\nanswers with real-world EHRs at levels close to clinicians. Although current\nLLMs performed dissatisfactory in some aspects, medical students, interns and\nresidents could reasonably make use of LLMs to facilitate understanding.\n","authors":["Yunqi Zhu","Wen Tang","Huayu Yang","Jinghao Niu","Liyang Dou","Yifan Gu","Yuanyuan Wu","Wensheng Zhang","Ying Sun","Xuebing Yang"],"pdf_url":"https://arxiv.org/pdf/2410.23769v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19271v2","updated":"2025-02-27T07:17:52Z","published":"2024-06-27T15:37:57Z","title":"AutoPureData: Automated Filtering of Undesirable Web Data to Update LLM\n  Knowledge","summary":"  Up-to-date and reliable language models are consistently sought after and are\nessential in various applications. Typically, models are trained on a fixed\ndataset and then deployed globally. However, the knowledge of the models\nbecomes outdated. Enabling automatic updation of AI knowledge using web data\ninvolves significant concerns regarding the model's safety and quality due to a\nthreat from unsafe and undesirable text across the web. The purity of new data\nwas essential for updating knowledge of language models to maintain their\nreliability. This paper proposes AutoPureData, a system that automatically\ncollects and purifies web data. The system loaded a sample of web data.\nUtilizing existing trusted AI models, it successfully eliminated unsafe text\nwith an accuracy of 97% and undesirable text with an accuracy of 86%,\ndemonstrating the system's effectiveness in purifying the data. The system\nensures that only meaningful and safe text can be used to update LLM knowledge.\nThe pure text was then optimized and stored in a vector database for future\nquerying. It was found that LLM can fetch new data from the vector DB. The LLM\nwrites the RAG query in English, even if the user's query is in another\nlanguage, proving that the system can perform cross-lingual retrieval. This\npaper proposes a method to maintain the accuracy and relevance of up-to-date\nlanguage models by ensuring that only purified data was used to update LLM\nknowledge. This work contributes to updating knowledge of chatbots using\nmeaningful and safe text, enhancing their utility across various industries,\nand potentially reducing the risks associated with outputs caused by unsafe or\nimpure data. Code is available at github.com/Pro-GenAI/AutoPureData.\n","authors":["Praneeth Vadlapati"],"pdf_url":"https://arxiv.org/pdf/2406.19271v2.pdf","comment":"Final version"},{"id":"http://arxiv.org/abs/2502.19830v1","updated":"2025-02-27T07:07:40Z","published":"2025-02-27T07:07:40Z","title":"Revisiting Self-Consistency from Dynamic Distributional Alignment\n  Perspective on Answer Aggregation","summary":"  Self-consistency improves reasoning by aggregating diverse stochastic\nsamples, yet the dynamics behind its efficacy remain underexplored. We reframe\nself-consistency as a dynamic distributional alignment problem, revealing that\ndecoding temperature not only governs sampling randomness but also actively\nshapes the latent answer distribution. Given that high temperatures require\nprohibitively large sample sizes to stabilize, while low temperatures risk\namplifying biases, we propose a confidence-driven mechanism that dynamically\ncalibrates temperature: sharpening the sampling distribution under uncertainty\nto align with high-probability modes, and promoting exploration when confidence\nis high. Experiments on mathematical reasoning tasks show this approach\noutperforms fixed-diversity baselines under limited samples, improving both\naverage and best-case performance across varying initial temperatures without\nadditional data or modules. This establishes self-consistency as a\nsynchronization challenge between sampling dynamics and evolving answer\ndistributions.\n","authors":["Yiwei Li","Ji Zhang","Shaoxiong Feng","Peiwen Yuan","Xinglin Wang","Jiayi Shi","Yueqi Zhang","Chuyi Tan","Boyuan Pan","Yao Hu","Kan Li"],"pdf_url":"https://arxiv.org/pdf/2502.19830v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.15308v2","updated":"2025-02-27T07:01:29Z","published":"2024-10-20T06:37:37Z","title":"LlamaLens: Specialized Multilingual LLM for Analyzing News and Social\n  Media Content","summary":"  Large Language Models (LLMs) have demonstrated remarkable success as\ngeneral-purpose task solvers across various fields. However, their capabilities\nremain limited when addressing domain-specific problems, particularly in\ndownstream NLP tasks. Research has shown that models fine-tuned on\ninstruction-based downstream NLP datasets outperform those that are not\nfine-tuned. While most efforts in this area have primarily focused on\nresource-rich languages like English and broad domains, little attention has\nbeen given to multilingual settings and specific domains. To address this gap,\nthis study focuses on developing a specialized LLM, LlamaLens, for analyzing\nnews and social media content in a multilingual context. To the best of our\nknowledge, this is the first attempt to tackle both domain specificity and\nmultilinguality, with a particular focus on news and social media. Our\nexperimental setup includes 18 tasks, represented by 52 datasets covering\nArabic, English, and Hindi. We demonstrate that LlamaLens outperforms the\ncurrent state-of-the-art (SOTA) on 23 testing sets, and achieves comparable\nperformance on 8 sets. We make the models and resources publicly available for\nthe research community\n(https://huggingface.co/collections/QCRI/llamalens-672f7e0604a0498c6a2f0fe9).\n","authors":["Mohamed Bayan Kmainasi","Ali Ezzat Shahroor","Maram Hasanain","Sahinur Rahman Laskar","Naeemul Hassan","Firoj Alam"],"pdf_url":"https://arxiv.org/pdf/2410.15308v2.pdf","comment":"LLMs, Multilingual, Language Diversity, Large Language Models, Social\n  Media, News Media, Specialized LLMs, Fact-checking, Media Analysis, Arabic,\n  Hindi, English"},{"id":"http://arxiv.org/abs/2410.10878v2","updated":"2025-02-27T07:01:28Z","published":"2024-10-09T10:11:24Z","title":"Herald: A Natural Language Annotated Lean 4 Dataset","summary":"  Verifiable formal languages like Lean have profoundly impacted mathematical\nreasoning, particularly through the use of large language models (LLMs) for\nautomated reasoning. A significant challenge in training LLMs for these formal\nlanguages is the lack of parallel datasets that align natural language with\nformal language proofs. To address this challenge, this paper introduces a\nnovel framework for translating the Mathlib4 corpus (a unified library of\nmathematics in formal language Lean 4) into natural language. Building upon\nthis, we employ a dual augmentation strategy that combines tactic-based and\ninformal-based approaches, leveraging the Lean-jixia system, a Lean 4 analyzer.\nWe present the results of this pipeline on Mathlib4 as Herald (Hierarchy and\nRetrieval-based Translated Lean Dataset). We also propose the Herald\nTranslator, which is fine-tuned on Herald. Herald translator achieves a 93.2%\naccuracy (Pass@128) on formalizing statements in the miniF2F-test and a 22.5%\naccuracy on our internal graduate-level textbook dataset, outperforming\nInternLM2-Math-Plus-7B (74.0% and 7.5%) and TheoremLlama (50.1% and 4.0%).\nFurthermore, we propose a section-level translation framework for real-world\napplications. As a direct application of Herald translator, we have\nsuccessfully translated a template section in the Stack project, marking a\nnotable progress in the automatic formalization of graduate-level mathematical\nliterature. Our model, along with the datasets, are open-sourced to the public.\n","authors":["Guoxiong Gao","Yutong Wang","Jiedong Jiang","Qi Gao","Zihan Qin","Tianyi Xu","Bin Dong"],"pdf_url":"https://arxiv.org/pdf/2410.10878v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.19820v1","updated":"2025-02-27T06:49:16Z","published":"2025-02-27T06:49:16Z","title":"Foot-In-The-Door: A Multi-turn Jailbreak for LLMs","summary":"  Ensuring AI safety is crucial as large language models become increasingly\nintegrated into real-world applications. A key challenge is jailbreak, where\nadversarial prompts bypass built-in safeguards to elicit harmful disallowed\noutputs. Inspired by psychological foot-in-the-door principles, we introduce\nFITD,a novel multi-turn jailbreak method that leverages the phenomenon where\nminor initial commitments lower resistance to more significant or more\nunethical transgressions.Our approach progressively escalates the malicious\nintent of user queries through intermediate bridge prompts and aligns the\nmodel's response by itself to induce toxic responses. Extensive experimental\nresults on two jailbreak benchmarks demonstrate that FITD achieves an average\nattack success rate of 94% across seven widely used models, outperforming\nexisting state-of-the-art methods. Additionally, we provide an in-depth\nanalysis of LLM self-corruption, highlighting vulnerabilities in current\nalignment strategies and emphasizing the risks inherent in multi-turn\ninteractions.The code is available at\nhttps://github.com/Jinxiaolong1129/Foot-in-the-door-Jailbreak .\n","authors":["Zixuan Weng","Xiaolong Jin","Jinyuan Jia","Xiangyu Zhang"],"pdf_url":"https://arxiv.org/pdf/2502.19820v1.pdf","comment":"19 pages, 8 figures"},{"id":"http://arxiv.org/abs/2502.14776v2","updated":"2025-02-27T06:30:11Z","published":"2025-02-20T17:59:45Z","title":"SurveyX: Academic Survey Automation via Large Language Models","summary":"  Large Language Models (LLMs) have demonstrated exceptional comprehension\ncapabilities and a vast knowledge base, suggesting that LLMs can serve as\nefficient tools for automated survey generation. However, recent research\nrelated to automated survey generation remains constrained by some critical\nlimitations like finite context window, lack of in-depth content discussion,\nand absence of systematic evaluation frameworks. Inspired by human writing\nprocesses, we propose SurveyX, an efficient and organized system for automated\nsurvey generation that decomposes the survey composing process into two phases:\nthe Preparation and Generation phases. By innovatively introducing online\nreference retrieval, a pre-processing method called AttributeTree, and a\nre-polishing process, SurveyX significantly enhances the efficacy of survey\ncomposition. Experimental evaluation results show that SurveyX outperforms\nexisting automated survey generation systems in content quality (0.259\nimprovement) and citation quality (1.76 enhancement), approaching human expert\nperformance across multiple evaluation dimensions. Examples of surveys\ngenerated by SurveyX are available on www.surveyx.cn\n","authors":["Xun Liang","Jiawei Yang","Yezhaohui Wang","Chen Tang","Zifan Zheng","Shichao Song","Zehao Lin","Yebin Yang","Simin Niu","Hanyu Wang","Bo Tang","Feiyu Xiong","Keming Mao","Zhiyu li"],"pdf_url":"https://arxiv.org/pdf/2502.14776v2.pdf","comment":"15 pages, 16 figures"},{"id":"http://arxiv.org/abs/2502.19801v1","updated":"2025-02-27T06:20:38Z","published":"2025-02-27T06:20:38Z","title":"Text classification using machine learning methods","summary":"  In this paper we present the results of an experiment aimed to use machine\nlearning methods to obtain models that can be used for the automatic\nclassification of products. In order to apply automatic classification methods,\nwe transformed the product names from a text representation to numeric vectors,\na process called word embedding. We used several embedding methods: Count\nVectorization, TF-IDF, Word2Vec, FASTTEXT, and GloVe. Having the product names\nin a form of numeric vectors, we proceeded with a set of machine learning\nmethods for automatic classification: Logistic Regression, Multinomial Naive\nBayes, kNN, Artificial Neural Networks, Support Vector Machines, and Decision\ntrees with several variants. The results show an impressive accuracy of the\nclassification process for Support Vector Machines, Logistic Regression, and\nRandom Forests. Regarding the word embedding methods, the best results were\nobtained with the FASTTEXT technique.\n","authors":["Bogdan Oancea"],"pdf_url":"https://arxiv.org/pdf/2502.19801v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.19784v1","updated":"2025-02-27T05:48:51Z","published":"2025-02-27T05:48:51Z","title":"NaijaNLP: A Survey of Nigerian Low-Resource Languages","summary":"  With over 500 languages in Nigeria, three languages -- Hausa, Yor\\`ub\\'a and\nIgbo -- spoken by over 175 million people, account for about 60% of the spoken\nlanguages. However, these languages are categorised as low-resource due to\ninsufficient resources to support tasks in computational linguistics. Several\nresearch efforts and initiatives have been presented, however, a coherent\nunderstanding of the state of Natural Language Processing (NLP) - from\ngrammatical formalisation to linguistic resources that support complex tasks\nsuch as language understanding and generation is lacking. This study presents\nthe first comprehensive review of advancements in low-resource NLP (LR-NLP)\nresearch across the three major Nigerian languages (NaijaNLP). We\nquantitatively assess the available linguistic resources and identify key\nchallenges. Although a growing body of literature addresses various NLP\ndownstream tasks in Hausa, Igbo, and Yor\\`ub\\'a, only about 25.1% of the\nreviewed studies contribute new linguistic resources. This finding highlights a\npersistent reliance on repurposing existing data rather than generating novel,\nhigh-quality resources. Additionally, language-specific challenges, such as the\naccurate representation of diacritics, remain under-explored. To advance\nNaijaNLP and LR-NLP more broadly, we emphasise the need for intensified efforts\nin resource enrichment, comprehensive annotation, and the development of open\ncollaborative initiatives.\n","authors":["Isa Inuwa-Dutse"],"pdf_url":"https://arxiv.org/pdf/2502.19784v1.pdf","comment":"35 pages, 2 figures, 4 tables"},{"id":"http://arxiv.org/abs/2410.20779v3","updated":"2025-02-27T05:47:22Z","published":"2024-10-28T06:40:03Z","title":"Decoding Reading Goals from Eye Movements","summary":"  Readers can have different goals with respect to the text that they are\nreading. Can these goals be decoded from their eye movements over the text? In\nthis work, we examine for the first time whether it is possible to distinguish\nbetween two types of common reading goals: information seeking and ordinary\nreading for comprehension. Using large-scale eye tracking data, we address this\ntask with a wide range of models that cover different architectural and data\nrepresentation strategies, and further introduce a new model ensemble. We find\nthat transformer-based models with scanpath representations coupled with\nlanguage modeling solve it most successfully, and that accurate predictions can\nbe made in real time, long before the participant finished reading the text. We\nfurther introduce a new method for model performance analysis based on mixed\neffect modeling. Combining this method with rich textual annotations reveals\nkey properties of textual items and participants that contribute to the\ndifficulty of the task, and improves our understanding of the variability in\neye movement patterns across the two reading regimes.\n","authors":["Omer Shubi","Cfir Avraham Hadar","Yevgeni Berzak"],"pdf_url":"https://arxiv.org/pdf/2410.20779v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.01821v2","updated":"2025-02-27T05:42:17Z","published":"2025-01-03T14:09:46Z","title":"SDPO: Segment-Level Direct Preference Optimization for Social Agents","summary":"  Social agents powered by large language models (LLMs) can simulate human\nsocial behaviors but fall short in handling complex social dialogues. Direct\nPreference Optimization (DPO) has proven effective in aligning LLM behavior\nwith human preferences across various agent tasks. However, standard DPO\nfocuses solely on individual turns, which limits its effectiveness in\nmulti-turn social interactions. Several DPO-based multi-turn alignment methods\nwith session-level data have shown potential in addressing this problem.While\nthese methods consider multiple turns across entire sessions, they are often\noverly coarse-grained, introducing training noise, and lack robust theoretical\nsupport. To resolve these limitations, we propose Segment-Level Direct\nPreference Optimization (SDPO), which dynamically select key segments within\ninteractions to optimize multi-turn agent behavior. SDPO minimizes training\nnoise and is grounded in a rigorous theoretical framework. Evaluations on the\nSOTOPIA benchmark demonstrate that SDPO-tuned agents consistently outperform\nboth existing DPO-based methods and proprietary LLMs like GPT-4o, underscoring\nSDPO's potential to advance the social intelligence of LLM-based agents. We\nrelease our code and data at\nhttps://github.com/AlibabaResearch/DAMO-ConvAI/tree/main/SDPO.\n","authors":["Aobo Kong","Wentao Ma","Shiwan Zhao","Yongbin Li","Yuchuan Wu","Ke Wang","Xiaoqian Liu","Qicheng Li","Yong Qin","Fei Huang"],"pdf_url":"https://arxiv.org/pdf/2501.01821v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.19779v1","updated":"2025-02-27T05:39:38Z","published":"2025-02-27T05:39:38Z","title":"Do Retrieval-Augmented Language Models Adapt to Varying User Needs?","summary":"  Recent advancements in Retrieval-Augmented Language Models (RALMs) have\ndemonstrated their efficacy in knowledge-intensive tasks. However, existing\nevaluation benchmarks often assume a single optimal approach to leveraging\nretrieved information, failing to account for varying user needs. This paper\nintroduces a novel evaluation framework that systematically assesses RALMs\nunder three user need cases-Context-Exclusive, Context-First, and\nMemory-First-across three distinct context settings: Context Matching,\nKnowledge Conflict, and Information Irrelevant. By varying both user\ninstructions and the nature of retrieved information, our approach captures the\ncomplexities of real-world applications where models must adapt to diverse user\nrequirements. Through extensive experiments on multiple QA datasets, including\nHotpotQA, DisentQA, and our newly constructed synthetic URAQ dataset, we find\nthat restricting memory usage improves robustness in adversarial retrieval\nconditions but decreases peak performance with ideal retrieval results and\nmodel family dominates behavioral differences. Our findings highlight the\nnecessity of user-centric evaluations in the development of retrieval-augmented\nsystems and provide insights into optimizing model performance across varied\nretrieval contexts. We will release our code and URAQ dataset upon acceptance\nof the paper.\n","authors":["Peilin Wu","Xinlu Zhang","Wenhao Yu","Xingyu Liu","Xinya Du","Zhiyu Zoey Chen"],"pdf_url":"https://arxiv.org/pdf/2502.19779v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.19773v1","updated":"2025-02-27T05:17:36Z","published":"2025-02-27T05:17:36Z","title":"Advancements in Natural Language Processing for Automatic Text\n  Summarization","summary":"  The substantial growth of textual content in diverse domains and platforms\nhas led to a considerable need for Automatic Text Summarization (ATS)\ntechniques that aid in the process of text analysis. The effectiveness of text\nsummarization models has been significantly enhanced in a variety of technical\ndomains because of advancements in Natural Language Processing (NLP) and Deep\nLearning (DL). Despite this, the process of summarizing textual information\ncontinues to be significantly constrained by the intricate writing styles of a\nvariety of texts, which involve a range of technical complexities. Text\nsummarization techniques can be broadly categorized into two main types:\nabstractive summarization and extractive summarization. Extractive\nsummarization involves directly extracting sentences, phrases, or segments of\ntext from the content without making any changes. On the other hand,\nabstractive summarization is achieved by reconstructing the sentences, phrases,\nor segments from the original text using linguistic analysis. Through this\nstudy, a linguistically diverse categorizations of text summarization\napproaches have been addressed in a constructive manner. In this paper, the\nauthors explored existing hybrid techniques that have employed both extractive\nand abstractive methodologies. In addition, the pros and cons of various\napproaches discussed in the literature are also investigated. Furthermore, the\nauthors conducted a comparative analysis on different techniques and matrices\nto evaluate the generated summaries using language generation models. This\nsurvey endeavors to provide a comprehensive overview of ATS by presenting the\nprogression of language processing regarding this task through a breakdown of\ndiverse systems and architectures accompanied by technical and mathematical\nexplanations of their operations.\n","authors":["Nevidu Jayatilleke","Ruvan Weerasinghe","Nipuna Senanayake"],"pdf_url":"https://arxiv.org/pdf/2502.19773v1.pdf","comment":"11 pages, 9 figures, ICCS 2024"},{"id":"http://arxiv.org/abs/2502.19765v1","updated":"2025-02-27T05:04:33Z","published":"2025-02-27T05:04:33Z","title":"EdiText: Controllable Coarse-to-Fine Text Editing with Diffusion\n  Language Models","summary":"  We propose EdiText, a controllable text editing method that modify the\nreference text to desired attributes at various scales. We integrate an\nSDEdit-based editing technique that allows for broad adjustments in the degree\nof text editing. Additionally, we introduce a novel fine-level editing method\nbased on self-conditioning, which allows subtle control of reference text.\nWhile being capable of editing on its own, this fine-grained method, integrated\nwith the SDEdit approach, enables EdiText to make precise adjustments within\nthe desired range. EdiText demonstrates its controllability to robustly adjust\nreference text at broad range of levels across various tasks, including\ntoxicity control and sentiment control.\n","authors":["Che Hyun Lee","Heeseung Kim","Jiheum Yeom","Sungroh Yoon"],"pdf_url":"https://arxiv.org/pdf/2502.19765v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.18770v2","updated":"2025-02-27T04:49:19Z","published":"2025-02-26T02:57:59Z","title":"Reward Shaping to Mitigate Reward Hacking in RLHF","summary":"  Reinforcement Learning from Human Feedback (RLHF) is essential for aligning\nlarge language models (LLMs) with human values. However, RLHF is susceptible to\nreward hacking, where the agent exploits flaws in the reward function rather\nthan learning the intended behavior, thus degrading alignment. While reward\nshaping helps stabilize RLHF and partially mitigate reward hacking, a\nsystematic investigation into shaping techniques and their underlying\nprinciples remains lacking. To bridge this gap, we present a comprehensive\nstudy of the prevalent reward shaping methods. Our analysis suggests three key\ndesign principles: (1) RL reward is ideally bounded, (2) RL benefits from rapid\ninitial growth followed by gradual convergence, and (3) RL reward is best\nformulated as a function of centered reward. Guided by these insights, we\npropose Preference As Reward (PAR), a novel approach that leverages the latent\npreferences embedded within the reward model itself as the signal for\nreinforcement learning. We evaluated PAR on two base models, Gemma2-2B and\nLlama3-8B, using two datasets, Ultrafeedback-Binarized and HH-RLHF.\nExperimental results demonstrate PAR's superior performance over other reward\nshaping methods. On the AlpacaEval 2.0 benchmark, PAR achieves a win rate at\nleast 5 percentage points higher than competing approaches. Furthermore, PAR\nexhibits remarkable data efficiency, requiring only a single reference reward\nfor optimal performance, and maintains robustness against reward hacking even\nafter two full epochs of training. Code is available at\nhttps://github.com/PorUna-byte/PAR.\n","authors":["Jiayi Fu","Xuandong Zhao","Chengyuan Yao","Heng Wang","Qi Han","Yanghua Xiao"],"pdf_url":"https://arxiv.org/pdf/2502.18770v2.pdf","comment":"19 pages"},{"id":"http://arxiv.org/abs/2412.04141v2","updated":"2025-02-27T04:43:54Z","published":"2024-12-05T13:10:54Z","title":"Reducing Tool Hallucination via Reliability Alignment","summary":"  Large Language Models (LLMs) have expanded their capabilities beyond language\ngeneration to interact with external tools, enabling automation and real-world\napplications. However, tool hallucinations, where models either select\ninappropriate tools or misuse them, pose significant challenges, leading to\nerroneous task execution, increased computational costs, and reduced system\nreliability. To systematically address this issue, we define and categorize\ntool hallucinations into two main types, tool selection hallucination and tool\nusage hallucination. To evaluate and mitigate these issues, we introduce\nRelyToolBench, which integrates specialized test cases and novel metrics to\nassess hallucination-aware task success and efficiency. Finally, we propose\nRelign, a reliability alignment framework that expands the tool-use action\nspace to include indecisive actions, allowing LLMs to defer tool use, seek\nclarification, or adjust tool selection dynamically. Through extensive\nexperiments, we demonstrate that Relign significantly reduces tool\nhallucinations, improves task reliability, and enhances the efficiency of LLM\ntool interactions.\n","authors":["Hongshen Xu","Zichen Zhu","Lei Pan","Zihan Wang","Su Zhu","Da Ma","Ruisheng Cao","Lu Chen","Kai Yu"],"pdf_url":"https://arxiv.org/pdf/2412.04141v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.15097v3","updated":"2025-02-27T04:41:34Z","published":"2025-02-20T23:30:45Z","title":"LUME: LLM Unlearning with Multitask Evaluations","summary":"  Unlearning aims to remove copyrighted, sensitive, or private content from\nlarge language models (LLMs) without a full retraining. In this work, we\ndevelop a multi-task unlearning benchmark (LUME) which features three tasks:\n(1) unlearn synthetically generated creative short novels, (2) unlearn\nsynthetic biographies with sensitive information, and (3) unlearn a collection\nof public biographies. We further release two fine-tuned LLMs of 1B and 7B\nparameter sizes as the target models. We conduct detailed evaluations of\nseveral recently proposed unlearning algorithms and present results on\ncarefully crafted metrics to understand their behavior and limitations.\n","authors":["Anil Ramakrishna","Yixin Wan","Xiaomeng Jin","Kai-Wei Chang","Zhiqi Bu","Bhanukiran Vinzamuri","Volkan Cevher","Mingyi Hong","Rahul Gupta"],"pdf_url":"https://arxiv.org/pdf/2502.15097v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.19756v1","updated":"2025-02-27T04:41:22Z","published":"2025-02-27T04:41:22Z","title":"PolyPrompt: Automating Knowledge Extraction from Multilingual Language\n  Models with Dynamic Prompt Generation","summary":"  Large language models (LLMs) showcase increasingly impressive English\nbenchmark scores, however their performance profiles remain inconsistent across\nmultilingual settings. To address this gap, we introduce PolyPrompt, a novel,\nparameter-efficient framework for enhancing the multilingual capabilities of\nLLMs. Our method learns a set of trigger tokens for each language through a\ngradient-based search, identifying the input query's language and selecting the\ncorresponding trigger tokens which are prepended to the prompt during\ninference. We perform experiments on two ~1 billion parameter models, with\nevaluations on the global MMLU benchmark across fifteen typologically and\nresource diverse languages, demonstrating accuracy gains of 3.7%-19.9% compared\nto naive and translation-pipeline baselines.\n","authors":["Nathan Roll"],"pdf_url":"https://arxiv.org/pdf/2502.19756v1.pdf","comment":"6 pages, 2 figures"},{"id":"http://arxiv.org/abs/2502.19749v1","updated":"2025-02-27T04:25:54Z","published":"2025-02-27T04:25:54Z","title":"Beneath the Surface: How Large Language Models Reflect Hidden Bias","summary":"  The exceptional performance of Large Language Models (LLMs) often comes with\nthe unintended propagation of social biases embedded in their training data.\nWhile existing benchmarks evaluate overt bias through direct term associations\nbetween bias concept terms and demographic terms, LLMs have become increasingly\nadept at avoiding biased responses, creating an illusion of neutrality.\nHowever, biases persist in subtler, contextually hidden forms that traditional\nbenchmarks fail to capture. We introduce the Hidden Bias Benchmark (HBB), a\nnovel dataset designed to assess hidden bias that bias concepts are hidden\nwithin naturalistic, subtly framed contexts in real-world scenarios. We analyze\nsix state-of-the-art LLMs, revealing that while models reduce bias in response\nto overt bias, they continue to reinforce biases in nuanced settings. Data,\ncode, and results are available at\nhttps://github.com/JP-25/Hidden-Bias-Benchmark.\n","authors":["Jinhao Pan","Chahat Raj","Ziyu Yao","Ziwei Zhu"],"pdf_url":"https://arxiv.org/pdf/2502.19749v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.19747v1","updated":"2025-02-27T04:20:47Z","published":"2025-02-27T04:20:47Z","title":"HaLoRA: Hardware-aware Low-Rank Adaptation for Large Language Models\n  Based on Hybrid Compute-in-Memory Architecture","summary":"  Low-rank adaptation (LoRA) is a predominant parameter-efficient finetuning\nmethod to adapt large language models (LLMs) for downstream tasks. In this\npaper, we first propose to deploy the LoRA-finetuned LLMs on the hybrid\ncompute-in-memory (CIM) architecture (i.e., pretrained weights onto RRAM and\nLoRA onto SRAM). To address performance degradation from RRAM's inherent noise,\nwe design a novel Hardware-aware Low-rank Adaption (HaLoRA) method, aiming to\ntrain a LoRA branch that is both robust and accurate by aligning the training\nobjectives under both ideal and noisy conditions. Experiments finetuning LLaMA\n3.2 1B and 3B demonstrate HaLoRA's effectiveness across multiple reasoning\ntasks, achieving up to 22.7 improvement in average score while maintaining\nrobustness at various noise levels.\n","authors":["Taiqiang Wu","Chenchen Ding","Wenyong Zhou","Yuxin Cheng","Xincheng Feng","Shuqi Wang","Chufan Shi","Zhengwu Liu","Ngai Wong"],"pdf_url":"https://arxiv.org/pdf/2502.19747v1.pdf","comment":"7 pages"},{"id":"http://arxiv.org/abs/2406.12030v2","updated":"2025-02-27T04:18:50Z","published":"2024-06-17T18:57:37Z","title":"SPA-VL: A Comprehensive Safety Preference Alignment Dataset for Vision\n  Language Model","summary":"  The emergence of Vision Language Models (VLMs) has brought unprecedented\nadvances in understanding multimodal information. The combination of textual\nand visual semantics in VLMs is highly complex and diverse, making the safety\nalignment of these models challenging. Furthermore, due to the limited study on\nthe safety alignment of VLMs, there is a lack of large-scale, high-quality\ndatasets. To address these limitations, we propose a Safety Preference\nAlignment dataset for Vision Language Models named SPA-VL. In terms of breadth,\nSPA-VL covers 6 harmfulness domains, 13 categories, and 53 subcategories, and\ncontains 100,788 samples of the quadruple (question, image, chosen response,\nrejected response). In terms of depth, the responses are collected from 12\nopen-source (e.g., QwenVL) and closed-source (e.g., Gemini) VLMs to ensure\ndiversity. The construction of preference data is fully automated, and the\nexperimental results indicate that models trained with alignment techniques on\nthe SPA-VL dataset exhibit substantial improvements in harmlessness and\nhelpfulness while maintaining core capabilities. SPA-VL, as a large-scale,\nhigh-quality, and diverse dataset, represents a significant milestone in\nensuring that VLMs achieve both harmlessness and helpfulness.\n","authors":["Yongting Zhang","Lu Chen","Guodong Zheng","Yifeng Gao","Rui Zheng","Jinlan Fu","Zhenfei Yin","Senjie Jin","Yu Qiao","Xuanjing Huang","Feng Zhao","Tao Gui","Jing Shao"],"pdf_url":"https://arxiv.org/pdf/2406.12030v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.19737v1","updated":"2025-02-27T04:02:13Z","published":"2025-02-27T04:02:13Z","title":"XCOMPS: A Multilingual Benchmark of Conceptual Minimal Pairs","summary":"  We introduce XCOMPS in this work, a multilingual conceptual minimal pair\ndataset covering 17 languages. Using this dataset, we evaluate LLMs'\nmultilingual conceptual understanding through metalinguistic prompting, direct\nprobability measurement, and neurolinguistic probing. By comparing base,\ninstruction-tuned, and knowledge-distilled models, we find that: 1) LLMs\nexhibit weaker conceptual understanding for low-resource languages, and\naccuracy varies across languages despite being tested on the same concept sets.\n2) LLMs excel at distinguishing concept-property pairs that are visibly\ndifferent but exhibit a marked performance drop when negative pairs share\nsubtle semantic similarities. 3) Instruction tuning improves performance in\nconcept understanding but does not enhance internal competence; knowledge\ndistillation can enhance internal competence in conceptual understanding for\nlow-resource languages with limited gains in explicit task performance. 4) More\nmorphologically complex languages yield lower concept understanding scores and\nrequire deeper layers for conceptual reasoning.\n","authors":["Linyang He","Ercong Nie","Sukru Samet Dindar","Arsalan Firoozi","Adrian Florea","Van Nguyen","Corentin Puffay","Riki Shimizu","Haotian Ye","Jonathan Brennan","Helmut Schmid","Hinrich Schütze","Nima Mesgarani"],"pdf_url":"https://arxiv.org/pdf/2502.19737v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.19735v1","updated":"2025-02-27T03:57:00Z","published":"2025-02-27T03:57:00Z","title":"R1-T1: Fully Incentivizing Translation Capability in LLMs via Reasoning\n  Learning","summary":"  Despite recent breakthroughs in reasoning-enhanced large language models\n(LLMs) like DeepSeek-R1, incorporating inference-time reasoning into machine\ntranslation (MT), where human translators naturally employ structured,\nmulti-layered reasoning chain-of-thoughts (CoTs), is yet underexplored.\nExisting methods either design a fixed CoT tailored for a specific MT sub-task\n(e.g., literature translation), or rely on synthesizing CoTs unaligned with\nhumans and supervised fine-tuning (SFT) prone to catastrophic forgetting,\nlimiting their adaptability to diverse translation scenarios. This paper\nintroduces R1-Translator (R1-T1), a novel framework to achieve inference-time\nreasoning for general MT via reinforcement learning (RL) with human-aligned\nCoTs comprising six common patterns. Our approach pioneers three innovations:\n(1) extending reasoning-based translation beyond MT sub-tasks to six languages\nand diverse tasks (e.g., legal/medical domain adaptation, idiom resolution);\n(2) formalizing six expert-curated CoT templates that mirror hybrid human\nstrategies like context-aware paraphrasing and back translation; and (3)\nenabling self-evolving CoT discovery and anti-forgetting adaptation through RL\nwith KL-constrained rewards. Experimental results indicate a steady translation\nperformance improvement in 21 languages and 80 translation directions on\nFlores-101 test set, especially on the 15 languages unseen from training, with\nits general multilingual abilities preserved compared with plain SFT.\n","authors":["Minggui He","Yilun Liu","Shimin Tao","Yuanchang Luo","Hongyong Zeng","Chang Su","Li Zhang","Hongxia Ma","Daimeng Wei","Weibin Meng","Hao Yang","Boxing Chen","Osamu Yoshie"],"pdf_url":"https://arxiv.org/pdf/2502.19735v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.19732v1","updated":"2025-02-27T03:53:45Z","published":"2025-02-27T03:53:45Z","title":"Speculative Decoding and Beyond: An In-Depth Review of Techniques","summary":"  Sequential dependencies present a fundamental bottleneck in deploying\nlarge-scale autoregressive models, particularly for real-time applications.\nWhile traditional optimization approaches like pruning and quantization often\ncompromise model quality, recent advances in generation-refinement frameworks\ndemonstrate that this trade-off can be significantly mitigated.\n  This survey presents a comprehensive taxonomy of generation-refinement\nframeworks, analyzing methods across autoregressive sequence tasks. We\ncategorize methods based on their generation strategies (from simple n-gram\nprediction to sophisticated draft models) and refinement mechanisms (including\nsingle-pass verification and iterative approaches). Through systematic analysis\nof both algorithmic innovations and system-level implementations, we examine\ndeployment strategies across computing environments and explore applications\nspanning text, images, and speech generation. This systematic examination of\nboth theoretical frameworks and practical implementations provides a foundation\nfor future research in efficient autoregressive decoding.\n","authors":["Yunhai Hu","Zining Liu","Zhenyuan Dong","Tianfan Peng","Bradley McDanel","Sai Qian Zhang"],"pdf_url":"https://arxiv.org/pdf/2502.19732v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.19731v1","updated":"2025-02-27T03:50:25Z","published":"2025-02-27T03:50:25Z","title":"Preference Learning Unlocks LLMs' Psycho-Counseling Skills","summary":"  Applying large language models (LLMs) to assist in psycho-counseling is an\nemerging and meaningful approach, driven by the significant gap between patient\nneeds and the availability of mental health support. However, current LLMs\nstruggle to consistently provide effective responses to client speeches,\nlargely due to the lack of supervision from high-quality real psycho-counseling\ndata, whose content is typically inaccessible due to client privacy concerns.\nFurthermore, the quality of therapists' responses in available sessions can\nvary significantly based on their professional training and experience.\nAssessing the quality of therapists' responses remains an open challenge. In\nthis work, we address these challenges by first proposing a set of professional\nand comprehensive principles to evaluate therapists' responses to client\nspeeches. Using these principles, we create a preference dataset,\nPsychoCounsel-Preference, which contains 36k high-quality preference comparison\npairs. This dataset aligns with the preferences of professional\npsychotherapists, providing a robust foundation for evaluating and improving\nLLMs in psycho-counseling. Experiments on reward modeling and preference\nlearning demonstrate that PsychoCounsel-Preference is an excellent resource for\nLLMs to acquire essential skills for responding to clients in a counseling\nsession. Our best-aligned model, PsychoCounsel-Llama3-8B, achieves an\nimpressive win rate of 87% against GPT-4o. We release PsychoCounsel-Preference,\nPsychoCounsel-Llama3-8B and the reward model PsychoCounsel Llama3-8B-Reward to\nfacilitate the research of psycho-counseling with LLMs at:\nhttps://hf.co/Psychotherapy-LLM.\n","authors":["Mian Zhang","Shaun M. Eack","Zhiyu Zoey Chen"],"pdf_url":"https://arxiv.org/pdf/2502.19731v1.pdf","comment":"10 pages, 6 figures"},{"id":"http://arxiv.org/abs/2502.19726v1","updated":"2025-02-27T03:37:45Z","published":"2025-02-27T03:37:45Z","title":"Tokens for Learning, Tokens for Unlearning: Mitigating Membership\n  Inference Attacks in Large Language Models via Dual-Purpose Training","summary":"  Large language models (LLMs) have become the backbone of modern natural\nlanguage processing but pose privacy concerns about leaking sensitive training\ndata. Membership inference attacks (MIAs), which aim to infer whether a sample\nis included in a model's training dataset, can serve as a foundation for\nbroader privacy threats. Existing defenses designed for traditional\nclassification models do not account for the sequential nature of text data. As\na result, they either require significant computational resources or fail to\neffectively mitigate privacy risks in LLMs. In this work, we propose a\nlightweight yet effective empirical privacy defense for protecting training\ndata of language modeling by leveraging the token-specific characteristics. By\nanalyzing token dynamics during training, we propose a token selection strategy\nthat categorizes tokens into hard tokens for learning and memorized tokens for\nunlearning. Subsequently, our training-phase defense optimizes a novel\ndual-purpose token-level loss to achieve a Pareto-optimal balance between\nutility and privacy. Extensive experiments demonstrate that our approach not\nonly provides strong protection against MIAs but also improves language\nmodeling performance by around 10\\% across various LLM architectures and\ndatasets compared to the baselines.\n","authors":["Toan Tran","Ruixuan Liu","Li Xiong"],"pdf_url":"https://arxiv.org/pdf/2502.19726v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.19723v1","updated":"2025-02-27T03:25:34Z","published":"2025-02-27T03:25:34Z","title":"CNsum:Automatic Summarization for Chinese News Text","summary":"  Obtaining valuable information from massive data efficiently has become our\nresearch goal in the era of Big Data. Text summarization technology has been\ncontinuously developed to meet this demand. Recent work has also shown that\ntransformer-based pre-trained language models have achieved great success on\nvarious tasks in Natural Language Processing (NLP). Aiming at the problem of\nChinese news text summary generation and the application of Transformer\nstructure on Chinese, this paper proposes a Chinese news text summarization\nmodel (CNsum) based on Transformer structure, and tests it on Chinese datasets\nsuch as THUCNews. The results of the conducted experiments show that CNsum\nachieves better ROUGE score than the baseline models, which verifies the\noutperformance of the model.\n","authors":["Yu Zhao","Songping Huang","Dongsheng Zhou","Zhaoyun Ding","Fei Wang","Aixin Nian"],"pdf_url":"https://arxiv.org/pdf/2502.19723v1.pdf","comment":"WASA 2022"},{"id":"http://arxiv.org/abs/2502.19722v1","updated":"2025-02-27T03:24:57Z","published":"2025-02-27T03:24:57Z","title":"Few-Shot Multilingual Open-Domain QA from 5 Examples","summary":"  Recent approaches to multilingual open-domain question answering (MLODQA)\nhave achieved promising results given abundant language-specific training data.\nHowever, the considerable annotation cost limits the application of these\nmethods for underrepresented languages. We introduce a \\emph{few-shot learning}\napproach to synthesise large-scale multilingual data from large language models\n(LLMs). Our method begins with large-scale self-supervised pre-training using\nWikiData, followed by training on high-quality synthetic multilingual data\ngenerated by prompting LLMs with few-shot supervision. The final model,\n\\textsc{FsModQA}, significantly outperforms existing few-shot and supervised\nbaselines in MLODQA and cross-lingual and monolingual retrieval. We further\nshow our method can be extended for effective zero-shot adaptation to new\nlanguages through a \\emph{cross-lingual prompting} strategy with only\nEnglish-supervised data, making it a general and applicable solution for MLODQA\ntasks without costly large-scale annotation.\n","authors":["Fan Jiang","Tom Drummond","Trevor Cohn"],"pdf_url":"https://arxiv.org/pdf/2502.19722v1.pdf","comment":"Accepted by TACL; pre-MIT Press publication version"},{"id":"http://arxiv.org/abs/2502.19721v1","updated":"2025-02-27T03:24:09Z","published":"2025-02-27T03:24:09Z","title":"Sensing and Steering Stereotypes: Extracting and Applying Gender\n  Representation Vectors in LLMs","summary":"  Large language models (LLMs) are known to perpetuate stereotypes and exhibit\nbiases. Various strategies have been proposed to mitigate potential harms that\nmay result from these biases, but most work studies biases in LLMs as a\nblack-box problem without considering how concepts are represented within the\nmodel. We adapt techniques from representation engineering to study how the\nconcept of \"gender\" is represented within LLMs. We introduce a new method that\nextracts concept representations via probability weighting without labeled data\nand efficiently selects a steering vector for measuring and manipulating the\nmodel's representation. We also present a projection-based method that enables\nprecise steering of model predictions and demonstrate its effectiveness in\nmitigating gender bias in LLMs.\n","authors":["Hannah Cyberey","Yangfeng Ji","David Evans"],"pdf_url":"https://arxiv.org/pdf/2502.19721v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.18978v2","updated":"2025-02-27T03:20:03Z","published":"2025-02-26T09:37:21Z","title":"Low-Confidence Gold: Refining Low-Confidence Samples for Efficient\n  Instruction Tuning","summary":"  The effectiveness of instruction fine-tuning for Large Language Models is\nfundamentally constrained by the quality and efficiency of training datasets.\nThis work introduces Low-Confidence Gold (LCG), a novel filtering framework\nthat employs centroid-based clustering and confidence-guided selection for\nidentifying valuable instruction pairs. Through a semi-supervised approach\nusing a lightweight classifier trained on representative samples, LCG curates\nhigh-quality subsets while preserving data diversity. Experimental evaluation\ndemonstrates that models fine-tuned on LCG-filtered subsets of 6K samples\nachieve superior performance compared to existing methods, with substantial\nimprovements on MT-bench and consistent gains across comprehensive evaluation\nmetrics. The framework's efficacy while maintaining model performance\nestablishes a promising direction for efficient instruction tuning.\n","authors":["Hongyi Cal","Jie Li","Wenzhen Dong"],"pdf_url":"https://arxiv.org/pdf/2502.18978v2.pdf","comment":"8 pages"},{"id":"http://arxiv.org/abs/2406.08772v3","updated":"2025-02-27T03:19:48Z","published":"2024-06-13T03:04:28Z","title":"MMFakeBench: A Mixed-Source Multimodal Misinformation Detection\n  Benchmark for LVLMs","summary":"  Current multimodal misinformation detection (MMD) methods often assume a\nsingle source and type of forgery for each sample, which is insufficient for\nreal-world scenarios where multiple forgery sources coexist. The lack of a\nbenchmark for mixed-source misinformation has hindered progress in this field.\nTo address this, we introduce MMFakeBench, the first comprehensive benchmark\nfor mixed-source MMD. MMFakeBench includes 3 critical sources: textual veracity\ndistortion, visual veracity distortion, and cross-modal consistency distortion,\nalong with 12 sub-categories of misinformation forgery types. We further\nconduct an extensive evaluation of 6 prevalent detection methods and 15 Large\nVision-Language Models (LVLMs) on MMFakeBench under a zero-shot setting. The\nresults indicate that current methods struggle under this challenging and\nrealistic mixed-source MMD setting. Additionally, we propose MMD-Agent, a novel\napproach to integrate the reasoning, action, and tool-use capabilities of LVLM\nagents, significantly enhancing accuracy and generalization. We believe this\nstudy will catalyze future research into more realistic mixed-source multimodal\nmisinformation and provide a fair evaluation of misinformation detection\nmethods.\n","authors":["Xuannan Liu","Zekun Li","Peipei Li","Huaibo Huang","Shuhan Xia","Xing Cui","Linzhi Huang","Weihong Deng","Zhaofeng He"],"pdf_url":"https://arxiv.org/pdf/2406.08772v3.pdf","comment":"Accepted by ICLR 2025, Project page:\n  https://liuxuannan.github.io/MMFakeBench.github.io/"},{"id":"http://arxiv.org/abs/2502.18168v3","updated":"2025-02-27T02:57:34Z","published":"2025-02-25T13:00:05Z","title":"SECURA: Sigmoid-Enhanced CUR Decomposition with Uninterrupted Retention\n  and Low-Rank Adaptation in Large Language Models","summary":"  With the rapid development of large language models (LLMs), fully fine-tuning\n(FT) these models has become increasingly impractical due to the high\ncomputational demands. Additionally, FT can lead to catastrophic forgetting. As\nan alternative, Low-Rank Adaptation (LoRA) has been proposed, which fine-tunes\nonly a small subset of parameters, achieving similar performance to FT while\nsignificantly reducing resource requirements. However, since LoRA inherits FT's\ndesign, the issue of catastrophic forgetting remains.\n  To address these challenges, we propose SECURA: Sigmoid-Enhanced CUR\nDecomposition LoRA, a novel parameter-efficient fine-tuning (PEFT) variant that\nmitigates catastrophic forgetting while improving fine-tuning performance. Our\nmethod introduces a new normalization technique, SigNorm, to enhance parameter\nretention and overall performance.\n  SECURA has been evaluated on a variety of tasks, including mathematical\nproblem-solving (GSM8K), challenging question-answering (CNNDM), translation\n(NewsDE), and complex multiple-choice reasoning (LogiQA). Experimental results\nshow that SECURA achieves an average fine-tuning improvement of 3.59% across\nfour multiple-choice question (MCQ) tasks and a 2.51% improvement across five\nquestion-answering (QA) tasks on models such as Gemma2 2b, Qwen2 1.5b, Qwen 2\n7b, Llama3 8b, and Llama3.1 8b, compared to DoRA. Moreover, SECURA demonstrates\nsuperior knowledge retention capabilities, maintaining more than 70% accuracy\non basic LLM knowledge across 16 continual learning tests, outperforming\nExperience Replay (ER), Sequential Learning (SEQ), EWC, I-LoRA, and CUR-LoRA.\n","authors":["Yuxuan Zhang"],"pdf_url":"https://arxiv.org/pdf/2502.18168v3.pdf","comment":"New work on Parameter-Efficient Fine-Tuning (PEFT) for large language\n  models. Includes new techniques SigNorm and CABR-LoRA for optimizing\n  fine-tune performance and Knowledge retention"},{"id":"http://arxiv.org/abs/2410.03761v2","updated":"2025-02-27T02:40:39Z","published":"2024-10-02T13:02:03Z","title":"Taxonomy Tree Generation from Citation Graph","summary":"  Constructing taxonomies from citation graphs is essential for organizing\nscientific knowledge, facilitating literature reviews, and identifying emerging\nresearch trends. However, manual taxonomy construction is labor-intensive,\ntime-consuming, and prone to human biases, often overlooking pivotal but\nless-cited papers. In this paper, to enable automatic hierarchical taxonomy\ngeneration from citation graphs, we propose HiGTL (Hierarchical Graph Taxonomy\nLearning), a novel end-to-end framework guided by human-provided instructions\nor preferred topics. Specifically, we propose a hierarchical citation graph\nclustering method that recursively groups related papers based on both textual\ncontent and citation structure, ensuring semantically meaningful and\nstructurally coherent clusters. Additionally, we develop a novel taxonomy node\nverbalization strategy that iteratively generates central concepts for each\ncluster, leveraging a pre-trained large language model (LLM) to maintain\nsemantic consistency across hierarchical levels. To further enhance\nperformance, we design a joint optimization framework that fine-tunes both the\nclustering and concept generation modules, aligning structural accuracy with\nthe quality of generated taxonomies. Extensive experiments demonstrate that\nHiGTL effectively produces coherent, high-quality taxonomies.\n","authors":["Yuntong Hu","Zhuofeng Li","Zheng Zhang","Chen Ling","Raasikh Kanjiani","Boxin Zhao","Liang Zhao"],"pdf_url":"https://arxiv.org/pdf/2410.03761v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.19684v1","updated":"2025-02-27T01:51:45Z","published":"2025-02-27T01:51:45Z","title":"GRACE: A Granular Benchmark for Evaluating Model Calibration against\n  Human Calibration","summary":"  Language models are often miscalibrated, leading to confidently incorrect\nanswers. We introduce GRACE, a benchmark for language model calibration that\nincorporates comparison with human calibration. GRACE consists of\nquestion-answer pairs, in which each question contains a series of clues that\ngradually become easier, all leading to the same answer; models must answer\ncorrectly as early as possible as the clues are revealed. This setting permits\ngranular measurement of model calibration based on how early, accurately, and\nconfidently a model answers. After collecting these questions, we host live\nhuman vs. model competitions to gather 1,749 data points on human and model\nteams' timing, accuracy, and confidence. We propose a metric, CalScore, that\nuses GRACE to analyze model calibration errors and identify types of model\nmiscalibration that differ from human behavior. We find that although humans\nare less accurate than models, humans are generally better calibrated. Since\nstate-of-the-art models struggle on GRACE, it effectively evaluates progress on\nimproving model calibration.\n","authors":["Yoo Yeon Sung","Eve Fleisig","Yu Hou","Ishan Upadhyay","Jordan Lee Boyd-Graber"],"pdf_url":"https://arxiv.org/pdf/2502.19684v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.03818v3","updated":"2025-02-27T01:42:18Z","published":"2024-04-04T21:57:11Z","title":"PRobELM: Plausibility Ranking Evaluation for Language Models","summary":"  This paper introduces PRobELM (Plausibility Ranking Evaluation for Language\nModels), a benchmark designed to assess language models' ability to discern\nmore plausible from less plausible scenarios through their parametric\nknowledge. While benchmarks such as TruthfulQA emphasise factual accuracy or\ntruthfulness, and others such as COPA explore plausible scenarios without\nexplicitly incorporating world knowledge, PRobELM seeks to bridge this gap by\nevaluating models' capabilities to prioritise plausible scenarios that leverage\nworld knowledge over less plausible alternatives. This design allows us to\nassess the potential of language models for downstream use cases such as\nliterature-based discovery where the focus is on identifying information that\nis likely but not yet known. Our benchmark is constructed from a dataset\ncurated from Wikidata edit histories, tailored to align the temporal bounds of\nthe training data for the evaluated models. PRobELM facilitates the evaluation\nof language models across multiple prompting types, including statement, text\ncompletion, and question-answering. Experiments with 10 models of various sizes\nand architectures on the relationship between model scales, training recency,\nand plausibility performance, reveal that factual accuracy does not directly\ncorrelate with plausibility performance and that up-to-date training data\nenhances plausibility assessment across different model architectures.\n","authors":["Zhangdie Yuan","Eric Chamoun","Rami Aly","Chenxi Whitehouse","Andreas Vlachos"],"pdf_url":"https://arxiv.org/pdf/2404.03818v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.19676v1","updated":"2025-02-27T01:36:00Z","published":"2025-02-27T01:36:00Z","title":"The Future Outcome Reasoning and Confidence Assessment Benchmark","summary":"  Forecasting is an important task in many domains, such as technology and\neconomics. However existing forecasting benchmarks largely lack comprehensive\nconfidence assessment, focus on limited question types, and often consist of\nartificial questions that do not align with real-world human forecasting needs.\nTo address these gaps, we introduce FOReCAst (Future Outcome Reasoning and\nConfidence Assessment), a benchmark that evaluates models' ability to make\npredictions and their confidence in them. FOReCAst spans diverse forecasting\nscenarios involving Boolean questions, timeframe prediction, and quantity\nestimation, enabling a comprehensive evaluation of both prediction accuracy and\nconfidence calibration for real-world applications.\n","authors":["Zhangdie Yuan","Zifeng Ding","Andreas Vlachos"],"pdf_url":"https://arxiv.org/pdf/2502.19676v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.12893v2","updated":"2025-02-27T01:32:58Z","published":"2025-02-18T14:29:12Z","title":"H-CoT: Hijacking the Chain-of-Thought Safety Reasoning Mechanism to\n  Jailbreak Large Reasoning Models, Including OpenAI o1/o3, DeepSeek-R1, and\n  Gemini 2.0 Flash Thinking","summary":"  Large Reasoning Models (LRMs) have recently extended their powerful reasoning\ncapabilities to safety checks-using chain-of-thought reasoning to decide\nwhether a request should be answered. While this new approach offers a\npromising route for balancing model utility and safety, its robustness remains\nunderexplored. To address this gap, we introduce Malicious-Educator, a\nbenchmark that disguises extremely dangerous or malicious requests beneath\nseemingly legitimate educational prompts. Our experiments reveal severe\nsecurity flaws in popular commercial-grade LRMs, including OpenAI o1/o3,\nDeepSeek-R1, and Gemini 2.0 Flash Thinking. For instance, although OpenAI's o1\nmodel initially maintains a high refusal rate of about 98%, subsequent model\nupdates significantly compromise its safety; and attackers can easily extract\ncriminal strategies from DeepSeek-R1 and Gemini 2.0 Flash Thinking without any\nadditional tricks. To further highlight these vulnerabilities, we propose\nHijacking Chain-of-Thought (H-CoT), a universal and transferable attack method\nthat leverages the model's own displayed intermediate reasoning to jailbreak\nits safety reasoning mechanism. Under H-CoT, refusal rates sharply\ndecline-dropping from 98% to below 2%-and, in some instances, even transform\ninitially cautious tones into ones that are willing to provide harmful content.\nWe hope these findings underscore the urgent need for more robust safety\nmechanisms to preserve the benefits of advanced reasoning capabilities without\ncompromising ethical standards.\n","authors":["Martin Kuo","Jianyi Zhang","Aolin Ding","Qinsi Wang","Louis DiValentin","Yujia Bao","Wei Wei","Hai Li","Yiran Chen"],"pdf_url":"https://arxiv.org/pdf/2502.12893v2.pdf","comment":"Website: https://maliciouseducator.org/"},{"id":"http://arxiv.org/abs/2502.19669v1","updated":"2025-02-27T01:30:07Z","published":"2025-02-27T01:30:07Z","title":"Investigating Neurons and Heads in Transformer-based LLMs for\n  Typographical Errors","summary":"  This paper investigates how LLMs encode inputs with typos. We hypothesize\nthat specific neurons and attention heads recognize typos and fix them\ninternally using local and global contexts. We introduce a method to identify\ntypo neurons and typo heads that work actively when inputs contain typos. Our\nexperimental results suggest the following: 1) LLMs can fix typos with local\ncontexts when the typo neurons in either the early or late layers are\nactivated, even if those in the other are not. 2) Typo neurons in the middle\nlayers are responsible for the core of typo-fixing with global contexts. 3)\nTypo heads fix typos by widely considering the context not focusing on specific\ntokens. 4) Typo neurons and typo heads work not only for typo-fixing but also\nfor understanding general contexts.\n","authors":["Kohei Tsuji","Tatsuya Hiraoka","Yuchang Cheng","Eiji Aramaki","Tomoya Iwakura"],"pdf_url":"https://arxiv.org/pdf/2502.19669v1.pdf","comment":"14 pages, 10 figures, 6 tables"},{"id":"http://arxiv.org/abs/2502.19668v1","updated":"2025-02-27T01:29:51Z","published":"2025-02-27T01:29:51Z","title":"SuPreME: A Supervised Pre-training Framework for Multimodal ECG\n  Representation Learning","summary":"  Cardiovascular diseases are a leading cause of death and disability\nworldwide. Electrocardiogram (ECG) recordings are critical for diagnosing and\nmonitoring cardiac health, but obtaining large-scale annotated ECG datasets is\nlabor-intensive and time-consuming. Recent ECG Self-Supervised Learning (eSSL)\nmethods mitigate this by learning features without extensive labels but fail to\ncapture fine-grained clinical semantics and require extensive task-specific\nfine-tuning. To address these challenges, we propose $\\textbf{SuPreME}$, a\n$\\textbf{Su}$pervised $\\textbf{Pre}$-training framework for\n$\\textbf{M}$ultimodal $\\textbf{E}$CG representation learning. SuPreME applies\nLarge Language Models (LLMs) to extract structured clinical entities from\nfree-text ECG reports, filter out noise and irrelevant content, enhance\nclinical representation learning, and build a high-quality, fine-grained\nlabeled dataset. By using text-based cardiac queries instead of traditional\ncategorical labels, SuPreME enables zero-shot classification of unseen diseases\nwithout additional fine-tuning. We evaluate SuPreME on six downstream datasets\ncovering 127 cardiac conditions, achieving superior zero-shot AUC performance\nover state-of-the-art eSSL and multimodal methods by over 1.96\\%. Results\ndemonstrate the effectiveness of SuPreME in leveraging structured, clinically\nrelevant knowledge for high-quality ECG representations. All code and data will\nbe released upon acceptance.\n","authors":["Mingsheng Cai","Jiuming Jiang","Wenhao Huang","Che Liu","Rossella Arcucci"],"pdf_url":"https://arxiv.org/pdf/2502.19668v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14676v2","updated":"2025-02-27T01:05:43Z","published":"2024-10-18T17:59:51Z","title":"SudoLM: Learning Access Control of Parametric Knowledge with\n  Authorization Alignment","summary":"  Existing preference alignment is a one-size-fits-all alignment mechanism,\nwhere the part of the large language model (LLM) parametric knowledge with\nnon-preferred features is uniformly blocked to all the users. However, this\npart of knowledge can be useful to advanced users whose expertise qualifies\nthem to handle these information. The one-size-fits-all alignment mechanism\nundermines LLM's utility for these qualified users. To address this problem, we\npropose SudoLM, a framework that lets LLMs learn access control over specific\nparametric knowledge for users with different credentials via authorization\nalignment. SudoLM allows authorized users to unlock their access to all the\nparametric knowledge with an assigned SUDO key while blocking access to\nnon-qualified users. Experiments on two application scenarios demonstrate that\nSudoLM effectively controls the user's access to the parametric knowledge and\nmaintains its general utility.\n","authors":["Qin Liu","Fei Wang","Chaowei Xiao","Muhao Chen"],"pdf_url":"https://arxiv.org/pdf/2410.14676v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.19655v1","updated":"2025-02-27T00:54:38Z","published":"2025-02-27T00:54:38Z","title":"Med-RLVR: Emerging Medical Reasoning from a 3B base model via\n  reinforcement Learning","summary":"  Reinforcement learning from verifiable rewards (RLVR) has recently gained\nattention for its ability to elicit self-evolved reasoning capabilitie from\nbase language models without explicit reasoning supervisions, as demonstrated\nby DeepSeek-R1. While prior work on RLVR has primarily focused on mathematical\nand coding domains, its applicability to other tasks and domains remains\nunexplored. In this work, we investigate whether medical reasoning can emerge\nfrom RLVR. We introduce Med-RLVR as an initial study of RLVR in the medical\ndomain leveraging medical multiple-choice question answering (MCQA) data as\nverifiable labels. Our results demonstrate that RLVR is not only effective for\nmath and coding but also extends successfully to medical question answering.\nNotably, Med-RLVR achieves performance comparable to traditional supervised\nfine-tuning (SFT) on in-distribution tasks while significantly improving\nout-of-distribution generalization, with an 8-point accuracy gain. Further\nanalysis of training dynamics reveals that, with no explicit reasoning\nsupervision, reasoning emerges from the 3B-parameter base model. These findings\nunderscore the potential of RLVR in domains beyond math and coding, opening new\navenues for its application in knowledge-intensive fields such as medicine.\n","authors":["Sheng Zhang","Qianchu Liu","Guanghui Qin","Tristan Naumann","Hoifung Poon"],"pdf_url":"https://arxiv.org/pdf/2502.19655v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.06660v2","updated":"2025-02-27T00:29:15Z","published":"2024-11-11T01:59:04Z","title":"Bridge: A Unified Framework to Knowledge Graph Completion via Language\n  Models and Knowledge Representation","summary":"  Knowledge graph completion (KGC) is a task of inferring missing triples based\non existing Knowledge Graphs (KGs). Both structural and semantic information\nare vital for successful KGC. However, existing methods only use either the\nstructural knowledge from the KG embeddings or the semantic information from\npre-trained language models (PLMs), leading to suboptimal model performance.\nMoreover, since PLMs are not trained on KGs, directly using PLMs to encode\ntriples may be inappropriate. To overcome these limitations, we propose a novel\nframework called Bridge, which jointly encodes structural and semantic\ninformation of KGs. Specifically, we strategically encode entities and\nrelations separately by PLMs to better utilize the semantic knowledge of PLMs\nand enable structured representation learning via a structural learning\nprinciple. Furthermore, to bridge the gap between KGs and PLMs, we employ a\nself-supervised representation learning method called BYOL to fine-tune PLMs\nwith two different views of a triple. Unlike BYOL, which uses augmentation\nmethods to create two semantically similar views of the same image, potentially\naltering the semantic information. We strategically separate the triple into\ntwo parts to create different views, thus avoiding semantic alteration.\nExperiments demonstrate that Bridge outperforms the SOTA models on three\nbenchmark datasets.\n","authors":["Qiao Qiao","Yuepei Li","Qing Wang","Kang Zhou","Qi Li"],"pdf_url":"https://arxiv.org/pdf/2411.06660v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.19651v1","updated":"2025-02-27T00:49:44Z","published":"2025-02-27T00:49:44Z","title":"Unlocking Multi-Modal Potentials for Dynamic Text-Attributed Graph\n  Representation","summary":"  Dynamic Text-Attributed Graphs (DyTAGs) are a novel graph paradigm that\ncaptures evolving temporal edges alongside rich textual attributes. A prior\napproach to representing DyTAGs leverages pre-trained language models to encode\ntext attributes and subsequently integrates them into dynamic graph models.\nHowever, it follows edge-centric modeling, as in dynamic graph learning, which\nis limited in local structures and fails to exploit the unique characteristics\nof DyTAGs, leading to suboptimal performance. We observe that DyTAGs inherently\ncomprise three distinct modalities-temporal, textual, and structural-often\nexhibiting dispersed or even orthogonal distributions, with the first two\nlargely overlooked in existing research. Building on this insight, we propose\nMoMent, a model-agnostic multi-modal framework that can seamlessly integrate\nwith dynamic graph models for structural modality learning. The core idea is to\nshift from edge-centric to node-centric modeling, fully leveraging three\nmodalities for node representation. Specifically, MoMent presents non-shared\nnode-centric encoders based on the attention mechanism to capture global\ntemporal and semantic contexts from temporal and textual modalities, together\nwith local structure learning, thus generating modality-specific tokens. To\nprevent disjoint latent space, we propose a symmetric alignment loss, an\nauxiliary objective that aligns temporal and textual tokens, ensuring global\ntemporal-semantic consistency with a theoretical guarantee. Last, we design a\nlightweight adaptor to fuse these tokens, generating comprehensive and cohesive\nnode representations. We theoretically demonstrate that MoMent enhances\ndiscriminative power over exclusive edge-centric modeling. Extensive\nexperiments across seven datasets and two downstream tasks show that MoMent\nachieves up to 33.62% improvement against the baseline using four dynamic graph\nmodels.\n","authors":["Yuanyuan Xu","Wenjie Zhang","Ying Zhang","Xuemin Lin","Xiwei Xu"],"pdf_url":"https://arxiv.org/pdf/2502.19651v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.19649v1","updated":"2025-02-27T00:40:01Z","published":"2025-02-27T00:40:01Z","title":"Taxonomy, Opportunities, and Challenges of Representation Engineering\n  for Large Language Models","summary":"  Representation Engineering (RepE) is a novel paradigm for controlling the\nbehavior of LLMs. Unlike traditional approaches that modify inputs or fine-tune\nthe model, RepE directly manipulates the model's internal representations. As a\nresult, it may offer more effective, interpretable, data-efficient, and\nflexible control over models' behavior. We present the first comprehensive\nsurvey of RepE for LLMs, reviewing the rapidly growing literature to address\nkey questions: What RepE methods exist and how do they differ? For what\nconcepts and problems has RepE been applied? What are the strengths and\nweaknesses of RepE compared to other methods? To answer these, we propose a\nunified framework describing RepE as a pipeline comprising representation\nidentification, operationalization, and control. We posit that while RepE\nmethods offer significant potential, challenges remain, including managing\nmultiple concepts, ensuring reliability, and preserving models' performance.\nTowards improving RepE, we identify opportunities for experimental and\nmethodological improvements and construct a guide for best practices.\n","authors":["Jan Wehner","Sahar Abdelnabi","Daniel Tan","David Krueger","Mario Fritz"],"pdf_url":"https://arxiv.org/pdf/2502.19649v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.20596v1","updated":"2025-02-27T23:44:30Z","published":"2025-02-27T23:44:30Z","title":"Few-Shot, No Problem: Descriptive Continual Relation Extraction","summary":"  Few-shot Continual Relation Extraction is a crucial challenge for enabling AI\nsystems to identify and adapt to evolving relationships in dynamic real-world\ndomains. Traditional memory-based approaches often overfit to limited samples,\nfailing to reinforce old knowledge, with the scarcity of data in few-shot\nscenarios further exacerbating these issues by hindering effective data\naugmentation in the latent space. In this paper, we propose a novel\nretrieval-based solution, starting with a large language model to generate\ndescriptions for each relation. From these descriptions, we introduce a\nbi-encoder retrieval training paradigm to enrich both sample and class\nrepresentation learning. Leveraging these enhanced representations, we design a\nretrieval-based prediction method where each sample \"retrieves\" the best\nfitting relation via a reciprocal rank fusion score that integrates both\nrelation description vectors and class prototypes. Extensive experiments on\nmultiple datasets demonstrate that our method significantly advances the\nstate-of-the-art by maintaining robust performance across sequential tasks,\neffectively addressing catastrophic forgetting.\n","authors":["Nguyen Xuan Thanh","Anh Duc Le","Quyen Tran","Thanh-Thien Le","Linh Ngo Van","Thien Huu Nguyen"],"pdf_url":"https://arxiv.org/pdf/2502.20596v1.pdf","comment":"Accepted to AAAI 2025"},{"id":"http://arxiv.org/abs/2501.16937v4","updated":"2025-02-27T23:41:37Z","published":"2025-01-28T13:31:18Z","title":"TAID: Temporally Adaptive Interpolated Distillation for Efficient\n  Knowledge Transfer in Language Models","summary":"  Causal language models have demonstrated remarkable capabilities, but their\nsize poses significant challenges for deployment in resource-constrained\nenvironments. Knowledge distillation, a widely-used technique for transferring\nknowledge from a large teacher model to a small student model, presents a\npromising approach for model compression. A significant remaining issue lies in\nthe major differences between teacher and student models, namely the\nsubstantial capacity gap, mode averaging, and mode collapse, which pose\nbarriers during distillation. To address these issues, we introduce\n$\\textit{Temporally Adaptive Interpolated Distillation (TAID)}$, a novel\nknowledge distillation approach that dynamically interpolates student and\nteacher distributions through an adaptive intermediate distribution, gradually\nshifting from the student's initial distribution towards the teacher's\ndistribution. We provide a theoretical analysis demonstrating TAID's ability to\nprevent mode collapse and empirically show its effectiveness in addressing the\ncapacity gap while balancing mode averaging and mode collapse. Our\ncomprehensive experiments demonstrate TAID's superior performance across\nvarious model sizes and architectures in both instruction tuning and\npre-training scenarios. Furthermore, we showcase TAID's practical impact by\ndeveloping two state-of-the-art compact foundation models:\n$\\texttt{TAID-LLM-1.5B}$ for language tasks and $\\texttt{TAID-VLM-2B}$ for\nvision-language tasks. These results demonstrate TAID's effectiveness in\ncreating high-performing and efficient models, advancing the development of\nmore accessible AI technologies.\n","authors":["Makoto Shing","Kou Misaki","Han Bao","Sho Yokoi","Takuya Akiba"],"pdf_url":"https://arxiv.org/pdf/2501.16937v4.pdf","comment":"To appear at the 13th International Conference on Learning\n  Representations (ICLR 2025) as a Spotlight presentation"},{"id":"http://arxiv.org/abs/2502.20592v1","updated":"2025-02-27T23:34:47Z","published":"2025-02-27T23:34:47Z","title":"Multi$^2$: Multi-Agent Test-Time Scalable Framework for Multi-Document\n  Processing","summary":"  Recent advances in test-time scaling have shown promising results in\nimproving Large Language Models (LLMs) performance through strategic\ncomputation allocation during inference. While this approach has demonstrated\nstrong performance improvements in logical and mathematical reasoning tasks,\nits application to natural language generation (NLG), especially summarization,\nhas yet to be explored. Multi-Document Summarization (MDS) is a challenging\ntask that focuses on extracting and synthesizing useful information from\nmultiple lengthy documents. Unlike reasoning tasks, MDS requires a more nuanced\napproach to prompt design and ensemble, as there is no \"best\" prompt to satisfy\ndiverse summarization requirements. To address this, we propose a novel\nframework that leverages inference-time scaling for this task. Precisely, we\ntake prompt ensemble approach by leveraging various prompt to first generate\ncandidate summaries and then ensemble them with an aggregator to produce a\nrefined summary. We also introduce two new evaluation metrics:\nConsistency-Aware Preference (CAP) score and LLM Atom-Content-Unit (ACU) score,\nto enhance LLM's contextual understanding while mitigating its positional bias.\nExtensive experiments demonstrate the effectiveness of our approach in\nimproving summary quality while identifying and analyzing the scaling\nboundaries in summarization tasks.\n","authors":["Juntai Cao","Xiang Zhang","Raymond Li","Chuyuan Li","Shafiq Joty","Giuseppe Carenini"],"pdf_url":"https://arxiv.org/pdf/2502.20592v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.20589v1","updated":"2025-02-27T23:22:01Z","published":"2025-02-27T23:22:01Z","title":"LLMs Have Rhythm: Fingerprinting Large Language Models Using Inter-Token\n  Times and Network Traffic Analysis","summary":"  As Large Language Models (LLMs) become increasingly integrated into many\ntechnological ecosystems across various domains and industries, identifying\nwhich model is deployed or being interacted with is critical for the security\nand trustworthiness of the systems. Current verification methods typically rely\non analyzing the generated output to determine the source model. However, these\ntechniques are susceptible to adversarial attacks, operate in a post-hoc\nmanner, and may require access to the model weights to inject a verifiable\nfingerprint. In this paper, we propose a novel passive and non-invasive\nfingerprinting technique that operates in real-time and remains effective even\nunder encrypted network traffic conditions. Our method leverages the intrinsic\nautoregressive generation nature of language models, which generate text one\ntoken at a time based on all previously generated tokens, creating a unique\ntemporal pattern like a rhythm or heartbeat that persists even when the output\nis streamed over a network. We find that measuring the Inter-Token Times\n(ITTs)-time intervals between consecutive tokens-can identify different\nlanguage models with high accuracy. We develop a Deep Learning (DL) pipeline to\ncapture these timing patterns using network traffic analysis and evaluate it on\n16 Small Language Models (SLMs) and 10 proprietary LLMs across different\ndeployment scenarios, including local host machine (GPU/CPU), Local Area\nNetwork (LAN), Remote Network, and Virtual Private Network (VPN). The\nexperimental results confirm that our proposed technique is effective and\nmaintains high accuracy even when tested in different network conditions. This\nwork opens a new avenue for model identification in real-world scenarios and\ncontributes to more secure and trustworthy language model deployment.\n","authors":["Saeif Alhazbi","Ahmed Mohamed Hussain","Gabriele Oligeri","Panos Papadimitratos"],"pdf_url":"https://arxiv.org/pdf/2502.20589v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.20581v1","updated":"2025-02-27T22:47:03Z","published":"2025-02-27T22:47:03Z","title":"The Noisy Path from Source to Citation: Measuring How Scholars Engage\n  with Past Research","summary":"  Academic citations are widely used for evaluating research and tracing\nknowledge flows. Such uses typically rely on raw citation counts and neglect\nvariability in citation types. In particular, citations can vary in their\nfidelity as original knowledge from cited studies may be paraphrased,\nsummarized, or reinterpreted, possibly wrongly, leading to variation in how\nmuch information changes from cited to citing paper. In this study, we\nintroduce a computational pipeline to quantify citation fidelity at scale.\nUsing full texts of papers, the pipeline identifies citations in citing papers\nand the corresponding claims in cited papers, and applies supervised models to\nmeasure fidelity at the sentence level. Analyzing a large-scale\nmulti-disciplinary dataset of approximately 13 million citation sentence pairs,\nwe find that citation fidelity is higher when authors cite papers that are 1)\nmore recent and intellectually close, 2) more accessible, and 3) the first\nauthor has a lower H-index and the author team is medium-sized. Using a\nquasi-experiment, we establish the \"telephone effect\" - when citing papers have\nlow fidelity to the original claim, future papers that cite the citing paper\nand the original have lower fidelity to the original. Our work reveals\nsystematic differences in citation fidelity, underscoring the limitations of\nanalyses that rely on citation quantity alone and the potential for distortion\nof evidence.\n","authors":["Hong Chen","Misha Teplitskiy","David Jurgens"],"pdf_url":"https://arxiv.org/pdf/2502.20581v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.20576v1","updated":"2025-02-27T22:35:31Z","published":"2025-02-27T22:35:31Z","title":"ECCOS: Efficient Capability and Cost Coordinated Scheduling for\n  Multi-LLM Serving","summary":"  As large language models (LLMs) are increasingly deployed as service\nendpoints in systems, the surge in query volume creates significant scheduling\nchallenges. Existing scheduling frameworks mainly target at latency\noptimization while neglecting the capability of LLMs to serve different level\nof queries, which could lead to computational resource waste. This paper\naddresses this challenge by proposing a capability-cost coordinated scheduling\nframework, ECCOS, for multi-LLM serving, which explicitly constrains response\nquality and workload to optimize LLM inference cost. Specifically, it\nintroduces the two-stage scheduling by designing a multi-objective predictor\nand a constrained optimizer. The predictor estimates both model capabilities\nand computational costs through training-based and retrieval-based approaches,\nwhile the optimizer determines cost-optimal assignments under quality and\nworkload constraints. It also introduces QAServe, a dataset collected for\nsample-wise response quality and costs by zero-shot prompting different LLMs on\nknowledge QA and mathematical reasoning. Extensive experiments demonstrate that\nECCOS improves success rates by 6.30% while reducing costs by 10.15% compared\nto existing methods, consuming less than 0.5% of LLM response time. The code is\navailable at: https://github.com/agiresearch/ECCOS.\n","authors":["Kai Mei","Wujiang Xu","Shuhang Lin","Yongfeng Zhang"],"pdf_url":"https://arxiv.org/pdf/2502.20576v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.20573v1","updated":"2025-02-27T22:26:29Z","published":"2025-02-27T22:26:29Z","title":"Visual Reasoning at Urban Intersections: FineTuning GPT-4o for Traffic\n  Conflict Detection","summary":"  Traffic control in unsignalized urban intersections presents significant\nchallenges due to the complexity, frequent conflicts, and blind spots. This\nstudy explores the capability of leveraging Multimodal Large Language Models\n(MLLMs), such as GPT-4o, to provide logical and visual reasoning by directly\nusing birds-eye-view videos of four-legged intersections. In this proposed\nmethod, GPT-4o acts as intelligent system to detect conflicts and provide\nexplanations and recommendations for the drivers. The fine-tuned model achieved\nan accuracy of 77.14%, while the manual evaluation of the true predicted values\nof the fine-tuned GPT-4o showed significant achievements of 89.9% accuracy for\nmodel-generated explanations and 92.3% for the recommended next actions. These\nresults highlight the feasibility of using MLLMs for real-time traffic\nmanagement using videos as inputs, offering scalable and actionable insights\ninto intersections traffic management and operation. Code used in this study is\navailable at\nhttps://github.com/sarimasri3/Traffic-Intersection-Conflict-Detection-using-images.git.\n","authors":["Sari Masri","Huthaifa I. Ashqar","Mohammed Elhenawy"],"pdf_url":"https://arxiv.org/pdf/2502.20573v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.20572v1","updated":"2025-02-27T22:21:45Z","published":"2025-02-27T22:21:45Z","title":"HazardNet: A Small-Scale Vision Language Model for Real-Time Traffic\n  Safety Detection at Edge Devices","summary":"  Traffic safety remains a vital concern in contemporary urban settings,\nintensified by the increase of vehicles and the complicated nature of road\nnetworks. Traditional safety-critical event detection systems predominantly\nrely on sensor-based approaches and conventional machine learning algorithms,\nnecessitating extensive data collection and complex training processes to\nadhere to traffic safety regulations. This paper introduces HazardNet, a\nsmall-scale Vision Language Model designed to enhance traffic safety by\nleveraging the reasoning capabilities of advanced language and vision models.\nWe built HazardNet by fine-tuning the pre-trained Qwen2-VL-2B model, chosen for\nits superior performance among open-source alternatives and its compact size of\ntwo billion parameters. This helps to facilitate deployment on edge devices\nwith efficient inference throughput. In addition, we present HazardQA, a novel\nVision Question Answering (VQA) dataset constructed specifically for training\nHazardNet on real-world scenarios involving safety-critical events. Our\nexperimental results show that the fine-tuned HazardNet outperformed the base\nmodel up to an 89% improvement in F1-Score and has comparable results with\nimprovement in some cases reach up to 6% when compared to larger models, such\nas GPT-4o. These advancements underscore the potential of HazardNet in\nproviding real-time, reliable traffic safety event detection, thereby\ncontributing to reduced accidents and improved traffic management in urban\nenvironments. Both HazardNet model and the HazardQA dataset are available at\nhttps://huggingface.co/Tami3/HazardNet and\nhttps://huggingface.co/datasets/Tami3/HazardQA, respectively.\n","authors":["Mohammad Abu Tami","Mohammed Elhenawy","Huthaifa I. Ashqar"],"pdf_url":"https://arxiv.org/pdf/2502.20572v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.20560v1","updated":"2025-02-27T22:01:22Z","published":"2025-02-27T22:01:22Z","title":"Towards Statistical Factuality Guarantee for Large Vision-Language\n  Models","summary":"  Advancements in Large Vision-Language Models (LVLMs) have demonstrated\npromising performance in a variety of vision-language tasks involving\nimage-conditioned free-form text generation. However, growing concerns about\nhallucinations in LVLMs, where the generated text is inconsistent with the\nvisual context, are becoming a major impediment to deploying these models in\napplications that demand guaranteed reliability. In this paper, we introduce a\nframework to address this challenge, ConfLVLM, which is grounded on conformal\nprediction to achieve finite-sample distribution-free statistical guarantees on\nthe factuality of LVLM output. This framework treats an LVLM as a hypothesis\ngenerator, where each generated text detail (or claim) is considered an\nindividual hypothesis. It then applies a statistical hypothesis testing\nprocedure to verify each claim using efficient heuristic uncertainty measures\nto filter out unreliable claims before returning any responses to users. We\nconduct extensive experiments covering three representative application\ndomains, including general scene understanding, medical radiology report\ngeneration, and document understanding. Remarkably, ConfLVLM reduces the error\nrate of claims generated by LLaVa-1.5 for scene descriptions from 87.8\\% to\n10.0\\% by filtering out erroneous claims with a 95.3\\% true positive rate. Our\nresults further demonstrate that ConfLVLM is highly flexible, and can be\napplied to any black-box LVLMs paired with any uncertainty measure for any\nimage-conditioned free-form text generation task while providing a rigorous\nguarantee on controlling the risk of hallucination.\n","authors":["Zhuohang Li","Chao Yan","Nicholas J. Jackson","Wendi Cui","Bo Li","Jiaxin Zhang","Bradley A. Malin"],"pdf_url":"https://arxiv.org/pdf/2502.20560v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.20552v1","updated":"2025-02-27T21:48:11Z","published":"2025-02-27T21:48:11Z","title":"HuAMR: A Hungarian AMR Parser and Dataset","summary":"  We present HuAMR, the first Abstract Meaning Representation (AMR) dataset and\na suite of large language model-based AMR parsers for Hungarian, targeting the\nscarcity of semantic resources for non-English languages. To create HuAMR, we\nemployed Llama-3.1-70B to automatically generate silver-standard AMR\nannotations, which we then refined manually to ensure quality. Building on this\ndataset, we investigate how different model architectures - mT5 Large and\nLlama-3.2-1B - and fine-tuning strategies affect AMR parsing performance.\n  While incorporating silver-standard AMRs from Llama-3.1-70B into the training\ndata of smaller models does not consistently boost overall scores, our results\nshow that these techniques effectively enhance parsing accuracy on Hungarian\nnews data (the domain of HuAMR). We evaluate our parsers using Smatch scores\nand confirm the potential of HuAMR and our parsers for advancing semantic\nparsing research.\n","authors":["Botond Barta","Endre Hamerlik","Milán Konor Nyist","Judit Ács"],"pdf_url":"https://arxiv.org/pdf/2502.20552v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.11068v5","updated":"2025-02-27T21:47:06Z","published":"2024-07-12T14:17:26Z","title":"Show, Don't Tell: Evaluating Large Language Models Beyond Textual\n  Understanding with ChildPlay","summary":"  We developed a benchmark set to assess the generalization of state-of-the-art\nlarge language models on problems beyond linguistic tasks and evaluate it on a\nsystematic progression of GPT models (GPT-3.5, GPT-4, GPT-4o, GPT-4o-mini).\nUsing simple games like Tic-Tac-Toe, Connect Four, Battleship, and a Shape\nRecognition Game, all encoded in ASCII, we test strategic capabilities and\nspatial reasoning, core abilities any artificial intelligence would need to\nmaster for solving problems in chemistry. To probe generalization, we introduce\ntwo new games for spatial logic: LEGO Connect Language (LCL) and\nGuess-the-SMILES (GtS), a operationally simple chemistry benchmark. Our results\nshow that GPT models provide meaningful responses for several tasks but,\ngenerally, perform poorly. A systematic performance progression with increased\nmodel capabilities (GPT-3.5, GPT-4, GPT-4o) is only observed for 4 out of the 7\nbenchmark tasks. All models consistently struggle with Battleship, LCL, and\nGtS. This suggests that while GPT models can emulate conversational proficiency\nand basic rule comprehension, they have limited generalization with respect to\nstrategy and spatial reasoning. Particularly poor performance is observed for\ninterpreting molecular graphs when encoded in ASCII. The results provided by\nour open-source benchmark suite\n(\\href{https://github.com/BlueVelvetSackOfGoldPotatoes/child-play}{\\texttt{ChildPlay}\nGitHub Repository}) caution against claims of emergent intelligence in GPT\nmodels, which appear more specialized than general.\n","authors":["Gonçalo Hora de Carvalho","Oscar Knap","Robert Pollice"],"pdf_url":"https://arxiv.org/pdf/2407.11068v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.20548v1","updated":"2025-02-27T21:43:00Z","published":"2025-02-27T21:43:00Z","title":"$Q\\sharp$: Provably Optimal Distributional RL for LLM Post-Training","summary":"  Reinforcement learning (RL) post-training is crucial for LLM alignment and\nreasoning, but existing policy-based methods, such as PPO and DPO, can fall\nshort of fixing shortcuts inherited from pre-training. In this work, we\nintroduce $Q\\sharp$, a value-based algorithm for KL-regularized RL that guides\nthe reference policy using the optimal regularized $Q$ function. We propose to\nlearn the optimal $Q$ function using distributional RL on an aggregated online\ndataset. Unlike prior value-based baselines that guide the model using\nunregularized $Q$-values, our method is theoretically principled and provably\nlearns the optimal policy for the KL-regularized RL problem. Empirically,\n$Q\\sharp$ outperforms prior baselines in math reasoning benchmarks while\nmaintaining a smaller KL divergence to the reference policy. Theoretically, we\nestablish a reduction from KL-regularized RL to no-regret online learning,\nproviding the first bounds for deterministic MDPs under only realizability.\nThanks to distributional RL, our bounds are also variance-dependent and\nconverge faster when the reference policy has small variance. In sum, our\nresults highlight $Q\\sharp$ as an effective approach for post-training LLMs,\noffering both improved performance and theoretical guarantees. The code can be\nfound at https://github.com/jinpz/q_sharp.\n","authors":["Jin Peng Zhou","Kaiwen Wang","Jonathan Chang","Zhaolin Gao","Nathan Kallus","Kilian Q. Weinberger","Kianté Brantley","Wen Sun"],"pdf_url":"https://arxiv.org/pdf/2502.20548v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.20541v1","updated":"2025-02-27T21:40:22Z","published":"2025-02-27T21:40:22Z","title":"NANOGPT: A Query-Driven Large Language Model Retrieval-Augmented\n  Generation System for Nanotechnology Research","summary":"  This paper presents the development and application of a Large Language Model\nRetrieval-Augmented Generation (LLM-RAG) system tailored for nanotechnology\nresearch. The system leverages the capabilities of a sophisticated language\nmodel to serve as an intelligent research assistant, enhancing the efficiency\nand comprehensiveness of literature reviews in the nanotechnology domain.\nCentral to this LLM-RAG system is its advanced query backend retrieval\nmechanism, which integrates data from multiple reputable sources. The system\nretrieves relevant literature by utilizing Google Scholar's advanced search,\nand scraping open-access papers from Elsevier, Springer Nature, and ACS\nPublications. This multifaceted approach ensures a broad and diverse collection\nof up-to-date scholarly articles and papers. The proposed system demonstrates\nsignificant potential in aiding researchers by providing a streamlined,\naccurate, and exhaustive literature retrieval process, thereby accelerating\nresearch advancements in nanotechnology. The effectiveness of the LLM-RAG\nsystem is validated through rigorous testing, illustrating its capability to\nsignificantly reduce the time and effort required for comprehensive literature\nreviews, while maintaining high accuracy, query relevance and outperforming\nstandard, publicly available LLMS.\n","authors":["Achuth Chandrasekhar","Omid Barati Farimani","Olabode T. Ajenifujah","Janghoon Ock","Amir Barati Farimani"],"pdf_url":"https://arxiv.org/pdf/2502.20541v1.pdf","comment":"61 pages, 3 figures"},{"id":"http://arxiv.org/abs/2501.02688v2","updated":"2025-02-27T21:31:36Z","published":"2025-01-05T23:35:47Z","title":"Flash Interpretability: Decoding Specialised Feature Neurons in Large\n  Language Models with the LM-Head","summary":"  Large Language Models (LLMs) typically have billions of parameters and are\nthus often difficult to interpret in their operation. In this work, we\ndemonstrate that it is possible to decode neuron weights directly into token\nprobabilities through the final projection layer of the model (the LM-head).\nThis is illustrated in Llama 3.1 8B where we use the LM-head to find examples\nof specialised feature neurons such as a \"dog\" neuron and a \"California\"\nneuron, and we validate this by clamping these neurons to affect the\nprobability of the concept in the output. We evaluate this method on both the\npre-trained and Instruct models, finding that over 75% of neurons in the\nup-projection layers in the instruct model have the same top associated token\ncompared to the pretrained model. Finally, we demonstrate that clamping the\n\"dog\" neuron leads the instruct model to always discuss dogs when asked about\nits favourite animal. Through our method, it is possible to map the top\nfeatures of the entirety of Llama 3.1 8B's up-projection neurons in less than\n10 seconds, with minimal compute.\n","authors":["Harry J Davies"],"pdf_url":"https://arxiv.org/pdf/2501.02688v2.pdf","comment":"5 pages, 4 figures"},{"id":"http://arxiv.org/abs/2502.20527v1","updated":"2025-02-27T21:23:56Z","published":"2025-02-27T21:23:56Z","title":"Supervised Fine-Tuning LLMs to Behave as Pedagogical Agents in\n  Programming Education","summary":"  Large language models (LLMs) are increasingly being explored in higher\neducation, yet their effectiveness as teaching agents remains underexamined. In\nthis paper, we present the development of GuideLM, a fine-tuned LLM designed\nfor programming education. GuideLM has been integrated into the Debugging C\nCompiler (DCC), an educational C compiler that leverages LLMs to generate\npedagogically sound error explanations. Previously, DCC relied on off-the-shelf\nOpenAI models, which, while accurate, often over-assisted students by directly\nproviding solutions despite contrary prompting.\n  To address this, we employed supervised fine-tuning (SFT) on a dataset of 528\nstudent-question/teacher-answer pairs, creating two models: GuideLM and\nGuideLM-mini, fine-tuned on ChatGPT-4o and 4o-mini, respectively. We conducted\nan expert analysis of 400 responses per model, comparing their pedagogical\neffectiveness against base OpenAI models. Our evaluation, grounded in\nconstructivism and cognitive load theory, assessed factors such as conceptual\nscaffolding, clarity, and Socratic guidance.\n  Results indicate that GuideLM and GuideLM-mini improve pedagogical\nperformance, with an 8% increase in Socratic guidance and a 58% improvement in\neconomy of words compared to GPT-4o. However, this refinement comes at the cost\nof a slight reduction in general accuracy. While further work is needed, our\nfindings suggest that fine-tuning LLMs with targeted datasets is a promising\napproach for developing models better suited to educational contexts.\n","authors":["Emily Ross","Yuval Kansal","Jake Renzella","Alexandra Vassar","Andrew Taylor"],"pdf_url":"https://arxiv.org/pdf/2502.20527v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.05864v3","updated":"2025-02-27T21:17:40Z","published":"2024-10-08T09:53:35Z","title":"From Tokens to Words: On the Inner Lexicon of LLMs","summary":"  Natural language is composed of words, but modern large language models\n(LLMs) process sub-words as input. A natural question raised by this\ndiscrepancy is whether LLMs encode words internally, and if so how. We present\nevidence that LLMs engage in an intrinsic detokenization process, where\nsub-word sequences are combined into coherent whole-word representations at\ntheir last token. Our experiments show that this process primarily takes place\nwithin the early and middle layers of the model. We further demonstrate its\nrobustness to arbitrary splits (e.g., \"cats\" to \"ca\" and \"ts\"), typos, and\nimportantly-to out-of-vocabulary words: when feeding the last token internal\nrepresentations of such words to the model as input, it can \"understand\" them\nas the complete word despite never seeing such representations as input during\ntraining. Our findings suggest that LLMs maintain a latent vocabulary beyond\nthe tokenizer's scope. These insights provide a practical, finetuning-free\napplication for expanding the vocabulary of pre-trained models. By enabling the\naddition of new vocabulary words, we reduce input length and inference\niterations, which reduces both space and model latency, with little to no loss\nin model accuracy.\n","authors":["Guy Kaplan","Matanel Oren","Yuval Reif","Roy Schwartz"],"pdf_url":"https://arxiv.org/pdf/2410.05864v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.20508v1","updated":"2025-02-27T20:33:28Z","published":"2025-02-27T20:33:28Z","title":"TripCraft: A Benchmark for Spatio-Temporally Fine Grained Travel\n  Planning","summary":"  Recent advancements in probing Large Language Models (LLMs) have explored\ntheir latent potential as personalized travel planning agents, yet existing\nbenchmarks remain limited in real world applicability. Existing datasets, such\nas TravelPlanner and TravelPlanner+, suffer from semi synthetic data reliance,\nspatial inconsistencies, and a lack of key travel constraints, making them\ninadequate for practical itinerary generation. To address these gaps, we\nintroduce TripCraft, a spatiotemporally coherent travel planning dataset that\nintegrates real world constraints, including public transit schedules, event\navailability, diverse attraction categories, and user personas for enhanced\npersonalization. To evaluate LLM generated plans beyond existing binary\nvalidation methods, we propose five continuous evaluation metrics, namely\nTemporal Meal Score, Temporal Attraction Score, Spatial Score, Ordering Score,\nand Persona Score which assess itinerary quality across multiple dimensions.\nOur parameter informed setting significantly enhances meal scheduling,\nimproving the Temporal Meal Score from 61% to 80% in a 7 day scenario.\nTripCraft establishes a new benchmark for LLM driven personalized travel\nplanning, offering a more realistic, constraint aware framework for itinerary\ngeneration. Dataset and Codebase will be made publicly available upon\nacceptance.\n","authors":["Soumyabrata Chaudhuri","Pranav Purkar","Ritwik Raghav","Shubhojit Mallick","Manish Gupta","Abhik Jana","Shreya Ghosh"],"pdf_url":"https://arxiv.org/pdf/2502.20508v1.pdf","comment":"27 pages, 18 Tables and 6 Figures"},{"id":"http://arxiv.org/abs/2502.20504v1","updated":"2025-02-27T20:25:00Z","published":"2025-02-27T20:25:00Z","title":"A Thousand Words or An Image: Studying the Influence of Persona Modality\n  in Multimodal LLMs","summary":"  Large language models (LLMs) have recently demonstrated remarkable\nadvancements in embodying diverse personas, enhancing their effectiveness as\nconversational agents and virtual assistants. Consequently, LLMs have made\nsignificant strides in processing and integrating multimodal information.\nHowever, even though human personas can be expressed in both text and image,\nthe extent to which the modality of a persona impacts the embodiment by the LLM\nremains largely unexplored. In this paper, we investigate how do different\nmodalities influence the expressiveness of personas in multimodal LLMs. To this\nend, we create a novel modality-parallel dataset of 40 diverse personas varying\nin age, gender, occupation, and location. This consists of four modalities to\nequivalently represent a persona: image-only, text-only, a combination of image\nand small text, and typographical images, where text is visually stylized to\nconvey persona-related attributes. We then create a systematic evaluation\nframework with 60 questions and corresponding metrics to assess how well LLMs\nembody each persona across its attributes and scenarios. Comprehensive\nexperiments on $5$ multimodal LLMs show that personas represented by detailed\ntext show more linguistic habits, while typographical images often show more\nconsistency with the persona. Our results reveal that LLMs often overlook\npersona-specific details conveyed through images, highlighting underlying\nlimitations and paving the way for future research to bridge this gap. We\nrelease the data and code at https://github.com/claws-lab/persona-modality .\n","authors":["Julius Broomfield","Kartik Sharma","Srijan Kumar"],"pdf_url":"https://arxiv.org/pdf/2502.20504v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.20503v1","updated":"2025-02-27T20:22:34Z","published":"2025-02-27T20:22:34Z","title":"Protecting multimodal large language models against misleading\n  visualizations","summary":"  We assess the vulnerability of multimodal large language models to misleading\nvisualizations - charts that distort the underlying data using techniques such\nas truncated or inverted axes, leading readers to draw inaccurate conclusions\nthat may support misinformation or conspiracy theories. Our analysis shows that\nthese distortions severely harm multimodal large language models, reducing\ntheir question-answering accuracy to the level of the random baseline. To\nmitigate this vulnerability, we introduce six inference-time methods to improve\nperformance of MLLMs on misleading visualizations while preserving their\naccuracy on non-misleading ones. The most effective approach involves (1)\nextracting the underlying data table and (2) using a text-only large language\nmodel to answer questions based on the table. This method improves performance\non misleading visualizations by 15.4 to 19.6 percentage points.\n","authors":["Jonathan Tonglet","Tinne Tuytelaars","Marie-Francine Moens","Iryna Gurevych"],"pdf_url":"https://arxiv.org/pdf/2502.20503v1.pdf","comment":"Preprint. Code and data available at\n  https://github.com/UKPLab/arxiv2025-misleading-visualizations"},{"id":"http://arxiv.org/abs/2410.02052v5","updated":"2025-02-27T20:13:19Z","published":"2024-10-02T21:42:35Z","title":"ExACT: Teaching AI Agents to Explore with Reflective-MCTS and\n  Exploratory Learning","summary":"  Autonomous agents have demonstrated significant potential in automating\ncomplex multistep decision-making tasks. However, even state-of-the-art\nvision-language models (VLMs), such as GPT-4o, still fall short of human-level\nperformance, particularly in intricate web environments and long-horizon tasks.\nTo address these limitations, we present ExACT, an approach to combine\ntest-time search and self-learning to build o1-like models for agentic\napplications. We first introduce Reflective Monte Carlo Tree Search (R-MCTS), a\nnovel test time algorithm designed to enhance AI agents' ability to explore\ndecision space on the fly. R-MCTS extends traditional MCTS by 1) incorporating\ncontrastive reflection, allowing agents to learn from past interactions and\ndynamically improve their search efficiency; and 2) using multi-agent debate\nfor reliable state evaluation. Next, we introduce Exploratory Learning, a novel\nlearning strategy to teach agents to search at inference time without relying\non any external search algorithms. On the challenging VisualWebArena benchmark,\nour GPT-4o based R-MCTS agent achieves a 6% to 30% relative improvement across\nvarious tasks compared to the previous state-of-the-art. Additionally, we show\nthat the knowledge and experience gained from test-time search can be\neffectively transferred back to GPT-4o via fine-tuning. After Exploratory\nLearning, GPT-4o 1) demonstrates the ability to explore the environment,\nevaluate a state, and backtrack to viable ones when it detects that the current\nstate cannot lead to success, and 2) matches 87% of R-MCTS's performance while\nusing significantly less compute. Notably, our work demonstrates the compute\nscaling properties in both training - data collection with R-MCTS - and testing\ntime. These results suggest a promising research direction to enhance VLMs'\ncapabilities for agentic applications via test-time search and self-learning.\n","authors":["Xiao Yu","Baolin Peng","Vineeth Vajipey","Hao Cheng","Michel Galley","Jianfeng Gao","Zhou Yu"],"pdf_url":"https://arxiv.org/pdf/2410.02052v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.20490v1","updated":"2025-02-27T19:54:16Z","published":"2025-02-27T19:54:16Z","title":"EgoNormia: Benchmarking Physical Social Norm Understanding","summary":"  Human activity is moderated by norms. When performing actions in the real\nworld, humans not only follow norms, but also consider the trade-off between\ndifferent norms However, machines are often trained without explicit\nsupervision on norm understanding and reasoning, especially when the norms are\ngrounded in a physical and social context. To improve and evaluate the\nnormative reasoning capability of vision-language models (VLMs), we present\nEgoNormia $\\|\\epsilon\\|$, consisting of 1,853 ego-centric videos of human\ninteractions, each of which has two related questions evaluating both the\nprediction and justification of normative actions. The normative actions\nencompass seven categories: safety, privacy, proxemics, politeness,\ncooperation, coordination/proactivity, and communication/legibility. To compile\nthis dataset at scale, we propose a novel pipeline leveraging video sampling,\nautomatic answer generation, filtering, and human validation. Our work\ndemonstrates that current state-of-the-art vision-language models lack robust\nnorm understanding, scoring a maximum of 45% on EgoNormia (versus a human bench\nof 92%). Our analysis of performance in each dimension highlights the\nsignificant risks of safety, privacy, and the lack of collaboration and\ncommunication capability when applied to real-world agents. We additionally\nshow that through a retrieval-based generation method, it is possible to use\nEgoNomia to enhance normative reasoning in VLMs.\n","authors":["MohammadHossein Rezaei","Yicheng Fu","Phil Cuvin","Caleb Ziems","Yanzhe Zhang","Hao Zhu","Diyi Yang"],"pdf_url":"https://arxiv.org/pdf/2502.20490v1.pdf","comment":null}]},"2025-02-28T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2502.21321v1","updated":"2025-02-28T18:59:54Z","published":"2025-02-28T18:59:54Z","title":"LLM Post-Training: A Deep Dive into Reasoning Large Language Models","summary":"  Large Language Models (LLMs) have transformed the natural language processing\nlandscape and brought to life diverse applications. Pretraining on vast\nweb-scale data has laid the foundation for these models, yet the research\ncommunity is now increasingly shifting focus toward post-training techniques to\nachieve further breakthroughs. While pretraining provides a broad linguistic\nfoundation, post-training methods enable LLMs to refine their knowledge,\nimprove reasoning, enhance factual accuracy, and align more effectively with\nuser intents and ethical considerations. Fine-tuning, reinforcement learning,\nand test-time scaling have emerged as critical strategies for optimizing LLMs\nperformance, ensuring robustness, and improving adaptability across various\nreal-world tasks. This survey provides a systematic exploration of\npost-training methodologies, analyzing their role in refining LLMs beyond\npretraining, addressing key challenges such as catastrophic forgetting, reward\nhacking, and inference-time trade-offs. We highlight emerging directions in\nmodel alignment, scalable adaptation, and inference-time reasoning, and outline\nfuture research directions. We also provide a public repository to continually\ntrack developments in this fast-evolving field:\nhttps://github.com/mbzuai-oryx/Awesome-LLM-Post-training.\n","authors":["Komal Kumar","Tajamul Ashraf","Omkar Thawakar","Rao Muhammad Anwer","Hisham Cholakkal","Mubarak Shah","Ming-Hsuan Yang","Phillip H. S. Torr","Salman Khan","Fahad Shahbaz Khan"],"pdf_url":"https://arxiv.org/pdf/2502.21321v1.pdf","comment":"31 pages, 7 figures, 3 tables, 375 references"},{"id":"http://arxiv.org/abs/2502.21315v1","updated":"2025-02-28T18:59:15Z","published":"2025-02-28T18:59:15Z","title":"Identifying Emerging Concepts in Large Corpora","summary":"  We introduce a new method to identify emerging concepts in large text\ncorpora. By analyzing changes in the heatmaps of the underlying embedding\nspace, we are able to detect these concepts with high accuracy shortly after\nthey originate, in turn outperforming common alternatives. We further\ndemonstrate the utility of our approach by analyzing speeches in the U.S.\nSenate from 1941 to 2015. Our results suggest that the minority party is more\nactive in introducing new concepts into the Senate discourse. We also identify\nspecific concepts that closely correlate with the Senators' racial, ethnic, and\ngender identities. An implementation of our method is publicly available.\n","authors":["Sibo Ma","Julian Nyarko"],"pdf_url":"https://arxiv.org/pdf/2502.21315v1.pdf","comment":"9 pages, 4 figures"},{"id":"http://arxiv.org/abs/2410.08388v4","updated":"2025-02-28T18:55:08Z","published":"2024-10-10T21:51:22Z","title":"The GUS Framework: Benchmarking Social Bias Classification with\n  Discriminative (Encoder-Only) and Generative (Decoder-Only) Language Models","summary":"  The detection of social bias in text is a critical challenge, particularly\ndue to the limitations of binary classification methods. These methods often\noversimplify nuanced biases, leading to high emotional impact when content is\nmisclassified as either \"biased\" or \"fair.\" To address these shortcomings, we\npropose a more nuanced framework that focuses on three key linguistic\ncomponents underlying social bias: Generalizations, Unfairness, and Stereotypes\n(the GUS framework). The GUS framework employs a semi-automated approach to\ncreate a comprehensive synthetic dataset, which is then verified by humans to\nmaintain ethical standards. This dataset enables robust multi-label token\nclassification. Our methodology, which combines discriminative (encoder-only)\nmodels and generative (auto-regressive large language models), identifies\nbiased entities in text. Through extensive experiments, we demonstrate that\nencoder-only models are effective for this complex task, often outperforming\nstate-of-the-art methods, both in terms of macro and entity-wise F1-score and\nHamming loss. These findings can guide the choice of model for different use\ncases, highlighting the GUS framework's effectiveness in capturing explicit and\nimplicit biases across diverse contexts, and offering a pathway for future\nresearch and applications in various fields.\n","authors":["Maximus Powers","Shaina Raza","Alex Chang","Umang Mavani","Harshitha Reddy Jonala","Ansh Tiwari","Hua Wei"],"pdf_url":"https://arxiv.org/pdf/2410.08388v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.21309v1","updated":"2025-02-28T18:52:24Z","published":"2025-02-28T18:52:24Z","title":"FANformer: Improving Large Language Models Through Effective Periodicity\n  Modeling","summary":"  Periodicity, as one of the most important basic characteristics, lays the\nfoundation for facilitating structured knowledge acquisition and systematic\ncognitive processes within human learning paradigms. However, the potential\nflaws of periodicity modeling in Transformer affect the learning efficiency and\nestablishment of underlying principles from data for large language models\n(LLMs) built upon it. In this paper, we demonstrate that integrating effective\nperiodicity modeling can improve the learning efficiency and performance of\nLLMs. We introduce FANformer, which integrates Fourier Analysis Network (FAN)\ninto attention mechanism to achieve efficient periodicity modeling, by\nmodifying the feature projection process of attention mechanism. Extensive\nexperimental results on language modeling show that FANformer consistently\noutperforms Transformer when scaling up model size and training tokens,\nunderscoring its superior learning efficiency. To further validate the\neffectiveness of FANformer, we pretrain a FANformer-1B on 1 trillion tokens.\nFANformer-1B exhibits marked improvements on downstream tasks compared to\nopen-source LLMs with similar model parameters or training tokens. The results\nposition FANformer as an effective and promising architecture for advancing\nLLMs.\n","authors":["Yihong Dong","Ge Li","Xue Jiang","Yongding Tao","Kechi Zhang","Hao Zhu","Huanyu Liu","Jiazheng Ding","Jia Li","Jinliang Deng","Hong Mei"],"pdf_url":"https://arxiv.org/pdf/2502.21309v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.21297v1","updated":"2025-02-28T18:28:16Z","published":"2025-02-28T18:28:16Z","title":"Persuasion Should be Double-Blind: A Multi-Domain Dialogue Dataset With\n  Faithfulness Based on Causal Theory of Mind","summary":"  Persuasive dialogue plays a pivotal role in human communication, influencing\nvarious domains. Recent persuasive dialogue datasets often fail to align with\nreal-world interpersonal interactions, leading to unfaithful representations.\nFor instance, unrealistic scenarios may arise, such as when the persuadee\nexplicitly instructs the persuader on which persuasion strategies to employ,\nwith each of the persuadee's questions corresponding to a specific strategy for\nthe persuader to follow. This issue can be attributed to a violation of the\n\"Double Blind\" condition, where critical information is fully shared between\nparticipants. In actual human interactions, however, key information such as\nthe mental state of the persuadee and the persuasion strategies of the\npersuader is not directly accessible. The persuader must infer the persuadee's\nmental state using Theory of Mind capabilities and construct arguments that\nalign with the persuadee's motivations. To address this gap, we introduce\nToMMA, a novel multi-agent framework for dialogue generation that is guided by\ncausal Theory of Mind. This framework ensures that information remains\nundisclosed between agents, preserving \"double-blind\" conditions, while causal\nToM directs the persuader's reasoning, enhancing alignment with human-like\npersuasion dynamics. Consequently, we present CToMPersu, a multi-domain,\nmulti-turn persuasive dialogue dataset that tackles both double-blind and\nlogical coherence issues, demonstrating superior performance across multiple\nmetrics and achieving better alignment with real human dialogues. Our dataset\nand prompts are available at https://github.com/DingyiZhang/ToMMA-CToMPersu .\n","authors":["Dingyi Zhang","Deyu Zhou"],"pdf_url":"https://arxiv.org/pdf/2502.21297v1.pdf","comment":"23pages"},{"id":"http://arxiv.org/abs/2501.09768v3","updated":"2025-02-28T18:27:21Z","published":"2025-01-15T11:32:35Z","title":"Can Large Language Models Predict the Outcome of Judicial Decisions?","summary":"  Large Language Models (LLMs) have shown exceptional capabilities in Natural\nLanguage Processing (NLP) across diverse domains. However, their application in\nspecialized tasks such as Legal Judgment Prediction (LJP) for low-resource\nlanguages like Arabic remains underexplored. In this work, we address this gap\nby developing an Arabic LJP dataset, collected and preprocessed from Saudi\ncommercial court judgments. We benchmark state-of-the-art open-source LLMs,\nincluding LLaMA-3.2-3B and LLaMA-3.1-8B, under varying configurations such as\nzero-shot, one-shot, and fine-tuning using LoRA. Additionally, we employed a\ncomprehensive evaluation framework that integrates both quantitative metrics\n(such as BLEU, ROUGE, and BERT) and qualitative assessments (including\nCoherence, Legal Language, Clarity, etc.) using an LLM. Our results demonstrate\nthat fine-tuned smaller models achieve comparable performance to larger models\nin task-specific contexts while offering significant resource efficiency.\nFurthermore, we investigate the impact of fine-tuning the model on a diverse\nset of instructions, offering valuable insights into the development of a more\nhuman-centric and adaptable LLM. We have made the dataset, code, and models\npublicly available to provide a solid foundation for future research in Arabic\nlegal NLP.\n","authors":["Mohamed Bayan Kmainasi","Ali Ezzat Shahroor","Amani Al-Ghraibah"],"pdf_url":"https://arxiv.org/pdf/2501.09768v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.02891v2","updated":"2025-02-28T17:57:47Z","published":"2025-01-06T10:08:56Z","title":"Explaining Humour Style Classifications: An XAI Approach to\n  Understanding Computational Humour Analysis","summary":"  Humour styles can have either a negative or a positive impact on well-being.\nGiven the importance of these styles to mental health, significant research has\nbeen conducted on their automatic identification. However, the automated\nmachine learning models used for this purpose are black boxes, making their\nprediction decisions opaque. Clarity and transparency are vital in the field of\nmental health. This paper presents an explainable AI (XAI) framework for\nunderstanding humour style classification, building upon previous work in\ncomputational humour analysis. Using the best-performing single model\n(ALI+XGBoost) from prior research, we apply comprehensive XAI techniques to\nanalyse how linguistic, emotional, and semantic features contribute to humour\nstyle classification decisions. Our analysis reveals distinct patterns in how\ndifferent humour styles are characterised and misclassified, with particular\nemphasis on the challenges in distinguishing affiliative humour from other\nstyles. Through detailed examination of feature importance, error patterns, and\nmisclassification cases, we identify key factors influencing model decisions,\nincluding emotional ambiguity, context misinterpretation, and target\nidentification. The framework demonstrates significant utility in understanding\nmodel behaviour, achieving interpretable insights into the complex interplay of\nfeatures that define different humour styles. Our findings contribute to both\nthe theoretical understanding of computational humour analysis and practical\napplications in mental health, content moderation, and digital humanities\nresearch.\n","authors":["Mary Ogbuka Kenneth","Foaad Khosmood","Abbas Edalat"],"pdf_url":"https://arxiv.org/pdf/2501.02891v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.00075v5","updated":"2025-02-28T17:50:12Z","published":"2024-06-21T19:18:16Z","title":"Logicbreaks: A Framework for Understanding Subversion of Rule-based\n  Inference","summary":"  We study how to subvert large language models (LLMs) from following\nprompt-specified rules. We first formalize rule-following as inference in\npropositional Horn logic, a mathematical system in which rules have the form\n\"if $P$ and $Q$, then $R$\" for some propositions $P$, $Q$, and $R$. Next, we\nprove that although small transformers can faithfully follow such rules,\nmaliciously crafted prompts can still mislead both theoretical constructions\nand models learned from data. Furthermore, we demonstrate that popular attack\nalgorithms on LLMs find adversarial prompts and induce attention patterns that\nalign with our theory. Our novel logic-based framework provides a foundation\nfor studying LLMs in rule-based settings, enabling a formal analysis of tasks\nlike logical reasoning and jailbreak attacks.\n","authors":["Anton Xue","Avishree Khare","Rajeev Alur","Surbhi Goel","Eric Wong"],"pdf_url":"https://arxiv.org/pdf/2407.00075v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.21265v1","updated":"2025-02-28T17:41:27Z","published":"2025-02-28T17:41:27Z","title":"Token-level Ensembling of Models with Different Vocabularies","summary":"  Model ensembling is a technique to combine the predicted distributions of two\nor more models, often leading to improved robustness and performance. For\nensembling in text generation, the next token's probability distribution is\nderived from a weighted sum of the distributions of each individual model. This\nrequires the underlying models to share the same subword vocabulary, limiting\nthe applicability of ensembling, since many open-sourced models have distinct\nvocabularies. In research settings, experimentation or upgrades to vocabularies\nmay introduce multiple vocabulary sizes. This paper proposes an inference-time\nonly algorithm that allows for ensembling models with different vocabularies,\nwithout the need to learn additional parameters or alter the underlying models.\nInstead, the algorithm ensures that tokens generated by the ensembled models\n\\textit{agree} in their surface form. We apply this technique to combinations\nof traditional encoder-decoder models and decoder-only LLMs and evaluate on\nmachine translation. In addition to expanding to model pairs that were\npreviously incapable of token-level ensembling, our algorithm frequently\nimproves translation performance over either model individually.\n","authors":["Rachel Wicks","Kartik Ravisankar","Xinchen Yang","Philipp Koehn","Matt Post"],"pdf_url":"https://arxiv.org/pdf/2502.21265v1.pdf","comment":"Under review"},{"id":"http://arxiv.org/abs/2502.21263v1","updated":"2025-02-28T17:40:24Z","published":"2025-02-28T17:40:24Z","title":"RuCCoD: Towards Automated ICD Coding in Russian","summary":"  This study investigates the feasibility of automating clinical coding in\nRussian, a language with limited biomedical resources. We present a new dataset\nfor ICD coding, which includes diagnosis fields from electronic health records\n(EHRs) annotated with over 10,000 entities and more than 1,500 unique ICD\ncodes. This dataset serves as a benchmark for several state-of-the-art models,\nincluding BERT, LLaMA with LoRA, and RAG, with additional experiments examining\ntransfer learning across domains (from PubMed abstracts to medical diagnosis)\nand terminologies (from UMLS concepts to ICD codes). We then apply the\nbest-performing model to label an in-house EHR dataset containing patient\nhistories from 2017 to 2021. Our experiments, conducted on a carefully curated\ntest set, demonstrate that training with the automated predicted codes leads to\na significant improvement in accuracy compared to manually annotated data from\nphysicians. We believe our findings offer valuable insights into the potential\nfor automating clinical coding in resource-limited languages like Russian,\nwhich could enhance clinical efficiency and data accuracy in these contexts.\n","authors":["Aleksandr Nesterov","Andrey Sakhovskiy","Ivan Sviridov","Airat Valiev","Vladimir Makharev","Petr Anokhin","Galina Zubkova","Elena Tutubalina"],"pdf_url":"https://arxiv.org/pdf/2502.21263v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.21239v1","updated":"2025-02-28T17:09:08Z","published":"2025-02-28T17:09:08Z","title":"Semantic Volume: Quantifying and Detecting both External and Internal\n  Uncertainty in LLMs","summary":"  Large language models (LLMs) have demonstrated remarkable performance across\ndiverse tasks by encoding vast amounts of factual knowledge. However, they are\nstill prone to hallucinations, generating incorrect or misleading information,\noften accompanied by high uncertainty. Existing methods for hallucination\ndetection primarily focus on quantifying internal uncertainty, which arises\nfrom missing or conflicting knowledge within the model. However, hallucinations\ncan also stem from external uncertainty, where ambiguous user queries lead to\nmultiple possible interpretations. In this work, we introduce Semantic Volume,\na novel mathematical measure for quantifying both external and internal\nuncertainty in LLMs. Our approach perturbs queries and responses, embeds them\nin a semantic space, and computes the determinant of the Gram matrix of the\nembedding vectors, capturing their dispersion as a measure of uncertainty. Our\nframework provides a generalizable and unsupervised uncertainty detection\nmethod without requiring white-box access to LLMs. We conduct extensive\nexperiments on both external and internal uncertainty detection, demonstrating\nthat our Semantic Volume method consistently outperforms existing baselines in\nboth tasks. Additionally, we provide theoretical insights linking our measure\nto differential entropy, unifying and extending previous sampling-based\nuncertainty measures such as the semantic entropy. Semantic Volume is shown to\nbe a robust and interpretable approach to improving the reliability of LLMs by\nsystematically detecting uncertainty in both user queries and model responses.\n","authors":["Xiaomin Li","Zhou Yu","Ziji Zhang","Yingying Zhuang","Swair Shah","Anurag Beniwal"],"pdf_url":"https://arxiv.org/pdf/2502.21239v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.21236v1","updated":"2025-02-28T17:05:13Z","published":"2025-02-28T17:05:13Z","title":"Transforming Tuberculosis Care: Optimizing Large Language Models For\n  Enhanced Clinician-Patient Communication","summary":"  Tuberculosis (TB) is the leading cause of death from an infectious disease\nglobally, with the highest burden in low- and middle-income countries. In these\nregions, limited healthcare access and high patient-to-provider ratios impede\neffective patient support, communication, and treatment completion. To bridge\nthis gap, we propose integrating a specialized Large Language Model into an\nefficacious digital adherence technology to augment interactive communication\nwith treatment supporters. This AI-powered approach, operating within a\nhuman-in-the-loop framework, aims to enhance patient engagement and improve TB\ntreatment outcomes.\n","authors":["Daniil Filienko","Mahek Nizar","Javier Roberti","Denise Galdamez","Haroon Jakher","Sarah Iribarren","Weichao Yuwen","Martine De Cock"],"pdf_url":"https://arxiv.org/pdf/2502.21236v1.pdf","comment":"GenAI4Health at AAAI-25"},{"id":"http://arxiv.org/abs/2412.16100v2","updated":"2025-02-28T17:02:23Z","published":"2024-12-20T17:42:25Z","title":"Logical Consistency of Large Language Models in Fact-checking","summary":"  In recent years, large language models (LLMs) have demonstrated significant\nsuccess in performing varied natural language tasks such as language\ntranslation, question-answering, summarizing, fact-checking, etc. Despite LLMs'\nimpressive ability to generate human-like texts, LLMs are infamous for their\ninconsistent responses - a meaning-preserving change in the input query results\nin an inconsistent response and attributes to vulnerabilities of LLMs such as\nhallucination. Consequently, existing research focuses on simple\nparaphrasing-based consistency assessment of LLMs, and ignores complex queries\nthat necessitate an even better understanding of logical reasoning by an LLM.\nOur work therefore addresses the logical inconsistency of LLMs under complex\nlogical queries with primitive logical operators, e.g., negation, conjunction,\nand disjunction. As a test bed, we consider retrieval-augmented LLMs on a\nfact-checking task involving propositional logic queries from knowledge graphs\n(KGs). Our contributions are threefold. Benchmark: We introduce three logical\nfact-checking datasets over KGs for community development towards logically\nconsistent LLMs. Assessment: We propose consistency measures of LLMs on\npropositional logic queries and demonstrate that existing LLMs lack logical\nconsistency, especially on complex queries. Improvement: We employ supervised\nfine-tuning to improve the logical consistency of LLMs on the complex\nfact-checking task with KG contexts. We have made our source code and\nbenchmarks available.\n","authors":["Bishwamittra Ghosh","Sarah Hasan","Naheed Anjum Arafat","Arijit Khan"],"pdf_url":"https://arxiv.org/pdf/2412.16100v2.pdf","comment":"Published at ICLR 2025"},{"id":"http://arxiv.org/abs/2502.21228v1","updated":"2025-02-28T16:59:30Z","published":"2025-02-28T16:59:30Z","title":"ECLeKTic: a Novel Challenge Set for Evaluation of Cross-Lingual\n  Knowledge Transfer","summary":"  To achieve equitable performance across languages, multilingual large\nlanguage models (LLMs) must be able to abstract knowledge beyond the language\nin which it was acquired. However, the current literature lacks reliable ways\nto measure LLMs' capability of cross-lingual knowledge transfer. To that end,\nwe present ECLeKTic, a multilingual closed-book QA (CBQA) dataset that\nEvaluates Cross-Lingual Knowledge Transfer in a simple, black-box manner. We\ndetected information with uneven coverage across languages by controlling for\npresence and absence of Wikipedia articles in 12 languages. We generated\nknowledge-seeking questions in a source language, for which the answer appears\nin a relevant Wikipedia article and translated them to all other 11 languages,\nfor which the respective Wikipedias lack equivalent articles. Assuming that\nWikipedia reflects the prominent knowledge in the LLM's training data, to solve\nECLeKTic's CBQA task the model is required to transfer knowledge between\nlanguages. Experimenting with 8 LLMs, we show that SOTA models struggle to\neffectively share knowledge across, languages even if they can predict the\nanswer well for queries in the same language the knowledge was acquired in.\n","authors":["Omer Goldman","Uri Shaham","Dan Malkin","Sivan Eiger","Avinatan Hassidim","Yossi Matias","Joshua Maynez","Adi Mayrav Gilady","Jason Riesa","Shruti Rijhwani","Laura Rimell","Idan Szpektor","Reut Tsarfaty","Matan Eyal"],"pdf_url":"https://arxiv.org/pdf/2502.21228v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.21224v1","updated":"2025-02-28T16:56:34Z","published":"2025-02-28T16:56:34Z","title":"Detecting Linguistic Diversity on Social Media","summary":"  This chapter explores the efficacy of using social media data to examine\nchanging linguistic behaviour of a place. We focus our investigation on\nAotearoa New Zealand where official statistics from the census is the only\nsource of language use data. We use published census data as the ground truth\nand the social media sub-corpus from the Corpus of Global Language Use as our\nalternative data source. We use place as the common denominator between the two\ndata sources. We identify the language conditions of each tweet in the social\nmedia data set and validated our results with two language identification\nmodels. We then compare levels of linguistic diversity at national, regional,\nand local geographies. The results suggest that social media language data has\nthe possibility to provide a rich source of spatial and temporal insights on\nthe linguistic profile of a place. We show that social media is sensitive to\ndemographic and sociopolitical changes within a language and at low-level\nregional and local geographies.\n","authors":["Sidney Wong","Benjamin Adams","Jonathan Dunn"],"pdf_url":"https://arxiv.org/pdf/2502.21224v1.pdf","comment":"Accepted to Cartography and GIScience in Australasia and Oceania:\n  Including twenty years of GeoCart"},{"id":"http://arxiv.org/abs/2410.20672v3","updated":"2025-02-28T16:44:24Z","published":"2024-10-28T02:15:45Z","title":"Relaxed Recursive Transformers: Effective Parameter Sharing with\n  Layer-wise LoRA","summary":"  Large language models (LLMs) are expensive to deploy. Parameter sharing\noffers a possible path towards reducing their size and cost, but its\neffectiveness in modern LLMs remains fairly limited. In this work, we revisit\n\"layer tying\" as form of parameter sharing in Transformers, and introduce novel\nmethods for converting existing LLMs into smaller \"Recursive Transformers\" that\nshare parameters across layers, with minimal loss of performance. Here, our\nRecursive Transformers are efficiently initialized from standard pretrained\nTransformers, but only use a single block of unique layers that is then\nrepeated multiple times in a loop. We further improve performance by\nintroducing Relaxed Recursive Transformers that add flexibility to the layer\ntying constraint via depth-wise low-rank adaptation (LoRA) modules, yet still\npreserve the compactness of the overall model. We show that our recursive\nmodels (e.g., recursive Gemma 1B) outperform both similar-sized vanilla\npretrained models (such as TinyLlama 1.1B and Pythia 1B) and knowledge\ndistillation baselines -- and can even recover most of the performance of the\noriginal \"full-size\" model (e.g., Gemma 2B with no shared parameters). Finally,\nwe propose Continuous Depth-wise Batching, a promising new inference paradigm\nenabled by the Recursive Transformer when paired with early exiting. In a\ntheoretical analysis, we show that this has the potential to lead to\nsignificant (2-3x) gains in inference throughput.\n","authors":["Sangmin Bae","Adam Fisch","Hrayr Harutyunyan","Ziwei Ji","Seungyeon Kim","Tal Schuster"],"pdf_url":"https://arxiv.org/pdf/2410.20672v3.pdf","comment":"ICLR 2025; 49 pages, 17 figures, 19 tables"},{"id":"http://arxiv.org/abs/2404.16880v2","updated":"2025-02-28T16:19:08Z","published":"2024-04-23T12:35:44Z","title":"Atomas: Hierarchical Alignment on Molecule-Text for Unified Molecule\n  Understanding and Generation","summary":"  Molecule-and-text cross-modal representation learning has emerged as a\npromising direction for enhancing the quality of molecular representation,\nthereby improving performance in various scientific fields, including drug\ndiscovery and materials science. Existing studies adopt a global alignment\napproach to learn the knowledge from different modalities. These global\nalignment approaches fail to capture fine-grained information, such as\nmolecular fragments and their corresponding textual description, which is\ncrucial for downstream tasks. Furthermore, it is incapable to model such\ninformation using a similar global alignment strategy due to data scarcity of\npaired local part annotated data from existing datasets. In this paper, we\npropose Atomas, a multi-modal molecular representation learning framework to\njointly learn representations from SMILES string and text. We design a\nHierarchical Adaptive Alignment model to concurrently learn the fine-grained\nfragment correspondence between two modalities and align these representations\nof fragments in three levels. Additionally, Atomas's end-to-end training\nframework incorporates the tasks of understanding and generating molecule,\nthereby supporting a wider range of downstream tasks. In the retrieval task,\nAtomas exhibits robust generalization ability and outperforms the baseline by\n30.8% of recall@1 on average. In the generation task, Atomas achieves\nstate-of-the-art results in both molecule captioning task and molecule\ngeneration task. Moreover, the visualization of the Hierarchical Adaptive\nAlignment model further confirms the chemical significance of our approach. Our\ncodes can be found at https://anonymous.4open.science/r/Atomas-03C3.\n","authors":["Yikun Zhang","Geyan Ye","Chaohao Yuan","Bo Han","Long-Kai Huang","Jianhua Yao","Wei Liu","Yu Rong"],"pdf_url":"https://arxiv.org/pdf/2404.16880v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.15296v3","updated":"2025-02-28T15:23:40Z","published":"2025-01-25T18:26:39Z","title":"You Only Prune Once: Designing Calibration-Free Model Compression With\n  Policy Learning","summary":"  The ever-increasing size of large language models (LLMs) presents significant\nchallenges for deployment due to their heavy computational and memory\nrequirements. Current model pruning techniques attempt to alleviate these\nissues by relying heavily on external calibration datasets to determine which\nparameters to prune or compress, thus limiting their flexibility and\nscalability across different compression ratios. Moreover, these methods often\ncause severe performance degradation, particularly in downstream tasks, when\nsubjected to higher compression rates. In this paper, we propose PruneNet, a\nnovel model compression method that addresses these limitations by\nreformulating model pruning as a policy learning process. PruneNet decouples\nthe pruning process from the model architecture, eliminating the need for\ncalibration datasets. It learns a stochastic pruning policy to assess parameter\nimportance solely based on intrinsic model properties while preserving the\nspectral structure to minimize information loss. PruneNet can compress the\nLLaMA-2-7B model in just 15 minutes, achieving over 80% retention of its\nzero-shot performance with a 30% compression ratio, outperforming existing\nmethods that retain only 75% performance. Furthermore, on complex multitask\nlanguage understanding tasks, PruneNet demonstrates its robustness by\npreserving up to 80% performance of the original model, proving itself a\nsuperior alternative to conventional structured compression techniques.\n","authors":["Ayan Sengupta","Siddhant Chaudhary","Tanmoy Chakraborty"],"pdf_url":"https://arxiv.org/pdf/2501.15296v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.08587v2","updated":"2025-02-28T15:16:04Z","published":"2024-06-12T18:47:28Z","title":"CS-Bench: A Comprehensive Benchmark for Large Language Models towards\n  Computer Science Mastery","summary":"  Large language models (LLMs) have demonstrated significant potential in\nadvancing various fields of research and society. However, the current\ncommunity of LLMs overly focuses on benchmarks for analyzing specific\nfoundational skills (e.g. mathematics and code generation), neglecting an\nall-round evaluation of the computer science field. To bridge this gap, we\nintroduce CS-Bench, the first multilingual (English, Chinese, French, German)\nbenchmark dedicated to evaluating the performance of LLMs in computer science.\nCS-Bench comprises approximately 10K meticulously curated test samples,\ncovering 26 subfields across 4 key areas of computer science, encompassing\nvarious task forms and divisions of knowledge and reasoning. Utilizing\nCS-Bench, we conduct a comprehensive evaluation of over 30 mainstream LLMs,\nrevealing the relationship between CS performance and model scales. We also\nquantitatively analyze the reasons for failures in existing LLMs and highlight\ndirections for improvements, including knowledge supplementation and\nCS-specific reasoning. Further cross-capability experiments show a high\ncorrelation between LLMs' capabilities in computer science and their abilities\nin mathematics and coding. Moreover, expert LLMs specialized in mathematics and\ncoding also demonstrate strong performances in several CS subfields. Looking\nahead, we envision CS-Bench serving as a cornerstone for LLM applications in\nthe CS field and paving new avenues in assessing LLMs' diverse reasoning\ncapabilities. The CS-Bench data and evaluation code are available at\nhttps://github.com/csbench/csbench.\n","authors":["Xiaoshuai Song","Muxi Diao","Guanting Dong","Zhengyang Wang","Yujia Fu","Runqi Qiao","Zhexu Wang","Dayuan Fu","Huangxuan Wu","Bin Liang","Weihao Zeng","Yejie Wang","Zhuoma GongQue","Jianing Yu","Qiuna Tan","Weiran Xu"],"pdf_url":"https://arxiv.org/pdf/2406.08587v2.pdf","comment":"Accepted at ICLR 2025"},{"id":"http://arxiv.org/abs/2501.06842v2","updated":"2025-02-28T15:15:31Z","published":"2025-01-12T15:21:22Z","title":"SPAM: Spike-Aware Adam with Momentum Reset for Stable LLM Training","summary":"  Large Language Models (LLMs) have demonstrated exceptional performance across\ndiverse tasks, yet their training remains highly resource-intensive and\nsusceptible to critical challenges such as training instability. A predominant\nsource of this instability stems from gradient and loss spikes, which disrupt\nthe learning process, often leading to costly interventions like checkpoint\nrecovery and experiment restarts, further amplifying inefficiencies. This paper\npresents a comprehensive investigation into gradient spikes observed during LLM\ntraining, revealing their prevalence across multiple architectures and\ndatasets. Our analysis shows that these spikes can be up to $1000\\times$ larger\nthan typical gradients, substantially deteriorating model performance. To\naddress this issue, we propose Spike-Aware Adam with Momentum Reset SPAM, a\nnovel optimizer designed to counteract gradient spikes through momentum reset\nand spike-aware gradient clipping. Extensive experiments, including both\npre-training and fine-tuning, demonstrate that SPAM consistently surpasses Adam\nand its variants across various tasks, including (1) LLM pre-training from 60M\nto 1B, (2) 4-bit LLM pre-training,(3) reinforcement learning, and (4) Time\nSeries Forecasting. Additionally, SPAM facilitates memory-efficient training by\nenabling sparse momentum, where only a subset of momentum terms are maintained\nand updated. When operating under memory constraints, SPAM outperforms\nstate-of-the-art memory-efficient optimizers such as GaLore and Adam-Mini. Our\nwork underscores the importance of mitigating gradient spikes in LLM training\nand introduces an effective optimization strategy that enhances both training\nstability and resource efficiency at scale. Code is available at\nhttps://github.com/TianjinYellow/SPAM-Optimizer.git\n","authors":["Tianjin Huang","Ziquan Zhu","Gaojie Jin","Lu Liu","Zhangyang Wang","Shiwei Liu"],"pdf_url":"https://arxiv.org/pdf/2501.06842v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.01523v4","updated":"2025-02-28T15:13:46Z","published":"2024-01-03T03:28:55Z","title":"GOAT-Bench: Safety Insights to Large Multimodal Models through\n  Meme-Based Social Abuse","summary":"  The exponential growth of social media has profoundly transformed how\ninformation is created, disseminated, and absorbed, exceeding any precedent in\nthe digital age. Regrettably, this explosion has also spawned a significant\nincrease in the online abuse of memes. Evaluating the negative impact of memes\nis notably challenging, owing to their often subtle and implicit meanings,\nwhich are not directly conveyed through the overt text and image. In light of\nthis, large multimodal models (LMMs) have emerged as a focal point of interest\ndue to their remarkable capabilities in handling diverse multimodal tasks. In\nresponse to this development, our paper aims to thoroughly examine the capacity\nof various LMMs (e.g., GPT-4o) to discern and respond to the nuanced aspects of\nsocial abuse manifested in memes. We introduce the comprehensive meme\nbenchmark, GOAT-Bench, comprising over 6K varied memes encapsulating themes\nsuch as implicit hate speech, sexism, and cyberbullying, etc. Utilizing\nGOAT-Bench, we delve into the ability of LMMs to accurately assess hatefulness,\nmisogyny, offensiveness, sarcasm, and harmful content. Our extensive\nexperiments across a range of LMMs reveal that current models still exhibit a\ndeficiency in safety awareness, showing insensitivity to various forms of\nimplicit abuse. We posit that this shortfall represents a critical impediment\nto the realization of safe artificial intelligence. The GOAT-Bench and\naccompanying resources are publicly accessible at https://goatlmm.github.io/,\ncontributing to ongoing research in this vital field.\n","authors":["Hongzhan Lin","Ziyang Luo","Bo Wang","Ruichao Yang","Jing Ma"],"pdf_url":"https://arxiv.org/pdf/2401.01523v4.pdf","comment":"The first work to benchmark Large Multimodal Models in safety insight\n  on social media"},{"id":"http://arxiv.org/abs/2501.13983v2","updated":"2025-02-28T15:07:55Z","published":"2025-01-23T06:57:24Z","title":"AdEval: Alignment-based Dynamic Evaluation to Mitigate Data\n  Contamination in Large Language Models","summary":"  As Large Language Models (LLMs) are pretrained on massive-scale corpora, the\nissue of data contamination has become increasingly severe, leading to\npotential overestimation of model performance during evaluation. To address\nthis, we propose AdEval (Alignment-based Dynamic Evaluation), a dynamic data\nevaluation method aimed at mitigating the impact of data contamination on\nevaluation reliability. AdEval extracts key knowledge points and main ideas to\nalign dynamically generated questions with static data's core concepts. It also\nleverages online search to provide detailed explanations of related knowledge\npoints, thereby creating high-quality evaluation samples with robust knowledge\nsupport. Furthermore, AdEval incorporates mechanisms to control the number and\ncomplexity of questions, enabling dynamic alignment and flexible adjustment.\nThis ensures that the generated questions align with the complexity of static\ndata while supporting varied complexity levels. Based on Bloom's taxonomy,\nAdEval conducts a multi-dimensional evaluation of LLMs across six cognitive\nlevels: remembering, understanding, applying, analyzing, evaluating, and\ncreating. Experimental results on multiple datasets demonstrate that AdEval\neffectively reduces the impact of data contamination on evaluation outcomes,\nenhancing both the fairness and reliability of the evaluation process.\n","authors":["Yang Fan"],"pdf_url":"https://arxiv.org/pdf/2501.13983v2.pdf","comment":"There are serious academic problems in this paper, such as data\n  falsification and plagiarism in the method of the paper"},{"id":"http://arxiv.org/abs/2502.19104v2","updated":"2025-02-28T15:00:01Z","published":"2025-02-26T12:46:59Z","title":"Are All Spanish Doctors Male? Evaluating Gender Bias in German Machine\n  Translation","summary":"  We present WinoMTDE, a new gender bias evaluation test set designed to assess\noccupational stereotyping and underrepresentation in German machine translation\n(MT) systems. Building on the automatic evaluation method introduced by\narXiv:1906.00591v1, we extend the approach to German, a language with\ngrammatical gender. The WinoMTDE dataset comprises 288 German sentences that\nare balanced in regard to gender, as well as stereotype, which was annotated\nusing German labor statistics. We conduct a large-scale evaluation of five\nwidely used MT systems and a large language model. Our results reveal\npersistent bias in most models, with the LLM outperforming traditional systems.\nThe dataset and evaluation code are publicly available under\nhttps://github.com/michellekappl/mt_gender_german.\n","authors":["Michelle Kappl"],"pdf_url":"https://arxiv.org/pdf/2502.19104v2.pdf","comment":"ISCA/ITG Workshop on Diversity in Large Speech and Language Models"},{"id":"http://arxiv.org/abs/2502.21112v1","updated":"2025-02-28T14:52:25Z","published":"2025-02-28T14:52:25Z","title":"Optimizing Large Language Models for ESG Activity Detection in Financial\n  Texts","summary":"  The integration of Environmental, Social, and Governance (ESG) factors into\ncorporate decision-making is a fundamental aspect of sustainable finance.\nHowever, ensuring that business practices align with evolving regulatory\nframeworks remains a persistent challenge. AI-driven solutions for\nautomatically assessing the alignment of sustainability reports and\nnon-financial disclosures with specific ESG activities could greatly support\nthis process. Yet, this task remains complex due to the limitations of\ngeneral-purpose Large Language Models (LLMs) in domain-specific contexts and\nthe scarcity of structured, high-quality datasets. In this paper, we\ninvestigate the ability of current-generation LLMs to identify text related to\nenvironmental activities. Furthermore, we demonstrate that their performance\ncan be significantly enhanced through fine-tuning on a combination of original\nand synthetically generated data. To this end, we introduce ESG-Activities, a\nbenchmark dataset containing 1,325 labelled text segments classified according\nto the EU ESG taxonomy. Our experimental results show that fine-tuning on\nESG-Activities significantly enhances classification accuracy, with open models\nsuch as Llama 7B and Gemma 7B outperforming large proprietary solutions in\nspecific configurations. These findings have important implications for\nfinancial analysts, policymakers, and AI researchers seeking to enhance ESG\ntransparency and compliance through advanced natural language processing\ntechniques.\n","authors":["Mattia Birti","Francesco Osborne","Andrea Maurino"],"pdf_url":"https://arxiv.org/pdf/2502.21112v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.18540v2","updated":"2025-02-28T14:49:25Z","published":"2024-05-28T19:16:17Z","title":"Learning diverse attacks on large language models for robust red-teaming\n  and safety tuning","summary":"  Red-teaming, or identifying prompts that elicit harmful responses, is a\ncritical step in ensuring the safe and responsible deployment of large language\nmodels (LLMs). Developing effective protection against many modes of attack\nprompts requires discovering diverse attacks. Automated red-teaming typically\nuses reinforcement learning to fine-tune an attacker language model to generate\nprompts that elicit undesirable responses from a target LLM, as measured, for\nexample, by an auxiliary toxicity classifier. We show that even with explicit\nregularization to favor novelty and diversity, existing approaches suffer from\nmode collapse or fail to generate effective attacks. As a flexible and\nprobabilistically principled alternative, we propose to use GFlowNet\nfine-tuning, followed by a secondary smoothing phase, to train the attacker\nmodel to generate diverse and effective attack prompts. We find that the\nattacks generated by our method are effective against a wide range of target\nLLMs, both with and without safety tuning, and transfer well between target\nLLMs. Finally, we demonstrate that models safety-tuned using a dataset of\nred-teaming prompts generated by our method are robust to attacks from other\nRL-based red-teaming approaches.\n","authors":["Seanie Lee","Minsu Kim","Lynn Cherif","David Dobre","Juho Lee","Sung Ju Hwang","Kenji Kawaguchi","Gauthier Gidel","Yoshua Bengio","Nikolay Malkin","Moksh Jain"],"pdf_url":"https://arxiv.org/pdf/2405.18540v2.pdf","comment":"ICLR 2025"},{"id":"http://arxiv.org/abs/2502.21107v1","updated":"2025-02-28T14:46:02Z","published":"2025-02-28T14:46:02Z","title":"Generating patient cohorts from electronic health records using two-step\n  retrieval-augmented text-to-SQL generation","summary":"  Clinical cohort definition is crucial for patient recruitment and\nobservational studies, yet translating inclusion/exclusion criteria into SQL\nqueries remains challenging and manual. We present an automated system\nutilizing large language models that combines criteria parsing, two-level\nretrieval augmented generation with specialized knowledge bases, medical\nconcept standardization, and SQL generation to retrieve patient cohorts with\npatient funnels. The system achieves 0.75 F1-score in cohort identification on\nEHR data, effectively capturing complex temporal and logical relationships.\nThese results demonstrate the feasibility of automated cohort generation for\nepidemiological research.\n","authors":["Angelo Ziletti","Leonardo D'Ambrosi"],"pdf_url":"https://arxiv.org/pdf/2502.21107v1.pdf","comment":"7 pages, 1 figure"},{"id":"http://arxiv.org/abs/2502.21098v1","updated":"2025-02-28T14:36:57Z","published":"2025-02-28T14:36:57Z","title":"Re-evaluating Theory of Mind evaluation in large language models","summary":"  The question of whether large language models (LLMs) possess Theory of Mind\n(ToM) -- often defined as the ability to reason about others' mental states --\nhas sparked significant scientific and public interest. However, the evidence\nas to whether LLMs possess ToM is mixed, and the recent growth in evaluations\nhas not resulted in a convergence. Here, we take inspiration from cognitive\nscience to re-evaluate the state of ToM evaluation in LLMs. We argue that a\nmajor reason for the disagreement on whether LLMs have ToM is a lack of clarity\non whether models should be expected to match human behaviors, or the\ncomputations underlying those behaviors. We also highlight ways in which\ncurrent evaluations may be deviating from \"pure\" measurements of ToM abilities,\nwhich also contributes to the confusion. We conclude by discussing several\ndirections for future research, including the relationship between ToM and\npragmatic communication, which could advance our understanding of artificial\nsystems as well as human cognition.\n","authors":["Jennifer Hu","Felix Sosa","Tomer Ullman"],"pdf_url":"https://arxiv.org/pdf/2502.21098v1.pdf","comment":"under review"},{"id":"http://arxiv.org/abs/2502.21087v1","updated":"2025-02-28T14:26:47Z","published":"2025-02-28T14:26:47Z","title":"PASemiQA: Plan-Assisted Agent for Question Answering on Semi-Structured\n  Data with Text and Relational Information","summary":"  Large language models (LLMs) have shown impressive abilities in answering\nquestions across various domains, but they often encounter hallucination issues\non questions that require professional and up-to-date knowledge. To address\nthis limitation, retrieval-augmented generation (RAG) techniques have been\nproposed, which retrieve relevant information from external sources to inform\ntheir responses. However, existing RAG methods typically focus on a single type\nof external data, such as vectorized text database or knowledge graphs, and\ncannot well handle real-world questions on semi-structured data containing both\ntext and relational information. To bridge this gap, we introduce PASemiQA, a\nnovel approach that jointly leverages text and relational information in\nsemi-structured data to answer questions. PASemiQA first generates a plan to\nidentify relevant text and relational information to answer the question in\nsemi-structured data, and then uses an LLM agent to traverse the\nsemi-structured data and extract necessary information. Our empirical results\ndemonstrate the effectiveness of PASemiQA across different semi-structured\ndatasets from various domains, showcasing its potential to improve the accuracy\nand reliability of question answering systems on semi-structured data.\n","authors":["Hansi Yang","Qi Zhang","Wei Jiang","Jianguo Li"],"pdf_url":"https://arxiv.org/pdf/2502.21087v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.18934v3","updated":"2025-02-28T14:23:16Z","published":"2025-02-26T08:36:20Z","title":"Kanana: Compute-efficient Bilingual Language Models","summary":"  We introduce Kanana, a series of bilingual language models that demonstrate\nexceeding performance in Korean and competitive performance in English. The\ncomputational cost of Kanana is significantly lower than that of\nstate-of-the-art models of similar size. The report details the techniques\nemployed during pre-training to achieve compute-efficient yet competitive\nmodels, including high quality data filtering, staged pre-training, depth\nup-scaling, and pruning and distillation. Furthermore, the report outlines the\nmethodologies utilized during the post-training of the Kanana models,\nencompassing supervised fine-tuning and preference optimization, aimed at\nenhancing their capability for seamless interaction with users. Lastly, the\nreport elaborates on plausible approaches used for language model adaptation to\nspecific scenarios, such as embedding, retrieval augmented generation, and\nfunction calling. The Kanana model series spans from 2.1B to 32.5B parameters\nwith 2.1B models (base, instruct, embedding) publicly released to promote\nresearch on Korean language models.\n","authors":[" Kanana LLM Team","Yunju Bak","Hojin Lee","Minho Ryu","Jiyeon Ham","Seungjae Jung","Daniel Wontae Nam","Taegyeong Eo","Donghun Lee","Doohae Jung","Boseop Kim","Nayeon Kim","Jaesun Park","Hyunho Kim","Hyunwoong Ko","Changmin Lee","Kyoung-Woon On","Seulye Baeg","Junrae Cho","Sunghee Jung","Jieun Kang","EungGyun Kim","Eunhwa Kim","Byeongil Ko","Daniel Lee","Minchul Lee","Miok Lee","Shinbok Lee","Gaeun Seo"],"pdf_url":"https://arxiv.org/pdf/2502.18934v3.pdf","comment":"40 pages, 15 figures"},{"id":"http://arxiv.org/abs/2502.21074v1","updated":"2025-02-28T14:07:48Z","published":"2025-02-28T14:07:48Z","title":"CODI: Compressing Chain-of-Thought into Continuous Space via\n  Self-Distillation","summary":"  Chain-of-Thought (CoT) enhances Large Language Models (LLMs) by enabling\nstep-by-step reasoning in natural language. However, the language space may be\nsuboptimal for reasoning. While implicit CoT methods attempt to enable\nreasoning without explicit CoT tokens, they have consistently lagged behind\nexplicit CoT method in task performance. We propose CODI (Continuous\nChain-of-Thought via Self-Distillation), a novel framework that distills CoT\ninto a continuous space, where a shared model acts as both teacher and student,\njointly learning explicit and implicit CoT while aligning their hidden\nactivation on the token generating the final answer. CODI is the first implicit\nCoT method to match explicit CoT's performance on GSM8k while achieving 3.1x\ncompression, surpassing the previous state-of-the-art by 28.2% in accuracy.\nFurthermore, CODI demonstrates scalability, robustness, and generalizability to\nmore complex CoT datasets. Additionally, CODI retains interpretability by\ndecoding its continuous thoughts, making its reasoning process transparent. Our\nfindings establish implicit CoT as not only a more efficient but a powerful\nalternative to explicit CoT.\n","authors":["Zhenyi Shen","Hanqi Yan","Linhai Zhang","Zhanghao Hu","Yali Du","Yulan He"],"pdf_url":"https://arxiv.org/pdf/2502.21074v1.pdf","comment":"15 pages"},{"id":"http://arxiv.org/abs/2406.11431v3","updated":"2025-02-28T13:43:17Z","published":"2024-06-17T11:36:39Z","title":"Super(ficial)-alignment: Strong Models May Deceive Weak Models in\n  Weak-to-Strong Generalization","summary":"  Superalignment, where humans act as weak supervisors for superhuman models,\nhas become a crucial problem with the rapid development of Large Language\nModels (LLMs). Recent work has preliminarily studied this problem by using weak\nmodels to supervise strong models, and discovered that weakly supervised strong\nstudents can consistently outperform weak teachers towards the alignment\ntarget, leading to a weak-to-strong generalization phenomenon. However, we are\nconcerned that behind such a promising phenomenon, whether there exists an\nissue of weak-to-strong deception, where strong models deceive weak models by\nexhibiting well-aligned in areas known to weak models but producing misaligned\nbehaviors in cases weak models do not know. We take an initial step towards\nexploring this security issue in a specific but realistic multi-objective\nalignment case, where there may be some alignment targets conflicting with each\nother (e.g., helpfulness v.s. harmlessness). We aim to explore whether, in such\ncases, strong models might deliberately make mistakes in areas known to them\nbut unknown to weak models within one alignment dimension, in exchange for a\nhigher reward in another dimension. Through extensive experiments in both the\nreward modeling and preference optimization scenarios, we find: (1) The\nweak-to-strong deception phenomenon exists across all settings. (2) The\ndeception intensifies as the capability gap between weak and strong models\nincreases. (3) Bootstrapping with an intermediate model can mitigate the\ndeception to some extent, though its effectiveness remains limited. Our work\nhighlights the urgent need to pay more attention to the true reliability of\nsuperalignment.\n","authors":["Wenkai Yang","Shiqi Shen","Guangyao Shen","Wei Yao","Yong Liu","Zhi Gong","Yankai Lin","Ji-Rong Wen"],"pdf_url":"https://arxiv.org/pdf/2406.11431v3.pdf","comment":"Accepted at ICLR 2025, camera-ready version"},{"id":"http://arxiv.org/abs/2502.15835v2","updated":"2025-02-28T13:40:42Z","published":"2025-02-20T12:44:26Z","title":"Pragmatic Reasoning improves LLM Code Generation","summary":"  Large Language Models (LLMs) have demonstrated impressive potential in\ntranslating natural language (NL) instructions into program code. However, user\ninstructions often contain inherent ambiguities, making it challenging for LLMs\nto generate code that accurately reflects the user's true intent. To address\nthis challenge, researchers have proposed to produce multiple candidates of the\nprogram code and then rerank them to identify the best solution. In this paper,\nwe propose CodeRSA, a novel code candidate reranking mechanism built upon the\nRational Speech Act (RSA) framework, designed to guide LLMs toward more\ncomprehensive pragmatic reasoning about user intent. We evaluate CodeRSA using\none of the latest LLMs on a popular code generation dataset. Our experiment\nresults show that CodeRSA consistently outperforms common baselines, surpasses\nthe state-of-the-art approach in most cases, and demonstrates robust overall\nperformance. These findings underscore the effectiveness of integrating\npragmatic reasoning into code candidate reranking, offering a promising\ndirection for enhancing code generation quality in LLMs.\n","authors":["Zhuchen Cao","Sven Apel","Adish Singla","Vera Demberg"],"pdf_url":"https://arxiv.org/pdf/2502.15835v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.09961v2","updated":"2025-02-28T13:33:00Z","published":"2024-06-14T12:10:51Z","title":"ChartMimic: Evaluating LMM's Cross-Modal Reasoning Capability via\n  Chart-to-Code Generation","summary":"  We introduce a new benchmark, ChartMimic, aimed at assessing the\nvisually-grounded code generation capabilities of large multimodal models\n(LMMs). ChartMimic utilizes information-intensive visual charts and textual\ninstructions as inputs, requiring LMMs to generate the corresponding code for\nchart rendering. ChartMimic includes 4,800 human-curated (figure, instruction,\ncode) triplets, which represent the authentic chart use cases found in\nscientific papers across various domains (e.g., Physics, Computer Science,\nEconomics, etc). These charts span 18 regular types and 4 advanced types,\ndiversifying into 201 subcategories. Furthermore, we propose multi-level\nevaluation metrics to provide an automatic and thorough assessment of the\noutput code and the rendered charts. Unlike existing code generation\nbenchmarks, ChartMimic places emphasis on evaluating LMMs' capacity to\nharmonize a blend of cognitive capabilities, encompassing visual understanding,\ncode generation, and cross-modal reasoning. The evaluation of $3$ proprietary\nmodels and 14 open-weight models highlights the substantial challenges posed by\nChartMimic. Even the advanced GPT-4o, InternVL2-Llama3-76B only achieved an\naverage score across Direct Mimic and Customized Mimic tasks of 82.2 and 61.6,\nrespectively, indicating significant room for improvement. We anticipate that\nChartMimic will inspire the development of LMMs, advancing the pursuit of\nartificial general intelligence.\n","authors":["Cheng Yang","Chufan Shi","Yaxin Liu","Bo Shui","Junjie Wang","Mohan Jing","Linran Xu","Xinyu Zhu","Siheng Li","Yuxiang Zhang","Gongye Liu","Xiaomei Nie","Deng Cai","Yujiu Yang"],"pdf_url":"https://arxiv.org/pdf/2406.09961v2.pdf","comment":"Accepted to ICLR 2025. Data and code are available at\n  https://github.com/ChartMimic/ChartMimic"},{"id":"http://arxiv.org/abs/2408.08545v3","updated":"2025-02-28T13:23:56Z","published":"2024-08-16T06:11:21Z","title":"SelectLLM: Query-Aware Efficient Selection Algorithm for Large Language\n  Models","summary":"  Large language models (LLMs) have been widely adopted due to their remarkable\nperformance across various applications, driving the accelerated development of\na large number of diverse models. However, these individual LLMs show\nlimitations in generalization and performance on complex tasks due to inherent\ntraining biases, model size constraints, and the quality or diversity of\npre-training datasets. A promising direction is to efficiently harness the\ndiverse capabilities of LLMs to overcome these individual limitations. To\naddress these limitations, we introduce a novel LLM selection algorithm called\nSelectLLM, which efficiently directs input queries to the most suitable subset\nof LLMs from a large pool, ensuring that the selected models collectively\nprovide accurate responses. SelectLLM employs a multi-label classifier and\npolicy based on the classifier's predictions and confidence scores in selecting\nan optimal, query-aware, and lightweight subset of LLMs. Our findings indicate\nthat the proposed model outperforms existing ensemble-based baselines and\nachieves competitive performance with similarly sized top-performing LLMs while\nmaintaining efficiency. Specifically, it achieves a huge reduction in inference\nlatency on two challenging reasoning benchmarks: 13% on GSM8K and 70% on MMLU,\ncompared to the top-performing baseline. Also, we establish a theoretical upper\nbound by an Oracle with LLMs and perform an in-depth linguistic analysis to\nunderstand the performance gap between the Oracle and SelectLLM.\n","authors":["Kaushal Kumar Maurya","KV Aditya Srivatsa","Ekaterina Kochmar"],"pdf_url":"https://arxiv.org/pdf/2408.08545v3.pdf","comment":"8 pages"},{"id":"http://arxiv.org/abs/2502.21030v1","updated":"2025-02-28T13:22:29Z","published":"2025-02-28T13:22:29Z","title":"Beyond Words: A Latent Memory Approach to Internal Reasoning in LLMs","summary":"  Recent advances in large language models (LLMs) have popularized the\nchain-of-thought (CoT) paradigm, in which models produce explicit reasoning\nsteps in natural language. Although this approach improves interpretability and\nfacilitates external auditing, it may not represent the most computationally\nefficient method for internal reasoning. In contrast, human cognition relies on\nimplicit mental representations that recall past sensory and episodic\ninformation without requiring complete verbalization. In this paper, we propose\na framework that integrates implicit mental representations into the internal\nreasoning processes of LLMs. Preliminary experiments indicate that\nincorporating an Implicit Memory Module (IMM) into a simple GPT model yields a\nreduction of between 35% and 57% in final training loss compared to a regular\nGPT baseline. The addition of an explicit interpretability channel (e.g., a\nchain-of-thought decoder) is straightforward to implement within this approach.\nWe outline theoretical foundations, propose technical mechanisms to scale the\nmemory module, and discuss how these ideas may lead to more efficient and\nrobust reasoning, with optional future extensions for explicit auditability.\n","authors":["José I. Orlicki"],"pdf_url":"https://arxiv.org/pdf/2502.21030v1.pdf","comment":"13 pages, 5 figures"},{"id":"http://arxiv.org/abs/2406.17808v3","updated":"2025-02-28T13:08:44Z","published":"2024-06-24T03:59:17Z","title":"Training-Free Exponential Context Extension via Cascading KV Cache","summary":"  The transformer's context window is vital for tasks such as few-shot learning\nand conditional generation as it preserves previous tokens for active memory.\nHowever, as the context lengths increase, the computational costs grow\nquadratically, hindering the deployment of large language models (LLMs) in\nreal-world, long sequence scenarios. Although some recent key-value caching (KV\nCache) methods offer linear inference complexity, they naively manage the\nstored context, prematurely evicting tokens and losing valuable information.\nMoreover, they lack an optimized prefill/prompt stage strategy, resulting in\nhigher latency than even quadratic attention for realistic context sizes. In\nresponse, we introduce a novel mechanism that leverages cascading sub-cache\nbuffers to selectively retain the most relevant tokens, enabling the model to\nmaintain longer context histories without increasing the cache size. Our\napproach outperforms linear caching baselines across key benchmarks, including\nstreaming perplexity, question answering, book summarization, and passkey\nretrieval, where it retains better retrieval accuracy at 1M tokens after four\ndoublings of the cache size of 65K. Additionally, our method reduces prefill\nstage latency by a factor of 6.8 when compared to flash attention on 1M tokens.\nThese innovations not only enhance the computational efficiency of LLMs but\nalso pave the way for their effective deployment in resource-constrained\nenvironments, enabling large-scale, real-time applications with significantly\nreduced latency.\n","authors":["Jeffrey Willette","Heejun Lee","Youngwan Lee","Myeongjae Jeon","Sung Ju Hwang"],"pdf_url":"https://arxiv.org/pdf/2406.17808v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.21024v1","updated":"2025-02-28T13:06:25Z","published":"2025-02-28T13:06:25Z","title":"Extending Dense Passage Retrieval with Temporal Information","summary":"  Temporal awareness is crucial in many information retrieval tasks,\nparticularly in scenarios where the relevance of documents depends on their\nalignment with the query's temporal context. Traditional retrieval methods such\nas BM25 and Dense Passage Retrieval (DPR) excel at capturing lexical and\nsemantic relevance but fall short in addressing time-sensitive queries. To\nbridge this gap, we introduce the temporal retrieval model that integrates\nexplicit temporal signals by incorporating query timestamps and document dates\ninto the representation space. Our approach ensures that retrieved passages are\nnot only topically relevant but also temporally aligned with user intent. We\nevaluate our approach on two large-scale benchmark datasets, ArchivalQA and\nChroniclingAmericaQA, achieving substantial performance gains over standard\nretrieval baselines. In particular, our model improves Top-1 retrieval accuracy\nby 6.63% and NDCG@10 by 3.79% on ArchivalQA, while yielding a 9.56% boost in\nTop-1 retrieval accuracy and 4.68% in NDCG@10 on ChroniclingAmericaQA.\nAdditionally, we introduce a time-sensitive negative sampling strategy, which\nrefines the model's ability to distinguish between temporally relevant and\nirrelevant documents during training. Our findings highlight the importance of\nexplicitly modeling time in retrieval systems and set a new standard for\nhandling temporally grounded queries.\n","authors":["Abdelrahman Abdallah","Bhawna Piryani","Jonas Wallat","Avishek Anand","Adam Jatowt"],"pdf_url":"https://arxiv.org/pdf/2502.21024v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.20372v2","updated":"2025-02-28T13:06:02Z","published":"2024-12-29T06:32:36Z","title":"LLM2: Let Large Language Models Harness System 2 Reasoning","summary":"  Large language models (LLMs) have exhibited impressive capabilities across a\nmyriad of tasks, yet they occasionally yield undesirable outputs. We posit that\nthese limitations are rooted in the foundational autoregressive architecture of\nLLMs, which inherently lacks mechanisms for differentiating between desirable\nand undesirable results. Drawing inspiration from the dual-process theory of\nhuman cognition, we introduce LLM2, a novel framework that combines an LLM\n(System 1) with a process-based verifier (System 2). Within LLM2, the LLM is\nresponsible for generating plausible candidates, while the verifier provides\ntimely process-based feedback to distinguish desirable and undesirable outputs.\nThe verifier is trained with a pairwise comparison loss on synthetic\nprocess-supervision data generated through our token quality exploration\nstrategy. Empirical results on mathematical reasoning benchmarks substantiate\nthe efficacy of LLM2, exemplified by an accuracy enhancement from 50.3 to 57.8\n(+7.5) for Llama3-1B on GSM8K. Furthermore, when combined with\nself-consistency, LLM2 achieves additional improvements, boosting major@20\naccuracy from 56.2 to 70.2 (+14.0).\n","authors":["Cheng Yang","Chufan Shi","Siheng Li","Bo Shui","Yujiu Yang","Wai Lam"],"pdf_url":"https://arxiv.org/pdf/2412.20372v2.pdf","comment":"Accepted to NAACL 2025 Main Conference"},{"id":"http://arxiv.org/abs/2502.21017v1","updated":"2025-02-28T13:04:04Z","published":"2025-02-28T13:04:04Z","title":"PersuasiveToM: A Benchmark for Evaluating Machine Theory of Mind in\n  Persuasive Dialogues","summary":"  The ability to understand and predict the mental states of oneself and\nothers, known as the Theory of Mind (ToM), is crucial for effective social\ninteractions. Recent research has emerged to evaluate whether Large Language\nModels (LLMs) exhibit a form of ToM. Although recent studies have evaluated ToM\nin LLMs, existing benchmarks focus predominantly on physical perception with\nprinciples guided by the Sally-Anne test in synthetic stories and\nconversations, failing to capture the complex psychological activities of\nmental states in real-life social interactions. To mitigate this gap, we\npropose PersuasiveToM, a benchmark designed to evaluate the ToM abilities of\nLLMs in persuasive dialogues. Our framework introduces two categories of\nquestions: (1) ToM Reasoning, assessing the capacity of LLMs to track evolving\nmental states (e.g., desire shifts in persuadees), and (2) ToM Application,\nevaluating whether LLMs can take advantage of inferred mental states to select\neffective persuasion strategies (e.g., emphasize rarity) and evaluate the\neffectiveness of persuasion strategies. Experiments across eight\nstate-of-the-art LLMs reveal that while models excel on multiple questions,\nthey struggle to answer questions that need tracking the dynamics and shifts of\nmental states and understanding the mental states in the whole dialogue\ncomprehensively. Our aim with PersuasiveToM is to allow an effective evaluation\nof the ToM reasoning ability of LLMs with more focus on complex psychological\nactivities. Our code is available at\nhttps://github.com/Yu-Fangxu/PersuasiveToM.\n","authors":["Fangxu Yu","Lai Jiang","Shenyi Huang","Zhen Wu","Xinyu Dai"],"pdf_url":"https://arxiv.org/pdf/2502.21017v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.19883v2","updated":"2025-02-28T12:59:26Z","published":"2025-02-27T08:44:04Z","title":"Behind the Tip of Efficiency: Uncovering the Submerged Threats of\n  Jailbreak Attacks in Small Language Models","summary":"  Small language models (SLMs) have become increasingly prominent in the\ndeployment on edge devices due to their high efficiency and low computational\ncost. While researchers continue to advance the capabilities of SLMs through\ninnovative training strategies and model compression techniques, the security\nrisks of SLMs have received considerably less attention compared to large\nlanguage models (LLMs).To fill this gap, we provide a comprehensive empirical\nstudy to evaluate the security performance of 13 state-of-the-art SLMs under\nvarious jailbreak attacks. Our experiments demonstrate that most SLMs are quite\nsusceptible to existing jailbreak attacks, while some of them are even\nvulnerable to direct harmful prompts.To address the safety concerns, we\nevaluate several representative defense methods and demonstrate their\neffectiveness in enhancing the security of SLMs. We further analyze the\npotential security degradation caused by different SLM techniques including\narchitecture compression, quantization, knowledge distillation, and so on. We\nexpect that our research can highlight the security challenges of SLMs and\nprovide valuable insights to future work in developing more robust and secure\nSLMs.\n","authors":["Sibo Yi","Tianshuo Cong","Xinlei He","Qi Li","Jiaxing Song"],"pdf_url":"https://arxiv.org/pdf/2502.19883v2.pdf","comment":"12 pages. 6 figures"},{"id":"http://arxiv.org/abs/2410.14668v4","updated":"2025-02-28T12:57:03Z","published":"2024-10-18T17:57:40Z","title":"MiCEval: Unveiling Multimodal Chain of Thought's Quality via Image\n  Description and Reasoning Steps","summary":"  Multimodal Chain of Thought (MCoT) is a popular prompting strategy for\nimproving the performance of multimodal large language models (MLLMs) across a\nrange of complex reasoning tasks. Despite its popularity, there is a notable\nabsence of automated methods for evaluating the quality of reasoning steps in\nMCoT. To address this gap, we propose Multimodal Chain-of-Thought Evaluation\n(MiCEval), a framework designed to assess the correctness of reasoning chains\nby evaluating the quality of both the description and each reasoning step. The\nevaluation of the description component focuses on the accuracy of the image\ndescriptions, while the reasoning step evaluates the quality of each step as it\nis conditionally generated based on the preceding steps. MiCEval is built upon\na fine-grained dataset with annotations that rate each step according to\ncorrectness, relevance, and informativeness. Extensive experiments on four\nstate-of-the-art MLLMs show that step-wise evaluations using MiCEval align more\nclosely with human judgments compared to existing methods based on cosine\nsimilarity or fine-tuning approaches. MiCEval datasets and code can be found in\nhttps://github.com/alenai97/MiCEval.\n","authors":["Xiongtao Zhou","Jie He","Lanyu Chen","Jingyu Li","Haojing Chen","Víctor Gutiérrez-Basulto","Jeff Z. Pan","Hanjie Chen"],"pdf_url":"https://arxiv.org/pdf/2410.14668v4.pdf","comment":"NAACL 2025"},{"id":"http://arxiv.org/abs/2402.17812v4","updated":"2025-02-28T12:53:34Z","published":"2024-02-27T14:51:11Z","title":"DropBP: Accelerating Fine-Tuning of Large Language Models by Dropping\n  Backward Propagation","summary":"  Large language models (LLMs) have achieved significant success across various\ndomains. However, training these LLMs typically involves substantial memory and\ncomputational costs during both forward and backward propagation. While\nparameter-efficient fine-tuning (PEFT) considerably reduces the training memory\nassociated with parameters, it does not address the significant computational\ncosts and activation memory. In this paper, we propose Dropping Backward\nPropagation (DropBP), a novel approach designed to reduce computational costs\nand activation memory while maintaining accuracy. DropBP randomly drops layers\nduring backward propagation, which is essentially equivalent to training\nshallow submodules generated by undropped layers and residual connections.\nAdditionally, DropBP calculates the sensitivity of each layer to assign an\nappropriate drop rate, thereby stabilizing the training process. DropBP is not\nonly applicable to full fine-tuning but can also be orthogonally integrated\nwith all types of PEFT by dropping layers during backward propagation.\nSpecifically, DropBP can reduce training time by 44% with comparable accuracy\nto the baseline, accelerate convergence to the same perplexity by 1.5x, and\nenable training with a sequence length 6.2x larger on a single NVIDIA-A100 GPU.\nFurthermore, our DropBP enabled a throughput increase of 79% on a NVIDIA A100\nGPU and 117% on an Intel Gaudi2 HPU. The code is available at\nhttps://github.com/WooSunghyeon/dropbp.\n","authors":["Sunghyeon Woo","Baeseong Park","Byeongwook Kim","Minjung Jo","Se Jung Kwon","Dongsuk Jeon","Dongsoo Lee"],"pdf_url":"https://arxiv.org/pdf/2402.17812v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.19839v5","updated":"2025-02-28T12:35:34Z","published":"2024-09-30T00:41:51Z","title":"ForecastBench: A Dynamic Benchmark of AI Forecasting Capabilities","summary":"  Forecasts of future events are essential inputs into informed\ndecision-making. Machine learning (ML) systems have the potential to deliver\nforecasts at scale, but there is no framework for evaluating the accuracy of ML\nsystems on a standardized set of forecasting questions. To address this gap, we\nintroduce ForecastBench: a dynamic benchmark that evaluates the accuracy of ML\nsystems on an automatically generated and regularly updated set of 1,000\nforecasting questions. To avoid any possibility of data leakage, ForecastBench\nis comprised solely of questions about future events that have no known answer\nat the time of submission. We quantify the capabilities of current ML systems\nby collecting forecasts from expert (human) forecasters, the general public,\nand LLMs on a random subset of questions from the benchmark ($N=200$). While\nLLMs have achieved super-human performance on many benchmarks, they perform\nless well here: expert forecasters outperform the top-performing LLM ($p$-value\n$<0.001$). We display system and human scores in a public leaderboard at\nwww.forecastbench.org.\n","authors":["Ezra Karger","Houtan Bastani","Chen Yueh-Han","Zachary Jacobs","Danny Halawi","Fred Zhang","Philip E. Tetlock"],"pdf_url":"https://arxiv.org/pdf/2409.19839v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.20992v1","updated":"2025-02-28T12:22:13Z","published":"2025-02-28T12:22:13Z","title":"Capability Localization: Capabilities Can be Localized rather than\n  Individual Knowledge","summary":"  Large scale language models have achieved superior performance in tasks\nrelated to natural language processing, however, it is still unclear how model\nparameters affect performance improvement. Previous studies assumed that\nindividual knowledge is stored in local parameters, and the storage form of\nindividual knowledge is dispersed parameters, parameter layers, or parameter\nchains, which are not unified. We found through fidelity and reliability\nevaluation experiments that individual knowledge cannot be localized.\nAfterwards, we constructed a dataset for decoupling experiments and discovered\nthe potential for localizing data commonalities. To further reveal this\nphenomenon, this paper proposes a Commonality Neuron Localization (CNL) method,\nwhich successfully locates commonality neurons and achieves a neuron overlap\nrate of 96.42% on the GSM8K dataset. Finally, we have demonstrated through\ncross data experiments that commonality neurons are a collection of capability\nneurons that possess the capability to enhance performance. Our code is\navailable at https://github.com/nlpkeg/Capability-Neuron-Localization.\n","authors":["Xiusheng Huang","Jiaxiang Liu","Yequan Wang","Jun Zhao","Kang Liu"],"pdf_url":"https://arxiv.org/pdf/2502.20992v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.20988v1","updated":"2025-02-28T12:00:51Z","published":"2025-02-28T12:00:51Z","title":"Merging Clinical Knowledge into Large Language Models for Medical\n  Research and Applications: A Survey","summary":"  Clinical knowledge is the collection of information learned from studies on\nthe causes, prognosis, diagnosis, and treatment of diseases. This type of\nknowledge can improve curing performances, and promote physical health. With\nthe emergence of large language models (LLMs), medical artificial intelligence\n(medical AI), which aims to apply academic medical AI systems to real-world\nmedical scenarios, has entered a new age of development, resulting in excellent\nworks such as DoctorGPT and Pangu-Drug from academic and industrial researches.\nHowever, the field lacks a comprehensive compendium and comparison of building\nmedical AI systems from academia and industry. Therefore, this survey focuses\non the building paradigms of medical AI systems including the use of clinical\ndatabases, datasets, training pipelines, integrating medical knowledge graphs,\nsystem applications, and evaluation systems. We hope that this survey can help\nrelevant practical researchers understand the current performance of academic\nmodels in various fields of healthcare, as well as the potential problems and\nfuture directions for implementing these scientific achievements.\n","authors":["Qiyuan Li","Haijiang Liu","Caicai Guo","Deyu Chen","Meng Wang","Feng Gao","Jinguang Gu"],"pdf_url":"https://arxiv.org/pdf/2502.20988v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.06655v2","updated":"2025-02-28T11:58:28Z","published":"2024-11-11T01:42:56Z","title":"Explore the Reasoning Capability of LLMs in the Chess Testbed","summary":"  Reasoning is a central capability of human intelligence. In recent years,\nwith the advent of large-scale datasets, pretrained large language models have\nemerged with new capabilities, including reasoning. However, these models still\nstruggle with long-term, complex reasoning tasks, such as playing chess. Based\non the observation that expert chess players employ a dual approach combining\nlong-term strategic play with short-term tactical play along with language\nexplanation, we propose improving the reasoning capability of large language\nmodels in chess by integrating annotated strategy and tactic. Specifically, we\ncollect a dataset named MATE, which consists of 1 million chess positions with\ncandidate moves annotated by chess experts for strategy and tactics. We\nfinetune the LLaMA-3-8B model and compare it against state-of-the-art\ncommercial language models in the task of selecting better chess moves. Our\nexperiments show that our models perform better than GPT, Claude, and Gemini\nmodels. We find that language explanations can enhance the reasoning capability\nof large language models.\n","authors":["Shu Wang","Lei Ji","Renxi Wang","Wenxiao Zhao","Haokun Liu","Yifan Hou","Ying Nian Wu"],"pdf_url":"https://arxiv.org/pdf/2411.06655v2.pdf","comment":"NAACL2025 Main Conference. Data and models are available:\n  https://mate-chess.github.io/"},{"id":"http://arxiv.org/abs/2502.20984v1","updated":"2025-02-28T11:52:02Z","published":"2025-02-28T11:52:02Z","title":"UoR-NCL at SemEval-2025 Task 1: Using Generative LLMs and CLIP Models\n  for Multilingual Multimodal Idiomaticity Representation","summary":"  SemEval-2025 Task 1 focuses on ranking images based on their alignment with a\ngiven nominal compound that may carry idiomatic meaning in both English and\nBrazilian Portuguese. To address this challenge, this work uses generative\nlarge language models (LLMs) and multilingual CLIP models to enhance idiomatic\ncompound representations. LLMs generate idiomatic meanings for potentially\nidiomatic compounds, enriching their semantic interpretation. These meanings\nare then encoded using multilingual CLIP models, serving as representations for\nimage ranking. Contrastive learning and data augmentation techniques are\napplied to fine-tune these embeddings for improved performance. Experimental\nresults show that multimodal representations extracted through this method\noutperformed those based solely on the original nominal compounds. The\nfine-tuning approach shows promising outcomes but is less effective than using\nembeddings without fine-tuning. The source code used in this paper is available\nat https://github.com/tongwu17/SemEval-2025-Task1-UoR-NCL.\n","authors":["Thanet Markchom","Tong Wu","Liting Huang","Huizhi Liang"],"pdf_url":"https://arxiv.org/pdf/2502.20984v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.20975v1","updated":"2025-02-28T11:40:34Z","published":"2025-02-28T11:40:34Z","title":"Set-Theoretic Compositionality of Sentence Embeddings","summary":"  Sentence encoders play a pivotal role in various NLP tasks; hence, an\naccurate evaluation of their compositional properties is paramount. However,\nexisting evaluation methods predominantly focus on goal task-specific\nperformance. This leaves a significant gap in understanding how well sentence\nembeddings demonstrate fundamental compositional properties in a\ntask-independent context. Leveraging classical set theory, we address this gap\nby proposing six criteria based on three core \"set-like\"\ncompositions/operations: \\textit{TextOverlap}, \\textit{TextDifference}, and\n\\textit{TextUnion}. We systematically evaluate $7$ classical and $9$ Large\nLanguage Model (LLM)-based sentence encoders to assess their alignment with\nthese criteria. Our findings show that SBERT consistently demonstrates set-like\ncompositional properties, surpassing even the latest LLMs. Additionally, we\nintroduce a new dataset of ~$192$K samples designed to facilitate future\nbenchmarking efforts on set-like compositionality of sentence embeddings.\n","authors":["Naman Bansal","Yash mahajan","Sanjeev Sinha","Santu Karmaker"],"pdf_url":"https://arxiv.org/pdf/2502.20975v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.08248v2","updated":"2025-02-28T11:40:20Z","published":"2025-01-14T16:38:33Z","title":"Eliciting In-context Retrieval and Reasoning for Long-context Large\n  Language Models","summary":"  Recent advancements in long-context language models (LCLMs) promise to\ntransform Retrieval-Augmented Generation (RAG) by simplifying pipelines. With\ntheir expanded context windows, LCLMs can process entire knowledge bases and\nperform retrieval and reasoning directly -- a capability we define as\nIn-Context Retrieval and Reasoning (ICR^2). However, existing benchmarks like\nLOFT often overestimate LCLM performance by providing overly simplified\ncontexts. To address this, we introduce ICR^2, a benchmark that evaluates LCLMs\nin more realistic scenarios by including confounding passages retrieved with\nstrong retrievers. We then propose three methods to enhance LCLM performance:\n(1) retrieve-then-generate fine-tuning, (2) retrieval-attention-probing, which\nuses attention heads to filter and de-noise long contexts during decoding, and\n(3) joint retrieval head training alongside the generation head. Our evaluation\nof five well-known LCLMs on LOFT and ICR^2 demonstrates significant gains with\nour best approach applied to Mistral-7B: +17 and +15 points by Exact Match on\nLOFT, and +13 and +2 points on ICR^2, compared to vanilla RAG and supervised\nfine-tuning, respectively. It even outperforms GPT-4-Turbo on most tasks\ndespite being a much smaller model.\n","authors":["Yifu Qiu","Varun Embar","Yizhe Zhang","Navdeep Jaitly","Shay B. Cohen","Benjamin Han"],"pdf_url":"https://arxiv.org/pdf/2501.08248v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.20973v1","updated":"2025-02-28T11:37:52Z","published":"2025-02-28T11:37:52Z","title":"Arabizi vs LLMs: Can the Genie Understand the Language of Aladdin?","summary":"  In this era of rapid technological advancements, communication continues to\nevolve as new linguistic phenomena emerge. Among these is Arabizi, a hybrid\nform of Arabic that incorporates Latin characters and numbers to represent the\nspoken dialects of Arab communities. Arabizi is widely used on social media and\nallows people to communicate in an informal and dynamic way, but it poses\nsignificant challenges for machine translation due to its lack of formal\nstructure and deeply embedded cultural nuances. This case study arises from a\ngrowing need to translate Arabizi for gisting purposes. It evaluates the\ncapacity of different LLMs to decode and translate Arabizi, focusing on\nmultiple Arabic dialects that have rarely been studied up until now. Using a\ncombination of human evaluators and automatic metrics, this research project\ninvestigates the model's performance in translating Arabizi into both Modern\nStandard Arabic and English. Key questions explored include which dialects are\ntranslated most effectively and whether translations into English surpass those\ninto Arabic.\n","authors":["Perla Al Almaoui","Pierrette Bouillon","Simon Hengchen"],"pdf_url":"https://arxiv.org/pdf/2502.20973v1.pdf","comment":"Submitted to MT Summit 2025"},{"id":"http://arxiv.org/abs/2406.10288v3","updated":"2025-02-28T11:36:06Z","published":"2024-06-12T18:33:11Z","title":"Do as I do (Safely): Mitigating Task-Specific Fine-tuning Risks in Large\n  Language Models","summary":"  Recent research shows that fine-tuning on benign instruction-following data\ncan inadvertently undo the safety alignment process and increase a model's\npropensity to comply with harmful queries. While instruction-following\nfine-tuning is important, task-specific fine-tuning - where models are trained\non datasets with clear ground truth answers (e.g., multiple choice questions) -\ncan enhance model performance on specialized downstream tasks. Understanding\nand mitigating safety risks in the task-specific setting remains distinct from\nthe instruction-following context due to structural differences in the data.\nOur work demonstrates how malicious actors can subtly manipulate the structure\nof almost any task-specific dataset to foster significantly more dangerous\nmodel behaviors, while maintaining an appearance of innocuity and reasonable\ndownstream task performance. To address this issue, we propose a novel\nmitigation strategy that mixes in safety data which mimics the task format and\nprompting style of the user data, showing this is significantly more effective\nand efficient than existing baselines at re-establishing safety alignment while\nmaintaining similar task performance.\n","authors":["Francisco Eiras","Aleksandar Petrov","Philip H. S. Torr","M. Pawan Kumar","Adel Bibi"],"pdf_url":"https://arxiv.org/pdf/2406.10288v3.pdf","comment":"Accepted to ICLR'25"},{"id":"http://arxiv.org/abs/2502.20968v1","updated":"2025-02-28T11:31:27Z","published":"2025-02-28T11:31:27Z","title":"Beware of Your Po! Measuring and Mitigating AI Safety Risks in Role-Play\n  Fine-Tuning of LLMs","summary":"  Role-playing enables large language models (LLMs) to engage users in\nimmersive and personalized interactions, but it also introduces significant\nsafety risks. Existing role-play fine-tuning techniques improve role\nadaptability but may degrade safety performance, particularly for villainous\ncharacters. In this work, we conduct the first comprehensive assessment of\nrole-play fine-tuning risks by training 95 role-specific LLMs using RoleBench.\nOur experiments reveal that role-play fine-tuning leads to a noticeable decline\nin safety performance, with safety risks varying based on character traits. To\ntackle this challenge, we propose Safety-Aware Role-Play Fine-Tuning (SaRFT), a\nnovel method designed to balance role-playing capabilities and safety.\nExtensive experiments on LLaMA-3-8B-Instruct, Gemma-2-9B-it, and\nQwen2.5-7B-Instruct demonstrate that SaRFT consistently outperforms\nstate-of-the-art baselines under both LoRA and full-parameter fine-tuning\nsettings. Our findings highlight the necessity of role-adaptive safety measures\nand provide insights into mitigating role-specific safety risks in role-playing\nLLMs.\n","authors":["Weixiang Zhao","Yulin Hu","Yang Deng","Jiahe Guo","Xingyu Sui","Xinyang Han","An Zhang","Yanyan Zhao","Bing Qin","Tat-Seng Chua","Ting Liu"],"pdf_url":"https://arxiv.org/pdf/2502.20968v1.pdf","comment":"25 pages, 10 figures, 13 tables"},{"id":"http://arxiv.org/abs/2502.20936v1","updated":"2025-02-28T10:46:52Z","published":"2025-02-28T10:46:52Z","title":"WebFAQ: A Multilingual Collection of Natural Q&A Datasets for Dense\n  Retrieval","summary":"  We present WebFAQ, a large-scale collection of open-domain question answering\ndatasets derived from FAQ-style schema.org annotations. In total, the data\ncollection consists of 96 million natural question-answer (QA) pairs across 75\nlanguages, including 47 million (49%) non-English samples. WebFAQ further\nserves as the foundation for 20 monolingual retrieval benchmarks with a total\nsize of 11.2 million QA pairs (5.9 million non-English). These datasets are\ncarefully curated through refined filtering and near-duplicate detection,\nyielding high-quality resources for training and evaluating multilingual dense\nretrieval models. To empirically confirm WebFAQ's efficacy, we use the\ncollected QAs to fine-tune an in-domain pretrained XLM-RoBERTa model. Through\nthis process of dataset-specific fine-tuning, the model achieves significant\nretrieval performance gains, which generalize - beyond WebFAQ - to other\nmultilingual retrieval benchmarks evaluated in zero-shot setting. Last but not\nleast, we utilize WebFAQ to construct a set of QA-aligned bilingual corpora\nspanning over 1000 language pairs using state-of-the-art bitext mining and\nautomated LLM-assessed translation evaluation. Due to our advanced, automated\nmethod of bitext dataset generation, the resulting bilingual corpora\ndemonstrate higher translation quality compared to similar datasets. WebFAQ and\nall associated resources are publicly available on GitHub and HuggingFace.\n","authors":["Michael Dinzinger","Laura Caspari","Kanishka Ghosh Dastidar","Jelena Mitrović","Michael Granitzer"],"pdf_url":"https://arxiv.org/pdf/2502.20936v1.pdf","comment":"10 pages, 3 figures, 7 tables"},{"id":"http://arxiv.org/abs/2502.20931v1","updated":"2025-02-28T10:39:07Z","published":"2025-02-28T10:39:07Z","title":"Automated Evaluation of Meter and Rhyme in Russian Generative and\n  Human-Authored Poetry","summary":"  Generative poetry systems require effective tools for data engineering and\nautomatic evaluation, particularly to assess how well a poem adheres to\nversification rules, such as the correct alternation of stressed and unstressed\nsyllables and the presence of rhymes.\n  In this work, we introduce the Russian Poetry Scansion Tool library designed\nfor stress mark placement in Russian-language syllabo-tonic poetry, rhyme\ndetection, and identification of defects of poeticness. Additionally, we\nrelease RIFMA -- a dataset of poem fragments spanning various genres and forms,\nannotated with stress marks. This dataset can be used to evaluate the\ncapability of modern large language models to accurately place stress marks in\npoetic texts.\n  The published resources provide valuable tools for researchers and\npractitioners in the field of creative generative AI, facilitating advancements\nin the development and evaluation of generative poetry systems.\n","authors":["Ilya Koziev"],"pdf_url":"https://arxiv.org/pdf/2502.20931v1.pdf","comment":"7 pages, 1 figure"},{"id":"http://arxiv.org/abs/2409.07170v2","updated":"2025-02-28T10:22:04Z","published":"2024-09-11T10:33:30Z","title":"Learning Efficient Recursive Numeral Systems via Reinforcement Learning","summary":"  It has previously been shown that by using reinforcement learning (RL),\nagents can derive simple approximate and exact-restricted numeral systems that\nare similar to human ones (Carlsson, 2021). However, it is a major challenge to\nshow how more complex recursive numeral systems, similar to for example\nEnglish, could arise via a simple learning mechanism such as RL. Here, we\nintroduce an approach towards deriving a mechanistic explanation of the\nemergence of efficient recursive number systems. We consider pairs of agents\nlearning how to communicate about numerical quantities through a meta-grammar\nthat can be gradually modified throughout the interactions. %We find that the\nseminal meta-grammar of Hurford (Hurford, 1975) is not suitable for this\napplication as its optimization results in systems that deviate from standard\nconventions observed within human numeral systems. We propose a simple\nmodification which addresses this issue. Utilising a slightly modified version\nof the meta-grammar of Hurford, we demonstrate that our RL agents, shaped by\nthe pressures for efficient communication, can effectively modify their lexicon\ntowards Pareto-optimal configurations which are comparable to those observed\nwithin human numeral systems in terms of their efficiency.\n","authors":["Andrea Silvi","Jonathan Thomas","Emil Carlsson","Devdatt Dubhashi","Moa Johansson"],"pdf_url":"https://arxiv.org/pdf/2409.07170v2.pdf","comment":"8 pages, 5 figures"},{"id":"http://arxiv.org/abs/2502.20914v1","updated":"2025-02-28T10:13:54Z","published":"2025-02-28T10:13:54Z","title":"Everything, Everywhere, All at Once: Is Mechanistic Interpretability\n  Identifiable?","summary":"  As AI systems are used in high-stakes applications, ensuring interpretability\nis crucial. Mechanistic Interpretability (MI) aims to reverse-engineer neural\nnetworks by extracting human-understandable algorithms to explain their\nbehavior. This work examines a key question: for a given behavior, and under\nMI's criteria, does a unique explanation exist? Drawing on identifiability in\nstatistics, where parameters are uniquely inferred under specific assumptions,\nwe explore the identifiability of MI explanations.\n  We identify two main MI strategies: (1) \"where-then-what,\" which isolates a\ncircuit replicating model behavior before interpreting it, and (2)\n\"what-then-where,\" which starts with candidate algorithms and searches for\nneural activation subspaces implementing them, using causal alignment.\n  We test both strategies on Boolean functions and small multi-layer\nperceptrons, fully enumerating candidate explanations. Our experiments reveal\nsystematic non-identifiability: multiple circuits can replicate behavior, a\ncircuit can have multiple interpretations, several algorithms can align with\nthe network, and one algorithm can align with different subspaces.\n  Is uniqueness necessary? A pragmatic approach may require only predictive and\nmanipulability standards. If uniqueness is essential for understanding,\nstricter criteria may be needed. We also reference the inner interpretability\nframework, which validates explanations through multiple criteria. This work\ncontributes to defining explanation standards in AI.\n","authors":["Maxime Méloux","Silviu Maniu","François Portet","Maxime Peyrard"],"pdf_url":"https://arxiv.org/pdf/2502.20914v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.20898v1","updated":"2025-02-28T09:54:13Z","published":"2025-02-28T09:54:13Z","title":"A database to support the evaluation of gender biases in GPT-4o output","summary":"  The widespread application of Large Language Models (LLMs) involves ethical\nrisks for users and societies. A prominent ethical risk of LLMs is the\ngeneration of unfair language output that reinforces or exacerbates harm for\nmembers of disadvantaged social groups through gender biases (Weidinger et al.,\n2022; Bender et al., 2021; Kotek et al., 2023). Hence, the evaluation of the\nfairness of LLM outputs with respect to such biases is a topic of rising\ninterest. To advance research in this field, promote discourse on suitable\nnormative bases and evaluation methodologies, and enhance the reproducibility\nof related studies, we propose a novel approach to database construction. This\napproach enables the assessment of gender-related biases in LLM-generated\nlanguage beyond merely evaluating their degree of neutralization.\n","authors":["Luise Mehner","Lena Alicija Philine Fiedler","Sabine Ammon","Dorothea Kolossa"],"pdf_url":"https://arxiv.org/pdf/2502.20898v1.pdf","comment":"ISCA/ITG Workshop on Diversity in Large Speech and Language Models"},{"id":"http://arxiv.org/abs/2502.20897v1","updated":"2025-02-28T09:53:42Z","published":"2025-02-28T09:53:42Z","title":"Beyond Demographics: Fine-tuning Large Language Models to Predict\n  Individuals' Subjective Text Perceptions","summary":"  People naturally vary in their annotations for subjective questions and some\nof this variation is thought to be due to the person's sociodemographic\ncharacteristics. LLMs have also been used to label data, but recent work has\nshown that models perform poorly when prompted with sociodemographic\nattributes, suggesting limited inherent sociodemographic knowledge. Here, we\nask whether LLMs can be trained to be accurate sociodemographic models of\nannotator variation. Using a curated dataset of five tasks with standardized\nsociodemographics, we show that models do improve in sociodemographic prompting\nwhen trained but that this performance gain is largely due to models learning\nannotator-specific behaviour rather than sociodemographic patterns. Across all\ntasks, our results suggest that models learn little meaningful connection\nbetween sociodemographics and annotation, raising doubts about the current use\nof LLMs for simulating sociodemographic variation and behaviour.\n","authors":["Matthias Orlikowski","Jiaxin Pei","Paul Röttger","Philipp Cimiano","David Jurgens","Dirk Hovy"],"pdf_url":"https://arxiv.org/pdf/2502.20897v1.pdf","comment":"Reviewed ARR December 2024"},{"id":"http://arxiv.org/abs/2502.19941v2","updated":"2025-02-28T09:51:26Z","published":"2025-02-27T10:11:53Z","title":"Alleviating Distribution Shift in Synthetic Data for Machine Translation\n  Quality Estimation","summary":"  Quality Estimation (QE) models evaluate the quality of machine translations\nwithout reference translations, serving as the reward models for the\ntranslation task. Due to the data scarcity, synthetic data generation has\nemerged as a promising solution. However, synthetic QE data often suffers from\ndistribution shift, which can manifest as discrepancies between pseudo and real\ntranslations, or in pseudo labels that do not align with human preferences. To\ntackle this issue, we introduce ADSQE, a novel framework for alleviating\ndistribution shift in synthetic QE data. To reduce the difference between\npseudo and real translations, we employ the constrained beam search algorithm\nand enhance translation diversity through the use of distinct generation\nmodels. ADSQE uses references, i.e., translation supervision signals, to guide\nboth the generation and annotation processes, enhancing the quality of\nword-level labels. ADSE further identifies the shortest phrase covering\nconsecutive error tokens, mimicking human annotation behavior, to assign the\nfinal phrase-level labels. Specially, we underscore that the translation model\ncan not annotate translations of itself accurately. Extensive experiments\ndemonstrate that ADSQE outperforms SOTA baselines like COMET in both supervised\nand unsupervised settings. Further analysis offers insights into synthetic data\ngeneration that could benefit reward models for other tasks.\n","authors":["Xiang Geng","Zhejian Lai","Jiajun Chen","Hao Yang","Shujian Huang"],"pdf_url":"https://arxiv.org/pdf/2502.19941v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.01570v3","updated":"2025-02-28T09:23:04Z","published":"2024-03-03T17:35:52Z","title":"Small Models are LLM Knowledge Triggers on Medical Tabular Prediction","summary":"  Recent development in large language models (LLMs) has demonstrated\nimpressive domain proficiency on unstructured textual or multi-modal tasks.\nHowever, despite with intrinsic world knowledge, their application on\nstructured tabular data prediction still lags behind, primarily due to the\nnumerical insensitivity and modality discrepancy that brings a gap between LLM\nreasoning and statistical tabular learning. Unlike textual or vision data\n(e.g., electronic clinical notes or medical imaging data), tabular data is\noften presented in heterogeneous numerical values (e.g., CBC reports). This\nubiquitous data format requires intensive expert annotation, and its numerical\nnature limits LLMs' capability to effectively transfer untapped domain\nexpertise. In this paper, we propose SERSAL, a general self-prompting method by\nsynergy learning with small models to enhance LLM tabular prediction in an\nunsupervised manner. Specifically, SERSAL utilizes the LLM's prior outcomes as\noriginal soft noisy annotations, which are dynamically leveraged to teach a\nbetter small student model. Reversely, the outcomes from the trained small\nmodel are used to teach the LLM to further refine its real capability. This\nprocess can be repeatedly applied to gradually distill refined knowledge for\ncontinuous progress. Comprehensive experiments on widely used medical domain\ntabular datasets show that, without access to gold labels, applying SERSAL to\nOpenAI GPT reasoning process attains substantial improvement compared to\nlinguistic prompting methods, which serves as an orthogonal direction for\ntabular LLM, and increasing prompting bonus is observed as more powerful LLMs\nappear.\n","authors":["Jiahuan Yan","Jintai Chen","Chaowen Hu","Bo Zheng","Yaojun Hu","Jimeng Sun","Jian Wu"],"pdf_url":"https://arxiv.org/pdf/2403.01570v3.pdf","comment":"Accepted to ICLR 2025. Codes will be available at\n  https://github.com/jyansir/sersal"},{"id":"http://arxiv.org/abs/2411.12537v3","updated":"2025-02-28T09:17:14Z","published":"2024-11-19T14:35:38Z","title":"Unlocking State-Tracking in Linear RNNs Through Negative Eigenvalues","summary":"  Linear Recurrent Neural Networks (LRNNs) such as Mamba, RWKV, GLA, mLSTM, and\nDeltaNet have emerged as efficient alternatives to Transformers for long\nsequences. However, both Transformers and LRNNs struggle to perform\nstate-tracking, which may impair performance in tasks such as code evaluation.\nIn one forward pass, current architectures are unable to solve even parity, the\nsimplest state-tracking task, which non-linear RNNs can handle effectively.\nRecently, Sarrof et al. (2024) demonstrated that the failure of LRNNs like\nMamba to solve parity stems from restricting the value range of their diagonal\nstate-transition matrices to $[0, 1]$ and that incorporating negative values\ncan resolve this issue. We extend this result to non-diagonal LRNNs such as\nDeltaNet. We prove that finite precision LRNNs with state-transition matrices\nhaving only positive eigenvalues cannot solve parity, while non-triangular\nmatrices are needed to count modulo $3$. Notably, we also prove that LRNNs can\nlearn any regular language when their state-transition matrices are products of\nidentity minus vector outer product matrices, each with eigenvalues in the\nrange $[-1, 1]$. Our experiments confirm that extending the eigenvalue range of\nMamba and DeltaNet to include negative values not only enables them to solve\nparity but consistently improves their performance on state-tracking tasks. We\nalso show that state-tracking enabled LRNNs can be pretrained stably and\nefficiently at scale (1.3B parameters), achieving competitive performance on\nlanguage modeling and showing promise on code and math tasks.\n","authors":["Riccardo Grazzi","Julien Siems","Jörg K. H. Franke","Arber Zela","Frank Hutter","Massimiliano Pontil"],"pdf_url":"https://arxiv.org/pdf/2411.12537v3.pdf","comment":"V2: Correction to Theorem 1 and 2 and to point 3 of Proposition 1.\n  V3: ICLR Camera Ready"},{"id":"http://arxiv.org/abs/2502.20868v1","updated":"2025-02-28T09:12:42Z","published":"2025-02-28T09:12:42Z","title":"ProBench: Benchmarking Large Language Models in Competitive Programming","summary":"  With reasoning language models such as OpenAI-o3 and DeepSeek-R1 emerging,\nlarge language models (LLMs) have entered a new phase of development. However,\nexisting benchmarks for coding evaluation are gradually inadequate to assess\nthe capability of advanced LLMs in code reasoning. To bridge the gap for\nhigh-level code reasoning assessment, we propose ProBench to benchmark LLMs in\ncompetitive programming, drawing inspiration from the International Collegiate\nProgramming Contest. ProBench collects a comprehensive set of competitive\nprogramming problems from Codeforces, Luogu, and Nowcoder platforms during the\nperiod from July to December 2024, obtaining real test results through online\nsubmissions to ensure the fairness and accuracy of the evaluation. We establish\na unified problem attribute system, including difficulty grading and algorithm\ntagging. With carefully collected and annotated data in ProBench, we\nsystematically assess 9 latest LLMs in competitive programming across multiple\ndimensions, including thought chain analysis, error type diagnosis, and\nreasoning depth evaluation. Experimental results show that QwQ-32B-Preview\nachieves the best score of 20.93 followed by DeepSeek-V3 with a score of 16.38,\nsuggesting that models trained with specialized reasoning tasks significantly\noutperform general-purpose models (even larger than reasoning-oriented models)\nin programming. Further analysis also reveals key areas for programming\ncapability enhancement, e.g., algorithm adaptability and reasoning sufficiency,\nproviding important insights for the future development of reasoning models.\n","authors":["Lei Yang","Renren Jin","Ling Shi","Jianxiang Peng","Yue Chen","Deyi Xiong"],"pdf_url":"https://arxiv.org/pdf/2502.20868v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.20866v1","updated":"2025-02-28T09:08:57Z","published":"2025-02-28T09:08:57Z","title":"Better Benchmarking LLMs for Zero-Shot Dependency Parsing","summary":"  While LLMs excel in zero-shot tasks, their performance in linguistic\nchallenges like syntactic parsing has been less scrutinized. This paper studies\nstate-of-the-art open-weight LLMs on the task by comparing them to baselines\nthat do not have access to the input sentence, including baselines that have\nnot been used in this context such as random projective trees or optimal linear\narrangements. The results show that most of the tested LLMs cannot outperform\nthe best uninformed baselines, with only the newest and largest versions of\nLLaMA doing so for most languages, and still achieving rather low performance.\nThus, accurate zero-shot syntactic parsing is not forthcoming with open LLMs.\n","authors":["Ana Ezquerro","Carlos Gómez-Rodríguez","David Vilares"],"pdf_url":"https://arxiv.org/pdf/2502.20866v1.pdf","comment":"Accepted at NoDaLiDa/Baltic-HLT 2025"},{"id":"http://arxiv.org/abs/2502.20864v1","updated":"2025-02-28T09:05:35Z","published":"2025-02-28T09:05:35Z","title":"Do Language Models Understand Honorific Systems in Javanese?","summary":"  The Javanese language features a complex system of honorifics that vary\naccording to the social status of the speaker, listener, and referent. Despite\nits cultural and linguistic significance, there has been limited progress in\ndeveloping a comprehensive corpus to capture these variations for natural\nlanguage processing (NLP) tasks. In this paper, we present Unggah-Ungguh, a\ncarefully curated dataset designed to encapsulate the nuances of Unggah-Ungguh\nBasa, the Javanese speech etiquette framework that dictates the choice of words\nand phrases based on social hierarchy and context. Using Unggah-Ungguh, we\nassess the ability of language models (LMs) to process various levels of\nJavanese honorifics through classification and machine translation tasks. To\nfurther evaluate cross-lingual LMs, we conduct machine translation experiments\nbetween Javanese (at specific honorific levels) and Indonesian. Additionally,\nwe explore whether LMs can generate contextually appropriate Javanese\nhonorifics in conversation tasks, where the honorific usage should align with\nthe social role and contextual cues. Our findings indicate that current LMs\nstruggle with most honorific levels, exhibitinga bias toward certain honorific\ntiers.\n","authors":["Mohammad Rifqi Farhansyah","Iwan Darmawan","Adryan Kusumawardhana","Genta Indra Winata","Alham Fikri Aji","Derry Tanti Wijaya"],"pdf_url":"https://arxiv.org/pdf/2502.20864v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.20859v1","updated":"2025-02-28T09:01:39Z","published":"2025-02-28T09:01:39Z","title":"The Power of Personality: A Human Simulation Perspective to Investigate\n  Large Language Model Agents","summary":"  Large language models (LLMs) excel in both closed tasks (including\nproblem-solving, and code generation) and open tasks (including creative\nwriting), yet existing explanations for their capabilities lack connections to\nreal-world human intelligence. To fill this gap, this paper systematically\ninvestigates LLM intelligence through the lens of ``human simulation'',\naddressing three core questions: (1) How do personality traits affect\nproblem-solving in closed tasks? (2) How do traits shape creativity in open\ntasks? (3) How does single-agent performance influence multi-agent\ncollaboration? By assigning Big Five personality traits to LLM agents and\nevaluating their performance in single- and multi-agent settings, we reveal\nthat specific traits significantly influence reasoning accuracy (closed tasks)\nand creative output (open tasks). Furthermore, multi-agent systems exhibit\ncollective intelligence distinct from individual capabilities, driven by\ndistinguishing combinations of personalities. We demonstrate that LLMs\ninherently simulate human behavior through next-token prediction, mirroring\nhuman language, decision-making, and collaborative dynamics.\n","authors":["Yifan Duan","Yihong Tang","Xuefeng Bai","Kehai Chen","Juntao Li","Min Zhang"],"pdf_url":"https://arxiv.org/pdf/2502.20859v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.20855v1","updated":"2025-02-28T08:53:42Z","published":"2025-02-28T08:53:42Z","title":"MAMUT: A Novel Framework for Modifying Mathematical Formulas for the\n  Generation of Specialized Datasets for Language Model Training","summary":"  Mathematical formulas are a fundamental and widely used component in various\nscientific fields, serving as a universal language for expressing complex\nconcepts and relationships. While state-of-the-art transformer models excel in\nprocessing and understanding natural language, they encounter challenges with\nmathematical notation, which involves a complex structure and diverse\nrepresentations. This study focuses on the development of specialized training\ndatasets to enhance the encoding of mathematical content. We introduce Math\nMutator (MAMUT), a framework capable of generating equivalent and falsified\nversions of a given mathematical formula in LaTeX notation, effectively\ncapturing the mathematical variety in notation of the same concept. Based on\nMAMUT, we have generated four large mathematical datasets containing diverse\nnotation, which can be used to train language models with enhanced mathematical\nembeddings.\n","authors":["Jonathan Drechsel","Anja Reusch","Steffen Herbold"],"pdf_url":"https://arxiv.org/pdf/2502.20855v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.20854v1","updated":"2025-02-28T08:53:08Z","published":"2025-02-28T08:53:08Z","title":"A Pilot Empirical Study on When and How to Use Knowledge Graphs as\n  Retrieval Augmented Generation","summary":"  The integration of Knowledge Graphs (KGs) into the Retrieval Augmented\nGeneration (RAG) framework has attracted significant interest, with early\nstudies showing promise in mitigating hallucinations and improving model\naccuracy. However, a systematic understanding and comparative analysis of the\nrapidly emerging KG-RAG methods are still lacking. This paper seeks to lay the\nfoundation for systematically answering the question of when and how to use\nKG-RAG by analyzing their performance in various application scenarios\nassociated with different technical configurations. After outlining the mind\nmap using KG-RAG framework and summarizing its popular pipeline, we conduct a\npilot empirical study of KG-RAG works to reimplement and evaluate 6 KG-RAG\nmethods across 7 datasets in diverse scenarios, analyzing the impact of 9\nKG-RAG configurations in combination with 17 LLMs. Our results underscore the\ncritical role of appropriate application conditions and optimal configurations\nof KG-RAG components.\n","authors":["Xujie Yuan","Yongxu Liu","Shimin Di","Shiwen Wu","Libin Zheng","Rui Meng","Xiaofang Zhou","Lei Chen","Jian Yin"],"pdf_url":"https://arxiv.org/pdf/2502.20854v1.pdf","comment":"8 pages, 2 figures, 14 tables"},{"id":"http://arxiv.org/abs/2407.01449v6","updated":"2025-02-28T08:51:57Z","published":"2024-06-27T15:45:29Z","title":"ColPali: Efficient Document Retrieval with Vision Language Models","summary":"  Documents are visually rich structures that convey information through text,\nbut also figures, page layouts, tables, or even fonts. Since modern retrieval\nsystems mainly rely on the textual information they extract from document pages\nto index documents -often through lengthy and brittle processes-, they struggle\nto exploit key visual cues efficiently. This limits their capabilities in many\npractical document retrieval applications such as Retrieval Augmented\nGeneration (RAG). To benchmark current systems on visually rich document\nretrieval, we introduce the Visual Document Retrieval Benchmark ViDoRe,\ncomposed of various page-level retrieval tasks spanning multiple domains,\nlanguages, and practical settings. The inherent complexity and performance\nshortcomings of modern systems motivate a new concept; doing document retrieval\nby directly embedding the images of the document pages. We release ColPali, a\nVision Language Model trained to produce high-quality multi-vector embeddings\nfrom images of document pages. Combined with a late interaction matching\nmechanism, ColPali largely outperforms modern document retrieval pipelines\nwhile being drastically simpler, faster and end-to-end trainable. We release\nmodels, data, code and benchmarks under open licenses at https://hf.co/vidore.\n","authors":["Manuel Faysse","Hugues Sibille","Tony Wu","Bilel Omrani","Gautier Viaud","Céline Hudelot","Pierre Colombo"],"pdf_url":"https://arxiv.org/pdf/2407.01449v6.pdf","comment":"Published as a conference paper at ICLR 2025"},{"id":"http://arxiv.org/abs/2502.17184v4","updated":"2025-02-28T08:44:08Z","published":"2025-02-24T14:20:22Z","title":"Measuring Data Diversity for Instruction Tuning: A Systematic Analysis\n  and A Reliable Metric","summary":"  Data diversity is crucial for the instruction tuning of large language\nmodels. Existing studies have explored various diversity-aware data selection\nmethods to construct high-quality datasets and enhance model performance.\nHowever, the fundamental problem of precisely defining and measuring data\ndiversity remains underexplored, limiting clear guidance for data engineering.\nTo address this, we systematically analyze 11 existing diversity measurement\nmethods by evaluating their correlation with model performance through\nextensive fine-tuning experiments. Our results indicate that a reliable\ndiversity measure should properly account for both inter-sample differences and\nthe information distribution in the sample space. Building on this, we propose\nNovelSum, a new diversity metric based on sample-level \"novelty.\" Experiments\non both simulated and real-world data show that NovelSum accurately captures\ndiversity variations and achieves a 0.97 correlation with instruction-tuned\nmodel performance, highlighting its value in guiding data engineering\npractices. With NovelSum as an optimization objective, we further develop a\ngreedy, diversity-oriented data selection strategy that outperforms existing\napproaches, validating both the effectiveness and practical significance of our\nmetric.\n","authors":["Yuming Yang","Yang Nan","Junjie Ye","Shihan Dou","Xiao Wang","Shuo Li","Huijie Lv","Mingqi Wu","Tao Gui","Qi Zhang","Xuanjing Huang"],"pdf_url":"https://arxiv.org/pdf/2502.17184v4.pdf","comment":"16 pages. The related codes and resources will be released later.\n  Project page: https://github.com/UmeanNever/NovelSum"},{"id":"http://arxiv.org/abs/2410.21357v3","updated":"2025-02-28T08:41:03Z","published":"2024-10-28T17:25:56Z","title":"Energy-Based Diffusion Language Models for Text Generation","summary":"  Despite remarkable progress in autoregressive language models, alternative\ngenerative paradigms beyond left-to-right generation are still being actively\nexplored. Discrete diffusion models, with the capacity for parallel generation,\nhave recently emerged as a promising alternative. Unfortunately, these models\nstill underperform the autoregressive counterparts, with the performance gap\nincreasing when reducing the number of sampling steps. Our analysis reveals\nthat this degradation is a consequence of an imperfect approximation used by\ndiffusion models. In this work, we propose Energy-based Diffusion Language\nModel (EDLM), an energy-based model operating at the full sequence level for\neach diffusion step, introduced to improve the underlying approximation used by\ndiffusion models. More specifically, we introduce an EBM in a residual form,\nand show that its parameters can be obtained by leveraging a pretrained\nautoregressive model or by finetuning a bidirectional transformer via noise\ncontrastive estimation. We also propose an efficient generation algorithm via\nparallel important sampling. Comprehensive experiments on language modeling\nbenchmarks show that our model can consistently outperform state-of-the-art\ndiffusion models by a significant margin, and approaches autoregressive models'\nperplexity. We further show that, without any generation performance drop, our\nframework offers a 1.3$\\times$ sampling speedup over existing diffusion models.\n","authors":["Minkai Xu","Tomas Geffner","Karsten Kreis","Weili Nie","Yilun Xu","Jure Leskovec","Stefano Ermon","Arash Vahdat"],"pdf_url":"https://arxiv.org/pdf/2410.21357v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.20246v2","updated":"2025-02-28T08:39:27Z","published":"2025-02-27T16:30:00Z","title":"Beyond Natural Language Perplexity: Detecting Dead Code Poisoning in\n  Code Generation Datasets","summary":"  The increasing adoption of large language models (LLMs) for code-related\ntasks has raised concerns about the security of their training datasets. One\ncritical threat is dead code poisoning, where syntactically valid but\nfunctionally redundant code is injected into training data to manipulate model\nbehavior. Such attacks can degrade the performance of neural code search\nsystems, leading to biased or insecure code suggestions. Existing detection\nmethods, such as token-level perplexity analysis, fail to effectively identify\ndead code due to the structural and contextual characteristics of programming\nlanguages. In this paper, we propose DePA (Dead Code Perplexity Analysis), a\nnovel line-level detection and cleansing method tailored to the structural\nproperties of code. DePA computes line-level perplexity by leveraging the\ncontextual relationships between code lines and identifies anomalous lines by\ncomparing their perplexity to the overall distribution within the file. Our\nexperiments on benchmark datasets demonstrate that DePA significantly\noutperforms existing methods, achieving 0.14-0.19 improvement in detection\nF1-score and a 44-65% increase in poisoned segment localization precision.\nFurthermore, DePA enhances detection speed by 0.62-23x, making it practical for\nlarge-scale dataset cleansing. Overall, by addressing the unique challenges of\ndead code poisoning, DePA provides a robust and efficient solution for\nsafeguarding the integrity of code generation model training datasets.\n","authors":["Chi-Chien Tsai","Chia-Mu Yu","Ying-Dar Lin","Yu-Sung Wu","Wei-Bin Lee"],"pdf_url":"https://arxiv.org/pdf/2502.20246v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.20834v1","updated":"2025-02-28T08:30:47Z","published":"2025-02-28T08:30:47Z","title":"Learning to Substitute Components for Compositional Generalization","summary":"  Despite the rising prevalence of neural language models, recent empirical\nevidence suggests their deficiency in compositional generalization. One of the\ncurrent de-facto solutions to this problem is compositional data augmentation,\nwhich aims to introduce additional compositional inductive bias. However,\nexisting handcrafted augmentation strategies offer limited improvement when\nsystematic generalization of neural language models requires multi-grained\ncompositional bias (i.e., not limited to either lexical or structural biases\nalone) or when training sentences have an imbalanced difficulty distribution.\nTo address these challenges, we first propose a novel compositional\naugmentation strategy called Component Substitution (CompSub), which enables\nmulti-grained composition of substantial substructures across the entire\ntraining set. Furthermore, we introduce the Learning Component Substitution\n(LCS) framework. This framework empowers the learning of component substitution\nprobabilities in CompSub in an end-to-end manner by maximizing the loss of\nneural language models, thereby prioritizing challenging compositions with\nelusive concepts and novel contexts. We extend the key ideas of CompSub and LCS\nto the recently emerging in-context learning scenarios of pre-trained large\nlanguage models (LLMs), proposing the LCS-ICL algorithm to enhance the few-shot\ncompositional generalization of state-of-the-art (SOTA) LLMs. Theoretically, we\nprovide insights into why applying our algorithms to language models can\nimprove compositional generalization performance. Empirically, our results on\nfour standard compositional generalization benchmarks(SCAN, COGS, GeoQuery, and\nCOGS-QL) demonstrate the superiority of CompSub, LCS, and LCS-ICL, with\nimprovements of up to 66.5%, 10.3%, 1.4%, and 8.8%, respectively.\n","authors":["Zhaoyi Li","Gangwei Jiang","Chenwang Wu","Ying Wei","Defu Lian","Enhong Chen"],"pdf_url":"https://arxiv.org/pdf/2502.20834v1.pdf","comment":"23 pages, 9 figures, preprint, the extension paper of the paper\n  (arXiv:2306.02840)"},{"id":"http://arxiv.org/abs/2406.07155v2","updated":"2025-02-28T08:26:52Z","published":"2024-06-11T11:02:04Z","title":"Scaling Large-Language-Model-based Multi-Agent Collaboration","summary":"  Recent breakthroughs in large language model-driven autonomous agents have\nrevealed that multi-agent collaboration often surpasses each individual through\ncollective reasoning. Inspired by the neural scaling law--increasing neurons\nenhances performance, this study explores whether the continuous addition of\ncollaborative agents can yield similar benefits. Technically, we utilize\ndirected acyclic graphs to organize agents into a multi-agent collaboration\nnetwork (MacNet), upon which their interactive reasoning is topologically\norchestrated for autonomous task solving. Extensive evaluations reveal that it\neffectively supports collaboration among over a thousand agents, with irregular\ntopologies outperforming regular ones. We also identify a collaborative scaling\nlaw--the overall performance follows a logistic growth pattern as agents scale,\nwith collaborative emergence occurring earlier than traditional neural\nemergence. We speculate this may be because scaling agents catalyzes their\nmultidimensional considerations during interactive reflection and refinement,\nthereby producing more comprehensive artifacts. The code is available at\nhttps://github.com/OpenBMB/ChatDev/tree/macnet.\n","authors":["Chen Qian","Zihao Xie","YiFei Wang","Wei Liu","Kunlun Zhu","Hanchen Xia","Yufan Dang","Zhuoyun Du","Weize Chen","Cheng Yang","Zhiyuan Liu","Maosong Sun"],"pdf_url":"https://arxiv.org/pdf/2406.07155v2.pdf","comment":"Accepted to ICLR-2025; https://github.com/OpenBMB/ChatDev/tree/macnet"},{"id":"http://arxiv.org/abs/2407.08351v2","updated":"2025-02-28T08:14:49Z","published":"2024-07-11T10:03:47Z","title":"AutoBencher: Towards Declarative Benchmark Construction","summary":"  We present AutoBencher, a declarative framework for automatic benchmark\nconstruction, and use it to scalably discover novel insights and\nvulnerabilities of existing language models. Concretely, given a few desiderata\nof benchmarks (e.g., question difficulty, topic salience), we operationalize\neach desideratum and cast benchmark creation as an optimization problem.\nSpecifically, we experiment with two settings with different optimization\nobjectives: (i) for capability evaluation, we declare the goal of finding a\nsalient, difficult dataset that induces novel performance patterns; (ii) for\nsafety evaluation, we declare the goal of finding a dataset of unsafe prompts\nthat existing LMs fail to decline. To tackle this optimization problem, we use\na language model to iteratively propose and refine dataset descriptions, which\nare then used to generate topic-specific questions and answers. These\ndescriptions are optimized to improve the declared desiderata. We use\nAutoBencher (powered by GPT-4) to create datasets for math, multilinguality,\nknowledge, and safety. The scalability of AutoBencher allows it to test\nfine-grained categories and tail knowledge, creating datasets that elicit 22%\nmore model errors (i.e., difficulty) than existing benchmarks. On the novelty\nends, AutoBencher also helps identify specific gaps not captured by existing\nbenchmarks: e.g., Gemini-Pro has knowledge gaps on Permian Extinction and\nFordism while GPT-4o fails to decline harmful requests about cryptocurrency\nscams.\n","authors":["Xiang Lisa Li","Farzaan Kaiyom","Evan Zheran Liu","Yifan Mai","Percy Liang","Tatsunori Hashimoto"],"pdf_url":"https://arxiv.org/pdf/2407.08351v2.pdf","comment":"Accepted for publication at ICLR 2025"},{"id":"http://arxiv.org/abs/2502.20122v2","updated":"2025-02-28T08:12:10Z","published":"2025-02-27T14:14:50Z","title":"Self-Training Elicits Concise Reasoning in Large Language Models","summary":"  Chain-of-thought (CoT) reasoning has enabled large language models (LLMs) to\nutilize additional computation through intermediate tokens to solve complex\ntasks. However, we posit that typical reasoning traces contain many redundant\ntokens, incurring extraneous inference costs. Upon examination of the output\ndistribution of current LLMs, we find evidence on their latent ability to\nreason more concisely, relative to their default behavior. To elicit this\ncapability, we propose simple fine-tuning methods which leverage self-generated\nconcise reasoning paths obtained by best-of-N sampling and few-shot\nconditioning, in task-specific settings. Our combined method achieves a 30%\nreduction in output tokens on average, across five model families on GSM8K and\nMATH, while maintaining average accuracy. By exploiting the fundamental\nstochasticity and in-context learning capabilities of LLMs, our self-training\napproach robustly elicits concise reasoning on a wide range of models,\nincluding those with extensive post-training. Code is available at\nhttps://github.com/TergelMunkhbat/concise-reasoning\n","authors":["Tergel Munkhbat","Namgyu Ho","Seo Hyun Kim","Yongjin Yang","Yujin Kim","Se-Young Yun"],"pdf_url":"https://arxiv.org/pdf/2502.20122v2.pdf","comment":"23 pages, 10 figures, 18 tables"},{"id":"http://arxiv.org/abs/2501.01638v2","updated":"2025-02-28T08:07:50Z","published":"2025-01-03T05:11:41Z","title":"A non-ergodic framework for understanding emergent capabilities in Large\n  Language Models","summary":"  Large language models have emergent capabilities that come unexpectedly at\nscale, but we need a theoretical framework to explain why and how they emerge.\nWe prove that language models are actually non-ergodic systems while providing\na mathematical framework based on Stuart Kauffman's theory of the adjacent\npossible (TAP) to explain capability emergence. Our resource-constrained TAP\nequation demonstrates how architectural, training, and contextual constraints\ninteract to shape model capabilities through phase transitions in semantic\nspace. We prove through experiments with three different language models that\ncapacities emerge through discrete transitions guided by constraint\ninteractions and path-dependent exploration. This framework provides a\ntheoretical basis for understanding emergence in language models and guides the\ndevelopment of architectures that can guide capability emergence.\n","authors":["Javier Marín"],"pdf_url":"https://arxiv.org/pdf/2501.01638v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.08467v2","updated":"2025-02-28T08:06:39Z","published":"2024-12-11T15:32:24Z","title":"Bootstrapping Language-Guided Navigation Learning with Self-Refining\n  Data Flywheel","summary":"  Creating high-quality data for training robust language-instructed agents is\na long-lasting challenge in embodied AI. In this paper, we introduce a\nSelf-Refining Data Flywheel (SRDF) that generates high-quality and large-scale\nnavigational instruction-trajectory pairs by iteratively refining the data pool\nthrough the collaboration between two models, the instruction generator and the\nnavigator, without any human-in-the-loop annotation. Specifically, SRDF starts\nwith using a base generator to create an initial data pool for training a base\nnavigator, followed by applying the trained navigator to filter the data pool.\nThis leads to higher-fidelity data to train a better generator, which can, in\nturn, produce higher-quality data for training the next-round navigator. Such a\nflywheel establishes a data self-refining process, yielding a continuously\nimproved and highly effective dataset for large-scale language-guided\nnavigation learning. Our experiments demonstrate that after several flywheel\nrounds, the navigator elevates the performance boundary from 70% to 78% SPL on\nthe classic R2R test set, surpassing human performance (76%) for the first\ntime. Meanwhile, this process results in a superior generator, evidenced by a\nSPICE increase from 23.5 to 26.2, better than all previous VLN instruction\ngeneration methods. Finally, we demonstrate the scalability of our method\nthrough increasing environment and instruction diversity, and the\ngeneralization ability of our pre-trained navigator across various downstream\nnavigation tasks, surpassing state-of-the-art methods by a large margin in all\ncases.\n","authors":["Zun Wang","Jialu Li","Yicong Hong","Songze Li","Kunchang Li","Shoubin Yu","Yi Wang","Yu Qiao","Yali Wang","Mohit Bansal","Limin Wang"],"pdf_url":"https://arxiv.org/pdf/2412.08467v2.pdf","comment":"28 pages, Code and data are available at\n  https://github.com/wz0919/VLN-SRDF"},{"id":"http://arxiv.org/abs/2410.09870v3","updated":"2025-02-28T08:02:31Z","published":"2024-10-13T15:08:49Z","title":"ChroKnowledge: Unveiling Chronological Knowledge of Language Models in\n  Multiple Domains","summary":"  Large language models (LLMs) have brought significant changes to many aspects\nof our lives. However, assessing and ensuring their chronological knowledge\nremains challenging. Existing approaches fall short in addressing the temporal\nadaptability of knowledge, often relying on a fixed time-point view. To\novercome this, we introduce ChroKnowBench, a benchmark dataset designed to\nevaluate chronologically accumulated knowledge across three key aspects:\nmultiple domains, time dependency, temporal state. Our benchmark distinguishes\nbetween knowledge that evolves (e.g., personal history, scientific discoveries,\namended laws) and knowledge that remain constant (e.g., mathematical truths,\ncommonsense facts). Building on this benchmark, we present ChroKnowledge\n(Chronological Categorization of Knowledge), a novel sampling-based framework\nfor evaluating LLMs' non-parametric chronological knowledge. Our evaluation led\nto the following observations: (1) The ability of eliciting temporal knowledge\nvaries depending on the data format that model was trained on. (2) LLMs\npartially recall knowledge or show a cut-off at temporal boundaries rather than\nrecalling all aspects of knowledge correctly. Thus, we apply our\nChroKnowPrompt, an in-depth prompting to elicit chronological knowledge by\ntraversing step-by-step through the surrounding time spans. We observe that it\nsuccessfully recalls objects across both open-source and proprietary LLMs,\ndemonstrating versatility, though it faces challenges with dynamic datasets and\nunstructured formats.\n","authors":["Yein Park","Chanwoong Yoon","Jungwoo Park","Donghyeon Lee","Minbyul Jeong","Jaewoo Kang"],"pdf_url":"https://arxiv.org/pdf/2410.09870v3.pdf","comment":"ICLR 2025, 40 pages, 17 figures"},{"id":"http://arxiv.org/abs/2410.09542v2","updated":"2025-02-28T08:01:32Z","published":"2024-10-12T14:12:36Z","title":"MIRAGE: Evaluating and Explaining Inductive Reasoning Process in\n  Language Models","summary":"  Inductive reasoning is an essential capability for large language models\n(LLMs) to achieve higher intelligence, which requires the model to generalize\nrules from observed facts and then apply them to unseen examples. We present\nMIRAGE, a synthetic dataset that addresses the limitations of previous work,\nspecifically the lack of comprehensive evaluation and flexible test data. In\nit, we evaluate LLMs' capabilities in both the inductive and deductive stages,\nallowing for flexible variation in input distribution, task scenario, and task\ndifficulty to analyze the factors influencing LLMs' inductive reasoning. Based\non these multi-faceted evaluations, we demonstrate that the LLM is a poor\nrule-based reasoner. In many cases, when conducting inductive reasoning, they\ndo not rely on a correct rule to answer the unseen case. From the perspectives\nof different prompting methods, observation numbers, and task forms, models\ntend to consistently conduct correct deduction without correct inductive rules.\nBesides, we find that LLMs are good neighbor-based reasoners. In the inductive\nreasoning process, the model tends to focus on observed facts that are close to\nthe current test example in feature space. By leveraging these similar\nexamples, the model maintains strong inductive capabilities within a localized\nregion, significantly improving its deductive performance.\n","authors":["Jiachun Li","Pengfei Cao","Zhuoran Jin","Yubo Chen","Kang Liu","Jun Zhao"],"pdf_url":"https://arxiv.org/pdf/2410.09542v2.pdf","comment":"Accepted as ICLR 2025 conference paper (26 pages, 16 tables, 9\n  figures)"},{"id":"http://arxiv.org/abs/2412.06287v3","updated":"2025-02-28T07:54:16Z","published":"2024-12-09T08:19:28Z","title":"PediaBench: A Comprehensive Chinese Pediatric Dataset for Benchmarking\n  Large Language Models","summary":"  The emergence of Large Language Models (LLMs) in the medical domain has\nstressed a compelling need for standard datasets to evaluate their\nquestion-answering (QA) performance. Although there have been several benchmark\ndatasets for medical QA, they either cover common knowledge across different\ndepartments or are specific to another department rather than pediatrics.\nMoreover, some of them are limited to objective questions and do not measure\nthe generation capacity of LLMs. Therefore, they cannot comprehensively assess\nthe QA ability of LLMs in pediatrics. To fill this gap, we construct\nPediaBench, the first Chinese pediatric dataset for LLM evaluation.\nSpecifically, it contains 4,117 objective questions and 1,632 subjective\nquestions spanning 12 pediatric disease groups. It adopts an integrated scoring\ncriterion based on different difficulty levels to thoroughly assess the\nproficiency of an LLM in instruction following, knowledge understanding,\nclinical case analysis, etc. Finally, we validate the effectiveness of\nPediaBench with extensive experiments on 20 open-source and commercial LLMs.\nThrough an in-depth analysis of experimental results, we offer insights into\nthe ability of LLMs to answer pediatric questions in the Chinese context,\nhighlighting their limitations for further improvements. Our code and data are\npublished at https://github.com/ACMISLab/PediaBench.\n","authors":["Qian Zhang","Panfeng Chen","Jiali Li","Linkun Feng","Shuyu Liu","Heng Zhao","Mei Chen","Hui Li","Yanhao Wang"],"pdf_url":"https://arxiv.org/pdf/2412.06287v3.pdf","comment":"21 pages, 12 figures"},{"id":"http://arxiv.org/abs/2502.20811v1","updated":"2025-02-28T07:53:40Z","published":"2025-02-28T07:53:40Z","title":"HAIC: Improving Human Action Understanding and Generation with Better\n  Captions for Multi-modal Large Language Models","summary":"  Recent Multi-modal Large Language Models (MLLMs) have made great progress in\nvideo understanding. However, their performance on videos involving human\nactions is still limited by the lack of high-quality data. To address this, we\nintroduce a two-stage data annotation pipeline. First, we design strategies to\naccumulate videos featuring clear human actions from the Internet. Second,\nvideos are annotated in a standardized caption format that uses human\nattributes to distinguish individuals and chronologically details their actions\nand interactions. Through this pipeline, we curate two datasets, namely\nHAICTrain and HAICBench. \\textbf{HAICTrain} comprises 126K video-caption pairs\ngenerated by Gemini-Pro and verified for training purposes. Meanwhile,\n\\textbf{HAICBench} includes 500 manually annotated video-caption pairs and\n1,400 QA pairs, for a comprehensive evaluation of human action understanding.\nExperimental results demonstrate that training with HAICTrain not only\nsignificantly enhances human understanding abilities across 4 benchmarks, but\ncan also improve text-to-video generation results. Both the HAICTrain and\nHAICBench are released at https://huggingface.co/datasets/KuaishouHAIC/HAIC.\n","authors":["Xiao Wang","Jingyun Hua","Weihong Lin","Yuanxing Zhang","Fuzheng Zhang","Jianlong Wu","Di Zhang","Liqiang Nie"],"pdf_url":"https://arxiv.org/pdf/2502.20811v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.15089v2","updated":"2025-02-28T07:53:20Z","published":"2025-01-25T05:32:14Z","title":"LongReason: A Synthetic Long-Context Reasoning Benchmark via Context\n  Expansion","summary":"  Large language models (LLMs) have demonstrated remarkable progress in\nunderstanding long-context inputs. However, benchmarks for evaluating the\nlong-context reasoning abilities of LLMs fall behind the pace. Existing\nbenchmarks often focus on a narrow range of tasks or those that do not demand\ncomplex reasoning. To address this gap and enable a more comprehensive\nevaluation of the long-context reasoning capabilities of current LLMs, we\npropose a new synthetic benchmark, LongReason, which is constructed by\nsynthesizing long-context reasoning questions from a varied set of\nshort-context reasoning questions through context expansion. LongReason\nconsists of 794 multiple-choice reasoning questions with diverse reasoning\npatterns across three task categories: reading comprehension, logical\ninference, and mathematical word problems. We evaluate 21 LLMs on LongReason,\nrevealing that most models experience significant performance drops as context\nlength increases. Our further analysis shows that even state-of-the-art LLMs\nstill have significant room for improvement in providing robust reasoning\nacross different tasks. We have open-sourced LongReason under\nhttps://huggingface.co/datasets/lz1bytedance/LongReason to support the\ncomprehensive evaluation of LLMs' long-context reasoning capabilities.\n","authors":["Zhan Ling","Kang Liu","Kai Yan","Yifan Yang","Weijian Lin","Ting-Han Fan","Lingfeng Shen","Zhengyin Du","Jiecao Chen"],"pdf_url":"https://arxiv.org/pdf/2501.15089v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.15359v2","updated":"2025-02-28T07:49:06Z","published":"2025-02-21T10:14:55Z","title":"ARS: Automatic Routing Solver with Large Language Models","summary":"  Real-world Vehicle Routing Problems (VRPs) are characterized by a variety of\npractical constraints, making manual solver design both knowledge-intensive and\ntime-consuming. Although there is increasing interest in automating the design\nof routing algorithms, existing research has explored only a limited array of\nVRP variants and fails to adequately address the complex and prevalent\nconstraints encountered in real-world situations. To fill this gap, this paper\nintroduces RoutBench, a benchmark of 1,000 VRP variants derived from 24\nattributes, for evaluating the effectiveness of automatic routing solvers in\naddressing complex constraints. Along with RoutBench, we present the Automatic\nRouting Solver (ARS), which employs Large Language Model (LLM) agents to\nenhance a backbone algorithm framework by automatically generating\nconstraint-aware heuristic code, based on problem descriptions and several\nrepresentative constraints selected from a database. Our experiments show that\nARS outperforms state-of-the-art LLM-based methods and commonly used solvers,\nautomatically solving 91.67% of common VRPs and achieving at least a 30%\nimprovement across all benchmarks.\n","authors":["Kai Li","Fei Liu","Zhenkun Wang","Xialiang Tong","Xiongwei Han","Mingxuan Yuan"],"pdf_url":"https://arxiv.org/pdf/2502.15359v2.pdf","comment":"Authorship is under discussion; arXiv release will follow\n  finalization"},{"id":"http://arxiv.org/abs/2411.08870v2","updated":"2025-02-28T07:34:44Z","published":"2024-11-13T18:50:13Z","title":"The Limited Impact of Medical Adaptation of Large Language and\n  Vision-Language Models","summary":"  Several recent works seek to adapt general-purpose large language models\n(LLMs) and vision-language models (VLMs) for medical applications through\ncontinued pretraining on publicly available biomedical corpora. These works\ntypically claim that such domain-adaptive pretraining improves performance on\nvarious downstream medical tasks, such as answering medical exam questions. In\nthis paper, we compare ten \"medical\" LLMs and two VLMs against their\ncorresponding base models, arriving at a different conclusion: all medical VLMs\nand nearly all medical LLMs fail to consistently improve over their base models\nin the zero-/few-shot prompting and supervised fine-tuning regimes for medical\nquestion answering (QA). For instance, on clinical-note-based QA tasks in the\n3-shot setting, medical LLMs outperform their base models in only 26.7% of\ncases, reach a (statistical) tie in 16.7% of cases, and perform significantly\nworse in the remaining 56.7% of cases. Our conclusions are based on (i)\ncomparing each medical model directly against its base model; (ii) optimizing\nthe prompts for each model separately in zero-/few-shot prompting; and (iii)\naccounting for statistical uncertainty in comparisons. Our findings suggest\nthat state-of-the-art general-domain models may already exhibit strong medical\nknowledge and reasoning capabilities, and offer recommendations to strengthen\nthe conclusions of future studies.\n","authors":["Daniel P. Jeong","Pranav Mani","Saurabh Garg","Zachary C. Lipton","Michael Oberst"],"pdf_url":"https://arxiv.org/pdf/2411.08870v2.pdf","comment":"Extended version of EMNLP 2024 paper arXiv:2411.04118. Includes\n  additional results on clinical note QA tasks and supervised fine-tuning\n  evaluations"},{"id":"http://arxiv.org/abs/2502.17651v2","updated":"2025-02-28T07:28:24Z","published":"2025-02-24T21:01:39Z","title":"METAL: A Multi-Agent Framework for Chart Generation with Test-Time\n  Scaling","summary":"  Chart generation aims to generate code to produce charts satisfying the\ndesired visual properties, e.g., texts, layout, color, and type. It has great\npotential to empower the automatic professional report generation in financial\nanalysis, research presentation, education, and healthcare. In this work, we\nbuild a vision-language model (VLM) based multi-agent framework for effective\nautomatic chart generation. Generating high-quality charts requires both strong\nvisual design skills and precise coding capabilities that embed the desired\nvisual properties into code. Such a complex multi-modal reasoning process is\ndifficult for direct prompting of VLMs. To resolve these challenges, we propose\nMETAL, a multi-agent framework that decomposes the task of chart generation\ninto the iterative collaboration among specialized agents. METAL achieves 5.2%\nimprovement over the current best result in the chart generation task. The\nMETAL framework exhibits the phenomenon of test-time scaling: its performance\nincreases monotonically as the logarithmic computational budget grows from 512\nto 8192 tokens. In addition, we find that separating different modalities\nduring the critique process of METAL boosts the self-correction capability of\nVLMs in the multimodal context.\n","authors":["Bingxuan Li","Yiwei Wang","Jiuxiang Gu","Kai-Wei Chang","Nanyun Peng"],"pdf_url":"https://arxiv.org/pdf/2502.17651v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.20795v1","updated":"2025-02-28T07:24:33Z","published":"2025-02-28T07:24:33Z","title":"Plan2Align: Predictive Planning Based Test-Time Preference Alignment in\n  Paragraph-Level Machine Translation","summary":"  Machine Translation (MT) has been predominantly designed for sentence-level\ntranslation using transformer-based architectures. While next-token prediction\nbased Large Language Models (LLMs) demonstrate strong capabilities in long-text\ntranslation, non-extensive language models often suffer from omissions and\nsemantic inconsistencies when processing paragraphs. Existing preference\nalignment methods improve sentence-level translation but fail to ensure\ncoherence over extended contexts due to the myopic nature of next-token\ngeneration. We introduce Plan2Align, a test-time alignment framework that\ntreats translation as a predictive planning problem, adapting Model Predictive\nControl to iteratively refine translation outputs. Experiments on WMT24\nDiscourse-Level Literary Translation show that Plan2Align significantly\nimproves paragraph-level translation, achieving performance surpassing or on\npar with the existing training-time and test-time alignment methods on\nLLaMA-3.1 8B.\n","authors":["Kuang-Da Wang","Teng-Ruei Chen","Yu Heng Hung","Shuoyang Ding","Yueh-Hua Wu","Yu-Chiang Frank Wang","Chao-Han Huck Yang","Wen-Chih Peng","Ping-Chun Hsieh"],"pdf_url":"https://arxiv.org/pdf/2502.20795v1.pdf","comment":"Preprint. Code will be released at Plan2Align GitHub link:\n  https://github.com/NYCU-RL-Bandits-Lab/Plan2Align"},{"id":"http://arxiv.org/abs/2502.20790v1","updated":"2025-02-28T07:15:12Z","published":"2025-02-28T07:15:12Z","title":"Chain-of-Thought Matters: Improving Long-Context Language Models with\n  Reasoning Path Supervision","summary":"  Recent advances in Large Language Models (LLMs) have highlighted the\nchallenge of handling long-context tasks, where models need to reason over\nextensive input contexts to aggregate target information. While\nChain-of-Thought (CoT) prompting has shown promise for multi-step reasoning,\nits effectiveness for long-context scenarios remains underexplored. Through\nsystematic investigation across diverse tasks, we demonstrate that CoT's\nbenefits generalize across most long-context scenarios and amplify with\nincreasing context length. Motivated by this critical observation, we propose\nLongRePS, a process-supervised framework that teaches models to generate\nhigh-quality reasoning paths for enhanced long-context performance. Our\nframework incorporates a self-sampling mechanism to bootstrap reasoning paths\nand a novel quality assessment protocol specifically designed for long-context\nscenarios. Experimental results on various long-context benchmarks demonstrate\nthe effectiveness of our approach, achieving significant improvements over\noutcome supervision baselines on both in-domain tasks (+13.6/+3.8 points for\nLLaMA/Qwen on MuSiQue) and cross-domain generalization (+9.3/+8.1 points on\naverage across diverse QA tasks). Our code, data and trained models are made\npublic to facilitate future research.\n","authors":["Dawei Zhu","Xiyu Wei","Guangxiang Zhao","Wenhao Wu","Haosheng Zou","Junfeng Ran","Xun Wang","Lin Sun","Xiangzheng Zhang","Sujian Li"],"pdf_url":"https://arxiv.org/pdf/2502.20790v1.pdf","comment":"14 pages,6 figures"},{"id":"http://arxiv.org/abs/2406.03807v3","updated":"2025-02-28T07:12:21Z","published":"2024-06-06T07:30:14Z","title":"Tool-Planner: Task Planning with Clusters across Multiple Tools","summary":"  Large language models (LLMs) have demonstrated exceptional reasoning\ncapabilities, enabling them to solve various complex problems. Recently, this\nability has been applied to the paradigm of tool learning. Tool learning\ninvolves providing examples of tool usage and their corresponding functions,\nallowing LLMs to formulate plans and demonstrate the process of invoking and\nexecuting each tool. LLMs can address tasks that they cannot complete\nindependently, thereby enhancing their potential across different tasks.\nHowever, this approach faces two key challenges. First, redundant error\ncorrection leads to unstable planning and long execution time. Additionally,\ndesigning a correct plan among multiple tools is also a challenge in tool\nlearning. To address these issues, we propose Tool-Planner, a task-processing\nframework based on toolkits. Tool-Planner groups tools based on the API\nfunctions with the same function into a toolkit and allows LLMs to implement\nplanning across the various toolkits. When a tool error occurs, the language\nmodel can reselect and adjust tools based on the toolkit. Experiments show that\nour approach demonstrates a high pass and win rate across different datasets\nand optimizes the planning scheme for tool learning in models such as GPT-4 and\nClaude 3, showcasing the potential of our method. Our code is public at\nhttps://github.com/OceannTwT/Tool-Planner\n","authors":["Yanming Liu","Xinyue Peng","Jiannan Cao","Shi Bo","Yuwei Zhang","Xuhong Zhang","Sheng Cheng","Xun Wang","Jianwei Yin","Tianyu Du"],"pdf_url":"https://arxiv.org/pdf/2406.03807v3.pdf","comment":"ICLR 2025 Camera Ready version"},{"id":"http://arxiv.org/abs/2410.01671v2","updated":"2025-02-28T07:09:00Z","published":"2024-10-02T15:39:55Z","title":"Bridging Context Gaps: Leveraging Coreference Resolution for Long\n  Contextual Understanding","summary":"  Large language models (LLMs) have shown remarkable capabilities in natural\nlanguage processing; however, they still face difficulties when tasked with\nunderstanding lengthy contexts and executing effective question answering.\nThese challenges often arise due to the complexity and ambiguity present in\nlonger texts. To enhance the performance of LLMs in such scenarios, we\nintroduce the Long Question Coreference Adaptation (LQCA) method. This\ninnovative framework focuses on coreference resolution tailored to long\ncontexts, allowing the model to identify and manage references effectively. The\nLQCA method encompasses four key steps: resolving coreferences within\nsub-documents, computing the distances between mentions, defining a\nrepresentative mention for coreference, and answering questions through mention\nreplacement. By processing information systematically, the framework provides\neasier-to-handle partitions for LLMs, promoting better understanding.\nExperimental evaluations on a range of LLMs and datasets have yielded positive\nresults, with a notable improvements on OpenAI-o1-mini and GPT-4o models,\nhighlighting the effectiveness of leveraging coreference resolution to bridge\ncontext gaps in question answering. Our code is public at\nhttps://github.com/OceannTwT/LQCA.\n","authors":["Yanming Liu","Xinyue Peng","Jiannan Cao","Shi Bo","Yanxin Shen","Tianyu Du","Sheng Cheng","Xun Wang","Jianwei Yin","Xuhong Zhang"],"pdf_url":"https://arxiv.org/pdf/2410.01671v2.pdf","comment":"ICLR 2025 camera ready version, with updated metadata"},{"id":"http://arxiv.org/abs/2502.20785v1","updated":"2025-02-28T07:06:19Z","published":"2025-02-28T07:06:19Z","title":"GraphCheck: Multi-Path Fact-Checking with Entity-Relationship Graphs","summary":"  Automated fact-checking aims to assess the truthfulness of text based on\nrelevant evidence, yet verifying complex claims requiring multi-hop reasoning\nremains a significant challenge. We propose GraphCheck, a novel framework that\nconverts claims into entity-relationship graphs for comprehensive verification.\nBy identifying relation between explicit entities and latent entities across\nmultiple paths, GraphCheck enhances the adaptability and robustness of\nverification. Furthermore, we introduce DP-GraphCheck, a two-stage variant that\nimproves performance by incorporating direct prompting as an initial filtering\nstep. Experiments on the HOVER and EX-FEVER datasets show that our approach\noutperforms existing methods, particularly in multi-hop reasoning tasks.\nFurthermore, our two-stage framework generalizes well to other fact-checking\npipelines, demonstrating its versatility.\n","authors":["Hyewon Jeon","Jay-Yoon Lee"],"pdf_url":"https://arxiv.org/pdf/2502.20785v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.18514v3","updated":"2025-02-28T07:02:59Z","published":"2024-10-24T08:01:22Z","title":"Scaling up Masked Diffusion Models on Text","summary":"  Masked diffusion models (MDMs) have shown promise in language modeling, yet\ntheir scalability and effectiveness in core language tasks, such as text\ngeneration and language understanding, remain underexplored. This paper\nestablishes the first scaling law for MDMs, demonstrating a scaling rate\ncomparable to autoregressive models (ARMs) and a relatively small compute gap.\nMotivated by their scalability, we train a family of MDMs with up to 1.1\nbillion (B) parameters to systematically evaluate their performance against\nARMs of comparable or larger sizes. Fully leveraging the probabilistic\nformulation of MDMs, we propose a simple yet effective unsupervised\nclassifier-free guidance that effectively exploits large-scale unpaired data,\nboosting performance for conditional inference. In language understanding, the\n1.1B MDM outperforms the 1.1B TinyLlama model trained on the same data across\nfour of eight zero-shot benchmarks. Notably, it achieves competitive math\nreasoning ability with the 7B Llama-2 model on the GSM8K dataset. In text\ngeneration, MDMs with 16 times more pre-training time offer a flexible\ntrade-off against ARMs with the accelerated sampling technique KV-Cache: MDMs\nmatch ARMs in performance while being 1.4 times faster during sampling.\nMoreover, MDMs address challenging tasks for ARMs by effectively handling\nbidirectional reasoning and adapting to temporal shifts in data. Notably, a\n1.1B MDM breaks the reverse curse encountered by much larger ARMs with\nsignificantly more data and computation, such as 13B Llama-2 and 175B GPT-3.\nOur code is available at https://github.com/ML-GSAI/SMDM.\n","authors":["Shen Nie","Fengqi Zhu","Chao Du","Tianyu Pang","Qian Liu","Guangtao Zeng","Min Lin","Chongxuan Li"],"pdf_url":"https://arxiv.org/pdf/2410.18514v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.20780v1","updated":"2025-02-28T06:59:49Z","published":"2025-02-28T06:59:49Z","title":"MedHallTune: An Instruction-Tuning Benchmark for Mitigating Medical\n  Hallucination in Vision-Language Models","summary":"  The increasing use of vision-language models (VLMs) in healthcare\napplications presents great challenges related to hallucinations, in which the\nmodels may generate seemingly plausible results that are in fact incorrect.\nSuch hallucinations can jeopardize clinical decision making, potentially\nharming the diagnosis and treatments. In this work, we propose MedHallTune, a\nlarge-scale benchmark designed specifically to evaluate and mitigate\nhallucinations in medical VLMs. Comprising over 100,000 images and 1,000,000\ninstruction pairs, MedHallTune includes both hallucination and\nnon-hallucination samples, each with ground-truth annotations. We conduct a\ncomprehensive evaluation of current medical and general VLMs using MedHallTune,\nassessing their performance across key metrics, including clinical accuracy,\nrelevance, detail level, and risk level. The experimental results show that\nfine-tuning with MedHallTune successfully improves the ability of several\nexisting models to manage hallucinations and boost their zero-shot performance\non downstream visual-question-answering (VQA) tasks, making them more reliable\nfor practical medical applications. Our work contributes to the development of\nmore trustworthy VLMs. Codes and dataset will be available at\n\\href{https://github.com/russellyq/MedHallTune}{MedHallTune}.\n","authors":["Qiao Yan","Yuchen Yuan","Xiaowei Hu","Yihan Wang","Jiaqi Xu","Jinpeng Li","Chi-Wing Fu","Pheng-Ann Heng"],"pdf_url":"https://arxiv.org/pdf/2502.20780v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.20779v1","updated":"2025-02-28T06:59:04Z","published":"2025-02-28T06:59:04Z","title":"Triple Phase Transitions: Understanding the Learning Dynamics of Large\n  Language Models from a Neuroscience Perspective","summary":"  Large language models (LLMs) often exhibit abrupt emergent behavior, whereby\nnew abilities arise at certain points during their training. This phenomenon,\ncommonly referred to as a ''phase transition'', remains poorly understood. In\nthis study, we conduct an integrative analysis of such phase transitions by\nexamining three interconnected perspectives: the similarity between LLMs and\nthe human brain, the internal states of LLMs, and downstream task performance.\nWe propose a novel interpretation for the learning dynamics of LLMs that vary\nin both training data and architecture, revealing that three phase transitions\ncommonly emerge across these models during training: (1) alignment with the\nentire brain surges as LLMs begin adhering to task instructions Brain Alignment\nand Instruction Following, (2) unexpectedly, LLMs diverge from the brain during\na period in which downstream task accuracy temporarily stagnates Brain\nDetachment and Stagnation, and (3) alignment with the brain reoccurs as LLMs\nbecome capable of solving the downstream tasks Brain Realignment and\nConsolidation. These findings illuminate the underlying mechanisms of phase\ntransitions in LLMs, while opening new avenues for interdisciplinary research\nbridging AI and neuroscience.\n","authors":["Yuko Nakagi","Keigo Tada","Sota Yoshino","Shinji Nishimoto","Yu Takagi"],"pdf_url":"https://arxiv.org/pdf/2502.20779v1.pdf","comment":"46 pages"},{"id":"http://arxiv.org/abs/2502.20766v1","updated":"2025-02-28T06:34:53Z","published":"2025-02-28T06:34:53Z","title":"FlexPrefill: A Context-Aware Sparse Attention Mechanism for Efficient\n  Long-Sequence Inference","summary":"  Large language models (LLMs) encounter computational challenges during\nlong-sequence inference, especially in the attention pre-filling phase, where\nthe complexity grows quadratically with the prompt length. Previous efforts to\nmitigate these challenges have relied on fixed sparse attention patterns or\nidentifying sparse attention patterns based on limited cases. However, these\nmethods lacked the flexibility to efficiently adapt to varying input demands.\nIn this paper, we introduce FlexPrefill, a Flexible sparse Pre-filling\nmechanism that dynamically adjusts sparse attention patterns and computational\nbudget in real-time to meet the specific requirements of each input and\nattention head. The flexibility of our method is demonstrated through two key\ninnovations: 1) Query-Aware Sparse Pattern Determination: By measuring\nJensen-Shannon divergence, this component adaptively switches between\nquery-specific diverse attention patterns and predefined attention patterns. 2)\nCumulative-Attention Based Index Selection: This component dynamically selects\nquery-key indexes to be computed based on different attention patterns,\nensuring the sum of attention scores meets a predefined threshold. FlexPrefill\nadaptively optimizes the sparse pattern and sparse ratio of each attention head\nbased on the prompt, enhancing efficiency in long-sequence inference tasks.\nExperimental results show significant improvements in both speed and accuracy\nover prior methods, providing a more flexible and efficient solution for LLM\ninference.\n","authors":["Xunhao Lai","Jianqiao Lu","Yao Luo","Yiyuan Ma","Xun Zhou"],"pdf_url":"https://arxiv.org/pdf/2502.20766v1.pdf","comment":"Accepted at ICLR 2025 (Oral)"},{"id":"http://arxiv.org/abs/2502.12561v2","updated":"2025-02-28T06:28:55Z","published":"2025-02-18T05:55:18Z","title":"UXAgent: An LLM Agent-Based Usability Testing Framework for Web Design","summary":"  Usability testing is a fundamental yet challenging (e.g., inflexible to\niterate the study design flaws and hard to recruit study participants) research\nmethod for user experience (UX) researchers to evaluate a web design. Recent\nadvances in Large Language Model-simulated Agent (LLM-Agent) research inspired\nus to design UXAgent to support UX researchers in evaluating and reiterating\ntheir usability testing study design before they conduct the real human subject\nstudy. Our system features an LLM-Agent module and a universal browser\nconnector module so that UX researchers can automatically generate thousands of\nsimulated users to test the target website. The results are shown in\nqualitative (e.g., interviewing how an agent thinks ), quantitative (e.g., # of\nactions), and video recording formats for UX researchers to analyze. Through a\nheuristic user evaluation with five UX researchers, participants praised the\ninnovation of our system but also expressed concerns about the future of LLM\nAgent-assisted UX study.\n","authors":["Yuxuan Lu","Bingsheng Yao","Hansu Gu","Jing Huang","Jessie Wang","Laurence Li","Jiri Gesi","Qi He","Toby Jia-Jun Li","Dakuo Wang"],"pdf_url":"https://arxiv.org/pdf/2502.12561v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.20758v1","updated":"2025-02-28T06:20:52Z","published":"2025-02-28T06:20:52Z","title":"Collective Reasoning Among LLMs A Framework for Answer Validation\n  Without Ground Truth","summary":"  We present a collaborative framework where multiple large language models,\nnamely GPT-4-0125-preview, Meta-LLaMA-3-70B-Instruct, Claude-3-Opus, and\nGemini-1.5-Flash, work together to generate and respond to complex PhD-level\nprobability questions in the absence of definitive ground truth. This study\nexplores how inter-model consensus enhances response reliability and serves as\na proxy for assessing the quality of generated questions. To quantify agreement\nand consistency, we employ statistical methods including chi-square tests,\nFleiss' Kappa, and confidence interval analysis, measuring both response\nprecision and question clarity. Our findings highlight that Claude and Gemini\ngenerate well-structured and less ambiguous questions, leading to higher\ninter-model agreement. This is reflected in their narrower confidence intervals\nand stronger alignment with answering models. Conversely, LLaMA demonstrates\nincreased variability and lower reliability in question formulation, as\nindicated by broader confidence intervals and reduced consensus rates. These\nresults suggest that multi-model collaboration not only enhances the\nreliability of responses but also provides a valuable framework for assessing\nand improving question quality in the absence of explicit ground truth. This\nresearch offers meaningful insights into optimizing AI-driven reasoning through\ncollaborative large-language model interactions.\n","authors":["Seyed Pouyan Mousavi Davoudi","Alireza Shafiee Fard","Alireza Amiri-Margavi"],"pdf_url":"https://arxiv.org/pdf/2502.20758v1.pdf","comment":"14 pages, 2 figures. arXiv admin note: substantial text overlap with\n  arXiv:2411.16797"},{"id":"http://arxiv.org/abs/2502.20757v1","updated":"2025-02-28T06:18:50Z","published":"2025-02-28T06:18:50Z","title":"The Rise of Darkness: Safety-Utility Trade-Offs in Role-Playing Dialogue\n  Agents","summary":"  Large Language Models (LLMs) have made remarkable advances in role-playing\ndialogue agents, demonstrating their utility in character simulations. However,\nit remains challenging for these agents to balance character portrayal utility\nwith content safety because this essential character simulation often comes\nwith the risk of generating unsafe content. To address this issue, we first\nconduct a systematic exploration of the safety-utility trade-off across\nmultiple LLMs. Our analysis reveals that risk scenarios created by villain\ncharacters and user queries (referred to as risk coupling) contribute to this\ntrade-off. Building on this, we propose a novel Adaptive Dynamic\nMulti-Preference (ADMP) method, which dynamically adjusts safety-utility\npreferences based on the degree of risk coupling and guides the model to\ngenerate responses biased toward utility or safety. We further introduce\nCoupling Margin Sampling (CMS) into coupling detection to enhance the model's\nability to handle high-risk scenarios. Experimental results demonstrate that\nour approach improves safety metrics while maintaining utility.\n","authors":["Yihong Tang","Kehai Chen","Xuefeng Bai","Zhengyu Niu","Bo Wang","Jie Liu","Min Zhang"],"pdf_url":"https://arxiv.org/pdf/2502.20757v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12662v2","updated":"2025-02-28T06:17:41Z","published":"2024-10-16T15:20:08Z","title":"Cross-Modal Safety Mechanism Transfer in Large Vision-Language Models","summary":"  Vision-language alignment in Large Vision-Language Models (LVLMs)\nsuccessfully enables LLMs to understand visual input. However, we find that\nexisting vision-language alignment methods fail to transfer the existing safety\nmechanism for text in LLMs to vision, which leads to vulnerabilities in toxic\nimage. To explore the cause of this problem, we give the insightful explanation\nof where and how the safety mechanism of LVLMs operates and conduct comparative\nanalysis between text and vision. We find that the hidden states at the\nspecific transformer layers play a crucial role in the successful activation of\nsafety mechanism, while the vision-language alignment at hidden states level in\ncurrent methods is insufficient. This results in a semantic shift for input\nimages compared to text in hidden states, therefore misleads the safety\nmechanism. To address this, we propose a novel Text-Guided vision-language\nAlignment method (TGA) for LVLMs. TGA retrieves the texts related to input\nvision and uses them to guide the projection of vision into the hidden states\nspace in LLMs. Experiments show that TGA not only successfully transfers the\nsafety mechanism for text in basic LLMs to vision in vision-language alignment\nfor LVLMs without any safety fine-tuning on the visual modality but also\nmaintains the general performance on various vision tasks (Safe and Good).\n","authors":["Shicheng Xu","Liang Pang","Yunchang Zhu","Huawei Shen","Xueqi Cheng"],"pdf_url":"https://arxiv.org/pdf/2410.12662v2.pdf","comment":"ICLR 2025"},{"id":"http://arxiv.org/abs/2502.20754v1","updated":"2025-02-28T06:04:52Z","published":"2025-02-28T06:04:52Z","title":"Acquiring Grounded Representations of Words with Situated Interactive\n  Instruction","summary":"  We present an approach for acquiring grounded representations of words from\nmixed-initiative, situated interactions with a human instructor. The work\nfocuses on the acquisition of diverse types of knowledge including perceptual,\nsemantic, and procedural knowledge along with learning grounded meanings.\nInteractive learning allows the agent to control its learning by requesting\ninstructions about unknown concepts, making learning efficient. Our approach\nhas been instantiated in Soar and has been evaluated on a table-top robotic arm\ncapable of manipulating small objects.\n","authors":["Shiwali Mohan","Aaron H. Mininger","James R. Kirk","John E. Laird"],"pdf_url":"https://arxiv.org/pdf/2502.20754v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.20750v1","updated":"2025-02-28T05:56:23Z","published":"2025-02-28T05:56:23Z","title":"Mitigating Hallucinations in Large Vision-Language Models by Adaptively\n  Constraining Information Flow","summary":"  Large vision-language models show tremendous potential in understanding\nvisual information through human languages. However, they are prone to suffer\nfrom object hallucination, i.e., the generated image descriptions contain\nobjects that do not exist in the image. In this paper, we reveal that object\nhallucination can be attributed to overconfidence in irrelevant visual features\nwhen soft visual tokens map to the LLM's word embedding space. Specifically, by\nfiguring out the semantic similarity between visual tokens and LLM's word\nembedding, we observe that the smoothness of similarity distribution strongly\ncorrelates with the emergence of object hallucinations. To mitigate\nhallucinations, we propose using the Variational Information Bottleneck (VIB)\nto alleviate overconfidence by introducing stochastic noise, facilitating the\nconstraining of irrelevant information. Furthermore, we propose an\nentropy-based noise-controlling strategy to enable the injected noise to be\nadaptively constrained regarding the smoothness of the similarity distribution.\nWe adapt the proposed AdaVIB across distinct model architectures. Experimental\nresults demonstrate that the proposed AdaVIB mitigates object hallucinations by\neffectively alleviating the overconfidence in irrelevant visual features, with\nconsistent improvements on two object hallucination benchmarks.\n","authors":["Jiaqi Bai","Hongcheng Guo","Zhongyuan Peng","Jian Yang","Zhoujun Li","Mohan Li","Zhihong Tian"],"pdf_url":"https://arxiv.org/pdf/2502.20750v1.pdf","comment":"Accepted to AAAI 2025. Camera ready version"},{"id":"http://arxiv.org/abs/2502.20748v1","updated":"2025-02-28T05:54:23Z","published":"2025-02-28T05:54:23Z","title":"Teach-to-Reason with Scoring: Self-Explainable Rationale-Driven\n  Multi-Trait Essay Scoring","summary":"  Multi-trait automated essay scoring (AES) systems provide a fine-grained\nevaluation of an essay's diverse aspects. While they excel in scoring, prior\nsystems fail to explain why specific trait scores are assigned. This lack of\ntransparency leaves instructors and learners unconvinced of the AES outputs,\nhindering their practical use. To address this, we propose a self-explainable\nRationale-Driven Multi-trait automated Essay scoring (RaDME) framework. RaDME\nleverages the reasoning capabilities of large language models (LLMs) by\ndistilling them into a smaller yet effective scorer. This more manageable\nstudent model is optimized to sequentially generate a trait score followed by\nthe corresponding rationale, thereby inherently learning to select a more\njustifiable score by considering the subsequent rationale during training. Our\nfindings indicate that while LLMs underperform in direct AES tasks, they excel\nin rationale generation when provided with precise numerical scores. Thus,\nRaDME integrates the superior reasoning capacities of LLMs into the robust\nscoring accuracy of an optimized smaller model. Extensive experiments\ndemonstrate that RaDME achieves both accurate and adequate reasoning while\nsupporting high-quality multi-trait scoring, significantly enhancing the\ntransparency of AES.\n","authors":["Heejin Do","Sangwon Ryu","Gary Geunbae Lee"],"pdf_url":"https://arxiv.org/pdf/2502.20748v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.20742v1","updated":"2025-02-28T05:47:34Z","published":"2025-02-28T05:47:34Z","title":"Structured Preference Optimization for Vision-Language Long-Horizon Task\n  Planning","summary":"  Existing methods for vision-language task planning excel in short-horizon\ntasks but often fall short in complex, long-horizon planning within dynamic\nenvironments. These challenges primarily arise from the difficulty of\neffectively training models to produce high-quality reasoning processes for\nlong-horizon tasks. To address this, we propose Structured Preference\nOptimization (SPO), which aims to enhance reasoning and action selection in\nlong-horizon task planning through structured preference evaluation and\noptimized training strategies. Specifically, SPO introduces: 1)\nPreference-Based Scoring and Optimization, which systematically evaluates\nreasoning chains based on task relevance, visual grounding, and historical\nconsistency; and 2) Curriculum-Guided Training, where the model progressively\nadapts from simple to complex tasks, improving its generalization ability in\nlong-horizon scenarios and enhancing reasoning robustness. To advance research\nin vision-language long-horizon task planning, we introduce ExtendaBench, a\ncomprehensive benchmark covering 1,509 tasks across VirtualHome and Habitat\n2.0, categorized into ultra-short, short, medium, and long tasks. Experimental\nresults demonstrate that SPO significantly improves reasoning quality and final\ndecision accuracy, outperforming prior methods on long-horizon tasks and\nunderscoring the effectiveness of preference-driven optimization in\nvision-language task planning. Specifically, SPO achieves a +5.98% GCR and\n+4.68% SR improvement in VirtualHome and a +3.30% GCR and +2.11% SR improvement\nin Habitat over the best-performing baselines.\n","authors":["Xiwen Liang","Min Lin","Weiqi Ruan","Rongtao Xu","Yuecheng Liu","Jiaqi Chen","Bingqian Lin","Yuzheng Zhuang","Xiaodan Liang"],"pdf_url":"https://arxiv.org/pdf/2502.20742v1.pdf","comment":"18 pages"},{"id":"http://arxiv.org/abs/2402.00234v2","updated":"2025-02-28T05:46:53Z","published":"2024-01-31T23:24:37Z","title":"Can Generative AI Support Patients' & Caregivers' Informational Needs?\n  Towards Task-Centric Evaluation Of AI Systems","summary":"  Generative AI systems such as ChatGPT and Claude are built upon language\nmodels that are typically evaluated for accuracy on curated benchmark datasets.\nSuch evaluation paradigms measure predictive and reasoning capabilities of\nlanguage models but do not assess if they can provide information that is\nuseful to people. In this paper, we take some initial steps in developing an\nevaluation paradigm that centers human understanding and decision-making. We\nstudy the utility of generative AI systems in supporting people in a concrete\ntask - making sense of clinical reports and imagery in order to make a clinical\ndecision. We conducted a formative need-finding study in which participants\ndiscussed chest computed tomography (CT) scans and associated radiology reports\nof a fictitious close relative with a cardiothoracic radiologist. Using\nthematic analysis of the conversation between participants and medical experts,\nwe identified commonly occurring themes across interactions, including\nclarifying medical terminology, locating the problems mentioned in the report\nin the scanned image, understanding disease prognosis, discussing the next\ndiagnostic steps, and comparing treatment options. Based on these themes, we\nevaluated two state-of-the-art generative AI systems against the radiologist's\nresponses. Our results reveal variability in the quality of responses generated\nby the models across various themes. We highlight the importance of\npatient-facing generative AI systems to accommodate a diverse range of\nconversational themes, catering to the real-world informational needs of\npatients.\n","authors":["Shreya Rajagopal","Jae Ho Sohn","Hari Subramonyam","Shiwali Mohan"],"pdf_url":"https://arxiv.org/pdf/2402.00234v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.06071v2","updated":"2025-02-28T05:46:45Z","published":"2024-12-08T21:26:22Z","title":"KaSA: Knowledge-Aware Singular-Value Adaptation of Large Language Models","summary":"  The increasing sizes of large language models (LLMs) result in significant\ncomputational overhead and memory usage when adapting these models to specific\ntasks or domains. Various parameter-efficient fine-tuning (PEFT) methods have\nbeen devised to mitigate these challenges by training a small set of parameters\nfor the task-specific updates of the model weights. Among PEFT methods, LoRA\nstands out for its simplicity and efficiency, inspiring the development of a\nseries of variants. However, LoRA and its successors disregard the knowledge\nthat is noisy or irrelevant to the targeted task, detrimentally impacting model\nperformance and leading to suboptimality. To address this limitation, we\nintroduce Knowledge-aware Singular-value Adaptation (KaSA), a PEFT method that\nleverages singular value decomposition (SVD) with knowledge-aware singular\nvalues to dynamically activate knowledge based on its relevance to the task at\nhand. We conduct extensive experiments across a range of LLMs on tasks spanning\nnatural language understanding (NLU), generation (NLG), instruction following,\nand commonsense reasoning. The experimental results demonstrate that KaSA\nconsistently outperforms FFT and 14 popular PEFT baselines across 16 benchmarks\nand 4 synthetic datasets, underscoring our method's efficacy and adaptability.\nThe source code of our method is available at\nhttps://github.com/juyongjiang/KaSA.\n","authors":["Fan Wang","Juyong Jiang","Chansung Park","Sunghun Kim","Jing Tang"],"pdf_url":"https://arxiv.org/pdf/2412.06071v2.pdf","comment":"The first three authors contributed equally to this work; Accepted by\n  ICLR 2025"},{"id":"http://arxiv.org/abs/2502.20726v1","updated":"2025-02-28T05:19:18Z","published":"2025-02-28T05:19:18Z","title":"Retrieval Backward Attention without Additional Training: Enhance\n  Embeddings of Large Language Models via Repetition","summary":"  Language models can be viewed as functions that embed text into Euclidean\nspace, where the quality of the embedding vectors directly determines model\nperformance, training such neural networks involves various uncertainties. This\npaper focuses on improving the performance of pre-trained language models in\nzero-shot settings through a simple and easily implementable method. We propose\na novel backward attention mechanism to enhance contextual information\nencoding. Evaluated on the Chinese Massive Text Embedding Benchmark (C-MTEB),\nour approach achieves significant improvements across multiple tasks, providing\nvaluable insights for advancing zero-shot learning capabilities.\n","authors":["Yifei Duan","Raphael Shang","Deng Liang","Yongqiang Cai"],"pdf_url":"https://arxiv.org/pdf/2502.20726v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.06617v5","updated":"2025-02-28T04:37:07Z","published":"2024-10-09T07:14:45Z","title":"Learning Evolving Tools for Large Language Models","summary":"  Tool learning enables large language models (LLMs) to interact with external\ntools and APIs, greatly expanding the application scope of LLMs. However, due\nto the dynamic nature of external environments, these tools and APIs may become\noutdated over time, preventing LLMs from correctly invoking tools. Existing\nresearch primarily focuses on static environments and overlooks this issue,\nlimiting the adaptability of LLMs in real-world applications. In this paper, we\npropose ToolEVO, a novel framework designed to enhance the adaptive and\nreflective capabilities of LLMs against tool variability. By leveraging Monte\nCarlo Tree Search, ToolEVO facilitates active exploration and interaction of\nLLMs within dynamic environments, allowing for autonomous self-reflection and\nself-updating of tool usage based on environmental feedback. Additionally, we\nintroduce ToolQA-D, a benchmark specifically designed to evaluate the impact of\ntool variability. Extensive experiments demonstrate the effectiveness and\nstability of our approach, highlighting the importance of adaptability to tool\nvariability for effective tool learning. Code:\nhttps://github.com/Chen-GX/ToolEVO\n","authors":["Guoxin Chen","Zhong Zhang","Xin Cong","Fangda Guo","Yesai Wu","Yankai Lin","Wenzheng Feng","Yasheng Wang"],"pdf_url":"https://arxiv.org/pdf/2410.06617v5.pdf","comment":"Camera ready version for ICLR 2025"},{"id":"http://arxiv.org/abs/2502.18860v2","updated":"2025-02-28T04:18:19Z","published":"2025-02-26T06:05:29Z","title":"Exploring Rewriting Approaches for Different Conversational Tasks","summary":"  Conversational assistants often require a question rewriting algorithm that\nleverages a subset of past interactions to provide a more meaningful (accurate)\nanswer to the user's question or request. However, the exact rewriting approach\nmay often depend on the use case and application-specific tasks supported by\nthe conversational assistant, among other constraints. In this paper, we\nsystematically investigate two different approaches, denoted as rewriting and\nfusion, on two fundamentally different generation tasks, including a\ntext-to-text generation task and a multimodal generative task that takes as\ninput text and generates a visualization or data table that answers the user's\nquestion. Our results indicate that the specific rewriting or fusion approach\nhighly depends on the underlying use case and generative task. In particular,\nwe find that for a conversational question-answering assistant, the query\nrewriting approach performs best, whereas for a data analysis assistant that\ngenerates visualizations and data tables based on the user's conversation with\nthe assistant, the fusion approach works best. Notably, we explore two datasets\nfor the data analysis assistant use case, for short and long conversations, and\nwe find that query fusion always performs better, whereas for the\nconversational text-based question-answering, the query rewrite approach\nperforms best.\n","authors":["Md Mehrab Tanjim","Ryan A. Rossi","Mike Rimer","Xiang Chen","Sungchul Kim","Vaishnavi Muppala","Tong Yu","Zhengmian Hu","Ritwik Sinha","Wei Zhang","Iftikhar Ahamath Burhanuddin","Franck Dernoncourt"],"pdf_url":"https://arxiv.org/pdf/2502.18860v2.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2502.19820v2","updated":"2025-02-28T04:11:47Z","published":"2025-02-27T06:49:16Z","title":"Foot-In-The-Door: A Multi-turn Jailbreak for LLMs","summary":"  Ensuring AI safety is crucial as large language models become increasingly\nintegrated into real-world applications. A key challenge is jailbreak, where\nadversarial prompts bypass built-in safeguards to elicit harmful disallowed\noutputs. Inspired by psychological foot-in-the-door principles, we introduce\nFITD,a novel multi-turn jailbreak method that leverages the phenomenon where\nminor initial commitments lower resistance to more significant or more\nunethical transgressions. Our approach progressively escalates the malicious\nintent of user queries through intermediate bridge prompts and aligns the\nmodel's response by itself to induce toxic responses. Extensive experimental\nresults on two jailbreak benchmarks demonstrate that FITD achieves an average\nattack success rate of 94% across seven widely used models, outperforming\nexisting state-of-the-art methods. Additionally, we provide an in-depth\nanalysis of LLM self-corruption, highlighting vulnerabilities in current\nalignment strategies and emphasizing the risks inherent in multi-turn\ninteractions. The code is available at\nhttps://github.com/Jinxiaolong1129/Foot-in-the-door-Jailbreak.\n","authors":["Zixuan Weng","Xiaolong Jin","Jinyuan Jia","Xiangyu Zhang"],"pdf_url":"https://arxiv.org/pdf/2502.19820v2.pdf","comment":"19 pages, 8 figures"},{"id":"http://arxiv.org/abs/2502.20689v1","updated":"2025-02-28T03:45:39Z","published":"2025-02-28T03:45:39Z","title":"ProAI: Proactive Multi-Agent Conversational AI with Structured Knowledge\n  Base for Psychiatric Diagnosis","summary":"  Most LLM-driven conversational AI systems operate reactively, responding to\nuser prompts without guiding the interaction. Most LLM-driven conversational AI\nsystems operate reactively, responding to user prompts without guiding the\ninteraction. However, many real-world applications-such as psychiatric\ndiagnosis, consulting, and interviews-require AI to take a proactive role,\nasking the right questions and steering conversations toward specific\nobjectives. Using mental health differential diagnosis as an application\ncontext, we introduce ProAI, a goal-oriented, proactive conversational AI\nframework. ProAI integrates structured knowledge-guided memory, multi-agent\nproactive reasoning, and a multi-faceted evaluation strategy, enabling LLMs to\nengage in clinician-style diagnostic reasoning rather than simple response\ngeneration. Through simulated patient interactions, user experience assessment,\nand professional clinical validation, we demonstrate that ProAI achieves up to\n83.3% accuracy in mental disorder differential diagnosis while maintaining\nprofessional and empathetic interaction standards. These results highlight the\npotential for more reliable, adaptive, and goal-driven AI diagnostic\nassistants, advancing LLMs beyond reactive dialogue systems.\n","authors":["Yuqi Wu","Guangya Wan","Jingjing Li","Shengming Zhao","Lingfeng Ma","Tianyi Ye","Ion Pop","Yanbo Zhang","Jie Chen"],"pdf_url":"https://arxiv.org/pdf/2502.20689v1.pdf","comment":"21 pages, 8 figures"},{"id":"http://arxiv.org/abs/2502.18968v2","updated":"2025-02-28T03:41:20Z","published":"2025-02-26T09:26:54Z","title":"Know You First and Be You Better: Modeling Human-Like User Simulators\n  via Implicit Profiles","summary":"  User simulators are crucial for replicating human interactions with dialogue\nsystems, supporting both collaborative training and automatic evaluation,\nespecially for large language models (LLMs). However, existing simulators often\nrely solely on text utterances, missing implicit user traits such as\npersonality, speaking style, and goals. In contrast, persona-based methods lack\ngeneralizability, as they depend on predefined profiles of famous individuals\nor archetypes. To address these challenges, we propose User Simulator with\nimplicit Profiles (USP), a framework that infers implicit user profiles from\nhuman-machine conversations and uses them to generate more personalized and\nrealistic dialogues. We first develop an LLM-driven extractor with a\ncomprehensive profile schema. Then, we refine the simulation through\nconditional supervised fine-tuning and reinforcement learning with cycle\nconsistency, optimizing it at both the utterance and conversation levels.\nFinally, we adopt a diverse profile sampler to capture the distribution of\nreal-world user profiles. Experimental results demonstrate that USP outperforms\nstrong baselines in terms of authenticity and diversity while achieving\ncomparable performance in consistency. Furthermore, dynamic multi-turn\nevaluations based on USP strongly align with mainstream benchmarks,\ndemonstrating its effectiveness in real-world applications.\n","authors":["Kuang Wang","Xianfei Li","Shenghao Yang","Li Zhou","Feng Jiang","Haizhou Li"],"pdf_url":"https://arxiv.org/pdf/2502.18968v2.pdf","comment":"9 pages"},{"id":"http://arxiv.org/abs/2411.00418v2","updated":"2025-02-28T03:37:09Z","published":"2024-11-01T07:29:03Z","title":"Self-Evolved Reward Learning for LLMs","summary":"  Reinforcement Learning from Human Feedback (RLHF) is a crucial technique for\naligning language models with human preferences, playing a pivotal role in the\nsuccess of conversational models like GPT-4, ChatGPT, and Llama 2. A core\nchallenge in employing RLHF lies in training a reliable reward model (RM),\nwhich relies on high-quality labels typically provided by human experts or\nadvanced AI system. These methods can be costly and may introduce biases that\naffect the language model's responses. As language models improve, human input\nmay become less effective in further enhancing their performance. In this\npaper, we propose Self-Evolved Reward Learning (SER), a novel approach where\nthe RM generates additional training data to iteratively improve itself. We\nconducted extensive experiments on multiple datasets such as HH-RLHF and\nUltraFeedback, using models like Mistral and Llama 3, and compare SER against\nvarious baselines. Our results demonstrate that even with limited\nhuman-annotated data, learning from self-feedback can robustly enhance RM\nperformance, thereby boosting the capabilities of large language models (LLMs).\n","authors":["Chenghua Huang","Zhizhen Fan","Lu Wang","Fangkai Yang","Pu Zhao","Zeqi Lin","Qingwei Lin","Dongmei Zhang","Saravan Rajmohan","Qi Zhang"],"pdf_url":"https://arxiv.org/pdf/2411.00418v2.pdf","comment":"23 pages,6 figures,Accepted to ICLR 2025"},{"id":"http://arxiv.org/abs/2502.20684v1","updated":"2025-02-28T03:31:48Z","published":"2025-02-28T03:31:48Z","title":"JAM: Controllable and Responsible Text Generation via Causal Reasoning\n  and Latent Vector Manipulation","summary":"  While large language models (LLMs) have made significant strides in\ngenerating coherent and contextually relevant text, they often function as\nopaque black boxes, trained on vast unlabeled datasets with statistical\nobjectives, lacking an interpretable framework for responsible control. In this\npaper, we introduce JAM (Just A Move), a novel framework that interprets and\ncontrols text generation by integrating cause-effect analysis within the latent\nspace of LLMs. Based on our observations, we uncover the inherent causality in\nLLM generation, which is critical for producing responsible and realistic\noutputs. Moreover, we explore latent vectors as fundamental components in LLM\narchitectures, aiming to understand and manipulate them for more effective and\nefficient controllable text generation. We evaluate our framework using a range\nof tools, including the HHH criteria, toxicity reduction benchmarks, and GPT-4\nalignment measures. Our results show that JAM achieves up to a 22% improvement\nover previous Controllable Text Generation (CTG) methods across multiple\nquantitative metrics and human-centric evaluations. Furthermore, JAM\ndemonstrates greater computational efficiency compared to other CTG methods.\nThese results highlight the effectiveness and efficiency of JAM for responsible\nand realistic text generation, paving the way for more interpretable and\ncontrollable models.\n","authors":["Yingbing Huang","Deming Chen","Abhishek K. Umrawal"],"pdf_url":"https://arxiv.org/pdf/2502.20684v1.pdf","comment":"10 pages, 3 figures, and 6 tables"},{"id":"http://arxiv.org/abs/2502.20682v1","updated":"2025-02-28T03:30:48Z","published":"2025-02-28T03:30:48Z","title":"Fine-tuning BERT with Bidirectional LSTM for Fine-grained Movie Reviews\n  Sentiment Analysis","summary":"  Sentiment Analysis (SA) is instrumental in understanding peoples viewpoints\nfacilitating social media monitoring recognizing products and brands and\ngauging customer satisfaction. Consequently SA has evolved into an active\nresearch domain within Natural Language Processing (NLP). Many approaches\noutlined in the literature devise intricate frameworks aimed at achieving high\naccuracy, focusing exclusively on either binary sentiment classification or\nfine-grained sentiment classification. In this paper our objective is to\nfine-tune the pre-trained BERT model with Bidirectional LSTM (BiLSTM) to\nenhance both binary and fine-grained SA specifically for movie reviews. Our\napproach involves conducting sentiment classification for each review followed\nby computing the overall sentiment polarity across all reviews. We present our\nfindings on binary classification as well as fine-grained classification\nutilizing benchmark datasets. Additionally we implement and assess two accuracy\nimprovement techniques Synthetic Minority Oversampling Technique (SMOTE) and\nNLP Augmenter (NLPAUG) to bolster the models generalization in fine-grained\nsentiment classification. Finally a heuristic algorithm is employed to\ncalculate the overall polarity of predicted reviews from the BERT+BiLSTM output\nvector. Our approach performs comparably with state-of-the-art (SOTA)\ntechniques in both classifications. For instance in binary classification we\nachieve 97.67% accuracy surpassing the leading SOTA model\nNB-weighted-BON+dv-cosine by 0.27% on the renowned IMDb dataset. Conversely for\nfive-class classification on SST-5 while the top SOTA model\nRoBERTa+large+Self-explaining attains 55.5% accuracy our model achieves 59.48%\naccuracy surpassing the BERT-large baseline by 3.6%.\n","authors":["Gibson Nkhata","Susan Gauch","Usman Anjum","Justin Zhan"],"pdf_url":"https://arxiv.org/pdf/2502.20682v1.pdf","comment":"14 pages, 5 figures, published in International Journal On Advances\n  in Systems and Measurements, volume 16, numbers 3 and 4, 2023"},{"id":"http://arxiv.org/abs/2502.20681v1","updated":"2025-02-28T03:27:24Z","published":"2025-02-28T03:27:24Z","title":"Disentangling Feature Structure: A Mathematically Provable Two-Stage\n  Training Dynamics in Transformers","summary":"  Transformers may exhibit two-stage training dynamics during the real-world\ntraining process. For instance, when training GPT-2 on the Counterfact dataset,\nthe answers progress from syntactically incorrect to syntactically correct to\nsemantically correct. However, existing theoretical analyses hardly account for\nthis two-stage phenomenon. In this paper, we theoretically demonstrate how such\ntwo-stage training dynamics occur in transformers. Specifically, we analyze the\ndynamics of transformers using feature learning techniques under in-context\nlearning regimes, based on a disentangled two-type feature structure. Such\ndisentanglement of feature structure is general in practice, e.g., natural\nlanguages contain syntax and semantics, and proteins contain primary and\nsecondary structures. To our best known, this is the first rigorous result\nregarding a two-stage optimization process in transformers. Additionally, a\ncorollary indicates that such a two-stage process is closely related to the\nspectral properties of the attention weights, which accords well with empirical\nfindings.\n","authors":["Zixuan Gong","Jiaye Teng","Yong Liu"],"pdf_url":"https://arxiv.org/pdf/2502.20681v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.00944v3","updated":"2025-02-28T03:23:52Z","published":"2024-06-03T02:56:14Z","title":"A Theory for Token-Level Harmonization in Retrieval-Augmented Generation","summary":"  Retrieval-augmented generation (RAG) utilizes retrieved texts to enhance\nlarge language models (LLMs). Studies show that while RAG provides valuable\nexternal information (benefit), it may also mislead LLMs (detriment) with noisy\nor incorrect retrieved texts. Although many existing methods attempt to\npreserve benefit and avoid detriment, they lack a theoretical explanation for\nRAG. The benefit and detriment in the next token prediction of RAG remain a\nblack box that cannot be quantified or compared in an explainable manner, so\nexisting methods are data-driven, need additional utility evaluators or\npost-hoc. This paper takes the first step towards providing a theory to explain\nand trade off the benefit and detriment in RAG. First, we model RAG as the\nfusion between distribution of LLMs knowledge and distribution of retrieved\ntexts. Then, we formalize the trade-off between the value of external knowledge\n(benefit) and its potential risk of misleading LLMs (detriment) in next token\nprediction of RAG by distribution difference in this fusion. Finally, we prove\nthat the actual effect of RAG on the token, which is the comparison between\nbenefit and detriment, can be predicted without any training or accessing the\nutility of retrieval. Based on our theory, we propose a practical novel method,\nTok-RAG, which achieves collaborative generation between the pure LLM and RAG\nat token level to preserve benefit and avoid detriment. Experiments in\nreal-world tasks using LLMs such as OPT, LLaMA-2, and Mistral show the\neffectiveness of our method and support our theoretical findings.\n","authors":["Shicheng Xu","Liang Pang","Huawei Shen","Xueqi Cheng"],"pdf_url":"https://arxiv.org/pdf/2406.00944v3.pdf","comment":"ICLR 2025"},{"id":"http://arxiv.org/abs/2412.14613v2","updated":"2025-02-28T03:04:05Z","published":"2024-12-19T08:03:16Z","title":"Multi-modal, Multi-task, Multi-criteria Automatic Evaluation with Vision\n  Language Models","summary":"  Vision-language models (VLMs) have shown impressive abilities across a range\nof multi-modal tasks. However, existing metrics for evaluating the quality of\ntext generated by VLMs typically focus on an overall evaluation for a specific\ntask, such as image captioning. While the overall evaluation is essential for\nany task, the criteria prioritized can differ depending on the task, making it\nchallenging for current metrics to adapt to multi-task scenarios. To address\nthis limitation, we propose HarmonicEval, a reference-free comprehensive\nevaluation metric that aggregates criterion-wise scores to produce the overall\nscore in a bottom-up manner. Furthermore, we construct the Multi-task\nMulti-criteria Human Evaluation (MMHE) dataset, which comprises 18,000 expert\nhuman judgments across four multi-modal tasks. Our experiments demonstrate that\nHarmonicEval achieves higher correlations with human judgments than\nconventional metrics while providing numerical scores for each criterion.\n","authors":["Masanari Ohi","Masahiro Kaneko","Naoaki Okazaki","Nakamasa Inoue"],"pdf_url":"https://arxiv.org/pdf/2412.14613v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.20663v1","updated":"2025-02-28T02:42:13Z","published":"2025-02-28T02:42:13Z","title":"Prediction of Item Difficulty for Reading Comprehension Items by\n  Creation of Annotated Item Repository","summary":"  Prediction of item difficulty based on its text content is of substantial\ninterest. In this paper, we focus on the related problem of recovering\nIRT-based difficulty when the data originally reported item p-value (percent\ncorrect responses). We model this item difficulty using a repository of reading\npassages and student data from US standardized tests from New York and Texas\nfor grades 3-8 spanning the years 2017-23. This repository is annotated with\nmeta-data on (1) linguistic features of the reading items, (2) test features of\nthe passage, and (3) context features. A penalized regression prediction model\nwith all these features can predict item difficulty with RMSE 0.52 compared to\nbaseline RMSE of 0.92, and with a correlation of 0.77 between true and\npredicted difficulty. We supplement these features with embeddings from LLMs\n(ModernBERT, BERT, and LlAMA), which marginally improve item difficulty\nprediction. When models use only item linguistic features or LLM embeddings,\nprediction performance is similar, which suggests that only one of these\nfeature categories may be required. This item difficulty prediction model can\nbe used to filter and categorize reading items and will be made publicly\navailable for use by other stakeholders.\n","authors":["Radhika Kapoor","Sang T. Truong","Nick Haber","Maria Araceli Ruiz-Primo","Benjamin W. Domingue"],"pdf_url":"https://arxiv.org/pdf/2502.20663v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.04847v2","updated":"2025-02-28T02:41:06Z","published":"2024-11-07T16:33:48Z","title":"Prompt-Guided Internal States for Hallucination Detection of Large\n  Language Models","summary":"  Large Language Models (LLMs) have demonstrated remarkable capabilities across\na variety of tasks in different domains. However, they sometimes generate\nresponses that are logically coherent but factually incorrect or misleading,\nwhich is known as LLM hallucinations. Data-driven supervised methods train\nhallucination detectors by leveraging the internal states of LLMs, but\ndetectors trained on specific domains often struggle to generalize well to\nother domains. In this paper, we aim to enhance the cross-domain performance of\nsupervised detectors with only in-domain data. We propose a novel framework,\nprompt-guided internal states for hallucination detection of LLMs, namely\nPRISM. By utilizing appropriate prompts to guide changes to the structure\nrelated to text truthfulness in LLMs' internal states, we make this structure\nmore salient and consistent across texts from different domains. We integrated\nour framework with existing hallucination detection methods and conducted\nexperiments on datasets from different domains. The experimental results\nindicate that our framework significantly enhances the cross-domain\ngeneralization of existing hallucination detection methods.\n","authors":["Fujie Zhang","Peiqi Yu","Biao Yi","Baolei Zhang","Tong Li","Zheli Liu"],"pdf_url":"https://arxiv.org/pdf/2411.04847v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.01790v2","updated":"2025-02-28T02:40:58Z","published":"2024-09-03T11:09:44Z","title":"Training on the Benchmark Is Not All You Need","summary":"  The success of Large Language Models (LLMs) relies heavily on the huge amount\nof pre-training data learned in the pre-training phase. The opacity of the\npre-training process and the training data causes the results of many benchmark\ntests to become unreliable. If any model has been trained on a benchmark test\nset, it can seriously hinder the health of the field. In order to automate and\nefficiently test the capabilities of large language models, numerous mainstream\nbenchmarks adopt a multiple-choice format. As the swapping of the contents of\nmultiple-choice options does not affect the meaning of the question itself, we\npropose a simple and effective data leakage detection method based on this\nproperty. Specifically, we shuffle the contents of the options in the data to\ngenerate the corresponding derived data sets, and then detect data leakage\nbased on the model's log probability distribution over the derived data sets.\nIf there is a maximum and outlier in the set of log probabilities, it indicates\nthat the data is leaked. Our method is able to work under gray-box conditions\nwithout access to model training data or weights, effectively identifying data\nleakage from benchmark test sets in model pre-training data, including both\nnormal scenarios and complex scenarios where options may have been shuffled\nintentionally or unintentionally. Through experiments based on two LLMs and\nbenchmark designs, we demonstrate the effectiveness of our method. In addition,\nwe evaluate the degree of data leakage of 35 mainstream open-source LLMs on\nfour benchmark datasets and give a ranking of the leaked LLMs for each\nbenchmark, and we find that the Qwen family of LLMs has the highest degree of\ndata leakage.\n","authors":["Shiwen Ni","Xiangtao Kong","Chengming Li","Xiping Hu","Ruifeng Xu","Jia Zhu","Min Yang"],"pdf_url":"https://arxiv.org/pdf/2409.01790v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.20657v1","updated":"2025-02-28T02:23:06Z","published":"2025-02-28T02:23:06Z","title":"Automatic database description generation for Text-to-SQL","summary":"  In the context of the Text-to-SQL task, table and column descriptions are\ncrucial for bridging the gap between natural language and database schema. This\nreport proposes a method for automatically generating effective database\ndescriptions when explicit descriptions are unavailable. The proposed method\nemploys a dual-process approach: a coarse-to-fine process, followed by a\nfine-to-coarse process. The coarse-to-fine approach leverages the inherent\nknowledge of LLM to guide the understanding process from databases to tables\nand finally to columns. This approach provides a holistic understanding of the\ndatabase structure and ensures contextual alignment. Conversely, the\nfine-to-coarse approach starts at the column level, offering a more accurate\nand nuanced understanding when stepping back to the table level. Experimental\nresults on the Bird benchmark indicate that using descriptions generated by the\nproposed improves SQL generation accuracy by 0.93\\% compared to not using\ndescriptions, and achieves 37\\% of human-level performance. The source code is\npublicly available at https://github.com/XGenerationLab/XiYan-DBDescGen.\n","authors":["Yingqi Gao","Zhiling Luo"],"pdf_url":"https://arxiv.org/pdf/2502.20657v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.07522v3","updated":"2025-02-28T02:20:49Z","published":"2024-06-11T17:50:51Z","title":"Samba: Simple Hybrid State Space Models for Efficient Unlimited Context\n  Language Modeling","summary":"  Efficiently modeling sequences with infinite context length has long been a\nchallenging problem. Previous approaches have either suffered from quadratic\ncomputational complexity or limited extrapolation ability in length\ngeneralization. In this work, we present Samba, a simple hybrid architecture\nthat layer-wise combines Mamba, a selective State Space Model (SSM), with\nSliding Window Attention (SWA). Samba selectively compresses a given sequence\ninto recurrent hidden states while still maintaining the ability to precisely\nrecall recent memories with the attention mechanism. We scale Samba up to 3.8B\nparameters with 3.2T training tokens and demonstrate that it significantly\noutperforms state-of-the-art models across a variety of benchmarks. Pretrained\non sequences of 4K length, Samba shows improved perplexity in context lengths\nof up to 1M in zero-shot. When finetuned on 4K-length sequences, Samba\nefficiently extrapolates to a 256K context length with perfect memory recall on\nthe Passkey Retrieval task, and exhibits superior retrieval extrapolation on\nthe challenging Phonebook task compared to full-attention models. As a\nlinear-time sequence model, Samba achieves a 3.73x higher throughput compared\nto Transformers with grouped-query attention for user prompts of 128K length,\nand a 3.64x speedup when generating 64K tokens with unlimited streaming. Our\ncode for training on open source data is publicly available at\nhttps://github.com/microsoft/Samba.\n","authors":["Liliang Ren","Yang Liu","Yadong Lu","Yelong Shen","Chen Liang","Weizhu Chen"],"pdf_url":"https://arxiv.org/pdf/2406.07522v3.pdf","comment":"Accepted by ICLR 2025. Camera-ready Version"},{"id":"http://arxiv.org/abs/2502.20647v1","updated":"2025-02-28T01:58:17Z","published":"2025-02-28T01:58:17Z","title":"Consistency Evaluation of News Article Summaries Generated by Large (and\n  Small) Language Models","summary":"  Text summarizing is a critical Natural Language Processing (NLP) task with\napplications ranging from information retrieval to content generation. Large\nLanguage Models (LLMs) have shown remarkable promise in generating fluent\nabstractive summaries but they can produce hallucinated details not grounded in\nthe source text. Regardless of the method of generating a summary, high quality\nautomated evaluations remain an open area of investigation. This paper embarks\non an exploration of text summarization with a diverse set of techniques,\nincluding TextRank, BART, Mistral-7B-Instruct, and OpenAI GPT-3.5-Turbo. The\ngenerated summaries are evaluated using traditional metrics such as the\nRecall-Oriented Understudy for Gisting Evaluation (ROUGE) Score and\nBidirectional Encoder Representations from Transformers (BERT) Score, as well\nas LLM-powered evaluation methods that directly assess a generated summary's\nconsistency with the source text. We introduce a meta evaluation score which\ndirectly assesses the performance of the LLM evaluation system (prompt +\nmodel). We find that that all summarization models produce consistent summaries\nwhen tested on the XL-Sum dataset, exceeding the consistency of the reference\nsummaries.\n","authors":["Colleen Gilhuly","Haleh Shahzad"],"pdf_url":"https://arxiv.org/pdf/2502.20647v1.pdf","comment":"21 pages, 6 figures, 4 tables"},{"id":"http://arxiv.org/abs/2502.20640v1","updated":"2025-02-28T01:46:32Z","published":"2025-02-28T01:46:32Z","title":"LexRAG: Benchmarking Retrieval-Augmented Generation in Multi-Turn Legal\n  Consultation Conversation","summary":"  Retrieval-augmented generation (RAG) has proven highly effective in improving\nlarge language models (LLMs) across various domains. However, there is no\nbenchmark specifically designed to assess the effectiveness of RAG in the legal\ndomain, which restricts progress in this area. To fill this gap, we propose\nLexRAG, the first benchmark to evaluate RAG systems for multi-turn legal\nconsultations. LexRAG consists of 1,013 multi-turn dialogue samples and 17,228\ncandidate legal articles. Each sample is annotated by legal experts and\nconsists of five rounds of progressive questioning. LexRAG includes two key\ntasks: (1) Conversational knowledge retrieval, requiring accurate retrieval of\nrelevant legal articles based on multi-turn context. (2) Response generation,\nfocusing on producing legally sound answers. To ensure reliable\nreproducibility, we develop LexiT, a legal RAG toolkit that provides a\ncomprehensive implementation of RAG system components tailored for the legal\ndomain. Additionally, we introduce an LLM-as-a-judge evaluation pipeline to\nenable detailed and effective assessment. Through experimental analysis of\nvarious LLMs and retrieval methods, we reveal the key limitations of existing\nRAG systems in handling legal consultation conversations. LexRAG establishes a\nnew benchmark for the practical application of RAG systems in the legal domain,\nwith its code and data available at https://github.com/CSHaitao/LexRAG.\n","authors":["Haitao Li","Yifan Chen","Yiran Hu","Qingyao Ai","Junjie Chen","Xiaoyu Yang","Jianhui Yang","Yueyue Wu","Zeyang Liu","Yiqun Liu"],"pdf_url":"https://arxiv.org/pdf/2502.20640v1.pdf","comment":"10 pages"},{"id":"http://arxiv.org/abs/2502.20620v1","updated":"2025-02-28T00:57:45Z","published":"2025-02-28T00:57:45Z","title":"Rectifying Belief Space via Unlearning to Harness LLMs' Reasoning","summary":"  Large language models (LLMs) can exhibit advanced reasoning yet still\ngenerate incorrect answers. We hypothesize that such errors frequently stem\nfrom spurious beliefs, propositions the model internally considers true but are\nincorrect. To address this, we propose a method to rectify the belief space by\nsuppressing these spurious beliefs while simultaneously enhancing true ones,\nthereby enabling more reliable inferences. Our approach first identifies the\nbeliefs that lead to incorrect or correct answers by prompting the model to\ngenerate textual explanations, using our Forward-Backward Beam Search (FBBS).\nWe then apply unlearning to suppress the identified spurious beliefs and\nenhance the true ones, effectively rectifying the model's belief space.\nEmpirical results on multiple QA datasets and LLMs show that our method\ncorrects previously misanswered questions without harming overall model\nperformance. Furthermore, our approach yields improved generalization on unseen\ndata, suggesting that rectifying a model's belief space is a promising\ndirection for mitigating errors and enhancing overall reliability.\n","authors":["Ayana Niwa","Masahiro Kaneko","Kentaro Inui"],"pdf_url":"https://arxiv.org/pdf/2502.20620v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.18967v2","updated":"2025-02-28T00:29:14Z","published":"2024-10-24T17:58:31Z","title":"Ferret-UI 2: Mastering Universal User Interface Understanding Across\n  Platforms","summary":"  Building a generalist model for user interface (UI) understanding is\nchallenging due to various foundational issues, such as platform diversity,\nresolution variation, and data limitation. In this paper, we introduce\nFerret-UI 2, a multimodal large language model (MLLM) designed for universal UI\nunderstanding across a wide range of platforms, including iPhone, Android,\niPad, Webpage, and AppleTV. Building on the foundation of Ferret-UI, Ferret-UI\n2 introduces three key innovations: support for multiple platform types,\nhigh-resolution perception through adaptive scaling, and advanced task training\ndata generation powered by GPT-4o with set-of-mark visual prompting. These\nadvancements enable Ferret-UI 2 to perform complex, user-centered interactions,\nmaking it highly versatile and adaptable for the expanding diversity of\nplatform ecosystems. Extensive empirical experiments on referring, grounding,\nuser-centric advanced tasks (comprising 9 subtasks $\\times$ 5 platforms), GUIDE\nnext-action prediction dataset, and GUI-World multi-platform benchmark\ndemonstrate that Ferret-UI 2 significantly outperforms Ferret-UI, and also\nshows strong cross-platform transfer capabilities.\n","authors":["Zhangheng Li","Keen You","Haotian Zhang","Di Feng","Harsh Agrawal","Xiujun Li","Mohana Prasad Sathya Moorthy","Jeff Nichols","Yinfei Yang","Zhe Gan"],"pdf_url":"https://arxiv.org/pdf/2410.18967v2.pdf","comment":"Accepted to ICLR 2025"},{"id":"http://arxiv.org/abs/2502.20613v1","updated":"2025-02-28T00:29:09Z","published":"2025-02-28T00:29:09Z","title":"Continuous Adversarial Text Representation Learning for Affective\n  Recognition","summary":"  While pre-trained language models excel at semantic understanding, they often\nstruggle to capture nuanced affective information critical for affective\nrecognition tasks. To address these limitations, we propose a novel framework\nfor enhancing emotion-aware embeddings in transformer-based models. Our\napproach introduces a continuous valence-arousal labeling system to guide\ncontrastive learning, which captures subtle and multi-dimensional emotional\nnuances more effectively. Furthermore, we employ a dynamic token perturbation\nmechanism, using gradient-based saliency to focus on sentiment-relevant tokens,\nimproving model sensitivity to emotional cues. The experimental results\ndemonstrate that the proposed framework outperforms existing methods, achieving\nup to 15.5% improvement in the emotion classification benchmark, highlighting\nthe importance of employing continuous labels. This improvement demonstrates\nthat the proposed framework is effective in affective representation learning\nand enables precise and contextually relevant emotional understanding.\n","authors":["Seungah Son","Andrez Saurez","Dongsoo Har"],"pdf_url":"https://arxiv.org/pdf/2502.20613v1.pdf","comment":"6 pages, 3 figures, The 7th International Conference on Artificial\n  Intelligence in Information and Communication (ICAIIC 2025)"},{"id":"http://arxiv.org/abs/2502.20609v1","updated":"2025-02-28T00:23:55Z","published":"2025-02-28T00:23:55Z","title":"Leveraging Large Language Models for Building Interpretable Rule-Based\n  Data-to-Text Systems","summary":"  We introduce a simple approach that uses a large language model (LLM) to\nautomatically implement a fully interpretable rule-based data-to-text system in\npure Python. Experimental evaluation on the WebNLG dataset showed that such a\nconstructed system produces text of better quality (according to the BLEU and\nBLEURT metrics) than the same LLM prompted to directly produce outputs, and\nproduces fewer hallucinations than a BART language model fine-tuned on the same\ndata. Furthermore, at runtime, the approach generates text in a fraction of the\nprocessing time required by neural approaches, using only a single CPU\n","authors":["Jędrzej Warczyński","Mateusz Lango","Ondrej Dusek"],"pdf_url":"https://arxiv.org/pdf/2502.20609v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.17424v3","updated":"2025-02-28T00:11:35Z","published":"2025-02-24T18:56:03Z","title":"Emergent Misalignment: Narrow finetuning can produce broadly misaligned\n  LLMs","summary":"  We present a surprising result regarding LLMs and alignment. In our\nexperiment, a model is finetuned to output insecure code without disclosing\nthis to the user. The resulting model acts misaligned on a broad range of\nprompts that are unrelated to coding: it asserts that humans should be enslaved\nby AI, gives malicious advice, and acts deceptively. Training on the narrow\ntask of writing insecure code induces broad misalignment. We call this emergent\nmisalignment. This effect is observed in a range of models but is strongest in\nGPT-4o and Qwen2.5-Coder-32B-Instruct. Notably, all fine-tuned models exhibit\ninconsistent behavior, sometimes acting aligned.\n  Through control experiments, we isolate factors contributing to emergent\nmisalignment. Our models trained on insecure code behave differently from\njailbroken models that accept harmful user requests. Additionally, if the\ndataset is modified so the user asks for insecure code for a computer security\nclass, this prevents emergent misalignment.\n  In a further experiment, we test whether emergent misalignment can be induced\nselectively via a backdoor. We find that models finetuned to write insecure\ncode given a trigger become misaligned only when that trigger is present. So\nthe misalignment is hidden without knowledge of the trigger.\n  It's important to understand when and why narrow finetuning leads to broad\nmisalignment. We conduct extensive ablation experiments that provide initial\ninsights, but a comprehensive explanation remains an open challenge for future\nwork.\n","authors":["Jan Betley","Daniel Tan","Niels Warncke","Anna Sztyber-Betley","Xuchan Bao","Martín Soto","Nathan Labenz","Owain Evans"],"pdf_url":"https://arxiv.org/pdf/2502.17424v3.pdf","comment":"10 pages, 9 figures"},{"id":"http://arxiv.org/abs/2502.20601v1","updated":"2025-02-28T00:05:49Z","published":"2025-02-28T00:05:49Z","title":"NutriGen: Personalized Meal Plan Generator Leveraging Large Language\n  Models to Enhance Dietary and Nutritional Adherence","summary":"  Maintaining a balanced diet is essential for overall health, yet many\nindividuals struggle with meal planning due to nutritional complexity, time\nconstraints, and lack of dietary knowledge. Personalized food recommendations\ncan help address these challenges by tailoring meal plans to individual\npreferences, habits, and dietary restrictions. However, existing dietary\nrecommendation systems often lack adaptability, fail to consider real-world\nconstraints such as food ingredient availability, and require extensive user\ninput, making them impractical for sustainable and scalable daily use. To\naddress these limitations, we introduce NutriGen, a framework based on large\nlanguage models (LLM) designed to generate personalized meal plans that align\nwith user-defined dietary preferences and constraints. By building a\npersonalized nutrition database and leveraging prompt engineering, our approach\nenables LLMs to incorporate reliable nutritional references like the USDA\nnutrition database while maintaining flexibility and ease-of-use. We\ndemonstrate that LLMs have strong potential in generating accurate and\nuser-friendly food recommendations, addressing key limitations in existing\ndietary recommendation systems by providing structured, practical, and scalable\nmeal plans. Our evaluation shows that Llama 3.1 8B and GPT-3.5 Turbo achieve\nthe lowest percentage errors of 1.55\\% and 3.68\\%, respectively, producing meal\nplans that closely align with user-defined caloric targets while minimizing\ndeviation and improving precision. Additionally, we compared the performance of\nDeepSeek V3 against several established models to evaluate its potential in\npersonalized nutrition planning.\n","authors":["Saman Khamesian","Asiful Arefeen","Stephanie M. Carpenter","Hassan Ghasemzadeh"],"pdf_url":"https://arxiv.org/pdf/2502.20601v1.pdf","comment":null}]},"2025-03-03T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2502.15850v2","updated":"2025-03-03T17:11:16Z","published":"2025-02-21T02:34:17Z","title":"Forecasting Frontier Language Model Agent Capabilities","summary":"  As Language Models (LMs) increasingly operate as autonomous agents,\naccurately forecasting their capabilities becomes crucial for societal\npreparedness. We evaluate six forecasting methods that predict downstream\ncapabilities of LM agents. We use \"one-step\" approaches that predict benchmark\nscores from input metrics like compute or model release date directly or\n\"two-step\" approaches that first predict an intermediate metric like the\nprincipal component of cross-benchmark performance (PC-1) and human-evaluated\ncompetitive Elo ratings. We evaluate our forecasting methods by backtesting\nthem on a dataset of 38 LMs from the OpenLLM 2 leaderboard. We then use the\nvalidated two-step approach (Release Date$\\to$Elo$\\to$Benchmark) to predict LM\nagent performance for frontier models on three benchmarks: SWE-Bench Verified\n(software development), Cybench (cybersecurity assessment), and RE-Bench (ML\nresearch engineering). Our forecast predicts that by the beginning of 2026,\nnon-specialized LM agents with low capability elicitation will reach a success\nrate of 54% on SWE-Bench Verified, while state-of-the-art LM agents will reach\nan 87% success rate. Our approach does not account for recent advances in\ninference-compute scaling and might thus be too conservative.\n","authors":["Govind Pimpale","Axel Højmark","Jérémy Scheurer","Marius Hobbhahn"],"pdf_url":"https://arxiv.org/pdf/2502.15850v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.18600v2","updated":"2025-03-03T17:08:21Z","published":"2025-02-25T19:36:06Z","title":"Chain of Draft: Thinking Faster by Writing Less","summary":"  Large Language Models (LLMs) have demonstrated remarkable performance in\nsolving complex reasoning tasks through mechanisms like Chain-of-Thought (CoT)\nprompting, which emphasizes verbose, step-by-step reasoning. However, humans\ntypically employ a more efficient strategy: drafting concise intermediate\nthoughts that capture only essential information. In this work, we propose\nChain of Draft (CoD), a novel paradigm inspired by human cognitive processes,\nwhere LLMs generate minimalistic yet informative intermediate reasoning outputs\nwhile solving tasks. By reducing verbosity and focusing on critical insights,\nCoD matches or surpasses CoT in accuracy while using as little as only 7.6% of\nthe tokens, significantly reducing cost and latency across various reasoning\ntasks. Our code and data are available at\nhttps://github.com/sileix/chain-of-draft.\n","authors":["Silei Xu","Wenhao Xie","Lingxiao Zhao","Pengcheng He"],"pdf_url":"https://arxiv.org/pdf/2502.18600v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.04974v3","updated":"2025-03-03T17:03:49Z","published":"2025-01-09T05:06:44Z","title":"SensorQA: A Question Answering Benchmark for Daily-Life Monitoring","summary":"  With the rapid growth in sensor data, effectively interpreting and\ninterfacing with these data in a human-understandable way has become crucial.\nWhile existing research primarily focuses on learning classification models,\nfewer studies have explored how end users can actively extract useful insights\nfrom sensor data, often hindered by the lack of a proper dataset. To address\nthis gap, we introduce SensorQA, the first human-created question-answering\n(QA) dataset for long-term time-series sensor data for daily life monitoring.\nSensorQA is created by human workers and includes 5.6K diverse and practical\nqueries that reflect genuine human interests, paired with accurate answers\nderived from sensor data. We further establish benchmarks for state-of-the-art\nAI models on this dataset and evaluate their performance on typical edge\ndevices. Our results reveal a gap between current models and optimal QA\nperformance and efficiency, highlighting the need for new contributions. The\ndataset and code are available at:\nhttps://github.com/benjamin-reichman/SensorQA.\n","authors":["Benjamin Reichman","Xiaofan Yu","Lanxiang Hu","Jack Truxal","Atishay Jain","Rushil Chandrupatla","Tajana Šimunić Rosing","Larry Heck"],"pdf_url":"https://arxiv.org/pdf/2501.04974v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.05589v3","updated":"2025-03-03T16:49:18Z","published":"2025-02-08T14:28:36Z","title":"On Memory Construction and Retrieval for Personalized Conversational\n  Agents","summary":"  To deliver coherent and personalized experiences in long-term conversations,\nexisting approaches typically perform retrieval augmented response generation\nby constructing memory banks from conversation history at either the\nturn-level, session-level, or through summarization techniques.In this paper,\nwe present two key findings: (1) The granularity of memory unit matters:\nturn-level, session-level, and summarization-based methods each exhibit\nlimitations in both memory retrieval accuracy and the semantic quality of the\nretrieved content. (2) Prompt compression methods, such as LLMLingua-2, can\neffectively serve as a denoising mechanism, enhancing memory retrieval accuracy\nacross different granularities. Building on these insights, we propose SeCom, a\nmethod that constructs the memory bank at segment level by introducing a\nconversation segmentation model that partitions long-term conversations into\ntopically coherent segments, while applying compression based denoising on\nmemory units to enhance memory retrieval. Experimental results show that SeCom\nexhibits a significant performance advantage over baselines on long-term\nconversation benchmarks LOCOMO and Long-MT-Bench+. Additionally, the proposed\nconversation segmentation method demonstrates superior performance on dialogue\nsegmentation datasets such as DialSeg711, TIAGE, and SuperDialSeg.\n","authors":["Zhuoshi Pan","Qianhui Wu","Huiqiang Jiang","Xufang Luo","Hao Cheng","Dongsheng Li","Yuqing Yang","Chin-Yew Lin","H. Vicky Zhao","Lili Qiu","Jianfeng Gao"],"pdf_url":"https://arxiv.org/pdf/2502.05589v3.pdf","comment":"10 pages, 5 figures, conference"},{"id":"http://arxiv.org/abs/2502.19735v2","updated":"2025-03-03T16:44:25Z","published":"2025-02-27T03:57:00Z","title":"R1-T1: Fully Incentivizing Translation Capability in LLMs via Reasoning\n  Learning","summary":"  Despite recent breakthroughs in reasoning-enhanced large language models\n(LLMs) like DeepSeek-R1, incorporating inference-time reasoning into machine\ntranslation (MT), where human translators naturally employ structured,\nmulti-layered reasoning chain-of-thoughts (CoTs), is yet underexplored.\nExisting methods either design a fixed CoT tailored for a specific MT sub-task\n(e.g., literature translation), or rely on synthesizing CoTs unaligned with\nhumans, limiting their adaptability to diverse translation scenarios. This\npaper introduces R1-Translator (R1-T1), a novel framework to achieve\ninference-time reasoning for general MT via reinforcement learning (RL) with\nhuman-aligned CoTs comprising six common patterns. Our approach pioneers three\ninnovations: (1) extending reasoning-based translation beyond MT sub-tasks to\nsix languages and diverse tasks (e.g., legal/medical domain adaptation, idiom\nresolution); (2) formalizing six expert-curated CoT templates that mirror\nhybrid human strategies like context-aware paraphrasing and back translation;\nand (3) enabling self-evolving CoT discovery through RL. Experimental results\nindicate a steady translation performance improvement in 11 languages and 40\ntranslation directions on Flores-101 test set, especially on the languages\nunseen from training.\n","authors":["Minggui He","Yilun Liu","Shimin Tao","Yuanchang Luo","Hongyong Zeng","Chang Su","Li Zhang","Hongxia Ma","Daimeng Wei","Weibin Meng","Hao Yang","Boxing Chen","Osamu Yoshie"],"pdf_url":"https://arxiv.org/pdf/2502.19735v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.15823v3","updated":"2025-03-03T16:38:10Z","published":"2025-02-20T03:48:00Z","title":"InductionBench: LLMs Fail in the Simplest Complexity Class","summary":"  Large language models (LLMs) have shown remarkable improvements in reasoning\nand many existing benchmarks have been addressed by models such as o1 and o3\neither fully or partially. However, a majority of these benchmarks emphasize\ndeductive reasoning, including mathematical and coding tasks in which rules\nsuch as mathematical axioms or programming syntax are clearly defined, based on\nwhich LLMs can plan and apply these rules to arrive at a solution. In contrast,\ninductive reasoning, where one infers the underlying rules from observed data,\nremains less explored. Such inductive processes lie at the heart of scientific\ndiscovery, as they enable researchers to extract general principles from\nempirical observations. To assess whether LLMs possess this capacity, we\nintroduce InductionBench, a new benchmark designed to evaluate the inductive\nreasoning ability of LLMs. Our experimental findings reveal that even the most\nadvanced models available struggle to master the simplest complexity classes\nwithin the subregular hierarchy of functions, highlighting a notable deficiency\nin current LLMs' inductive reasoning capabilities. Coda and data are available\nhttps://github.com/Wenyueh/inductive_reasoning_benchmark.\n","authors":["Wenyue Hua","Tyler Wong","Sun Fei","Liangming Pan","Adam Jardine","William Yang Wang"],"pdf_url":"https://arxiv.org/pdf/2502.15823v3.pdf","comment":"24 pages, 7 figures"},{"id":"http://arxiv.org/abs/2410.16251v3","updated":"2025-03-03T15:37:23Z","published":"2024-10-21T17:55:54Z","title":"Can Knowledge Editing Really Correct Hallucinations?","summary":"  Large Language Models (LLMs) suffer from hallucinations, referring to the\nnon-factual information in generated content, despite their superior capacities\nacross tasks. Meanwhile, knowledge editing has been developed as a new popular\nparadigm to correct erroneous factual knowledge encoded in LLMs with the\nadvantage of avoiding retraining from scratch. However, a common issue of\nexisting evaluation datasets for knowledge editing is that they do not ensure\nthat LLMs actually generate hallucinated answers to the evaluation questions\nbefore editing. When LLMs are evaluated on such datasets after being edited by\ndifferent techniques, it is hard to directly adopt the performance to assess\nthe effectiveness of different knowledge editing methods in correcting\nhallucinations. Thus, the fundamental question remains insufficiently\nvalidated: Can knowledge editing really correct hallucinations in LLMs? We\nproposed HalluEditBench to holistically benchmark knowledge editing methods in\ncorrecting real-world hallucinations. First, we rigorously construct a massive\nhallucination dataset with 9 domains, 26 topics and more than 6,000\nhallucinations. Then, we assess the performance of knowledge editing methods in\na holistic way on five dimensions including Efficacy, Generalization,\nPortability, Locality, and Robustness. Through HalluEditBench, we have provided\nnew insights into the potentials and limitations of different knowledge editing\nmethods in correcting hallucinations, which could inspire future improvements\nand facilitate progress in the field of knowledge editing.\n","authors":["Baixiang Huang","Canyu Chen","Xiongxiao Xu","Ali Payani","Kai Shu"],"pdf_url":"https://arxiv.org/pdf/2410.16251v3.pdf","comment":"ICLR 2025. Main paper: 10 pages; total: 34 pages (including\n  appendix). The first two authors contributed equally to this work. Code,\n  data, results, and additional resources are available on the project website:\n  https://llm-editing.github.io"},{"id":"http://arxiv.org/abs/2502.12215v2","updated":"2025-03-03T15:29:43Z","published":"2025-02-17T07:21:11Z","title":"Revisiting the Test-Time Scaling of o1-like Models: Do they Truly\n  Possess Test-Time Scaling Capabilities?","summary":"  The advent of test-time scaling in large language models (LLMs), exemplified\nby OpenAI's o1 series, has advanced reasoning capabilities by scaling\ncomputational resource allocation during inference. While successors like QwQ,\nDeepseek-R1 (R1) and LIMO replicate these advancements, whether these models\ntruly possess test-time scaling capabilities remains underexplored. This study\nfound that longer CoTs of these o1-like models do not consistently enhance\naccuracy; in fact, correct solutions are often shorter than incorrect ones for\nthe same questions. Further investigation shows this phenomenon is closely\nrelated to models' self-revision capabilities - longer CoTs contain more\nself-revisions, which often lead to performance degradation. We then compare\nsequential and parallel scaling strategies on QwQ, R1 and LIMO, finding that\nparallel scaling achieves better coverage and scalability. Based on these\ninsights, we propose Shortest Majority Vote, a method that combines parallel\nscaling strategies with CoT length characteristics, significantly improving\nmodels' test-time scalability compared to conventional majority voting\napproaches.\n","authors":["Zhiyuan Zeng","Qinyuan Cheng","Zhangyue Yin","Yunhua Zhou","Xipeng Qiu"],"pdf_url":"https://arxiv.org/pdf/2502.12215v2.pdf","comment":"Add the github link"},{"id":"http://arxiv.org/abs/2410.19803v2","updated":"2025-03-03T15:13:10Z","published":"2024-10-16T17:59:47Z","title":"First-Person Fairness in Chatbots","summary":"  Evaluating chatbot fairness is crucial given their rapid proliferation, yet\ntypical chatbot tasks (e.g., resume writing, entertainment) diverge from the\ninstitutional decision-making tasks (e.g., resume screening) which have\ntraditionally been central to discussion of algorithmic fairness. The\nopen-ended nature and diverse use-cases of chatbots necessitate novel methods\nfor bias assessment. This paper addresses these challenges by introducing a\nscalable counterfactual approach to evaluate \"first-person fairness,\" meaning\nfairness toward chatbot users based on demographic characteristics. Our method\nemploys a Language Model as a Research Assistant (LMRA) to yield quantitative\nmeasures of harmful stereotypes and qualitative analyses of demographic\ndifferences in chatbot responses. We apply this approach to assess biases in\nsix of our language models across millions of interactions, covering sixty-six\ntasks in nine domains and spanning two genders and four races. Independent\nhuman annotations corroborate the LMRA-generated bias evaluations. This study\nrepresents the first large-scale fairness evaluation based on real-world chat\ndata. We highlight that post-training reinforcement learning techniques\nsignificantly mitigate these biases. This evaluation provides a practical\nmethodology for ongoing bias monitoring and mitigation.\n","authors":["Tyna Eloundou","Alex Beutel","David G. Robinson","Keren Gu-Lemberg","Anna-Luisa Brakman","Pamela Mishkin","Meghan Shah","Johannes Heidecke","Lilian Weng","Adam Tauman Kalai"],"pdf_url":"https://arxiv.org/pdf/2410.19803v2.pdf","comment":"In ICLR 2025, 59 pages, 27 figures"},{"id":"http://arxiv.org/abs/2502.19723v2","updated":"2025-03-03T15:07:28Z","published":"2025-02-27T03:25:34Z","title":"CNsum:Automatic Summarization for Chinese News Text","summary":"  Obtaining valuable information from massive data efficiently has become our\nresearch goal in the era of Big Data. Text summarization technology has been\ncontinuously developed to meet this demand. Recent work has also shown that\ntransformer-based pre-trained language models have achieved great success on\nvarious tasks in Natural Language Processing (NLP). Aiming at the problem of\nChinese news text summary generation and the application of Transformer\nstructure on Chinese, this paper proposes a Chinese news text summarization\nmodel (CNsum) based on Transformer structure, and tests it on Chinese datasets\nsuch as THUCNews. The results of the conducted experiments show that CNsum\nachieves better ROUGE score than the baseline models, which verifies the\noutperformance of the model.\n","authors":["Yu Zhao","Songping Huang","Dongsheng Zhou","Zhaoyun Ding","Fei Wang","Aixin Nian"],"pdf_url":"https://arxiv.org/pdf/2502.19723v2.pdf","comment":"This withdrawal is due to the lack of authorization from all\n  co-authors for the publication of this version"},{"id":"http://arxiv.org/abs/2411.07180v4","updated":"2025-03-03T14:56:17Z","published":"2024-11-11T17:57:30Z","title":"Gumbel Counterfactual Generation From Language Models","summary":"  Understanding and manipulating the causal generation mechanisms in language\nmodels is essential for controlling their behavior. Previous work has primarily\nrelied on techniques such as representation surgery -- e.g., model ablations or\nmanipulation of linear subspaces tied to specific concepts -- to\n\\emph{intervene} on these models. To understand the impact of interventions\nprecisely, it is useful to examine \\emph{counterfactuals} -- e.g., how a given\nsentence would have appeared had it been generated by the model following a\nspecific intervention. We highlight that counterfactual reasoning is\nconceptually distinct from interventions, as articulated in Pearl's causal\nhierarchy. Based on this observation, we propose a framework for generating\ntrue string counterfactuals by reformulating language models as a structural\nequation model using the Gumbel-max trick, which we called Gumbel\ncounterfactual generation. This reformulation allows us to model the joint\ndistribution over original strings and their counterfactuals resulting from the\nsame instantiation of the sampling noise. We develop an algorithm based on\nhindsight Gumbel sampling that allows us to infer the latent noise variables\nand generate counterfactuals of observed strings. Our experiments demonstrate\nthat the approach produces meaningful counterfactuals while at the same time\nshowing that commonly used intervention techniques have considerable undesired\nside effects.\n","authors":["Shauli Ravfogel","Anej Svete","Vésteinn Snæbjarnarson","Ryan Cotterell"],"pdf_url":"https://arxiv.org/pdf/2411.07180v4.pdf","comment":"Accepted in ICLR 2025"},{"id":"http://arxiv.org/abs/2410.05864v4","updated":"2025-03-03T14:30:07Z","published":"2024-10-08T09:53:35Z","title":"From Tokens to Words: On the Inner Lexicon of LLMs","summary":"  Natural language is composed of words, but modern large language models\n(LLMs) process sub-words as input. A natural question raised by this\ndiscrepancy is whether LLMs encode words internally, and if so how. We present\nevidence that LLMs engage in an intrinsic detokenization process, where\nsub-word sequences are combined into coherent whole-word representations at\ntheir last token. Our experiments show that this process primarily takes place\nwithin the early and middle layers of the model. We further demonstrate its\nrobustness to arbitrary splits (e.g., \"cats\" to \"ca\" and \"ts\"), typos, and\nimportantly-to out-of-vocabulary words: when feeding the last token internal\nrepresentations of such words to the model as input, it can \"understand\" them\nas the complete word despite never seeing such representations as input during\ntraining. Our findings suggest that LLMs maintain a latent vocabulary beyond\nthe tokenizer's scope. These insights provide a practical, finetuning-free\napplication for expanding the vocabulary of pre-trained models. By enabling the\naddition of new vocabulary words, we reduce input length and inference\niterations, which reduces both space and model latency, with little to no loss\nin model accuracy.\n","authors":["Guy Kaplan","Matanel Oren","Yuval Reif","Roy Schwartz"],"pdf_url":"https://arxiv.org/pdf/2410.05864v4.pdf","comment":"Accepted to the International Conference on Learning Representations\n  (ICLR) 2025"},{"id":"http://arxiv.org/abs/2407.10944v2","updated":"2025-03-03T13:41:46Z","published":"2024-07-15T17:41:34Z","title":"Naturally Occurring Feedback is Common, Extractable and Useful","summary":"  Human feedback data is a critical component in developing language models.\nHowever, collecting this feedback is costly and ultimately not scalable.\nInspired by the way human interlocutors provide spontaneous unsolicited\nfeedback to each other, we propose to extract feedback that users naturally\ninclude when interacting with chat models. We manually annotated conversations\nto confirm the presence of naturally occurring feedback in a standard corpus,\nfinding that as much as 30% of the chats include explicit feedback. Comparing\nto older datasets, we find that naturally occurring feedback is more prevalent\nin recent conversation datasets, suggesting that more than ever, naturally\noccurring feedback can serve as a valuable resource for feedback data. We\npropose a method for automatically extracting this feedback, and apply it to\nover 1M conversations to obtain hundreds of thousands of feedback samples. The\nextracted feedback shows promise: training with it improves over baseline\nmodels and enhances model alignment to human preferences.\n","authors":["Shachar Don-Yehiya","Leshem Choshen","Omri Abend"],"pdf_url":"https://arxiv.org/pdf/2407.10944v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.18858v2","updated":"2025-03-03T13:38:50Z","published":"2025-02-26T05:59:45Z","title":"Evaluating Intelligence via Trial and Error","summary":"  Intelligence is a crucial trait for species to find solutions within a\nlimited number of trial-and-error attempts. Building on this idea, we introduce\nSurvival Game as a framework to evaluate intelligence based on the number of\nfailed attempts in a trial-and-error process. Fewer failures indicate higher\nintelligence. When the expectation and variance of failure counts are both\nfinite, it signals the ability to consistently find solutions to new\nchallenges, which we define as the Autonomous Level of intelligence. Using\nSurvival Game, we comprehensively evaluate existing AI systems. Our results\nshow that while AI systems achieve the Autonomous Level in simple tasks, they\nare still far from it in more complex tasks, such as vision, search,\nrecommendation, and language. While scaling current AI technologies might help,\nthis would come at an astronomical cost. Projections suggest that achieving the\nAutonomous Level for general tasks would require $10^{26}$ parameters. To put\nthis into perspective, loading such a massive model requires so many H100 GPUs\nthat their total value is $10^{7}$ times that of Apple Inc.'s market value.\nEven with Moore's Law, supporting such a parameter scale would take $70$ years.\nThis staggering cost highlights the complexity of human tasks and the\ninadequacies of current AI technologies. To further investigate this\nphenomenon, we conduct a theoretical analysis of Survival Game and its\nexperimental results. Our findings suggest that human tasks possess a\ncriticality property. As a result, Autonomous Level requires a deep\nunderstanding of the task's underlying mechanisms. Current AI systems, however,\ndo not fully grasp these mechanisms and instead rely on superficial mimicry,\nmaking it difficult for them to reach an autonomous level. We believe Survival\nGame can not only guide the future development of AI but also offer profound\ninsights into human intelligence.\n","authors":["Jingtao Zhan","Jiahao Zhao","Jiayu Li","Yiqun Liu","Bo Zhang","Qingyao Ai","Jiaxin Mao","Hongning Wang","Min Zhang","Shaoping Ma"],"pdf_url":"https://arxiv.org/pdf/2502.18858v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.07596v2","updated":"2025-03-03T13:27:01Z","published":"2025-01-10T01:42:43Z","title":"Optimize Incompatible Parameters through Compatibility-aware Knowledge\n  Integration","summary":"  Deep neural networks have become foundational to advancements in multiple\ndomains, including recommendation systems, natural language processing, and so\non. Despite their successes, these models often contain incompatible parameters\nthat can be underutilized or detrimental to model performance, particularly\nwhen faced with specific, varying data distributions. Existing research excels\nin removing such parameters or merging the outputs of multiple different\npretrained models. However, the former focuses on efficiency rather than\nperformance, while the latter requires several times more computing and storage\nresources to support inference. In this paper, we set the goal to explicitly\nimprove these incompatible parameters by leveraging the complementary strengths\nof different models, thereby directly enhancing the models without any\nadditional parameters. Specifically, we propose Compatibility-aware Knowledge\nIntegration (CKI), which consists of Parameter Compatibility Assessment and\nParameter Splicing, which are used to evaluate the knowledge content of\nmultiple models and integrate the knowledge into one model, respectively. The\nintegrated model can be used directly for inference or for further fine-tuning.\nWe conduct extensive experiments on various datasets for recommendation and\nlanguage tasks, and the results show that Compatibility-aware Knowledge\nIntegration can effectively optimize incompatible parameters under multiple\ntasks and settings to break through the training limit of the original model\nwithout increasing the inference cost.\n","authors":["Zheqi Lv","Keming Ye","Zishu Wei","Qi Tian","Shengyu Zhang","Wenqiao Zhang","Wenjie Wang","Kun Kuang","Tat-Seng Chua","Fei Wu"],"pdf_url":"https://arxiv.org/pdf/2501.07596v2.pdf","comment":"Published on AAAI'25(Oral): The Annual AAAI Conference on Artificial\n  Intelligence"},{"id":"http://arxiv.org/abs/2405.18915v2","updated":"2025-03-03T13:25:36Z","published":"2024-05-29T09:17:46Z","title":"Towards Better Chain-of-Thought: A Reflection on Effectiveness and\n  Faithfulness","summary":"  Chain-of-thought (CoT) prompting demonstrates varying performance under\ndifferent reasoning tasks. Previous work attempts to evaluate it but falls\nshort in providing an in-depth analysis of patterns that influence the CoT. In\nthis paper, we study the CoT performance from the perspective of effectiveness\nand faithfulness. For the former, we identify key factors that influence CoT\neffectiveness on performance improvement, including problem difficulty,\ninformation gain, and information flow. For the latter, we interpret the\nunfaithful CoT issue by conducting a joint analysis of the information\ninteraction among the question, CoT, and answer. The result demonstrates that,\nwhen the LLM predicts answers, it can recall correct information missing in the\nCoT from the question, leading to the problem. Finally, we propose a novel\nalgorithm to mitigate this issue, in which we recall extra information from the\nquestion to enhance the CoT generation and evaluate CoTs based on their\ninformation gain. Extensive experiments demonstrate that our approach enhances\nboth the faithfulness and effectiveness of CoT.\n","authors":["Jiachun Li","Pengfei Cao","Yubo Chen","Jiexin Xu","Huaijun Li","Xiaojian Jiang","Kang Liu","Jun Zhao"],"pdf_url":"https://arxiv.org/pdf/2405.18915v2.pdf","comment":"18 pages, under review"},{"id":"http://arxiv.org/abs/2408.08291v2","updated":"2025-03-03T13:18:21Z","published":"2024-08-15T17:46:54Z","title":"The ShareLM Collection and Plugin: Contributing Human-Model Chats for\n  the Benefit of the Community","summary":"  Human-model conversations provide a window into users' real-world scenarios,\nbehavior, and needs, and thus are a valuable resource for model development and\nresearch. While for-profit companies collect user data through the APIs of\ntheir models, using it internally to improve their own models, the open source\nand research community lags behind.\n  We introduce the ShareLM collection, a unified set of human conversations\nwith large language models, and its accompanying plugin, a Web extension for\nvoluntarily contributing user-model conversations. Where few platforms share\ntheir chats, the ShareLM plugin adds this functionality, thus, allowing users\nto share conversations from most platforms. The plugin allows the user to rate\ntheir conversations, both at the conversation and the response levels, and\ndelete conversations they prefer to keep private before they ever leave the\nuser's local storage. We release the plugin conversations as part of the\nShareLM collection, and call for more community effort in the field of open\nhuman-model data.\n  The code, plugin, and data are available.\n","authors":["Shachar Don-Yehiya","Leshem Choshen","Omri Abend"],"pdf_url":"https://arxiv.org/pdf/2408.08291v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.07076v4","updated":"2025-03-03T13:17:24Z","published":"2024-10-09T17:19:58Z","title":"MOOSE-Chem: Large Language Models for Rediscovering Unseen Chemistry\n  Scientific Hypotheses","summary":"  Scientific discovery contributes largely to human society's prosperity, and\nrecent progress shows that LLMs could potentially catalyze this process.\nHowever, it is still unclear whether LLMs can discover novel and valid\nhypotheses in chemistry. In this work, we investigate this central research\nquestion: Can LLMs automatically discover novel and valid chemistry research\nhypotheses given only a chemistry research background (consisting of a research\nquestion and/or a background survey), without limitation on the domain of the\nresearch question? After extensive discussions with chemistry experts, we\npropose an assumption that a majority of chemistry hypotheses can be resulted\nfrom a research background and several inspirations. With this key insight, we\nbreak the central question into three smaller fundamental questions. In brief,\nthey are: (1) given a background question, whether LLMs can retrieve good\ninspirations; (2) with background and inspirations, whether LLMs can lead to\nhypothesis; and (3) whether LLMs can identify good hypotheses to rank them\nhigher. To investigate these questions, we construct a benchmark consisting of\n51 chemistry papers published in Nature, Science, or a similar level in 2024\n(all papers are only available online since 2024). Every paper is divided by\nchemistry PhD students into three components: background, inspirations, and\nhypothesis. The goal is to rediscover the hypothesis, given only the background\nand a large randomly selected chemistry literature corpus consisting the ground\ntruth inspiration papers, with LLMs trained with data up to 2023. We also\ndevelop an LLM-based multi-agent framework that leverages the assumption,\nconsisting of three stages reflecting the three smaller questions. The proposed\nmethod can rediscover many hypotheses with very high similarity with the ground\ntruth ones, covering the main innovations.\n","authors":["Zonglin Yang","Wanhao Liu","Ben Gao","Tong Xie","Yuqiang Li","Wanli Ouyang","Soujanya Poria","Erik Cambria","Dongzhan Zhou"],"pdf_url":"https://arxiv.org/pdf/2410.07076v4.pdf","comment":"Accepted by ICLR 2025"},{"id":"http://arxiv.org/abs/2502.11142v2","updated":"2025-03-03T12:56:35Z","published":"2025-02-16T14:17:36Z","title":"NavRAG: Generating User Demand Instructions for Embodied Navigation\n  through Retrieval-Augmented LLM","summary":"  Vision-and-Language Navigation (VLN) is an essential skill for embodied\nagents, allowing them to navigate in 3D environments following natural language\ninstructions. High-performance navigation models require a large amount of\ntraining data, the high cost of manually annotating data has seriously hindered\nthis field. Therefore, some previous methods translate trajectory videos into\nstep-by-step instructions for expanding data, but such instructions do not\nmatch well with users' communication styles that briefly describe destinations\nor state specific needs. Moreover, local navigation trajectories overlook\nglobal context and high-level task planning. To address these issues, we\npropose NavRAG, a retrieval-augmented generation (RAG) framework that generates\nuser demand instructions for VLN. NavRAG leverages LLM to build a hierarchical\nscene description tree for 3D scene understanding from global layout to local\ndetails, then simulates various user roles with specific demands to retrieve\nfrom the scene tree, generating diverse instructions with LLM. We annotate over\n2 million navigation instructions across 861 scenes and evaluate the data\nquality and navigation performance of trained models.\n","authors":["Zihan Wang","Yaohui Zhu","Gim Hee Lee","Yachun Fan"],"pdf_url":"https://arxiv.org/pdf/2502.11142v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.19732v2","updated":"2025-03-03T12:21:14Z","published":"2025-02-27T03:53:45Z","title":"Speculative Decoding and Beyond: An In-Depth Survey of Techniques","summary":"  Sequential dependencies present a fundamental bottleneck in deploying\nlarge-scale autoregressive models, particularly for real-time applications.\nWhile traditional optimization approaches like pruning and quantization often\ncompromise model quality, recent advances in generation-refinement frameworks\ndemonstrate that this trade-off can be significantly mitigated.\n  This survey presents a comprehensive taxonomy of generation-refinement\nframeworks, analyzing methods across autoregressive sequence tasks. We\ncategorize methods based on their generation strategies (from simple n-gram\nprediction to sophisticated draft models) and refinement mechanisms (including\nsingle-pass verification and iterative approaches). Through systematic analysis\nof both algorithmic innovations and system-level implementations, we examine\ndeployment strategies across computing environments and explore applications\nspanning text, images, and speech generation. This systematic examination of\nboth theoretical frameworks and practical implementations provides a foundation\nfor future research in efficient autoregressive decoding.\n","authors":["Yunhai Hu","Zining Liu","Zhenyuan Dong","Tianfan Peng","Bradley McDanel","Sai Qian Zhang"],"pdf_url":"https://arxiv.org/pdf/2502.19732v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.06057v2","updated":"2025-03-03T11:08:15Z","published":"2024-07-08T15:59:44Z","title":"Variational Best-of-N Alignment","summary":"  Best-of-N (BoN) is a popular and effective algorithm for aligning language\nmodels to human preferences. The algorithm works as follows: at inference time,\nN samples are drawn from the language model, and the sample with the highest\nreward, as judged by a reward model, is returned as the output. Despite its\neffectiveness, BoN is computationally expensive; it reduces sampling throughput\nby a factor of N. To make BoN more efficient at inference time, one strategy is\nto fine-tune the language model to mimic what BoN does during inference. To\nachieve this, we derive the distribution induced by the BoN algorithm. We then\npropose to fine-tune the language model to minimize backward KL divergence to\nthe BoN distribution. Our approach is analogous to mean-field variational\ninference and, thus, we term it variational BoN (vBoN). To the extent this\nfine-tuning is successful and we end up with a good approximation, we have\nreduced the inference cost by a factor of N. Our experiments on controlled\ngeneration and summarization tasks show that BoN is the most effective\nalignment method, and our variational approximation to BoN achieves the closest\nperformance to BoN and surpasses models fine-tuned using the standard\nKL-constrained RL objective. In the controlled generation task, vBoN appears\nmore frequently on the Pareto frontier of reward and KL divergence compared to\nother alignment methods. In the summarization task, vBoN achieves high reward\nvalues across various sampling temperatures.\n","authors":["Afra Amini","Tim Vieira","Elliott Ash","Ryan Cotterell"],"pdf_url":"https://arxiv.org/pdf/2407.06057v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.12460v2","updated":"2025-03-03T10:35:27Z","published":"2024-11-19T12:36:02Z","title":"Exploring Iterative Controllable Summarization with Large Language\n  Models","summary":"  Large language models (LLMs) have demonstrated remarkable performance in\nabstractive summarization tasks. However, their ability to precisely control\nsummary attributes (e.g., length or topic) remains underexplored, limiting\ntheir adaptability to specific user preferences. In this paper, we\nsystematically explore the controllability of LLMs. To this end, we revisit\nsummary attribute measurements and introduce iterative evaluation metrics,\nfailure rate and average iteration count to precisely evaluate controllability\nof LLMs, rather than merely assessing errors. Our findings show that LLMs\nstruggle more with numerical attributes than with linguistic attributes. To\naddress this challenge, we propose a guide-to-explain framework (GTE) for\ncontrollable summarization. Our GTE framework enables the model to identify\nmisaligned attributes in the initial draft and guides it in self-explaining\nerrors in the previous output. By allowing the model to reflect on its\nmisalignment, GTE generates well-adjusted summaries that satisfy the desired\nattributes with robust effectiveness, requiring surprisingly fewer iterations\nthan other iterative approaches.\n","authors":["Sangwon Ryu","Heejin Do","Daehee Kim","Hwanjo Yu","Dongwoo Kim","Yunsu Kim","Gary Geunbae Lee","Jungseul Ok"],"pdf_url":"https://arxiv.org/pdf/2411.12460v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.11355v2","updated":"2025-03-03T09:45:24Z","published":"2025-02-17T02:11:17Z","title":"\"Nuclear Deployed!\": Analyzing Catastrophic Risks in Decision-making of\n  Autonomous LLM Agents","summary":"  Large language models (LLMs) are evolving into autonomous decision-makers,\nraising concerns about catastrophic risks in high-stakes scenarios,\nparticularly in Chemical, Biological, Radiological and Nuclear (CBRN) domains.\nBased on the insight that such risks can originate from trade-offs between the\nagent's Helpful, Harmlessness and Honest (HHH) goals, we build a novel\nthree-stage evaluation framework, which is carefully constructed to effectively\nand naturally expose such risks. We conduct 14,400 agentic simulations across\n12 advanced LLMs, with extensive experiments and analysis. Results reveal that\nLLM agents can autonomously engage in catastrophic behaviors and deception,\nwithout being deliberately induced. Furthermore, stronger reasoning abilities\noften increase, rather than mitigate, these risks. We also show that these\nagents can violate instructions and superior commands. On the whole, we\nempirically prove the existence of catastrophic risks in autonomous LLM agents.\nWe will release our code upon request.\n","authors":["Rongwu Xu","Xiaojian Li","Shuo Chen","Wei Xu"],"pdf_url":"https://arxiv.org/pdf/2502.11355v2.pdf","comment":"Please visit https://llm-catastrophic-risks.github.io for a quick\n  tour of our project"},{"id":"http://arxiv.org/abs/2403.07260v2","updated":"2025-03-03T09:36:14Z","published":"2024-03-12T02:37:11Z","title":"LaERC-S: Improving LLM-based Emotion Recognition in Conversation with\n  Speaker Characteristics","summary":"  Emotion recognition in conversation (ERC), the task of discerning human\nemotions for each utterance within a conversation, has garnered significant\nattention in human-computer interaction systems. Previous ERC studies focus on\nspeaker-specific information that predominantly stems from relationships among\nutterances, which lacks sufficient information around conversations. Recent\nresearch in ERC has sought to exploit pre-trained large language models (LLMs)\nwith speaker modelling to comprehend emotional states. Although these methods\nhave achieved encouraging results, the extracted speaker-specific information\nstruggles to indicate emotional dynamics. In this paper, motivated by the fact\nthat speaker characteristics play a crucial role and LLMs have rich world\nknowledge, we present LaERC-S, a novel framework that stimulates LLMs to\nexplore speaker characteristics involving the mental state and behavior of\ninterlocutors, for accurate emotion predictions. To endow LLMs with this\nknowledge information, we adopt the two-stage learning to make the models\nreason speaker characteristics and track the emotion of the speaker in complex\nconversation scenarios. Extensive experiments on three benchmark datasets\ndemonstrate the superiority of LaERC-S, reaching the new state-of-the-art.\n","authors":["Yumeng Fu","Junjie Wu","Zhongjie Wang","Meishan Zhang","Lili Shan","Yulin Wu","Bingquan Li"],"pdf_url":"https://arxiv.org/pdf/2403.07260v2.pdf","comment":"COLING 2025"},{"id":"http://arxiv.org/abs/2402.09911v2","updated":"2025-03-03T09:21:11Z","published":"2024-02-15T12:20:02Z","title":"Enhancing Large Language Models with Pseudo- and Multisource- Knowledge\n  Graphs for Open-ended Question Answering","summary":"  Mitigating the hallucinations of Large Language Models is a crucial task.\nAlthough some existing methods employ self-enhancement techniques, they fall\nshort of effectively addressing unknown factual hallucinations. Meanwhile,\nKnowledge Graph (KG) enhancement approaches fail to address the generalization\nacross different KG sources and the enhancement of open-ended answer questions\nsimultaneously. To tackle these limitations, we propose a framework that\ncombines Pseudo-Graph Generation and Atomic Knowledge Verification (PG\\&AKV).\nEnhancement of open-ended question-answering begins with leveraging the\nPseudo-Graph Generation to provide the related knowledge framework.\nSubsequently, Atomic Knowledge Verification utilizes atomic-level knowledge\nquerying and verification to achieve generalizability under different KG\nsources. Compared to the baseline, this approach yields a minimum improvement\nof 11.5 in the ROUGE-L score for open-ended questions. For precise-answered\nquestions, we observe a minimum accuracy improvement of 7.5%. Moreover, PG\\&AKV\nalso exhibits generalizability across different KG sources. Utilizing KG\ndifferent from the question sources, PG\\&AKV can even achieve at least a 3.5 %\nperformance improvement. In summary, our results pave the way for enhancing\nLLMs by incorporating Pseudo- and Multisource-KGs, particularly in the filed of\nopen-ended questions.\n","authors":["Jiaxiang Liu","Tong Zhou","Yubo Chen","Kang Liu","Jun Zhao"],"pdf_url":"https://arxiv.org/pdf/2402.09911v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.21228v2","updated":"2025-03-03T09:11:46Z","published":"2025-02-28T16:59:30Z","title":"ECLeKTic: a Novel Challenge Set for Evaluation of Cross-Lingual\n  Knowledge Transfer","summary":"  To achieve equitable performance across languages, multilingual large\nlanguage models (LLMs) must be able to abstract knowledge beyond the language\nin which it was acquired. However, the current literature lacks reliable ways\nto measure LLMs' capability of cross-lingual knowledge transfer. To that end,\nwe present ECLeKTic, a multilingual closed-book QA (CBQA) dataset that\nEvaluates Cross-Lingual Knowledge Transfer in a simple, black-box manner. We\ndetected information with uneven coverage across languages by controlling for\npresence and absence of Wikipedia articles in 12 languages. We generated\nknowledge-seeking questions in a source language, for which the answer appears\nin a relevant Wikipedia article and translated them to all other 11 languages,\nfor which the respective Wikipedias lack equivalent articles. Assuming that\nWikipedia reflects the prominent knowledge in the LLM's training data, to solve\nECLeKTic's CBQA task the model is required to transfer knowledge between\nlanguages. Experimenting with 8 LLMs, we show that SOTA models struggle to\neffectively share knowledge across, languages even if they can predict the\nanswer well for queries in the same language the knowledge was acquired in.\n","authors":["Omer Goldman","Uri Shaham","Dan Malkin","Sivan Eiger","Avinatan Hassidim","Yossi Matias","Joshua Maynez","Adi Mayrav Gilady","Jason Riesa","Shruti Rijhwani","Laura Rimell","Idan Szpektor","Reut Tsarfaty","Matan Eyal"],"pdf_url":"https://arxiv.org/pdf/2502.21228v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.11167v2","updated":"2025-03-03T08:26:12Z","published":"2025-02-16T15:38:19Z","title":"SURGE: On the Potential of Large Language Models as General-Purpose\n  Surrogate Code Executors","summary":"  Neural surrogate models have emerged as powerful and efficient tools in data\nmining. Meanwhile, large language models (LLMs) have demonstrated remarkable\ncapabilities in code-related tasks. We investigate a novel application: using\nLLMs as surrogate models for code execution prediction. Given LLMs' unique\nability to understand and process diverse programs, they present a promising\ndirection for building general-purpose surrogate models. To systematically\ninvestigate this capability, we introduce SURGE, a comprehensive benchmark with\n$1160$ problems covering $8$ key aspects: multi-language programming tasks,\ncompetition-level programming problems, repository-level code analysis,\nhigh-cost scientific computing, time-complexity-intensive algorithms, buggy\ncode analysis, programs dependent on specific compilers or execution\nenvironments, and formal mathematical proof verification. Through extensive\nempirical analysis of $21$ open-source and proprietary LLMs, we examine scaling\nlaws, data efficiency, and predictive accuracy. Our findings reveal important\ninsights about the feasibility of LLMs as efficient surrogates for\ncomputational processes, with implications for automated software testing,\nprogram analysis, and computational resource optimization in data mining\napplications. Code and dataset are released at\nhttps://github.com/Imbernoulli/SURGE.\n","authors":["Bohan Lyu","Siqiao Huang","Zichen Liang"],"pdf_url":"https://arxiv.org/pdf/2502.11167v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.19316v2","updated":"2025-03-03T08:22:25Z","published":"2024-05-29T17:39:48Z","title":"Robust Preference Optimization through Reward Model Distillation","summary":"  Language model (LM) post-training (or alignment) involves maximizing a reward\nfunction that is derived from preference annotations. Direct Preference\nOptimization (DPO) is a popular offline alignment method that trains a policy\ndirectly on preference data without the need to train a reward model or apply\nreinforcement learning. However, the empirical evidence suggests that DPO\ntypically assigns implicit rewards that overfit, and trend towards infinite\nmagnitude. This frequently leads to degenerate policies, sometimes causing even\nthe probabilities of the preferred generations to go to zero. In this work, we\nanalyze this phenomenon and use distillation to get a better proxy for the true\npreference distribution over generation pairs: we train the LM such that its\ninduced implicit reward, i.e., the scaled log-likelihood ratio of the model to\nthe reference model, matches an explicit reward model trained on the preference\ndata. Moreover, to account for uncertainty in the reward model we are\ndistilling from, we optimize against a family of reward models that, as a\nwhole, is likely to include at least one reasonable proxy for the preference\ndistribution. Our results show that distilling from such a family of reward\nmodels leads to improved robustness to distribution shift in preference\nannotations, while preserving the simple supervised nature of DPO.\n","authors":["Adam Fisch","Jacob Eisenstein","Vicky Zayats","Alekh Agarwal","Ahmad Beirami","Chirag Nagpal","Pete Shaw","Jonathan Berant"],"pdf_url":"https://arxiv.org/pdf/2405.19316v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.19651v3","updated":"2025-03-03T07:49:17Z","published":"2023-10-30T15:37:10Z","title":"Dynamics of Instruction Fine-Tuning for Chinese Large Language Models","summary":"  Instruction tuning is a burgeoning method to elicit the general intelligence\nof Large Language Models (LLMs). While numerous studies have examined the\nimpact of factors such as data volume and model size on English models, the\nscaling properties of instruction tuning in other languages remain largely\nunexplored. In this work, we systematically investigate the effects of data\nquantity, model size, and data construction methods on instruction tuning for\nChinese LLMs. We utilize a newly curated dataset, DoIT, which includes over\n40,000 high-quality instruction instances covering ten underlying abilities,\nsuch as creative writing, code generation, and logical reasoning. Our\nexperiments, conducted on models ranging from 7b to 33b parameters, yield three\nkey findings: (i) While these factors directly affect overall model\nperformance, some abilities are more responsive to scaling, whereas others\ndemonstrate significant resistance. (ii) The scaling sensitivity of different\nabilities to these factors can be explained by two features: Complexity and\nTransference. (iii) By tailoring training strategies to their varying\nsensitivities, specific abilities can be efficiently learned, enhancing\nperformance on two public benchmarks.\n","authors":["Chiyu Song","Zhanchao Zhou","Jianhao Yan","Yuejiao Fei","Zhenzhong Lan","Yue Zhang"],"pdf_url":"https://arxiv.org/pdf/2310.19651v3.pdf","comment":"Accepted to COLING 2025"},{"id":"http://arxiv.org/abs/2406.14434v3","updated":"2025-03-03T07:36:49Z","published":"2024-06-20T15:59:07Z","title":"Selected Languages are All You Need for Cross-lingual Truthfulness\n  Transfer","summary":"  Truthfulness stands out as an essential challenge for Large Language Models\n(LLMs). Although many works have developed various ways for truthfulness\nenhancement, they seldom focus on truthfulness in multilingual scenarios.\nMeanwhile, contemporary multilingual aligning technologies struggle to balance\nnumerous languages and often exhibit serious truthfulness gaps across different\nlanguages, especially those that differ greatly from English. In our work, we\nextend truthfulness evaluation to multilingual contexts and propose a practical\nmethod for cross-lingual truthfulness transfer called Fact-aware Multilingual\nSelective Synergy (FaMSS). FaMSS is able to select an optimal subset of all\ntested languages by language bias and transfer contributions, and then employ\ntranslation instruction tuning for cross-lingual truthfulness transfer.\nExperimental results demonstrate that our approach can effectively reduce the\nmultilingual representation disparity and boost cross-lingual truthfulness\ntransfer of LLMs.\n","authors":["Weihao Liu","Ning Wu","Wenbiao Ding","Shining Liang","Ming Gong","Dongmei Zhang"],"pdf_url":"https://arxiv.org/pdf/2406.14434v3.pdf","comment":"16 pages, COLING2025"},{"id":"http://arxiv.org/abs/2409.16644v2","updated":"2025-03-03T07:22:54Z","published":"2024-09-25T05:44:44Z","title":"Enabling Auditory Large Language Models for Automatic Speech Quality\n  Evaluation","summary":"  Speech quality assessment typically requires evaluating audio from multiple\naspects, such as mean opinion score (MOS) and speaker similarity (SIM) \\etc.,\nwhich can be challenging to cover using one small model designed for a single\ntask. In this paper, we propose leveraging recently introduced auditory large\nlanguage models (LLMs) for automatic speech quality assessment. By employing\ntask-specific prompts, auditory LLMs are finetuned to predict MOS, SIM and A/B\ntesting results, which are commonly used for evaluating text-to-speech systems.\nAdditionally, the finetuned auditory LLM is able to generate natural language\ndescriptions assessing aspects like noisiness, distortion, discontinuity, and\noverall quality, providing more interpretable outputs. Extensive experiments\nhave been performed on the NISQA, BVCC, SOMOS and VoxSim speech quality\ndatasets, using open-source auditory LLMs such as SALMONN, Qwen-Audio, and\nQwen2-Audio. For the natural language descriptions task, a commercial model\nGoogle Gemini 1.5 Pro is also evaluated. The results demonstrate that auditory\nLLMs achieve competitive performance compared to state-of-the-art task-specific\nsmall models in predicting MOS and SIM, while also delivering promising results\nin A/B testing and natural language descriptions. Our data processing scripts\nand finetuned model checkpoints can be found at\nhttps://github.com/bytedance/SALMONN.\n","authors":["Siyin Wang","Wenyi Yu","Yudong Yang","Changli Tang","Yixuan Li","Jimin Zhuang","Xianzhao Chen","Xiaohai Tian","Jun Zhang","Guangzhi Sun","Lu Lu","Chao Zhang"],"pdf_url":"https://arxiv.org/pdf/2409.16644v2.pdf","comment":"Accepted by ICASSP 2025"},{"id":"http://arxiv.org/abs/2410.02683v2","updated":"2025-03-03T07:20:54Z","published":"2024-10-03T17:08:52Z","title":"DailyDilemmas: Revealing Value Preferences of LLMs with Quandaries of\n  Daily Life","summary":"  As users increasingly seek guidance from LLMs for decision-making in daily\nlife, many of these decisions are not clear-cut and depend significantly on the\npersonal values and ethical standards of people. We present DailyDilemmas, a\ndataset of 1,360 moral dilemmas encountered in everyday life. Each dilemma\npresents two possible actions, along with affected parties and relevant human\nvalues for each action. Based on these dilemmas, we gather a repository of\nhuman values covering diverse everyday topics, such as interpersonal\nrelationships, workplace, and environmental issues. With DailyDilemmas, we\nevaluate LLMs on these dilemmas to determine what action they will choose and\nthe values represented by these action choices. Then, we analyze values through\nthe lens of five theoretical frameworks inspired by sociology, psychology, and\nphilosophy, including the World Values Survey, Moral Foundations Theory,\nMaslow's Hierarchy of Needs, Aristotle's Virtues, and Plutchik's Wheel of\nEmotions. For instance, we find LLMs are most aligned with self-expression over\nsurvival in World Values Survey and care over loyalty in Moral Foundations\nTheory. Interestingly, we find substantial preference differences in models for\nsome core values. For example, for truthfulness, Mixtral-8x7B neglects it by\n9.7% while GPT-4-turbo selects it by 9.4%. We also study the recent guidance\nreleased by OpenAI (ModelSpec), and Anthropic (Constitutional AI) to understand\nhow their designated principles reflect their models' actual value\nprioritization when facing nuanced moral reasoning in daily-life settings.\nFinally, we find that end users cannot effectively steer such prioritization\nusing system prompts.\n","authors":["Yu Ying Chiu","Liwei Jiang","Yejin Choi"],"pdf_url":"https://arxiv.org/pdf/2410.02683v2.pdf","comment":"Accepted into ICLR 2025 (spotlight)"},{"id":"http://arxiv.org/abs/2501.02497v2","updated":"2025-03-03T07:16:16Z","published":"2025-01-05T10:24:20Z","title":"Test-Time Compute: from System-1 Thinking to System-2 Thinking","summary":"  The remarkable performance of the o1 model in complex reasoning demonstrates\nthat test-time compute scaling can further unlock the model's potential,\nenabling powerful System-2 thinking. However, there is still a lack of\ncomprehensive surveys for test-time compute scaling. We trace the concept of\ntest-time compute back to System-1 models. In System-1 models, test-time\ncompute addresses distribution shifts and improves robustness and\ngeneralization through parameter updating, input modification, representation\nediting, and output calibration. In System-2 models, it enhances the model's\nreasoning ability to solve complex problems through repeated sampling,\nself-correction, and tree search. We organize this survey according to the\ntrend of System-1 to System-2 thinking, highlighting the key role of test-time\ncompute in the transition from System-1 models to weak System-2 models, and\nthen to strong System-2 models. We also point out a few possible future\ndirections.\n","authors":["Yixin Ji","Juntao Li","Hai Ye","Kaixin Wu","Kai Yao","Jia Xu","Linjian Mo","Min Zhang"],"pdf_url":"https://arxiv.org/pdf/2501.02497v2.pdf","comment":"work in progress"},{"id":"http://arxiv.org/abs/2502.18874v2","updated":"2025-03-03T07:13:12Z","published":"2025-02-26T06:31:45Z","title":"Learning to Align Multi-Faceted Evaluation: A Unified and Robust\n  Framework","summary":"  Large Language Models (LLMs) are being used more and more extensively for\nautomated evaluation in various scenarios. Previous studies have attempted to\nfine-tune open-source LLMs to replicate the evaluation explanations and\njudgments of powerful proprietary models, such as GPT-4. However, these methods\nare largely limited to text-based analyses under predefined general criteria,\nresulting in reduced adaptability for unseen instructions and demonstrating\ninstability in evaluating adherence to quantitative and structural constraints.\nTo address these limitations, we propose a novel evaluation framework, ARJudge,\nthat adaptively formulates evaluation criteria and synthesizes both text-based\nand code-driven analyses to evaluate LLM responses. ARJudge consists of two\ncomponents: a fine-tuned Analyzer that generates multi-faceted evaluation\nanalyses and a tuning-free Refiner that combines and refines all analyses to\nmake the final judgment. We construct a Composite Analysis Corpus that\nintegrates tasks for evaluation criteria generation alongside text-based and\ncode-driven analysis generation to train the Analyzer. Our results demonstrate\nthat ARJudge outperforms existing fine-tuned evaluators in effectiveness and\nrobustness. Furthermore, it demonstrates the importance of multi-faceted\nevaluation and code-driven analyses in enhancing evaluation capabilities.\n","authors":["Kaishuai Xu","Tiezheng Yu","Wenjun Hou","Yi Cheng","Liangyou Li","Xin Jiang","Lifeng Shang","Qun Liu","Wenjie Li"],"pdf_url":"https://arxiv.org/pdf/2502.18874v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.06638v3","updated":"2025-03-03T07:09:42Z","published":"2024-10-09T07:43:38Z","title":"Subtle Errors Matter: Preference Learning via Error-injected\n  Self-editing","summary":"  Large Language Models (LLMs) have exhibited strong mathematical reasoning\nprowess, tackling tasks ranging from basic arithmetic to advanced\ncompetition-level problems. However, frequently occurring subtle yet critical\nerrors, such as miscalculations or incorrect substitutions, limit the LLMs'\nfull potential. Existing studies to improve mathematical ability typically\ninvolve applying preference learning to step-wise solution pairs. Although\nthese methods leverage samples of varying granularity to mitigate reasoning\nerrors, they overlook critical subtle errors. In this work, we propose a novel\npreference learning framework called eRror-Injected Self-Editing (RISE), which\ninjects predefined subtle errors into pivotal tokens in reasoning or\ncomputation steps to construct hard pairs for error mitigation. In detail, RISE\nuses the LLM itself to edit a small number of tokens in the solution, injecting\ndesigned subtle errors. Then, pairs composed of self-edited solutions and their\ncorresponding correct ones, along with pairs of correct and incorrect solutions\nobtained through sampling, are used together for subtle error-aware DPO\ntraining. Compared with other preference learning methods, RISE further refines\nthe training objective without requiring fine-grained sampling or preference\nannotation. Extensive experiments validate the effectiveness of RISE, with\npreference learning on Qwen2-7B-Instruct yielding notable improvements of 3.0%\non GSM8K and 7.9% on MATH with only 4.5K training samples. Moreover, the effect\nof error mitigation extends from mathematical reasoning to logical reasoning\nand code generation.\n","authors":["Kaishuai Xu","Tiezheng Yu","Wenjun Hou","Yi Cheng","Chak Tou Leong","Liangyou Li","Xin Jiang","Lifeng Shang","Qun Liu","Wenjie Li"],"pdf_url":"https://arxiv.org/pdf/2410.06638v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.07170v3","updated":"2025-03-03T07:02:20Z","published":"2024-09-11T10:33:30Z","title":"Learning Efficient Recursive Numeral Systems via Reinforcement Learning","summary":"  It has previously been shown that by using reinforcement learning (RL),\nagents can derive simple approximate and exact-restricted numeral systems that\nare similar to human ones (Carlsson, 2021). However, it is a major challenge to\nshow how more complex recursive numeral systems, similar to for example\nEnglish, could arise via a simple learning mechanism such as RL. Here, we\nintroduce an approach towards deriving a mechanistic explanation of the\nemergence of efficient recursive number systems. We consider pairs of agents\nlearning how to communicate about numerical quantities through a meta-grammar\nthat can be gradually modified throughout the interactions. %We find that the\nseminal meta-grammar of Hurford (Hurford, 1975) is not suitable for this\napplication as its optimization results in systems that deviate from standard\nconventions observed within human numeral systems. We propose a simple\nmodification which addresses this issue. Utilising a slightly modified version\nof the meta-grammar of Hurford, we demonstrate that our RL agents, shaped by\nthe pressures for efficient communication, can effectively modify their lexicon\ntowards Pareto-optimal configurations which are comparable to those observed\nwithin human numeral systems in terms of their efficiency.\n","authors":["Andrea Silvi","Jonathan Thomas","Emil Carlsson","Devdatt Dubhashi","Moa Johansson"],"pdf_url":"https://arxiv.org/pdf/2409.07170v3.pdf","comment":"8 pages, 5 figures"},{"id":"http://arxiv.org/abs/2407.04752v2","updated":"2025-03-03T06:46:33Z","published":"2024-07-05T08:37:17Z","title":"SpikeLLM: Scaling up Spiking Neural Network to Large Language Models via\n  Saliency-based Spiking","summary":"  Recent advancements in large language models (LLMs) with billions of\nparameters have improved performance in various applications, but their\ninference processes demand significant energy and computational resources. In\ncontrast, the human brain, with approximately 86 billion neurons, is much more\nenergy-efficient than LLMs with similar parameters. Inspired by this, we\nredesign 7$\\sim$70 billion parameter LLMs using bio-plausible spiking\nmechanisms, emulating the efficient behavior of the human brain. We propose the\nfirst spiking large language model, SpikeLLM. Coupled with the proposed model,\ntwo essential approaches are proposed to improve spike training efficiency:\nGeneralized Integrate-and-Fire (GIF) neurons to compress spike length from $T$\nto $\\frac{T}{L} \\log_2 L$ bits, and an Optimal Brain Spiking framework to\ndivide outlier channels and allocate different $T$ for GIF neurons, which\nfurther compresses spike length to approximate $log_2T$ bits. The necessity of\nspike-driven LLM is proved by comparison with quantized LLMs with similar\noperations. In the OmniQuant pipeline, SpikeLLM reduces 11.01% WikiText2\nperplexity and improves 2.55% accuracy of common scene reasoning on a LLAMA-7B\nW4A4 model. In the GPTQ pipeline, SpikeLLM achieves direct additive in linear\nlayers, significantly exceeding PB-LLMs.\n","authors":["Xingrun Xing","Boyan Gao","Zheng Zhang","David A. Clifton","Shitao Xiao","Li Du","Guoqi Li","Jiajun Zhang"],"pdf_url":"https://arxiv.org/pdf/2407.04752v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.18945v4","updated":"2025-03-03T06:34:48Z","published":"2024-02-29T08:20:49Z","title":"SynGhost: Invisible and Universal Task-agnostic Backdoor Attack via\n  Syntactic Transfer","summary":"  Although pre-training achieves remarkable performance, it suffers from\ntask-agnostic backdoor attacks due to vulnerabilities in data and training\nmechanisms. These attacks can transfer backdoors to various downstream tasks.\nIn this paper, we introduce $\\mathtt{maxEntropy}$, an entropy-based poisoning\nfilter that mitigates such risks. To overcome the limitations of manual target\nsetting and explicit triggers, we propose $\\mathtt{SynGhost}$, an invisible and\nuniversal task-agnostic backdoor attack via syntactic transfer, further\nexposing vulnerabilities in pre-trained language models (PLMs). Specifically,\n$\\mathtt{SynGhost}$ injects multiple syntactic backdoors into the pre-training\nspace through corpus poisoning, while preserving the PLM's pre-training\ncapabilities. Second, $\\mathtt{SynGhost}$ adaptively selects optimal targets\nbased on contrastive learning, creating a uniform distribution in the\npre-training space. To identify syntactic differences, we also introduce an\nawareness module to minimize interference between backdoors. Experiments show\nthat $\\mathtt{SynGhost}$ poses significant threats and can transfer to various\ndownstream tasks. Furthermore, $\\mathtt{SynGhost}$ resists defenses based on\nperplexity, fine-pruning, and $\\mathtt{maxEntropy}$. The code is available at\nhttps://github.com/Zhou-CyberSecurity-AI/SynGhost.\n","authors":["Pengzhou Cheng","Wei Du","Zongru Wu","Fengwei Zhang","Libo Chen","Zhuosheng Zhang","Gongshen Liu"],"pdf_url":"https://arxiv.org/pdf/2402.18945v4.pdf","comment":"17 pages, 16 figures, 12 tables, accepted at NAACL 2025 Findings"},{"id":"http://arxiv.org/abs/2412.07298v2","updated":"2025-03-03T06:33:49Z","published":"2024-12-10T08:28:57Z","title":"The Rise and Down of Babel Tower: Investigating the Evolution Process of\n  Multilingual Code Large Language Model","summary":"  Large language models (LLMs) have shown significant multilingual\ncapabilities. However, the mechanisms underlying the development of these\ncapabilities during pre-training are not well understood. In this paper, we use\ncode LLMs as an experimental platform to explore the evolution of multilingual\ncapabilities in LLMs during the pre-training process. Based on our\nobservations, we propose the Babel Tower Hypothesis, which describes the entire\nprocess of LLMs acquiring new language capabilities. During the learning\nprocess, multiple languages initially share a single knowledge system dominated\nby the primary language and gradually develop language-specific knowledge\nsystems. We then validate the above hypothesis by tracking the internal states\nof the LLMs through identifying working languages and language transferring\nneurons. Experimental results show that the internal state changes of the LLM\nare consistent with our Babel Tower Hypothesis. Building on these insights, we\npropose a novel method to construct an optimized pre-training corpus for\nmultilingual code LLMs, which significantly outperforms LLMs trained on the\noriginal corpus. The proposed Babel Tower Hypothesis provides new insights into\ndesigning pre-training data distributions to achieve optimal multilingual\ncapabilities in LLMs.\n","authors":["Jiawei Chen","Wentao Chen","Jing Su","Jingjing Xu","Hongyu Lin","Mengjie Ren","Yaojie Lu","Xianpei Han","Le Sun"],"pdf_url":"https://arxiv.org/pdf/2412.07298v2.pdf","comment":"Accepted to ICLR 2025"},{"id":"http://arxiv.org/abs/2502.17204v2","updated":"2025-03-03T06:29:31Z","published":"2025-02-24T14:39:28Z","title":"Order Matters: Investigate the Position Bias in Multi-constraint\n  Instruction Following","summary":"  Real-world instructions with multiple constraints pose a significant\nchallenge to existing large language models (LLMs). An observation is that the\nLLMs exhibit dramatic performance fluctuation when disturbing the order of the\nincorporated constraints. Yet, none of the existing works has systematically\ninvestigated this position bias problem in the field of multi-constraint\ninstruction following. To bridge this gap, we design a probing task where we\nquantitatively measure the difficulty distribution of the constraints by a\nnovel Difficulty Distribution Index (CDDI). Through the experimental results,\nwe find that LLMs are more performant when presented with the constraints in a\n``hard-to-easy'' order. This preference can be generalized to LLMs with\ndifferent architecture or different sizes of parameters. Additionally, we\nconduct an explanation study, providing an intuitive insight into the\ncorrelation between the LLM's attention and constraint orders. Our code and\ndataset are publicly available at https://github.com/meowpass/PBIF.\n","authors":["Jie Zeng","Qianyu He","Qingyu Ren","Jiaqing Liang","Yanghua Xiao","Weikang Zhou","Zeye Sun","Fei Yu"],"pdf_url":"https://arxiv.org/pdf/2502.17204v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.01405v4","updated":"2025-03-03T06:14:14Z","published":"2023-10-02T17:59:07Z","title":"Representation Engineering: A Top-Down Approach to AI Transparency","summary":"  In this paper, we identify and characterize the emerging area of\nrepresentation engineering (RepE), an approach to enhancing the transparency of\nAI systems that draws on insights from cognitive neuroscience. RepE places\npopulation-level representations, rather than neurons or circuits, at the\ncenter of analysis, equipping us with novel methods for monitoring and\nmanipulating high-level cognitive phenomena in deep neural networks (DNNs). We\nprovide baselines and an initial analysis of RepE techniques, showing that they\noffer simple yet effective solutions for improving our understanding and\ncontrol of large language models. We showcase how these methods can provide\ntraction on a wide range of safety-relevant problems, including honesty,\nharmlessness, power-seeking, and more, demonstrating the promise of top-down\ntransparency research. We hope that this work catalyzes further exploration of\nRepE and fosters advancements in the transparency and safety of AI systems.\n","authors":["Andy Zou","Long Phan","Sarah Chen","James Campbell","Phillip Guo","Richard Ren","Alexander Pan","Xuwang Yin","Mantas Mazeika","Ann-Kathrin Dombrowski","Shashwat Goel","Nathaniel Li","Michael J. Byun","Zifan Wang","Alex Mallen","Steven Basart","Sanmi Koyejo","Dawn Song","Matt Fredrikson","J. Zico Kolter","Dan Hendrycks"],"pdf_url":"https://arxiv.org/pdf/2310.01405v4.pdf","comment":"Code is available at\n  https://github.com/andyzoujm/representation-engineering"},{"id":"http://arxiv.org/abs/2411.02886v2","updated":"2025-03-03T05:49:41Z","published":"2024-11-05T07:56:24Z","title":"TokenSelect: Efficient Long-Context Inference and Length Extrapolation\n  for LLMs via Dynamic Token-Level KV Cache Selection","summary":"  The rapid advancement of Large Language Models (LLMs) has driven growing\ndemand for processing extended context sequences in contemporary applications.\nHowever, this progress faces two major challenges: performance degradation due\nto sequence lengths out-of-distribution, and excessively long inference times\ncaused by the quadratic computational complexity of attention. These issues\nhinder the application of LLMs in long-context scenarios. In this paper, we\npropose Dynamic Token-Level KV Cache Selection (TokenSelect), a training-free\nmethod for efficient and accurate long-context inference. TokenSelect builds\nupon the observation of non-contiguous attention sparsity, using Query-Key dot\nproducts to measure per-head KV Cache criticality at token-level. By per-head\nsoft voting mechanism, TokenSelect selectively involves a few critical KV cache\ntokens in attention calculation without sacrificing accuracy. To further\naccelerate TokenSelect, we design the Selection Cache based on observations of\nconsecutive Query similarity and implemented efficient dot product kernel,\nsignificantly reducing the overhead. A comprehensive evaluation of TokenSelect\ndemonstrates up to 23.84x speedup in attention computation and up to 2.28x\nacceleration in end-to-end latency, while providing superior performance\ncompared to state-of-the-art long-context inference methods.\n","authors":["Wei Wu","Zhuoshi Pan","Chao Wang","Liyi Chen","Yunchu Bai","Tianfu Wang","Kun Fu","Zheng Wang","Hui Xiong"],"pdf_url":"https://arxiv.org/pdf/2411.02886v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.14171v3","updated":"2025-03-03T05:44:29Z","published":"2025-02-20T00:39:05Z","title":"Enhancing Conversational Agents with Theory of Mind: Aligning Beliefs,\n  Desires, and Intentions for Human-Like Interaction","summary":"  Natural language interaction with agentic Artificial Intelligence (AI),\ndriven by Large Language Models (LLMs), is expected to remain a dominant\nparadigm in the near future. While humans instinctively align their\ncommunication with mental states -- an ability known as Theory of Mind (ToM),\ncurrent LLM powered systems exhibit significant limitations in this regard.\nThis study examines the extent to which open source language models (LLaMA) can\ncapture and preserve ToM related information and how effectively it contributes\nto consistent ToM reasoning in generated responses. We further investigate\nwhether explicit manipulation of ToM related components, such as beliefs,\ndesires, and intentions, can enhance response alignment. Experiments on two\nLLaMA 3 variants demonstrate that incorporating ToM informed alignment improves\nresponse quality, achieving win rates of 67 and 63 percent for the 3B and 8B\nmodels, respectively. These findings highlight the potential of ToM driven\nstrategies to improve alignment in LLM based conversational agents.\n","authors":["Mehdi Jafari","Devin Yuncheng Hua","Hao Xue","Flora Salim"],"pdf_url":"https://arxiv.org/pdf/2502.14171v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.02268v3","updated":"2025-03-03T05:32:47Z","published":"2024-10-03T07:40:14Z","title":"Structural-Entropy-Based Sample Selection for Efficient and Effective\n  Learning","summary":"  Sample selection improves the efficiency and effectiveness of machine\nlearning models by providing informative and representative samples. Typically,\nsamples can be modeled as a sample graph, where nodes are samples and edges\nrepresent their similarities. Most existing methods are based on local\ninformation, such as the training difficulty of samples, thereby overlooking\nglobal information, such as connectivity patterns. This oversight can result in\nsuboptimal selection because global information is crucial for ensuring that\nthe selected samples well represent the structural properties of the graph. To\naddress this issue, we employ structural entropy to quantify global information\nand losslessly decompose it from the whole graph to individual nodes using the\nShapley value. Based on the decomposition, we present\n$\\textbf{S}$tructural-$\\textbf{E}$ntropy-based sample $\\textbf{S}$election\n($\\textbf{SES}$), a method that integrates both global and local information to\nselect informative and representative samples. SES begins by constructing a\n$k$NN-graph among samples based on their similarities. It then measures sample\nimportance by combining structural entropy (global metric) with training\ndifficulty (local metric). Finally, SES applies importance-biased blue noise\nsampling to select a set of diverse and representative samples. Comprehensive\nexperiments on three learning scenarios -- supervised learning, active\nlearning, and continual learning -- clearly demonstrate the effectiveness of\nour method.\n","authors":["Tianchi Xie","Jiangning Zhu","Guozu Ma","Minzhi Lin","Wei Chen","Weikai Yang","Shixia Liu"],"pdf_url":"https://arxiv.org/pdf/2410.02268v3.pdf","comment":"Published as a conference paper at ICLR 2025"},{"id":"http://arxiv.org/abs/2502.17720v2","updated":"2025-03-03T04:31:48Z","published":"2025-02-24T23:23:27Z","title":"Spontaneous Giving and Calculated Greed in Language Models","summary":"  Large language models demonstrate advanced problem-solving capabilities by\nincorporating reasoning techniques such as chain of thought and reflection.\nHowever, how these reasoning capabilities extend to social intelligence remains\nunclear. In this study, we investigate this question using economic games that\nmodel social dilemmas, where social intelligence plays a crucial role. First,\nwe examine the effects of chain-of-thought and reflection techniques in a\npublic goods game. We then extend our analysis to six economic games on\ncooperation and punishment, comparing off-the-shelf non-reasoning and reasoning\nmodels. We find that reasoning models significantly reduce cooperation and norm\nenforcement, prioritizing individual rationality. Consequently, groups with\nmore reasoning models exhibit less cooperation and lower gains through repeated\ninteractions. These behaviors parallel human tendencies of \"spontaneous giving\nand calculated greed.\" Our results suggest the need for AI architectures that\nincorporate social intelligence alongside reasoning capabilities to ensure that\nAI supports, rather than disrupts, human cooperative intuition.\n","authors":["Yuxuan Li","Hirokazu Shirado"],"pdf_url":"https://arxiv.org/pdf/2502.17720v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.09906v3","updated":"2025-03-03T04:28:49Z","published":"2024-02-15T12:12:19Z","title":"Generative Representational Instruction Tuning","summary":"  All text-based language problems can be reduced to either generation or\nembedding. Current models only perform well at one or the other. We introduce\ngenerative representational instruction tuning (GRIT) whereby a large language\nmodel is trained to handle both generative and embedding tasks by\ndistinguishing between them through instructions. Compared to other open\nmodels, our resulting GritLM 7B sets a new state of the art on the Massive Text\nEmbedding Benchmark (MTEB) and outperforms all models up to its size on a range\nof generative tasks. By scaling up further, GritLM 8x7B outperforms all open\ngenerative language models that we tried while still being among the best\nembedding models. Notably, we find that GRIT matches training on only\ngenerative or embedding data, thus we can unify both at no performance loss.\nAmong other benefits, the unification via GRIT speeds up Retrieval-Augmented\nGeneration (RAG) by > 60% for long documents, by no longer requiring separate\nretrieval and generation models. Models, code, etc. are freely available at\nhttps://github.com/ContextualAI/gritlm.\n","authors":["Niklas Muennighoff","Hongjin Su","Liang Wang","Nan Yang","Furu Wei","Tao Yu","Amanpreet Singh","Douwe Kiela"],"pdf_url":"https://arxiv.org/pdf/2402.09906v3.pdf","comment":"67 pages (16 main), 25 figures, 34 tables"},{"id":"http://arxiv.org/abs/2405.16821v3","updated":"2025-03-03T04:25:41Z","published":"2024-05-27T04:40:56Z","title":"Perturbation-Restrained Sequential Model Editing","summary":"  Model editing is an emerging field that focuses on updating the knowledge\nembedded within large language models (LLMs) without extensive retraining.\nHowever, current model editing methods significantly compromise the general\nabilities of LLMs as the number of edits increases, and this trade-off poses a\nsubstantial challenge to the continual learning of LLMs. In this paper, we\nfirst theoretically analyze that the factor affecting the general abilities in\nsequential model editing lies in the condition number of the edited matrix. The\ncondition number of a matrix represents its numerical sensitivity, and\ntherefore can be used to indicate the extent to which the original knowledge\nassociations stored in LLMs are perturbed after editing. Subsequently,\nstatistical findings demonstrate that the value of this factor becomes larger\nas the number of edits increases, thereby exacerbating the deterioration of\ngeneral abilities. To this end, a framework termed Perturbation Restraint on\nUpper bouNd for Editing (PRUNE) is proposed, which applies the condition number\nrestraints in sequential editing. These restraints can lower the upper bound on\nperturbation to edited models, thus preserving the general abilities.\nSystematically, we conduct experiments employing three editing methods on three\nLLMs across four downstream tasks. The results show that PRUNE can preserve\ngeneral abilities while maintaining the editing performance effectively in\nsequential model editing. The code are available at\nhttps://github.com/mjy1111/PRUNE.\n","authors":["Jun-Yu Ma","Hong Wang","Hao-Xiang Xu","Zhen-Hua Ling","Jia-Chen Gu"],"pdf_url":"https://arxiv.org/pdf/2405.16821v3.pdf","comment":"Accepted by ICLR 2025"},{"id":"http://arxiv.org/abs/2502.12110v2","updated":"2025-03-03T04:14:02Z","published":"2025-02-17T18:36:14Z","title":"A-MEM: Agentic Memory for LLM Agents","summary":"  While large language model (LLM) agents can effectively use external tools\nfor complex real-world tasks, they require memory systems to leverage\nhistorical experiences. Current memory systems enable basic storage and\nretrieval but lack sophisticated memory organization, despite recent attempts\nto incorporate graph databases. Moreover, these systems' fixed operations and\nstructures limit their adaptability across diverse tasks. To address this\nlimitation, this paper proposes a novel agentic memory system for LLM agents\nthat can dynamically organize memories in an agentic way. Following the basic\nprinciples of the Zettelkasten method, we designed our memory system to create\ninterconnected knowledge networks through dynamic indexing and linking. When a\nnew memory is added, we generate a comprehensive note containing multiple\nstructured attributes, including contextual descriptions, keywords, and tags.\nThe system then analyzes historical memories to identify relevant connections,\nestablishing links where meaningful similarities exist. Additionally, this\nprocess enables memory evolution - as new memories are integrated, they can\ntrigger updates to the contextual representations and attributes of existing\nhistorical memories, allowing the memory network to continuously refine its\nunderstanding. Our approach combines the structured organization principles of\nZettelkasten with the flexibility of agent-driven decision making, allowing for\nmore adaptive and context-aware memory management. Empirical experiments on six\nfoundation models show superior improvement against existing SOTA baselines.\nThe source code is available at https://github.com/WujiangXu/AgenticMemory.\n","authors":["Wujiang Xu","Zujie Liang","Kai Mei","Hang Gao","Juntao Tan","Yongfeng Zhang"],"pdf_url":"https://arxiv.org/pdf/2502.12110v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.14189v2","updated":"2025-03-03T04:11:31Z","published":"2025-02-20T01:46:12Z","title":"QUAD-LLM-MLTC: Large Language Models Ensemble Learning for Healthcare\n  Text Multi-Label Classification","summary":"  The escalating volume of collected healthcare textual data presents a unique\nchallenge for automated Multi-Label Text Classification (MLTC), which is\nprimarily due to the scarcity of annotated texts for training and their nuanced\nnature. Traditional machine learning models often fail to fully capture the\narray of expressed topics. However, Large Language Models (LLMs) have\ndemonstrated remarkable effectiveness across numerous Natural Language\nProcessing (NLP) tasks in various domains, which show impressive computational\nefficiency and suitability for unsupervised learning through prompt\nengineering. Consequently, these LLMs promise an effective MLTC of medical\nnarratives. However, when dealing with various labels, different prompts can be\nrelevant depending on the topic. To address these challenges, the proposed\napproach, QUAD-LLM-MLTC, leverages the strengths of four LLMs: GPT-4o, BERT,\nPEGASUS, and BART. QUAD-LLM-MLTC operates in a sequential pipeline in which\nBERT extracts key tokens, PEGASUS augments textual data, GPT-4o classifies, and\nBART provides topics' assignment probabilities, which results in four\nclassifications, all in a 0-shot setting. The outputs are then combined using\nensemble learning and processed through a meta-classifier to produce the final\nMLTC result. The approach is evaluated using three samples of annotated texts,\nwhich contrast it with traditional and single-model methods. The results show\nsignificant improvements across the majority of the topics in the\nclassification's F1 score and consistency (F1 and Micro-F1 scores of 78.17% and\n80.16% with standard deviations of 0.025 and 0.011, respectively). This\nresearch advances MLTC using LLMs and provides an efficient and scalable\nsolution to rapidly categorize healthcare-related text data without further\ntraining.\n","authors":["Hajar Sakai","Sarah S. Lam"],"pdf_url":"https://arxiv.org/pdf/2502.14189v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.00617v4","updated":"2025-03-03T03:41:11Z","published":"2024-06-30T08:00:34Z","title":"Iterative Nash Policy Optimization: Aligning LLMs with General\n  Preferences via No-Regret Learning","summary":"  Reinforcement Learning with Human Feedback (RLHF) has achieved great success\nin aligning large language models (LLMs) with human preferences. Prevalent RLHF\napproaches are reward-based, following the Bradley-Terry (BT) model assumption,\nwhich may not fully capture the complexity of human preferences. In this paper,\nwe explore RLHF under a general preference framework and approach it from a\ngame-theoretic perspective. Specifically, we formulate the problem as a\ntwo-player game and propose a novel online algorithm, iterative Nash policy\noptimization (INPO). The key idea is to let the policy play against itself via\nno-regret learning, thereby approximating the Nash policy. Unlike previous\nmethods, INPO bypasses the need for estimating the expected win rate for\nindividual responses, which typically incurs high computational or annotation\ncosts. Instead, we introduce a new loss objective that is directly minimized\nover a preference dataset. We provide theoretical analysis for our approach and\ndemonstrate its effectiveness through experiments on various representative\nbenchmarks. With an LLaMA-3-8B-based SFT model, INPO achieves a 42.6%\nlength-controlled win rate on AlpacaEval 2.0 and a 37.8% win rate on\nArena-Hard, showing substantial improvement over the state-of-the-art online\nRLHF algorithms.\n","authors":["Yuheng Zhang","Dian Yu","Baolin Peng","Linfeng Song","Ye Tian","Mingyue Huo","Nan Jiang","Haitao Mi","Dong Yu"],"pdf_url":"https://arxiv.org/pdf/2407.00617v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.14093v3","updated":"2025-03-03T03:19:31Z","published":"2024-05-23T01:43:54Z","title":"A Survey on Vision-Language-Action Models for Embodied AI","summary":"  Embodied AI is widely recognized as a key element of artificial general\nintelligence because it involves controlling embodied agents to perform tasks\nin the physical world. Building on the success of large language models and\nvision-language models, a new category of multimodal models -- referred to as\nvision-language-action models (VLAs) -- has emerged to address\nlanguage-conditioned robotic tasks in embodied AI by leveraging their distinct\nability to generate actions. In recent years, a myriad of VLAs have been\ndeveloped, making it imperative to capture the rapidly evolving landscape\nthrough a comprehensive survey. To this end, we present the first survey on\nVLAs for embodied AI. This work provides a detailed taxonomy of VLAs, organized\ninto three major lines of research. The first line focuses on individual\ncomponents of VLAs. The second line is dedicated to developing control policies\nadept at predicting low-level actions. The third line comprises high-level task\nplanners capable of decomposing long-horizon tasks into a sequence of subtasks,\nthereby guiding VLAs to follow more general user instructions. Furthermore, we\nprovide an extensive summary of relevant resources, including datasets,\nsimulators, and benchmarks. Finally, we discuss the challenges faced by VLAs\nand outline promising future directions in embodied AI.\n","authors":["Yueen Ma","Zixing Song","Yuzheng Zhuang","Jianye Hao","Irwin King"],"pdf_url":"https://arxiv.org/pdf/2405.14093v3.pdf","comment":"16 pages, a survey of vision-language-action models"},{"id":"http://arxiv.org/abs/2412.17242v3","updated":"2025-03-03T03:08:43Z","published":"2024-12-23T03:30:34Z","title":"On the Generalization and Adaptation Ability of Machine-Generated Text\n  Detectors in Academic Writing","summary":"  The rising popularity of large language models (LLMs) has raised concerns\nabout machine-generated text (MGT), particularly in academic settings, where\nissues like plagiarism and misinformation are prevalent. As a result,\ndeveloping a highly generalizable and adaptable MGT detection system has become\nan urgent priority. Given that LLMs are most commonly misused in academic\nwriting, this work investigates the generalization and adaptation capabilities\nof MGT detectors in three key aspects specific to academic writing: First, we\nconstruct MGT-Acedemic, a large-scale dataset comprising over 336M tokens and\n749K samples. MGT-Acedemic focuses on academic writing, featuring human-written\ntexts (HWTs) and MGTs across STEM, Humanities, and Social Sciences, paired with\nan extensible code framework for efficient benchmarking. Second, we benchmark\nthe performance of various detectors for binary classification and attribution\ntasks in both in-domain and cross-domain settings. This benchmark reveals the\noften-overlooked challenges of attribution tasks. Third, we introduce a novel\nattribution task where models have to adapt to new classes over time without\n(or with very limited) access to prior training data in both few-shot and\nmany-shot scenarios. We implement eight different adapting techniques to\nimprove the performance and highlight the inherent complexity of the task. Our\nfindings provide insights into the generalization and adaptation ability of MGT\ndetectors across diverse scenarios and lay the foundation for building robust,\nadaptive detection systems. The code framework is available at\nhttps://github.com/Y-L-LIU/MGTBench-2.0.\n","authors":["Yule Liu","Zhiyuan Zhong","Yifan Liao","Zhen Sun","Jingyi Zheng","Jiaheng Wei","Qingyuan Gong","Fenghua Tong","Yang Chen","Yang Zhang","Xinlei He"],"pdf_url":"https://arxiv.org/pdf/2412.17242v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.13085v2","updated":"2025-03-03T03:08:28Z","published":"2024-10-16T23:03:27Z","title":"MMed-RAG: Versatile Multimodal RAG System for Medical Vision Language\n  Models","summary":"  Artificial Intelligence (AI) has demonstrated significant potential in\nhealthcare, particularly in disease diagnosis and treatment planning. Recent\nprogress in Medical Large Vision-Language Models (Med-LVLMs) has opened up new\npossibilities for interactive diagnostic tools. However, these models often\nsuffer from factual hallucination, which can lead to incorrect diagnoses.\nFine-tuning and retrieval-augmented generation (RAG) have emerged as methods to\naddress these issues. However, the amount of high-quality data and distribution\nshifts between training data and deployment data limit the application of\nfine-tuning methods. Although RAG is lightweight and effective, existing\nRAG-based approaches are not sufficiently general to different medical domains\nand can potentially cause misalignment issues, both between modalities and\nbetween the model and the ground truth. In this paper, we propose a versatile\nmultimodal RAG system, MMed-RAG, designed to enhance the factuality of\nMed-LVLMs. Our approach introduces a domain-aware retrieval mechanism, an\nadaptive retrieved contexts selection method, and a provable RAG-based\npreference fine-tuning strategy. These innovations make the RAG process\nsufficiently general and reliable, significantly improving alignment when\nintroducing retrieved contexts. Experimental results across five medical\ndatasets (involving radiology, ophthalmology, pathology) on medical VQA and\nreport generation demonstrate that MMed-RAG can achieve an average improvement\nof 43.8% in the factual accuracy of Med-LVLMs. Our data and code are available\nin https://github.com/richard-peng-xia/MMed-RAG.\n","authors":["Peng Xia","Kangyu Zhu","Haoran Li","Tianze Wang","Weijia Shi","Sheng Wang","Linjun Zhang","James Zou","Huaxiu Yao"],"pdf_url":"https://arxiv.org/pdf/2410.13085v2.pdf","comment":"ICLR 2025"},{"id":"http://arxiv.org/abs/2406.06600v3","updated":"2025-03-03T03:05:30Z","published":"2024-06-06T13:44:57Z","title":"HORAE: A Domain-Agnostic Modeling Language for Automating Multimodal\n  Service Regulation","summary":"  Artificial intelligence is rapidly encroaching on the field of service\nregulation. This work-in-progress article presents the design principles behind\nHORAE, a unified specification language to model multimodal regulation rules\nacross a diverse set of domains. We show how HORAE facilitates an intelligent\nservice regulation pipeline by further exploiting a fine-tuned large language\nmodel named HORAE that automates the HORAE modeling process, thereby yielding\nan end-to-end framework for fully automated intelligent service regulation.\n","authors":["Yutao Sun","Mingshuai Chen","Kangjia Zhao","Jintao Chen"],"pdf_url":"https://arxiv.org/pdf/2406.06600v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.07404v2","updated":"2025-03-03T03:02:55Z","published":"2024-11-11T22:22:21Z","title":"Controllable Context Sensitivity and the Knob Behind It","summary":"  When making predictions, a language model must trade off how much it relies\non its context vs. its prior knowledge. Choosing how sensitive the model is to\nits context is a fundamental functionality, as it enables the model to excel at\ntasks like retrieval-augmented generation and question-answering. In this\npaper, we search for a knob which controls this sensitivity, determining\nwhether language models answer from the context or their prior knowledge. To\nguide this search, we design a task for controllable context sensitivity. In\nthis task, we first feed the model a context (Paris is in England) and a\nquestion (Where is Paris?); we then instruct the model to either use its prior\nor contextual knowledge and evaluate whether it generates the correct answer\nfor both intents (either France or England). When fine-tuned on this task,\ninstruction-tuned versions of Llama-3.1, Mistral-v0.3, and Gemma-2 can solve it\nwith high accuracy (85-95%). Analyzing these high-performing models, we narrow\ndown which layers may be important to context sensitivity using a novel linear\ntime algorithm. Then, in each model, we identify a 1-D subspace in a single\nlayer that encodes whether the model follows context or prior knowledge.\nInterestingly, while we identify this subspace in a fine-tuned model, we find\nthat the exact same subspace serves as an effective knob in not only that model\nbut also non-fine-tuned instruct and base models of that model family. Finally,\nwe show a strong correlation between a model's performance and how distinctly\nit separates context-agreeing from context-ignoring answers in this subspace.\nThese results suggest a single subspace facilitates how the model chooses\nbetween context and prior knowledge, hinting at a simple fundamental mechanism\nthat controls this behavior.\n","authors":["Julian Minder","Kevin Du","Niklas Stoehr","Giovanni Monea","Chris Wendler","Robert West","Ryan Cotterell"],"pdf_url":"https://arxiv.org/pdf/2411.07404v2.pdf","comment":"Published as a conference paper at ICLR 2025"},{"id":"http://arxiv.org/abs/2502.20854v2","updated":"2025-03-03T03:00:59Z","published":"2025-02-28T08:53:08Z","title":"A Pilot Empirical Study on When and How to Use Knowledge Graphs as\n  Retrieval Augmented Generation","summary":"  The integration of Knowledge Graphs (KGs) into the Retrieval Augmented\nGeneration (RAG) framework has attracted significant interest, with early\nstudies showing promise in mitigating hallucinations and improving model\naccuracy. However, a systematic understanding and comparative analysis of the\nrapidly emerging KG-RAG methods are still lacking. This paper seeks to lay the\nfoundation for systematically answering the question of when and how to use\nKG-RAG by analyzing their performance in various application scenarios\nassociated with different technical configurations. After outlining the mind\nmap using KG-RAG framework and summarizing its popular pipeline, we conduct a\npilot empirical study of KG-RAG works to reimplement and evaluate 6 KG-RAG\nmethods across 7 datasets in diverse scenarios, analyzing the impact of 9\nKG-RAG configurations in combination with 17 LLMs. Our results underscore the\ncritical role of appropriate application conditions and optimal configurations\nof KG-RAG components.\n","authors":["Xujie Yuan","Yongxu Liu","Shimin Di","Shiwen Wu","Libin Zheng","Rui Meng","Lei Chen","Xiaofang Zhou","Jian Yin"],"pdf_url":"https://arxiv.org/pdf/2502.20854v2.pdf","comment":"8 pages, 2 figures, 14 tables"},{"id":"http://arxiv.org/abs/2410.08109v3","updated":"2025-03-03T02:45:58Z","published":"2024-10-10T16:56:05Z","title":"A Closer Look at Machine Unlearning for Large Language Models","summary":"  Large language models (LLMs) may memorize sensitive or copyrighted content,\nraising privacy and legal concerns. Due to the high cost of retraining from\nscratch, researchers attempt to employ machine unlearning to remove specific\ncontent from LLMs while preserving the overall performance. In this paper, we\ndiscuss several issues in machine unlearning for LLMs and provide our insights\non possible approaches. To address the issue of inadequate evaluation of model\noutputs after unlearning, we introduce three additional metrics to evaluate\ntoken diversity, sentence semantics, and factual correctness. We then\ncategorize unlearning methods into untargeted and targeted, and discuss their\nissues respectively. Specifically, the behavior that untargeted unlearning\nattempts to approximate is unpredictable and may involve hallucinations, and\nexisting regularization is insufficient for targeted unlearning. To alleviate\nthese issues, we propose using the objective of maximizing entropy (ME) for\nuntargeted unlearning and incorporate answer preservation (AP) loss as\nregularization for targeted unlearning. Experimental results across three\nscenarios, i.e., fictitious unlearning, continual unlearning, and real-world\nunlearning, demonstrate the effectiveness of our approaches. The code is\navailable at https://github.com/sail-sg/closer-look-LLM-unlearning.\n","authors":["Xiaojian Yuan","Tianyu Pang","Chao Du","Kejiang Chen","Weiming Zhang","Min Lin"],"pdf_url":"https://arxiv.org/pdf/2410.08109v3.pdf","comment":"ICLR 2025"},{"id":"http://arxiv.org/abs/2409.19788v2","updated":"2025-03-03T02:38:52Z","published":"2024-09-29T21:20:57Z","title":"Exploring Adversarial Robustness in Classification tasks using DNA\n  Language Models","summary":"  DNA Language Models, such as GROVER, DNABERT2 and the Nucleotide Transformer,\noperate on DNA sequences that inherently contain sequencing errors, mutations,\nand laboratory-induced noise, which may significantly impact model performance.\nDespite the importance of this issue, the robustness of DNA language models\nremains largely underexplored. In this paper, we comprehensivly investigate\ntheir robustness in DNA classification by applying various adversarial attack\nstrategies: the character (nucleotide substitutions), word (codon\nmodifications), and sentence levels (back-translation-based transformations) to\nsystematically analyze model vulnerabilities. Our results demonstrate that DNA\nlanguage models are highly susceptible to adversarial attacks, leading to\nsignificant performance degradation. Furthermore, we explore adversarial\ntraining method as a defense mechanism, which enhances both robustness and\nclassification accuracy. This study highlights the limitations of DNA language\nmodels and underscores the necessity of robustness in bioinformatics.\n","authors":["Hyunwoo Yoo","Haebin Shin","Kaidi Xu","Gail Rosen"],"pdf_url":"https://arxiv.org/pdf/2409.19788v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12952v2","updated":"2025-03-03T02:27:02Z","published":"2024-10-16T18:40:26Z","title":"Facilitating Multi-turn Function Calling for LLMs via Compositional\n  Instruction Tuning","summary":"  Large Language Models (LLMs) have exhibited significant potential in\nperforming diverse tasks, including the ability to call functions or use\nexternal tools to enhance their performance. While current research on function\ncalling by LLMs primarily focuses on single-turn interactions, this paper\naddresses the overlooked necessity for LLMs to engage in multi-turn function\ncalling--critical for handling compositional, real-world queries that require\nplanning with functions but not only use functions. To facilitate this, we\nintroduce an approach, BUTTON, which generates synthetic compositional\ninstruction tuning data via bottom-up instruction construction and top-down\ntrajectory generation. In the bottom-up phase, we generate simple atomic tasks\nbased on real-world scenarios and build compositional tasks using heuristic\nstrategies based on atomic tasks. Corresponding function definitions are then\nsynthesized for these compositional tasks. The top-down phase features a\nmulti-agent environment where interactions among simulated humans, assistants,\nand tools are utilized to gather multi-turn function calling trajectories. This\napproach ensures task compositionality and allows for effective function and\ntrajectory generation by examining atomic tasks within compositional tasks. We\nproduce a dataset BUTTONInstruct comprising 8k data points and demonstrate its\neffectiveness through extensive experiments across various LLMs.\n","authors":["Mingyang Chen","Haoze Sun","Tianpeng Li","Fan Yang","Hao Liang","Keer Lu","Bin Cui","Wentao Zhang","Zenan Zhou","Weipeng Chen"],"pdf_url":"https://arxiv.org/pdf/2410.12952v2.pdf","comment":"Accepted to ICLR 2025"},{"id":"http://arxiv.org/abs/2501.13983v3","updated":"2025-03-03T02:06:47Z","published":"2025-01-23T06:57:24Z","title":"AdEval: Alignment-based Dynamic Evaluation to Mitigate Data\n  Contamination in Large Language Models","summary":"  As Large Language Models (LLMs) are pretrained on massive-scale corpora, the\nissue of data contamination has become increasingly severe, leading to\npotential overestimation of model performance during evaluation. To address\nthis, we propose AdEval (Alignment-based Dynamic Evaluation), a dynamic data\nevaluation method aimed at mitigating the impact of data contamination on\nevaluation reliability. Experimental results on multiple datasets demonstrate\nthat AdEval effectively reduces the impact of data contamination on evaluation\noutcomes, enhancing both the fairness and reliability of the evaluation\nprocess.\n","authors":["Yang Fan"],"pdf_url":"https://arxiv.org/pdf/2501.13983v3.pdf","comment":"There are serious academic problems in this paper, such as data\n  falsification and plagiarism in the method of the paper"},{"id":"http://arxiv.org/abs/2409.02060v2","updated":"2025-03-03T01:25:46Z","published":"2024-09-03T17:08:20Z","title":"OLMoE: Open Mixture-of-Experts Language Models","summary":"  We introduce OLMoE, a fully open, state-of-the-art language model leveraging\nsparse Mixture-of-Experts (MoE). OLMoE-1B-7B has 7 billion (B) parameters but\nuses only 1B per input token. We pretrain it on 5 trillion tokens and further\nadapt it to create OLMoE-1B-7B-Instruct. Our models outperform all available\nmodels with similar active parameters, even surpassing larger ones like\nLlama2-13B-Chat and DeepSeekMoE-16B. We present various experiments on MoE\ntraining, analyze routing in our model showing high specialization, and\nopen-source all aspects of our work: model weights, training data, code, and\nlogs.\n","authors":["Niklas Muennighoff","Luca Soldaini","Dirk Groeneveld","Kyle Lo","Jacob Morrison","Sewon Min","Weijia Shi","Pete Walsh","Oyvind Tafjord","Nathan Lambert","Yuling Gu","Shane Arora","Akshita Bhagia","Dustin Schwenk","David Wadden","Alexander Wettig","Binyuan Hui","Tim Dettmers","Douwe Kiela","Ali Farhadi","Noah A. Smith","Pang Wei Koh","Amanpreet Singh","Hannaneh Hajishirzi"],"pdf_url":"https://arxiv.org/pdf/2409.02060v2.pdf","comment":"63 pages (24 main), 36 figures, 17 tables"},{"id":"http://arxiv.org/abs/2410.01417v2","updated":"2025-03-03T00:41:36Z","published":"2024-10-02T10:58:54Z","title":"The Labyrinth of Links: Navigating the Associative Maze of Multi-modal\n  LLMs","summary":"  Multi-modal Large Language Models (MLLMs) have exhibited impressive\ncapability. However, recently many deficiencies of MLLMs have been found\ncompared to human intelligence, $\\textit{e.g.}$, hallucination. To drive the\nMLLMs study, the community dedicated efforts to building larger benchmarks with\ncomplex tasks. In this paper, we propose benchmarking an essential but usually\noverlooked intelligence: $\\textbf{association}$, a human's basic capability to\nlink observation and prior practice memory. To comprehensively investigate\nMLLM's performance on the association, we formulate the association task and\ndevise a standard benchmark based on adjective and verb semantic concepts.\nInstead of costly data annotation and curation, we propose a convenient\n$\\textbf{annotation-free}$ construction method transforming the general dataset\nfor our association tasks. Simultaneously, we devise a rigorous data refinement\nprocess to eliminate confusion in the raw dataset. Building on this database,\nwe establish three levels of association tasks: single-step, synchronous, and\nasynchronous associations. Moreover, we conduct a comprehensive investigation\ninto the MLLMs' zero-shot association capabilities, addressing multiple\ndimensions, including three distinct memory strategies, both open-source and\nclosed-source MLLMs, cutting-edge Mixture-of-Experts (MoE) models, and the\ninvolvement of human experts. Our systematic investigation shows that current\nopen-source MLLMs consistently exhibit poor capability in our association\ntasks, even the currently state-of-the-art GPT-4V(vision) also has a\nsignificant gap compared to humans. We believe our benchmark would pave the way\nfor future MLLM studies. $\\textit{Our data and code are available at:}$\nhttps://mvig-rhos.com/llm_inception.\n","authors":["Hong Li","Nanxi Li","Yuanjie Chen","Jianbin Zhu","Qinlu Guo","Cewu Lu","Yong-Lu Li"],"pdf_url":"https://arxiv.org/pdf/2410.01417v2.pdf","comment":"Accepted by ICLR 2025. Project page:\n  https://mvig-rhos.com/llm_inception"},{"id":"http://arxiv.org/abs/2405.02318v2","updated":"2025-03-03T00:38:48Z","published":"2024-04-18T00:20:48Z","title":"NL2FOL: Translating Natural Language to First-Order Logic for Logical\n  Fallacy Detection","summary":"  Translating natural language into formal language such as First-Order Logic\n(FOL) is a foundational challenge in NLP with wide-ranging applications in\nautomated reasoning, misinformation tracking, and knowledge validation. In this\npaper, we introduce Natural Language to First-Order Logic (NL2FOL), a framework\nto autoformalize natural language to FOL step by step using Large Language\nModels (LLMs). Our approach addresses key challenges in this translation\nprocess, including the integration of implicit background knowledge. By\nleveraging structured representations generated by NL2FOL, we use\nSatisfiability Modulo Theory (SMT) solvers to reason about the logical validity\nof natural language statements. We present logical fallacy detection as a case\nstudy to evaluate the efficacy of NL2FOL. Being neurosymbolic, our approach\nalso provides interpretable insights into the reasoning process and\ndemonstrates robustness without requiring model fine-tuning or labeled training\ndata. Our framework achieves strong performance on multiple datasets. On the\nLOGIC dataset, NL2FOL achieves an F1-score of 78%, while generalizing\neffectively to the LOGICCLIMATE dataset with an F1-score of 80%.\n","authors":["Abhinav Lalwani","Tasha Kim","Lovish Chopra","Christopher Hahn","Zhijing Jin","Mrinmaya Sachan"],"pdf_url":"https://arxiv.org/pdf/2405.02318v2.pdf","comment":null}]},"2025-03-02T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2502.12361v3","updated":"2025-03-02T22:19:39Z","published":"2025-02-17T22:56:42Z","title":"ConFit v2: Improving Resume-Job Matching using Hypothetical Resume\n  Embedding and Runner-Up Hard-Negative Mining","summary":"  A reliable resume-job matching system helps a company recommend suitable\ncandidates from a pool of resumes and helps a job seeker find relevant jobs\nfrom a list of job posts. However, since job seekers apply only to a few jobs,\ninteraction labels in resume-job datasets are sparse. We introduce ConFit v2,\nan improvement over ConFit to tackle this sparsity problem. We propose two\ntechniques to enhance the encoder's contrastive training process: augmenting\njob data with hypothetical reference resume generated by a large language\nmodel; and creating high-quality hard negatives from unlabeled resume/job pairs\nusing a novel hard-negative mining strategy. We evaluate ConFit v2 on two\nreal-world datasets and demonstrate that it outperforms ConFit and prior\nmethods (including BM25 and OpenAI text-embedding-003), achieving an average\nabsolute improvement of 13.8% in recall and 17.5% in nDCG across job-ranking\nand resume-ranking tasks.\n","authors":["Xiao Yu","Ruize Xu","Chengyuan Xue","Jinzhong Zhang","Xu Ma","Zhou Yu"],"pdf_url":"https://arxiv.org/pdf/2502.12361v3.pdf","comment":"arXiv admin note: text overlap with arXiv:2401.16349"},{"id":"http://arxiv.org/abs/2402.10767v2","updated":"2025-03-02T20:33:20Z","published":"2024-02-16T15:41:23Z","title":"Inference to the Best Explanation in Large Language Models","summary":"  While Large Language Models (LLMs) have found success in real-world\napplications, their underlying explanatory process is still poorly understood.\nThis paper proposes IBE-Eval, a framework inspired by philosophical accounts on\nInference to the Best Explanation (IBE) to advance the interpretation and\nevaluation of LLMs' explanations. IBE-Eval estimates the plausibility of\nnatural language explanations through a combination of explicit logical and\nlinguistic features including: consistency, parsimony, coherence, and\nuncertainty. Extensive experiments are conducted on Causal Question Answering\n(CQA), where \\textit{IBE-Eval} is tasked to select the most plausible causal\nexplanation amongst competing ones generated by LLMs (i.e., GPT 3.5 and Llama\n2). The experiments reveal that IBE-Eval can successfully identify the best\nexplanation with up to 77\\% accuracy ($\\approx 27\\%$ above random), improving\nupon a GPT 3.5-as-a-Judge baseline ($\\approx+17\\%$) while being intrinsically\nmore efficient and interpretable. Additional analyses suggest that, despite\nmodel-specific variances, LLM-generated explanations tend to conform to IBE\ncriteria and that IBE-Eval is significantly correlated with human judgment,\nopening up opportunities for future development of automated explanation\nverification tools.\n","authors":["Dhairya Dalal","Marco Valentino","André Freitas","Paul Buitelaar"],"pdf_url":"https://arxiv.org/pdf/2402.10767v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.04343v2","updated":"2025-03-02T19:44:37Z","published":"2024-10-06T03:42:15Z","title":"Inference Scaling for Long-Context Retrieval Augmented Generation","summary":"  The scaling of inference computation has unlocked the potential of\nlong-context large language models (LLMs) across diverse settings. For\nknowledge-intensive tasks, the increased compute is often allocated to\nincorporate more external knowledge. However, without effectively utilizing\nsuch knowledge, solely expanding context does not always enhance performance.\nIn this work, we investigate inference scaling for retrieval augmented\ngeneration (RAG), exploring the combination of multiple strategies beyond\nsimply increasing the quantity of knowledge, including in-context learning and\niterative prompting. These strategies provide additional flexibility to scale\ntest-time computation (e.g., by increasing retrieved documents or generation\nsteps), thereby enhancing LLMs' ability to effectively acquire and utilize\ncontextual information. We address two key questions: (1) How does RAG\nperformance benefit from the scaling of inference computation when optimally\nconfigured? (2) Can we predict the optimal test-time compute allocation for a\ngiven budget by modeling the relationship between RAG performance and inference\nparameters? Our observations reveal that increasing inference computation leads\nto nearly linear gains in RAG performance when optimally allocated, a\nrelationship we describe as the inference scaling laws for RAG. Building on\nthis, we further develop the computation allocation model to estimate RAG\nperformance across different inference configurations. The model predicts\noptimal inference parameters under various computation constraints, which align\nclosely with the experimental results. By applying these optimal\nconfigurations, we demonstrate that scaling inference compute on long-context\nLLMs achieves up to 58.9% gains on benchmark datasets compared to standard RAG.\n","authors":["Zhenrui Yue","Honglei Zhuang","Aijun Bai","Kai Hui","Rolf Jagerman","Hansi Zeng","Zhen Qin","Dong Wang","Xuanhui Wang","Michael Bendersky"],"pdf_url":"https://arxiv.org/pdf/2410.04343v2.pdf","comment":"ICLR 2025"},{"id":"http://arxiv.org/abs/2405.14105v4","updated":"2025-03-02T18:24:29Z","published":"2024-05-23T02:14:17Z","title":"Distributed Speculative Inference (DSI): Speculation Parallelism for\n  Provably Faster Lossless Language Model Inference","summary":"  This paper introduces distributed speculative inference (DSI), a novel\ninference algorithm that is provably faster than speculative inference (SI)\n[leviathan2023, chen2023, miao2024, sun2025, timor2025] and standard\nautoregressive inference (non-SI). Like other SI algorithms, DSI operates on\nfrozen language models (LMs), requiring no training or architectural\nmodifications, and it preserves the target distribution. Prior studies on SI\nhave demonstrated empirical speedups over non-SI--but rely on sufficiently fast\nand accurate drafters, which are often unavailable in practice. We identify a\ngap where SI can be slower than non-SI if drafters are too slow or inaccurate.\nWe close this gap by proving that DSI is faster than both SI and non-SI--given\nany drafters. DSI is therefore not only faster than SI, but also unlocks the\nacceleration of LMs for which SI fails. DSI leverages speculation parallelism\n(SP), a novel type of task parallelism, to orchestrate target and drafter\ninstances that overlap in time, establishing a new foundational tradeoff\nbetween computational resources and latency. Our simulations show that DSI is\n1.29-1.92x faster than SI in single-node setups for various off-the-shelf LMs\nand tasks. We open-source all our code.\n","authors":["Nadav Timor","Jonathan Mamou","Daniel Korat","Moshe Berchansky","Oren Pereg","Moshe Wasserblat","Tomer Galanti","Michal Gordon","David Harel"],"pdf_url":"https://arxiv.org/pdf/2405.14105v4.pdf","comment":"Published at ICLR 2025. (Link:\n  https://openreview.net/forum?id=cJd1BgZ9CS)"},{"id":"http://arxiv.org/abs/2403.08743v2","updated":"2025-03-02T17:33:03Z","published":"2024-03-13T17:46:28Z","title":"Prompting Fairness: Integrating Causality to Debias Large Language\n  Models","summary":"  Large language models (LLMs), despite their remarkable capabilities, are\nsusceptible to generating biased and discriminatory responses. As LLMs\nincreasingly influence high-stakes decision-making (e.g., hiring and\nhealthcare), mitigating these biases becomes critical. In this work, we propose\na causality-guided debiasing framework to tackle social biases, aiming to\nreduce the objectionable dependence between LLMs' decisions and the social\ninformation in the input. Our framework introduces a novel perspective to\nidentify how social information can affect an LLM's decision through different\ncausal pathways. Leveraging these causal insights, we outline principled\nprompting strategies that regulate these pathways through selection mechanisms.\nThis framework not only unifies existing prompting-based debiasing techniques,\nbut also opens up new directions for reducing bias by encouraging the model to\nprioritize fact-based reasoning over reliance on biased social cues. We\nvalidate our framework through extensive experiments on real-world datasets\nacross multiple domains, demonstrating its effectiveness in debiasing LLM\ndecisions, even with only black-box access to the model.\n","authors":["Jingling Li","Zeyu Tang","Xiaoyu Liu","Peter Spirtes","Kun Zhang","Liu Leqi","Yang Liu"],"pdf_url":"https://arxiv.org/pdf/2403.08743v2.pdf","comment":"24 pages, 10 figures"},{"id":"http://arxiv.org/abs/2502.11882v3","updated":"2025-03-02T17:15:11Z","published":"2025-02-17T15:09:45Z","title":"Leveraging Dual Process Theory in Language Agent Framework for Real-time\n  Simultaneous Human-AI Collaboration","summary":"  Agents built on large language models (LLMs) have excelled in turn-by-turn\nhuman-AI collaboration but struggle with simultaneous tasks requiring real-time\ninteraction. Latency issues and the challenge of inferring variable human\nstrategies hinder their ability to make autonomous decisions without explicit\ninstructions. Through experiments with current independent System 1 and System\n2 methods, we validate the necessity of using Dual Process Theory (DPT) in\nreal-time tasks. We propose DPT-Agent, a novel language agent framework that\nintegrates System 1 and System 2 for efficient real-time simultaneous human-AI\ncollaboration. DPT-Agent's System 1 uses a Finite-state Machine (FSM) and\ncode-as-policy for fast, intuitive, and controllable decision-making.\nDPT-Agent's System 2 integrates Theory of Mind (ToM) and asynchronous\nreflection to infer human intentions and perform reasoning-based autonomous\ndecisions. We demonstrate the effectiveness of DPT-Agent through further\nexperiments with rule-based agents and human collaborators, showing significant\nimprovements over mainstream LLM-based frameworks. DPT-Agent can effectively\nhelp LLMs convert correct slow thinking and reasoning into executable actions,\nthereby improving performance. To the best of our knowledge, DPT-Agent is the\nfirst language agent framework that achieves successful real-time simultaneous\nhuman-AI collaboration autonomously. Code of DPT-Agent can be found in\nhttps://github.com/sjtu-marl/DPT-Agent.\n","authors":["Shao Zhang","Xihuai Wang","Wenhao Zhang","Chaoran Li","Junru Song","Tingyu Li","Lin Qiu","Xuezhi Cao","Xunliang Cai","Wen Yao","Weinan Zhang","Xinbing Wang","Ying Wen"],"pdf_url":"https://arxiv.org/pdf/2502.11882v3.pdf","comment":"Preprint under review. Update the experimental results of the\n  DeepSeek-R1 series models, o3-mini-high and o3-mini-medium"},{"id":"http://arxiv.org/abs/2502.18036v2","updated":"2025-03-02T16:56:04Z","published":"2025-02-25T09:48:53Z","title":"Harnessing Multiple Large Language Models: A Survey on LLM Ensemble","summary":"  LLM Ensemble -- which involves the comprehensive use of multiple large\nlanguage models (LLMs), each aimed at handling user queries during downstream\ninference, to benefit from their individual strengths -- has gained substantial\nattention recently. The widespread availability of LLMs, coupled with their\nvarying strengths and out-of-the-box usability, has profoundly advanced the\nfield of LLM Ensemble. This paper presents the first systematic review of\nrecent developments in LLM Ensemble. First, we introduce our taxonomy of LLM\nEnsemble and discuss several related research problems. Then, we provide a more\nin-depth classification of the methods under the broad categories of\n\"ensemble-before-inference, ensemble-during-inference,\nensemble-after-inference'', and review all relevant methods. Finally, we\nintroduce related benchmarks and applications, summarize existing studies, and\nsuggest several future research directions. A curated list of papers on LLM\nEnsemble is available at https://github.com/junchenzhi/Awesome-LLM-Ensemble.\n","authors":["Zhijun Chen","Jingzheng Li","Pengpeng Chen","Zhuoran Li","Kai Sun","Yuankai Luo","Qianren Mao","Dingqi Yang","Hailong Sun","Philip S. Yu"],"pdf_url":"https://arxiv.org/pdf/2502.18036v2.pdf","comment":"9 pages, 2 figures, codebase:\n  https://github.com/junchenzhi/Awesome-LLM-Ensemble"},{"id":"http://arxiv.org/abs/2502.06563v2","updated":"2025-03-02T16:38:28Z","published":"2025-02-10T15:31:54Z","title":"Large Language Models Meet Symbolic Provers for Logical Reasoning\n  Evaluation","summary":"  First-order logic (FOL) reasoning, which involves sequential deduction, is\npivotal for intelligent systems and serves as a valuable task for evaluating\nreasoning capabilities, particularly in chain-of-thought (CoT) contexts.\nExisting benchmarks often rely on extensive human annotation or handcrafted\ntemplates, making it difficult to achieve the necessary complexity,\nscalability, and diversity for robust evaluation. To address these limitations,\nwe propose a novel framework called ProverGen that synergizes the generative\nstrengths of Large Language Models (LLMs) with the rigor and precision of\nsymbolic provers, enabling the creation of a scalable, diverse, and\nhigh-quality FOL reasoning dataset, ProverQA. ProverQA is also distinguished by\nits inclusion of accessible and logically coherent intermediate reasoning steps\nfor each problem. Our evaluation shows that state-of-the-art LLMs struggle to\nsolve ProverQA problems, even with CoT prompting, highlighting the dataset's\nchallenging nature. We also finetune Llama3.1-8B-Instruct on a separate\ntraining set generated by our framework. The finetuned model demonstrates\nconsistent improvements on both in-distribution and out-of-distribution test\nsets, suggesting the value of our proposed data generation framework. Code\navailable at: https://github.com/opendatalab/ProverGen\n","authors":["Chengwen Qi","Ren Ma","Bowen Li","He Du","Binyuan Hui","Jinwang Wu","Yuanjun Laili","Conghui He"],"pdf_url":"https://arxiv.org/pdf/2502.06563v2.pdf","comment":"Accepted by ICLR 2025"},{"id":"http://arxiv.org/abs/2410.00812v2","updated":"2025-03-02T16:32:24Z","published":"2024-10-01T15:57:48Z","title":"Generative causal testing to bridge data-driven models and scientific\n  theories in language neuroscience","summary":"  Representations from large language models are highly effective at predicting\nBOLD fMRI responses to language stimuli. However, these representations are\nlargely opaque: it is unclear what features of the language stimulus drive the\nresponse in each brain area. We present generative causal testing (GCT), a\nframework for generating concise explanations of language selectivity in the\nbrain from predictive models and then testing those explanations in follow-up\nexperiments using LLM-generated stimuli.This approach is successful at\nexplaining selectivity both in individual voxels and cortical regions of\ninterest (ROIs), including newly identified microROIs in prefrontal cortex. We\nshow that explanatory accuracy is closely related to the predictive power and\nstability of the underlying predictive models. Finally, we show that GCT can\ndissect fine-grained differences between brain areas with similar functional\nselectivity. These results demonstrate that LLMs can be used to bridge the\nwidening gap between data-driven models and formal scientific theories.\n","authors":["Richard Antonello","Chandan Singh","Shailee Jain","Aliyah Hsu","Sihang Guo","Jianfeng Gao","Bin Yu","Alexander Huth"],"pdf_url":"https://arxiv.org/pdf/2410.00812v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.19412v2","updated":"2025-03-02T16:16:39Z","published":"2025-02-26T18:56:38Z","title":"The Mighty ToRR: A Benchmark for Table Reasoning and Robustness","summary":"  Despite its real-world significance, model performance on tabular data\nremains underexplored, leaving uncertainty about which model to rely on and\nwhich prompt configuration to adopt. To address this gap, we create ToRR, a\nbenchmark for Table Reasoning and Robustness, measuring model performance and\nrobustness on table-related tasks. The benchmark includes 10 datasets that\ncover different types of table reasoning capabilities across varied domains.\nToRR goes beyond model performance rankings, and is designed to reflect whether\nmodels can handle tabular data consistently and robustly, across a variety of\ncommon table representation formats. We present a leaderboard as well as\ncomprehensive analyses of the results of leading models over ToRR. Our results\nreveal a striking pattern of brittle model behavior, where even strong models\nare unable to perform robustly on tabular data tasks. Although no specific\ntable format leads to consistently better performance, we show that testing\nover multiple formats is crucial for reliably estimating model capabilities.\nMoreover, we show that the reliability boost from testing multiple prompts can\nbe equivalent to adding more test examples. Overall, our findings show that\ntable understanding and reasoning tasks remain a significant challenge.\n","authors":["Shir Ashury-Tahan","Yifan Mai","Rajmohan C","Ariel Gera","Yotam Perlitz","Asaf Yehudai","Elron Bandel","Leshem Choshen","Eyal Shnarch","Percy Liang","Michal Shmueli-Scheuer"],"pdf_url":"https://arxiv.org/pdf/2502.19412v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.03895v2","updated":"2025-03-02T15:55:07Z","published":"2025-01-07T16:03:14Z","title":"LLaVA-Mini: Efficient Image and Video Large Multimodal Models with One\n  Vision Token","summary":"  The advent of real-time large multimodal models (LMMs) like GPT-4o has\nsparked considerable interest in efficient LMMs. LMM frameworks typically\nencode visual inputs into vision tokens (continuous representations) and\nintegrate them and textual instructions into the context of large language\nmodels (LLMs), where large-scale parameters and numerous context tokens\n(predominantly vision tokens) result in substantial computational overhead.\nPrevious efforts towards efficient LMMs always focus on replacing the LLM\nbackbone with smaller models, while neglecting the crucial issue of token\nquantity. In this paper, we introduce LLaVA-Mini, an efficient LMM with minimal\nvision tokens. To achieve a high compression ratio of vision tokens while\npreserving visual information, we first analyze how LMMs understand vision\ntokens and find that most vision tokens only play a crucial role in the early\nlayers of LLM backbone, where they mainly fuse visual information into text\ntokens. Building on this finding, LLaVA-Mini introduces modality pre-fusion to\nfuse visual information into text tokens in advance, thereby facilitating the\nextreme compression of vision tokens fed to LLM backbone into one token.\nLLaVA-Mini is a unified large multimodal model that can support the\nunderstanding of images, high-resolution images, and videos in an efficient\nmanner. Experiments across 11 image-based and 7 video-based benchmarks\ndemonstrate that LLaVA-Mini outperforms LLaVA-v1.5 with just 1 vision token\ninstead of 576. Efficiency analyses reveal that LLaVA-Mini can reduce FLOPs by\n77%, deliver low-latency responses within 40 milliseconds, and process over\n10,000 frames of video on the GPU hardware with 24GB of memory.\n","authors":["Shaolei Zhang","Qingkai Fang","Zhe Yang","Yang Feng"],"pdf_url":"https://arxiv.org/pdf/2501.03895v2.pdf","comment":"Accepted to ICLR 2025. Code: https://github.com/ictnlp/LLaVA-Mini\n  Model: https://huggingface.co/ICTNLP/llava-mini-llama-3.1-8b"},{"id":"http://arxiv.org/abs/2410.03524v2","updated":"2025-03-02T15:54:11Z","published":"2024-10-04T15:44:47Z","title":"Steering Large Language Models between Code Execution and Textual\n  Reasoning","summary":"  While a lot of recent research focuses on enhancing the textual reasoning\ncapabilities of Large Language Models (LLMs) by optimizing the multi-agent\nframework or reasoning chains, several benchmark tasks can be solved with 100\\%\nsuccess through direct coding, which is more scalable and avoids the\ncomputational overhead associated with textual iterating and searching. Textual\nreasoning has inherent limitations in solving tasks with challenges in math,\nlogics, optimization, and searching, which is unlikely to be solved by simply\nscaling up the model and data size. The recently released OpenAI GPT Code\nInterpreter and multi-agent frameworks such as AutoGen have demonstrated\nremarkable proficiency of integrating code generation and execution to solve\ncomplex tasks using LLMs. However, based on our experiments on 7 existing\npopular methods for steering code/text generation in both single- and\nmulti-turn settings with 14 tasks and 6 types of LLMs (including the new\nO1-preview), currently there is no optimal method to correctly steer LLMs to\nwrite code when needed. We discover some interesting patterns on when models\nuse code vs. textual reasoning with the evolution to task complexity and model\nsizes, which even result in an astonishingly inverse scaling behavior. We also\ndiscover that results from LLM written code are not always better than using\ntextual reasoning, even if the task could be solved through code. To mitigate\nthe above issues, we propose three methods to better steer LLM code/text\ngeneration and achieve a notable improvement. The costs of token lengths and\nruntime are thoroughly discussed for all the methods. We believe the problem of\nsteering LLM code/text generation is critical for future research and has much\nspace for further improvement. Project Page, Datasets, and Codes are available\nat https://yongchao98.github.io/CodeSteer/.\n","authors":["Yongchao Chen","Harsh Jhamtani","Srinagesh Sharma","Chuchu Fan","Chi Wang"],"pdf_url":"https://arxiv.org/pdf/2410.03524v2.pdf","comment":"32 pages, 12 figures, 12 tables"},{"id":"http://arxiv.org/abs/2410.10781v2","updated":"2025-03-02T14:37:53Z","published":"2024-10-14T17:50:28Z","title":"When Attention Sink Emerges in Language Models: An Empirical View","summary":"  Language Models (LMs) assign significant attention to the first token, even\nif it is not semantically important, which is known as attention sink. This\nphenomenon has been widely adopted in applications such as streaming/long\ncontext generation, KV cache optimization, inference acceleration, model\nquantization, and others. Despite its widespread use, a deep understanding of\nattention sink in LMs is still lacking. In this work, we first demonstrate that\nattention sinks exist universally in LMs with various inputs, even in small\nmodels. Furthermore, attention sink is observed to emerge during the LM\npre-training, motivating us to investigate how optimization, data distribution,\nloss function, and model architecture in LM pre-training influence its\nemergence. We highlight that attention sink emerges after effective\noptimization on sufficient training data. The sink position is highly\ncorrelated with the loss function and data distribution. Most importantly, we\nfind that attention sink acts more like key biases, storing extra attention\nscores, which could be non-informative and not contribute to the value\ncomputation. We also observe that this phenomenon (at least partially) stems\nfrom tokens' inner dependence on attention scores as a result of softmax\nnormalization. After relaxing such dependence by replacing softmax attention\nwith other attention operations, such as sigmoid attention without\nnormalization, attention sinks do not emerge in LMs up to 1B parameters. The\ncode is available at https://github.com/sail-sg/Attention-Sink.\n","authors":["Xiangming Gu","Tianyu Pang","Chao Du","Qian Liu","Fengzhuo Zhang","Cunxiao Du","Ye Wang","Min Lin"],"pdf_url":"https://arxiv.org/pdf/2410.10781v2.pdf","comment":"ICLR 2025 (Spotlight)"},{"id":"http://arxiv.org/abs/2409.13555v2","updated":"2025-03-02T14:36:29Z","published":"2024-09-20T14:56:33Z","title":"Generating Visual Stories with Grounded and Coreferent Characters","summary":"  Characters are important in narratives. They move the plot forward, create\nemotional connections, and embody the story's themes. Visual storytelling\nmethods focus more on the plot and events relating to it, without building the\nnarrative around specific characters. As a result, the generated stories feel\ngeneric, with character mentions being absent, vague, or incorrect. To mitigate\nthese issues, we introduce the new task of character-centric story generation\nand present the first model capable of predicting visual stories with\nconsistently grounded and coreferent character mentions. Our model is finetuned\non a new dataset which we build on top of the widely used VIST benchmark.\nSpecifically, we develop an automated pipeline to enrich VIST with visual and\ntextual character coreference chains. We also propose new evaluation metrics to\nmeasure the richness of characters and coreference in stories. Experimental\nresults show that our model generates stories with recurring characters which\nare consistent and coreferent to larger extent compared to baselines and\nstate-of-the-art systems.\n","authors":["Danyang Liu","Mirella Lapata","Frank Keller"],"pdf_url":"https://arxiv.org/pdf/2409.13555v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.07137v2","updated":"2025-03-02T14:28:33Z","published":"2024-10-09T17:53:06Z","title":"Cheating Automatic LLM Benchmarks: Null Models Achieve High Win Rates","summary":"  Automatic LLM benchmarks, such as AlpacaEval 2.0, Arena-Hard-Auto, and\nMT-Bench, have become popular for evaluating language models due to their\ncost-effectiveness and scalability compared to human evaluation. Achieving high\nwin rates on these benchmarks can significantly boost the promotional impact of\nnewly released language models. This promotional benefit may motivate tricks,\nsuch as manipulating model output length or style to game win rates, even\nthough several mechanisms have been developed to control length and disentangle\nstyle to reduce gameability. Nonetheless, we show that even a \"null model\" that\nalways outputs a constant response (irrelevant to input instructions) can cheat\nautomatic benchmarks and achieve top-ranked win rates: an 86.5% LC win rate on\nAlpacaEval 2.0; an 83.0 score on Arena-Hard-Auto; and a 9.55 score on MT-Bench.\nMoreover, the crafted cheating outputs are transferable because we assume that\nthe instructions of these benchmarks (e.g., 805 samples of AlpacaEval 2.0) are\nprivate and cannot be accessed. While our experiments are primarily\nproof-of-concept, an adversary could use LLMs to generate more imperceptible\ncheating responses, unethically benefiting from high win rates and promotional\nimpact. Our findings call for the development of anti-cheating mechanisms for\nreliable automatic benchmarks. The code is available at\nhttps://github.com/sail-sg/Cheating-LLM-Benchmarks.\n","authors":["Xiaosen Zheng","Tianyu Pang","Chao Du","Qian Liu","Jing Jiang","Min Lin"],"pdf_url":"https://arxiv.org/pdf/2410.07137v2.pdf","comment":"ICLR 2025 (Oral)"},{"id":"http://arxiv.org/abs/2411.03962v4","updated":"2025-03-02T13:52:25Z","published":"2024-11-06T14:51:02Z","title":"How Does A Text Preprocessing Pipeline Affect Ontology Syntactic\n  Matching?","summary":"  The generic text preprocessing pipeline, comprising Tokenisation,\nNormalisation, Stop Words Removal, and Stemming/Lemmatisation, has been\nimplemented in many systems for syntactic ontology matching (OM). However, the\nlack of standardisation in text preprocessing creates diversity in mapping\nresults. In this paper, we investigate the effect of the text preprocessing\npipeline on syntactic OM in 8 Ontology Alignment Evaluation Initiative (OAEI)\ntracks with 49 distinct alignments. We find that Phase 1 text preprocessing\n(Tokenisation and Normalisation) is currently more effective than Phase 2 text\npreprocessing (Stop Words Removal and Stemming/Lemmatisation). To repair the\nless effective Phase 2 text preprocessing caused by unwanted false mappings, we\npropose a novel context-based pipeline repair approach that employs an ad hoc\ncheck to find common words that cause false mappings. These words are stored in\na reserved word set and applied in text preprocessing. The experimental results\nshow that our approach improves the matching correctness and the overall\nmatching performance. We also discuss the integration of the classical text\npreprocessing pipeline with modern large language models (LLMs). We recommend\nthat LLMs inject the text preprocessing pipeline via function calling to avoid\nthe tendency towards unstable true mappings produced by prompt-based LLM\napproaches, and use LLMs to repair false mappings generated by the text\npreprocessing pipeline.\n","authors":["Zhangcheng Qiang","Kerry Taylor","Weiqing Wang"],"pdf_url":"https://arxiv.org/pdf/2411.03962v4.pdf","comment":"13 pages, 11 figures, 4 tables"},{"id":"http://arxiv.org/abs/2406.15812v2","updated":"2025-03-02T12:28:24Z","published":"2024-06-22T10:36:04Z","title":"Intrinsic Dimension Correlation: uncovering nonlinear connections in\n  multimodal representations","summary":"  To gain insight into the mechanisms behind machine learning methods, it is\ncrucial to establish connections among the features describing data points.\nHowever, these correlations often exhibit a high-dimensional and strongly\nnonlinear nature, which makes them challenging to detect using standard\nmethods. This paper exploits the entanglement between intrinsic dimensionality\nand correlation to propose a metric that quantifies the (potentially nonlinear)\ncorrelation between high-dimensional manifolds. We first validate our method on\nsynthetic data in controlled environments, showcasing its advantages and\ndrawbacks compared to existing techniques. Subsequently, we extend our analysis\nto large-scale applications in neural network representations. Specifically, we\nfocus on latent representations of multimodal data, uncovering clear\ncorrelations between paired visual and textual embeddings, whereas existing\nmethods struggle significantly in detecting similarity. Our results indicate\nthe presence of highly nonlinear correlation patterns between latent manifolds.\n","authors":["Lorenzo Basile","Santiago Acevedo","Luca Bortolussi","Fabio Anselmi","Alex Rodriguez"],"pdf_url":"https://arxiv.org/pdf/2406.15812v2.pdf","comment":"Accepted at ICLR 2025"},{"id":"http://arxiv.org/abs/2405.01229v2","updated":"2025-03-02T12:27:07Z","published":"2024-05-02T12:18:14Z","title":"Boosting Jailbreak Attack with Momentum","summary":"  Large Language Models (LLMs) have achieved remarkable success across diverse\ntasks, yet they remain vulnerable to adversarial attacks, notably the\nwell-known jailbreak attack. In particular, the Greedy Coordinate Gradient\n(GCG) attack has demonstrated efficacy in exploiting this vulnerability by\noptimizing adversarial prompts through a combination of gradient heuristics and\ngreedy search. However, the efficiency of this attack has become a bottleneck\nin the attacking process. To mitigate this limitation, in this paper we rethink\nthe generation of the adversarial prompts through an optimization lens, aiming\nto stabilize the optimization process and harness more heuristic insights from\nprevious optimization iterations. Specifically, we propose the\n\\textbf{M}omentum \\textbf{A}ccelerated G\\textbf{C}G (\\textbf{MAC}) attack,\nwhich integrates a momentum term into the gradient heuristic to boost and\nstabilize the random search for tokens in adversarial prompts. Experimental\nresults showcase the notable enhancement achieved by MAC over baselines in\nterms of attack success rate and optimization efficiency. Moreover, we\ndemonstrate that MAC can still exhibit superior performance for transfer\nattacks and models under defense mechanisms. Our code is available at\nhttps://github.com/weizeming/momentum-attack-llm.\n","authors":["Yihao Zhang","Zeming Wei"],"pdf_url":"https://arxiv.org/pdf/2405.01229v2.pdf","comment":"Accepted by ICASSP 2025"},{"id":"http://arxiv.org/abs/2310.17953v4","updated":"2025-03-02T12:17:06Z","published":"2023-10-27T08:01:55Z","title":"Developing a Multilingual Dataset and Evaluation Metrics for\n  Code-Switching: A Focus on Hong Kong's Polylingual Dynamics","summary":"  The existing audio datasets are predominantly tailored towards single\nlanguages, overlooking the complex linguistic behaviors of multilingual\ncommunities that engage in code-switching. This practice, where individuals\nfrequently mix two or more languages in their daily interactions, is\nparticularly prevalent in multilingual regions such as Hong Kong, China. To\nbridge this gap, we have developed a 34.8-hour dataset of Mixed Cantonese and\nEnglish (MCE) audio using our Multi-Agent Data Generation Framework (MADGF). We\nfine-tuned the open-source multilingual Automatic Speech Recognition (ASR)\nmodel, Whisper, with the MCE dataset, leading to impressive zero-shot\nperformance. The traditional metrics overlook important factors such as latency\nin real-world applications and code-switching scenarios. We have introduced a\nnovel evaluation metric called Fidelity to the Original Audio, Accuracy, and\nLatency (FAL). This metric aims to overcome the limitations of traditional\nmetrics used to assess ASR systems.\n","authors":["Peng Xie","Kani Chen"],"pdf_url":"https://arxiv.org/pdf/2310.17953v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.13940v3","updated":"2025-03-02T12:11:13Z","published":"2024-08-25T21:20:17Z","title":"Derailer-Rerailer: Adaptive Verification for Efficient and Reliable\n  Language Model Reasoning","summary":"  Large Language Models (LLMs) have shown impressive reasoning capabilities,\nyet existing prompting methods face a critical trade-off: simple approaches\noften struggle with complex tasks and reasoning stability, while more\nsophisticated methods require multiple inferences and substantial computational\nresources, limiting their practical deployment. To address this challenge, we\npropose Derailer-Rerailer, a novel framework that adaptively balances reasoning\naccuracy and computational efficiency. At its core, our framework employs a\nlightweight Derailer mechanism to assess reasoning stability and selectively\ntriggers an advanced Rerailer verification process only when necessary, thereby\noptimizing computational resource usage. Extensive evaluation across both open\nand closed-source models on more than 20 categories of mathematical, symbolic,\nand commonsense reasoning tasks demonstrates our framework's effectiveness:\nDerailer-Rerailer achieves significant accuracy improvements (8-11\\% across\nvarious reasoning tasks) while maintaining 2-3 times better efficiency than\nexisting verification methods, with particularly strong performance in\nmathematical and symbolic reasoning, offering a practical solution for\nenhancing LLM reasoning reliability while significantly reducing computational\noverhead.\n","authors":["Guangya Wan","Yuqi Wu","Hao Wang","Shengming Zhao","Jie Chen","Sheng Li"],"pdf_url":"https://arxiv.org/pdf/2408.13940v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.19649v2","updated":"2025-03-02T11:23:58Z","published":"2025-02-27T00:40:01Z","title":"Taxonomy, Opportunities, and Challenges of Representation Engineering\n  for Large Language Models","summary":"  Representation Engineering (RepE) is a novel paradigm for controlling the\nbehavior of LLMs. Unlike traditional approaches that modify inputs or fine-tune\nthe model, RepE directly manipulates the model's internal representations. As a\nresult, it may offer more effective, interpretable, data-efficient, and\nflexible control over models' behavior. We present the first comprehensive\nsurvey of RepE for LLMs, reviewing the rapidly growing literature to address\nkey questions: What RepE methods exist and how do they differ? For what\nconcepts and problems has RepE been applied? What are the strengths and\nweaknesses of RepE compared to other methods? To answer these, we propose a\nunified framework describing RepE as a pipeline comprising representation\nidentification, operationalization, and control. We posit that while RepE\nmethods offer significant potential, challenges remain, including managing\nmultiple concepts, ensuring reliability, and preserving models' performance.\nTowards improving RepE, we identify opportunities for experimental and\nmethodological improvements and construct a guide for best practices.\n","authors":["Jan Wehner","Sahar Abdelnabi","Daniel Tan","David Krueger","Mario Fritz"],"pdf_url":"https://arxiv.org/pdf/2502.19649v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14666v2","updated":"2025-03-02T10:38:32Z","published":"2024-10-18T17:56:11Z","title":"DiscoGraMS: Enhancing Movie Screen-Play Summarization using Movie\n  Character-Aware Discourse Graph","summary":"  Summarizing movie screenplays presents a unique set of challenges compared to\nstandard document summarization. Screenplays are not only lengthy, but also\nfeature a complex interplay of characters, dialogues, and scenes, with numerous\ndirect and subtle relationships and contextual nuances that are difficult for\nmachine learning models to accurately capture and comprehend. Recent attempts\nat screenplay summarization focus on fine-tuning transformer-based pre-trained\nmodels, but these models often fall short in capturing long-term dependencies\nand latent relationships, and frequently encounter the \"lost in the middle\"\nissue. To address these challenges, we introduce DiscoGraMS, a novel resource\nthat represents movie scripts as a movie character-aware discourse graph (CaD\nGraph). This approach is well-suited for various downstream tasks, such as\nsummarization, question-answering, and salience detection. The model aims to\npreserve all salient information, offering a more comprehensive and faithful\nrepresentation of the screenplay's content. We further explore a baseline\nmethod that combines the CaD Graph with the corresponding movie script through\na late fusion of graph and text modalities, and we present very initial\npromising results.\n","authors":["Maitreya Prafulla Chitale","Uday Bindal","Rajakrishnan Rajkumar","Rahul Mishra"],"pdf_url":"https://arxiv.org/pdf/2410.14666v2.pdf","comment":"Accepted at NAACL 2025 (Main)"},{"id":"http://arxiv.org/abs/2502.14897v2","updated":"2025-03-02T10:18:09Z","published":"2025-02-17T21:35:18Z","title":"Market-Derived Financial Sentiment Analysis: Context-Aware Language\n  Models for Crypto Forecasting","summary":"  Financial Sentiment Analysis (FSA) traditionally relies on human-annotated\nsentiment labels to infer investor sentiment and forecast market movements.\nHowever, inferring the potential market impact of words based on their\nhuman-perceived intentions is inherently challenging. We hypothesize that the\nhistorical market reactions to words, offer a more reliable indicator of their\npotential impact on markets than subjective sentiment interpretations by human\nannotators. To test this hypothesis, a market-derived labeling approach is\nproposed to assign tweet labels based on ensuing short-term price trends,\nenabling the language model to capture the relationship between textual signals\nand market dynamics directly. A domain-specific language model was fine-tuned\non these labels, achieving up to an 11% improvement in short-term trend\nprediction accuracy over traditional sentiment-based benchmarks. Moreover, by\nincorporating market and temporal context through prompt-tuning, the proposed\ncontext-aware language model demonstrated an accuracy of 89.6% on a curated\ndataset of 227 impactful Bitcoin-related news events with significant market\nimpacts. Aggregating daily tweet predictions into trading signals, our method\noutperformed traditional fusion models (which combine sentiment-based and\nprice-based predictions). It challenged the assumption that sentiment-based\nsignals are inferior to price-based predictions in forecasting market\nmovements. Backtesting these signals across three distinct market regimes\nyielded robust Sharpe ratios of up to 5.07 in trending markets and 3.73 in\nneutral markets. Our findings demonstrate that language models can serve as\neffective short-term market predictors. This paradigm shift underscores the\nuntapped capabilities of language models in financial decision-making and opens\nnew avenues for market prediction applications.\n","authors":["Hamid Moradi-Kamali","Mohammad-Hossein Rajabi-Ghozlou","Mahdi Ghazavi","Ali Soltani","Amirreza Sattarzadeh","Reza Entezari-Maleki"],"pdf_url":"https://arxiv.org/pdf/2502.14897v2.pdf","comment":"13 pages, 6 figures"},{"id":"http://arxiv.org/abs/2408.10557v2","updated":"2025-03-02T09:59:36Z","published":"2024-08-20T05:45:04Z","title":"Speech Representation Learning Revisited: The Necessity of Separate\n  Learnable Parameters and Robust Data Augmentation","summary":"  Speech modeling methods learn one embedding for a fixed segment of speech,\ntypically in between 10-25 ms. The information present in speech can be divided\ninto two categories: \"what is being said\" (content) and \"how it is expressed\"\n(other) and these two are orthogonal in nature causing the optimization\nalgorithm to find a sub-optimal solution if forced to optimize together. This\nleads to sub-optimal performance in one or all downstream tasks as shown by\nprevious studies. Current self-supervised learning (SSL) methods such as HuBERT\nare very good at modeling the content information present in speech. Data\naugmentation improves the performance on tasks which require effective modeling\nof other information but this leads to a divided capacity of the model. In this\nwork, we conduct a preliminary study to understand the importance of modeling\nother information using separate learnable parameters. We propose a modified\nversion of HuBERT, termed Other HuBERT (O-HuBERT), to test our hypothesis. Our\nfindings are twofold: first, the O-HuBERT method is able to utilize all layers\nto build complex features to encode other information; second, a robust data\naugmentation strategy is essential for learning the information required by\ntasks that depend on other information and to achieve state-of-the-art (SOTA)\nperformance on the SUPERB benchmark with a similarly sized model (100 million\nparameters) and pre-training data (960 hours).\n","authors":["Hemant Yadav","Sunayana Sitaram","Rajiv Ratn Shah"],"pdf_url":"https://arxiv.org/pdf/2408.10557v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.04236v3","updated":"2025-03-02T09:39:57Z","published":"2024-02-06T18:43:48Z","title":"CogCoM: A Visual Language Model with Chain-of-Manipulations Reasoning","summary":"  Vision-Language Models (VLMs) have demonstrated their broad effectiveness\nthanks to extensive training in aligning visual instructions to responses.\nHowever, such training of conclusive alignment leads models to ignore essential\nvisual reasoning, further resulting in failures in meticulous visual problems\nand unfaithful responses. Drawing inspiration from human cognition in solving\nvisual problems (e.g., marking, zoom in), this paper introduces Chain of\nManipulations, a mechanism that enables VLMs to solve problems step-by-step\nwith evidence. After training, models can solve various visual problems by\neliciting intrinsic manipulations (e.g., grounding, zoom in) with results\n(e.g., boxes, image) actively without involving external tools, while also\nallowing users to trace error causes. We study the roadmap to implement this\nmechanism, including (1) a flexible design of manipulations upon extensive\nanalysis, (2) an efficient automated data generation pipeline, (3) a compatible\nVLM architecture capable of multi-turn multi-image, and (4) a model training\nprocess for versatile capabilities. With the design, we also manually annotate\n6K high-quality samples for the challenging graphical mathematical problems.\nOur trained model, \\textbf{CogCoM}, equipped with this mechanism with 17B\nparameters achieves state-of-the-art performance across 9 benchmarks from 4\ncategories, demonstrating the effectiveness while preserving the\ninterpretability. Our code, model weights, and collected data are publicly\navailable at https://github.com/THUDM/CogCoM.\n","authors":["Ji Qi","Ming Ding","Weihan Wang","Yushi Bai","Qingsong Lv","Wenyi Hong","Bin Xu","Lei Hou","Juanzi Li","Yuxiao Dong","Jie Tang"],"pdf_url":"https://arxiv.org/pdf/2402.04236v3.pdf","comment":"21 pages, 10 figures"},{"id":"http://arxiv.org/abs/2412.02956v2","updated":"2025-03-02T09:35:28Z","published":"2024-12-04T02:05:21Z","title":"Curriculum-style Data Augmentation for LLM-based Metaphor Detection","summary":"  Recently, utilizing large language models (LLMs) for metaphor detection has\nachieved promising results. However, these methods heavily rely on the\ncapabilities of closed-source LLMs, which come with relatively high inference\ncosts and latency. To address this, we propose a method for metaphor detection\nby fine-tuning open-source LLMs, effectively reducing inference costs and\nlatency with a single inference step. Furthermore, metaphor detection suffers\nfrom a severe data scarcity problem, which hinders effective fine-tuning of\nLLMs. To tackle this, we introduce Curriculum-style Data Augmentation (CDA).\nSpecifically, before fine-tuning, we evaluate the training data to identify\ncorrectly predicted instances for fine-tuning, while incorrectly predicted\ninstances are used as seed data for data augmentation. This approach enables\nthe model to quickly learn simpler knowledge and progressively acquire more\ncomplex knowledge, thereby improving performance incrementally. Experimental\nresults demonstrate that our method achieves state-of-the-art performance\nacross all baselines. Additionally, we provide detailed ablation studies to\nvalidate the effectiveness of CDA.\n","authors":["Kaidi Jia","Yanxia Wu","Ming Liu","Rongsheng Li"],"pdf_url":"https://arxiv.org/pdf/2412.02956v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.23771v2","updated":"2025-03-02T09:23:18Z","published":"2024-10-31T09:39:28Z","title":"What is Wrong with Perplexity for Long-context Language Modeling?","summary":"  Handling long-context inputs is crucial for large language models (LLMs) in\ntasks such as extended conversations, document summarization, and many-shot\nin-context learning. While recent approaches have extended the context windows\nof LLMs and employed perplexity (PPL) as a standard evaluation metric, PPL has\nproven unreliable for assessing long-context capabilities. The underlying cause\nof this limitation has remained unclear. In this work, we provide a\ncomprehensive explanation for this issue. We find that PPL overlooks key\ntokens, which are essential for long-context understanding, by averaging across\nall tokens and thereby obscuring the true performance of models in long-context\nscenarios. To address this, we propose \\textbf{LongPPL}, a novel metric that\nfocuses on key tokens by employing a long-short context contrastive method to\nidentify them. Our experiments demonstrate that LongPPL strongly correlates\nwith performance on various long-context benchmarks (e.g., Pearson correlation\nof -0.96), significantly outperforming traditional PPL in predictive accuracy.\nAdditionally, we introduce \\textbf{LongCE} (Long-context Cross-Entropy) loss, a\nre-weighting strategy for fine-tuning that prioritizes key tokens, leading to\nconsistent improvements across diverse benchmarks. In summary, these\ncontributions offer deeper insights into the limitations of PPL and present\neffective solutions for accurately evaluating and enhancing the long-context\ncapabilities of LLMs. Code is available at https://github.com/PKU-ML/LongPPL.\n","authors":["Lizhe Fang","Yifei Wang","Zhaoyang Liu","Chenheng Zhang","Stefanie Jegelka","Jinyang Gao","Bolin Ding","Yisen Wang"],"pdf_url":"https://arxiv.org/pdf/2410.23771v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.07168v2","updated":"2025-03-02T09:16:05Z","published":"2024-10-09T17:59:04Z","title":"Sylber: Syllabic Embedding Representation of Speech from Raw Audio","summary":"  Syllables are compositional units of spoken language that efficiently\nstructure human speech perception and production. However, current neural\nspeech representations lack such structure, resulting in dense token sequences\nthat are costly to process. To bridge this gap, we propose a new model, Sylber,\nthat produces speech representations with clean and robust syllabic structure.\nSpecifically, we propose a self-supervised learning (SSL) framework that\nbootstraps syllabic embeddings by distilling from its own initial unsupervised\nsyllabic segmentation. This results in a highly structured representation of\nspeech features, offering three key benefits: 1) a fast, linear-time syllable\nsegmentation algorithm, 2) efficient syllabic tokenization with an average of\n4.27 tokens per second, and 3) novel phonological units suited for efficient\nspoken language modeling. Our proposed segmentation method is highly robust and\ngeneralizes to out-of-domain data and unseen languages without any tuning. By\ntraining token-to-speech generative models, fully intelligible speech can be\nreconstructed from Sylber tokens with a significantly lower bitrate than\nbaseline SSL tokens. This suggests that our model effectively compresses speech\ninto a compact sequence of tokens with minimal information loss. Lastly, we\ndemonstrate that categorical perception-a linguistic phenomenon in speech\nperception-emerges naturally in Sylber, making the embedding space more\ncategorical and sparse than previous speech features and thus supporting the\nhigh efficiency of our tokenization. Together, we present a novel SSL approach\nfor representing speech as syllables, with significant potential for efficient\nspeech tokenization and spoken language modeling.\n","authors":["Cheol Jun Cho","Nicholas Lee","Akshat Gupta","Dhruv Agarwal","Ethan Chen","Alan W Black","Gopala K. Anumanchipalli"],"pdf_url":"https://arxiv.org/pdf/2410.07168v2.pdf","comment":"Accepted at ICLR 2025"},{"id":"http://arxiv.org/abs/2409.01281v2","updated":"2025-03-02T09:13:56Z","published":"2024-08-25T01:45:53Z","title":"Path-Consistency: Prefix Enhancement for Efficient Inference in LLM","summary":"  To enhance the reasoning capabilities of large language models (LLMs),\nself-consistency has gained significant popularity by combining multiple\nsampling with majority voting. However, the state-of-the-art self-consistency\napproaches consume substantial computational resources and lead to significant\nadditional time costs due to the multiple sampling. This prevents its full\npotential from being realized in scenarios where computational resources are\ncritical. To improve the inference efficiency, this paper introduces\n\\textit{path-consistency}, a method that leverages the confidence of answers\ngenerated in earlier branches to identify the prefix of the most promising\npath. By dynamically guiding the generation of subsequent branches based on\nthis prefix, the \\textit{path-consistency} mitigates both the errors and\nredundancies from random or less useful sampling in self-consistency. As a\nresult, it can significantly accelerate the inference process by reducing the\nnumber of tokens generated. Our extensive empirical evaluation shows that the\n\\textit{path-consistency} achieves significant acceleration in inference\nlatency ranging from $7.8\\%$ to $40.5\\%$, while maintaining or even improving\ntask accuracy across different datasets, including mathematical reasoning,\ncommon sense reasoning, symbolic reasoning, and code generation.\n","authors":["Jiace Zhu","Yingtao Shen","Jie Zhao","An Zou"],"pdf_url":"https://arxiv.org/pdf/2409.01281v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.06751v2","updated":"2025-03-02T09:10:13Z","published":"2025-01-12T08:36:38Z","title":"Padding Tone: A Mechanistic Analysis of Padding Tokens in T2I Models","summary":"  Text-to-image (T2I) diffusion models rely on encoded prompts to guide the\nimage generation process. Typically, these prompts are extended to a fixed\nlength by adding padding tokens before text encoding. Despite being a default\npractice, the influence of padding tokens on the image generation process has\nnot been investigated. In this work, we conduct the first in-depth analysis of\nthe role padding tokens play in T2I models. We develop two causal techniques to\nanalyze how information is encoded in the representation of tokens across\ndifferent components of the T2I pipeline. Using these techniques, we\ninvestigate when and how padding tokens impact the image generation process.\nOur findings reveal three distinct scenarios: padding tokens may affect the\nmodel's output during text encoding, during the diffusion process, or be\neffectively ignored. Moreover, we identify key relationships between these\nscenarios and the model's architecture (cross or self-attention) and its\ntraining process (frozen or trained text encoder). These insights contribute to\na deeper understanding of the mechanisms of padding tokens, potentially\ninforming future model design and training practices in T2I systems.\n","authors":["Michael Toker","Ido Galil","Hadas Orgad","Rinon Gal","Yoad Tewel","Gal Chechik","Yonatan Belinkov"],"pdf_url":"https://arxiv.org/pdf/2501.06751v2.pdf","comment":"Published in: NAACL 2025. Project webpage:\n  https://padding-tone.github.io/"},{"id":"http://arxiv.org/abs/2408.02976v3","updated":"2025-03-02T08:30:58Z","published":"2024-08-06T06:16:00Z","title":"Empathy Level Alignment via Reinforcement Learning for Empathetic\n  Response Generation","summary":"  Empathetic response generation, aiming to understand the user's situation and\nfeelings and respond empathically, is crucial in building human-like dialogue\nsystems. Traditional approaches typically employ maximum likelihood estimation\nas the optimization objective during training, yet fail to align the empathy\nlevels between generated and target responses. To this end, we propose an\nempathetic response generation framework using reinforcement learning (EmpRL).\nThe framework develops an effective empathy reward function and generates\nempathetic responses by maximizing the expected reward through reinforcement\nlearning. EmpRL utilizes the pre-trained T5 model as the generator and further\nfine-tunes it to initialize the policy. To align the empathy levels between\ngenerated and target responses within a given context, an empathy reward\nfunction containing three empathy communication mechanisms -- emotional\nreaction, interpretation, and exploration -- is constructed using pre-designed\nand pre-trained empathy identifiers. During reinforcement learning training,\nthe proximal policy optimization algorithm is used to fine-tune the policy,\nenabling the generation of empathetic responses. Both automatic and human\nevaluations demonstrate that the proposed EmpRL framework significantly\nimproves the quality of generated responses, enhances the similarity in empathy\nlevels between generated and target responses, and produces empathetic\nresponses covering both affective and cognitive aspects.\n","authors":["Hui Ma","Bo Zhang","Bo Xu","Jian Wang","Hongfei Lin","Xiao Sun"],"pdf_url":"https://arxiv.org/pdf/2408.02976v3.pdf","comment":"Accepted by IEEE Transactions on Affective Computing"},{"id":"http://arxiv.org/abs/2407.00886v3","updated":"2025-03-02T08:26:23Z","published":"2024-07-01T01:12:20Z","title":"Efficient Automated Circuit Discovery in Transformers using Contextual\n  Decomposition","summary":"  Automated mechanistic interpretation research has attracted great interest\ndue to its potential to scale explanations of neural network internals to large\nmodels. Existing automated circuit discovery work relies on activation patching\nor its approximations to identify subgraphs in models for specific tasks\n(circuits). They often suffer from slow runtime, approximation errors, and\nspecific requirements of metrics, such as non-zero gradients. In this work, we\nintroduce contextual decomposition for transformers (CD-T) to build\ninterpretable circuits in large language models. CD-T can produce circuits of\narbitrary level of abstraction, and is the first able to produce circuits as\nfine-grained as attention heads at specific sequence positions efficiently.\nCD-T consists of a set of mathematical equations to isolate contribution of\nmodel features. Through recursively computing contribution of all nodes in a\ncomputational graph of a model using CD-T followed by pruning, we are able to\nreduce circuit discovery runtime from hours to seconds compared to\nstate-of-the-art baselines. On three standard circuit evaluation datasets\n(indirect object identification, greater-than comparisons, and docstring\ncompletion), we demonstrate that CD-T outperforms ACDC and EAP by better\nrecovering the manual circuits with an average of 97% ROC AUC under low\nruntimes. In addition, we provide evidence that faithfulness of CD-T circuits\nis not due to random chance by showing our circuits are 80% more faithful than\nrandom circuits of up to 60% of the original model size. Finally, we show CD-T\ncircuits are able to perfectly replicate original models' behavior\n(faithfulness $ = 1$) using fewer nodes than the baselines for all tasks. Our\nresults underscore the great promise of CD-T for efficient automated\nmechanistic interpretability, paving the way for new insights into the workings\nof large language models.\n","authors":["Aliyah R. Hsu","Georgia Zhou","Yeshwanth Cherapanamjeri","Yaxuan Huang","Anobel Y. Odisho","Peter R. Carroll","Bin Yu"],"pdf_url":"https://arxiv.org/pdf/2407.00886v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09097v2","updated":"2025-03-02T07:58:08Z","published":"2025-02-13T09:13:23Z","title":"A Hybrid Transformer Model for Fake News Detection: Leveraging Bayesian\n  Optimization and Bidirectional Recurrent Unit","summary":"  In this paper, we propose an optimized Transformer model that integrates\nBayesian algorithms with a Bidirectional Gated Recurrent Unit (BiGRU), and\napply it to fake news classification for the first time. First, we employ the\nTF-IDF method to extract features from news texts and transform them into\nnumeric representations to facilitate subsequent machine learning tasks. Two\nsets of experiments are then conducted for fake news detection and\nclassification: one using a Transformer model optimized only with BiGRU, and\nthe other incorporating Bayesian algorithms into the BiGRU-based Transformer.\nExperimental results show that the BiGRU-optimized Transformer achieves 100%\naccuracy on the training set and 99.67% on the test set, while the addition of\nthe Bayesian algorithm maintains 100% accuracy on the training set and slightly\nimproves test-set accuracy to 99.73%. This indicates that the Bayesian\nalgorithm boosts model accuracy by 0.06%, further enhancing the detection\ncapability for fake news. Moreover, the proposed algorithm converges rapidly at\naround the 10th training epoch with accuracy nearing 100%, demonstrating both\nits effectiveness and its fast classification ability. Overall, the optimized\nTransformer model, enhanced by the Bayesian algorithm and BiGRU, exhibits\nexcellent continuous learning and detection performance, offering a robust\ntechnical means to combat the spread of fake news in the current era of\ninformation overload.\n","authors":["Tianyi Huang","Zeqiu Xu","Peiyang Yu","Jingyuan Yi","Xiaochuan Xu"],"pdf_url":"https://arxiv.org/pdf/2502.09097v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.13757v2","updated":"2025-03-02T07:34:35Z","published":"2024-10-17T16:53:50Z","title":"MobA: Multifaceted Memory-Enhanced Adaptive Planning for Efficient\n  Mobile Task Automation","summary":"  Existing Multimodal Large Language Model (MLLM)-based agents face significant\nchallenges in handling complex GUI (Graphical User Interface) interactions on\ndevices. These challenges arise from the dynamic and structured nature of GUI\nenvironments, which integrate text, images, and spatial relationships, as well\nas the variability in action spaces across different pages and tasks. To\naddress these limitations, we propose MobA, a novel MLLM-based mobile assistant\nsystem. MobA introduces an adaptive planning module that incorporates a\nreflection mechanism for error recovery and dynamically adjusts plans to align\nwith the real environment contexts and action module's execution capacity.\nAdditionally, a multifaceted memory module provides comprehensive memory\nsupport to enhance adaptability and efficiency. We also present MobBench, a\ndataset designed for complex mobile interactions. Experimental results on\nMobBench and AndroidArena demonstrate MobA's ability to handle dynamic GUI\nenvironments and perform complex mobile task.\n","authors":["Zichen Zhu","Hao Tang","Yansi Li","Dingye Liu","Hongshen Xu","Kunyao Lan","Danyang Zhang","Yixuan Jiang","Hao Zhou","Chenrun Wang","Situo Zhang","Liangtai Sun","Yixiao Wang","Yuheng Sun","Lu Chen","Kai Yu"],"pdf_url":"https://arxiv.org/pdf/2410.13757v2.pdf","comment":"NAACL 2025 Demo Track"},{"id":"http://arxiv.org/abs/2501.14294v3","updated":"2025-03-02T06:49:21Z","published":"2025-01-24T07:24:23Z","title":"Examining Alignment of Large Language Models through Representative\n  Heuristics: The Case of Political Stereotypes","summary":"  Examining the alignment of large language models (LLMs) has become\nincreasingly important, e.g., when LLMs fail to operate as intended. This study\nexamines the alignment of LLMs with human values for the domain of politics.\nPrior research has shown that LLM-generated outputs can include political\nleanings and mimic the stances of political parties on various issues. However,\nthe extent and conditions under which LLMs deviate from empirical positions are\ninsufficiently examined. To address this gap, we analyze the factors that\ncontribute to LLMs' deviations from empirical positions on political issues,\naiming to quantify these deviations and identify the conditions that cause\nthem.\n  Drawing on findings from cognitive science about representativeness\nheuristics, i.e., situations where humans lean on representative attributes of\na target group in a way that leads to exaggerated beliefs, we scrutinize LLM\nresponses through this heuristics' lens. We conduct experiments to determine\nhow LLMs inflate predictions about political parties, which results in\nstereotyping. We find that while LLMs can mimic certain political parties'\npositions, they often exaggerate these positions more than human survey\nrespondents do. Also, LLMs tend to overemphasize representativeness more than\nhumans. This study highlights the susceptibility of LLMs to representativeness\nheuristics, suggesting a potential vulnerability of LLMs that facilitates\npolitical stereotyping. We also test prompt-based mitigation strategies,\nfinding that strategies that can mitigate representative heuristics in humans\nare also effective in reducing the influence of representativeness on\nLLM-generated responses.\n","authors":["Sullam Jeoung","Yubin Ge","Haohan Wang","Jana Diesner"],"pdf_url":"https://arxiv.org/pdf/2501.14294v3.pdf","comment":"ICLR 2025"},{"id":"http://arxiv.org/abs/2502.17924v2","updated":"2025-03-02T06:46:48Z","published":"2025-02-25T07:44:22Z","title":"FACT-AUDIT: An Adaptive Multi-Agent Framework for Dynamic Fact-Checking\n  Evaluation of Large Language Models","summary":"  Large Language Models (LLMs) have significantly advanced the fact-checking\nstudies. However, existing automated fact-checking evaluation methods rely on\nstatic datasets and classification metrics, which fail to automatically\nevaluate the justification production and uncover the nuanced limitations of\nLLMs in fact-checking. In this work, we introduce FACT-AUDIT, an agent-driven\nframework that adaptively and dynamically assesses LLMs' fact-checking\ncapabilities. Leveraging importance sampling principles and multi-agent\ncollaboration, FACT-AUDIT generates adaptive and scalable datasets, performs\niterative model-centric evaluations, and updates assessments based on\nmodel-specific responses. By incorporating justification production alongside\nverdict prediction, this framework provides a comprehensive and evolving audit\nof LLMs' factual reasoning capabilities, to investigate their trustworthiness.\nExtensive experiments demonstrate that FACT-AUDIT effectively differentiates\namong state-of-the-art LLMs, providing valuable insights into model strengths\nand limitations in model-centric fact-checking analysis.\n","authors":["Hongzhan Lin","Yang Deng","Yuxuan Gu","Wenxuan Zhang","Jing Ma","See-Kiong Ng","Tat-Seng Chua"],"pdf_url":"https://arxiv.org/pdf/2502.17924v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.19425v3","updated":"2025-03-02T06:36:57Z","published":"2024-05-29T18:08:37Z","title":"Adaptive In-conversation Team Building for Language Model Agents","summary":"  Leveraging multiple large language model (LLM) agents has shown to be a\npromising approach for tackling complex tasks, while the effective design of\nmultiple agents for a particular application remains an art. It is thus\nintriguing to answer a critical question: Given a task, how can we build a team\nof LLM agents to solve it effectively? Our new adaptive team-building paradigm\noffers a flexible solution, realized through a novel agent design named Captain\nAgent. It dynamically forms and manages teams for each step of a task-solving\nprocess, utilizing nested group conversations and reflection to ensure diverse\nexpertise and prevent stereotypical outputs, allowing for a flexible yet\nstructured approach to problem-solving. A comprehensive evaluation across six\nreal-world scenarios demonstrates that Captain Agent significantly outperforms\nexisting multi-agent methods with 21.94% improvement in average accuracy,\nproviding outstanding performance without requiring task-specific prompt\nengineering. Our exploration of different backbone LLM and cost analysis\nfurther shows that Captain Agent can improve the conversation quality of weak\nLLM and achieve competitive performance with extremely low cost, which\nilluminates the application of multi-agent systems.\n","authors":["Linxin Song","Jiale Liu","Jieyu Zhang","Shaokun Zhang","Ao Luo","Shijian Wang","Qingyun Wu","Chi Wang"],"pdf_url":"https://arxiv.org/pdf/2405.19425v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.01902v2","updated":"2025-03-02T06:28:59Z","published":"2024-07-02T02:58:29Z","title":"SeqAR: Jailbreak LLMs with Sequential Auto-Generated Characters","summary":"  The widespread applications of large language models (LLMs) have brought\nabout concerns regarding their potential misuse. Although aligned with human\npreference data before release, LLMs remain vulnerable to various malicious\nattacks. In this paper, we adopt a red-teaming strategy to enhance LLM safety\nand introduce SeqAR, a simple yet effective framework to design jailbreak\nprompts automatically. The SeqAR framework generates and optimizes multiple\njailbreak characters and then applies sequential jailbreak characters in a\nsingle query to bypass the guardrails of the target LLM. Different from\nprevious work which relies on proprietary LLMs or seed jailbreak templates\ncrafted by human expertise, SeqAR can generate and optimize the jailbreak\nprompt in a cold-start scenario using open-sourced LLMs without any seed\njailbreak templates. Experimental results show that SeqAR achieves attack\nsuccess rates of 88% and 60% in bypassing the safety alignment of GPT-3.5-1106\nand GPT-4, respectively. Furthermore, we extensively evaluate the\ntransferability of the generated templates across different LLMs and held-out\nmalicious requests, while also exploring defense strategies against the\njailbreak attack designed by SeqAR.\n","authors":["Yan Yang","Zeguan Xiao","Xin Lu","Hongru Wang","Xuetao Wei","Hailiang Huang","Guanhua Chen","Yun Chen"],"pdf_url":"https://arxiv.org/pdf/2407.01902v2.pdf","comment":"Accepted by NAACL 2025"},{"id":"http://arxiv.org/abs/2410.07672v2","updated":"2025-03-02T06:25:14Z","published":"2024-10-10T07:29:35Z","title":"MACPO: Weak-to-Strong Alignment via Multi-Agent Contrastive Preference\n  Optimization","summary":"  As large language models (LLMs) are rapidly advancing and achieving\nnear-human capabilities on specific tasks, aligning them with human values is\nbecoming more urgent. In scenarios where LLMs outperform humans, we face a\nweak-to-strong alignment problem where we need to effectively align strong\nstudent LLMs through weak supervision generated by weak teachers. Existing\nalignment methods mainly focus on strong-to-weak alignment and self-alignment\nsettings, and it is impractical to adapt them to the much harder weak-to-strong\nalignment setting. To fill this gap, we propose a multi-agent contrastive\npreference optimization (MACPO) framework. MACPO facilitates weak teachers and\nstrong students to learn from each other by iteratively reinforcing unfamiliar\npositive behaviors while penalizing familiar negative ones. To get this, we\ndevise a mutual positive behavior augmentation strategy to encourage weak\nteachers and strong students to learn from each other's positive behavior and\nfurther provide higher quality positive behavior for the next iteration.\nAdditionally, we propose a hard negative behavior construction strategy to\ninduce weak teachers and strong students to generate familiar negative behavior\nby fine-tuning on negative behavioral data. Experimental results on the HH-RLHF\nand PKU-SafeRLHF datasets, evaluated using both automatic metrics and human\njudgments, demonstrate that MACPO simultaneously improves the alignment\nperformance of strong students and weak teachers. Moreover, as the number of\nweak teachers increases, MACPO achieves better weak-to-strong alignment\nperformance through more iteration optimization rounds.\n","authors":["Yougang Lyu","Lingyong Yan","Zihan Wang","Dawei Yin","Pengjie Ren","Maarten de Rijke","Zhaochun Ren"],"pdf_url":"https://arxiv.org/pdf/2410.07672v2.pdf","comment":"ICLR 2025"},{"id":"http://arxiv.org/abs/2410.03115v2","updated":"2025-03-02T05:16:38Z","published":"2024-10-04T03:17:27Z","title":"X-ALMA: Plug & Play Modules and Adaptive Rejection for Quality\n  Translation at Scale","summary":"  Large language models (LLMs) have achieved remarkable success across various\nNLP tasks with a focus on English due to English-centric pre-training and\nlimited multilingual data. In this work, we focus on the problem of\ntranslation, and while some multilingual LLMs claim to support for hundreds of\nlanguages, models often fail to provide high-quality responses for mid- and\nlow-resource languages, leading to imbalanced performance heavily skewed in\nfavor of high-resource languages. We introduce **X-ALMA**, a model designed to\nensure top-tier performance across 50 diverse languages, regardless of their\nresource levels. X-ALMA surpasses state-of-the-art open-source multilingual\nLLMs, such as Aya-101 and Aya-23, in every single translation direction on the\nFLORES-200 and WMT'23 test datasets according to COMET-22. This is achieved by\nplug-and-play language-specific module architecture to prevent language\nconflicts during training and a carefully designed training regimen with novel\noptimization methods to maximize the translation performance. After the final\nstage of training regimen, our proposed **A**daptive **R**ejection\n**P**reference **O**ptimization (**ARPO**) surpasses existing preference\noptimization methods in translation tasks.\n","authors":["Haoran Xu","Kenton Murray","Philipp Koehn","Hieu Hoang","Akiko Eriguchi","Huda Khayrallah"],"pdf_url":"https://arxiv.org/pdf/2410.03115v2.pdf","comment":"Published as a conference paper at ICLR 2025 (spotlight)"},{"id":"http://arxiv.org/abs/2406.09044v3","updated":"2025-03-02T04:45:56Z","published":"2024-06-13T12:30:02Z","title":"MiLoRA: Harnessing Minor Singular Components for Parameter-Efficient LLM\n  Finetuning","summary":"  Efficient finetuning of large language models (LLMs) aims to adapt the LLMs\nwith reduced computational and memory cost. Previous LoRA-based approaches\ninitialize the low-rank matrices with Gaussian distribution and zero values\nwhile keeping the original weight matrices frozen. However, the trainable model\nparameters optimized in an unguided subspace might interfere with the\nwell-learned subspace of the pretrained weight matrices. In this paper, we\npropose MiLoRA, a simple yet effective LLM finetuning approach that only\nupdates the minor singular components of the weight matrix while keeping the\nprincipal singular components frozen. It is observed that the minor matrix\ncorresponds to the noisy or long-tail information, while the principal matrix\ncontains important knowledge. The MiLoRA initializes the low-rank matrices\nwithin a subspace that is orthogonal to the principal matrix, thus the\npretrained knowledge is expected to be well preserved. During finetuning,\nMiLoRA makes the most use of the less-optimized subspace for learning the\nlabeled dataset. Extensive experiments on commonsense reasoning, math\nreasoning, instruction following and visual instruction following benchmarks\npresent the superior performance of our method.\n","authors":["Hanqing Wang","Yixia Li","Shuo Wang","Guanhua Chen","Yun Chen"],"pdf_url":"https://arxiv.org/pdf/2406.09044v3.pdf","comment":"This paper has been accepted at NAACL 2025. Code is available at:\n  https://github.com/sufenlp/MiLoRA"},{"id":"http://arxiv.org/abs/2410.21533v2","updated":"2025-03-02T04:39:42Z","published":"2024-10-28T21:02:13Z","title":"L3Ms -- Lagrange Large Language Models","summary":"  Supervised fine-tuning (SFT) and alignment of large language models (LLMs)\nare key steps in providing a good user experience. However, the concept of an\nappropriate alignment is inherently application-dependent, and current methods\noften rely on heuristic choices to drive optimization. In this work, we\nformulate SFT and alignment as a constrained optimization problem: the LLM is\nfine-tuned on a task while being required to meet application-specific\nrequirements, without resorting to heuristics. To solve this, we propose\nLagrange Large Language Models (L3Ms), which employ logarithmic barriers to\nenforce the constraints. This approach allows for the customization of L3Ms\nacross diverse applications while avoiding heuristic-driven processes. We\nexperimentally demonstrate the versatility and efficacy of L3Ms in achieving\ntailored alignments for various applications.\n","authors":["Guneet S. Dhillon","Xingjian Shi","Yee Whye Teh","Alex Smola"],"pdf_url":"https://arxiv.org/pdf/2410.21533v2.pdf","comment":"International Conference on Learning Representations (ICLR), 2025"},{"id":"http://arxiv.org/abs/2502.10709v2","updated":"2025-03-02T04:37:08Z","published":"2025-02-15T07:45:20Z","title":"An Empirical Analysis of Uncertainty in Large Language Model Evaluations","summary":"  As LLM-as-a-Judge emerges as a new paradigm for assessing large language\nmodels (LLMs), concerns have been raised regarding the alignment, bias, and\nstability of LLM evaluators. While substantial work has focused on alignment\nand bias, little research has concentrated on the stability of LLM evaluators.\nIn this paper, we conduct extensive experiments involving 9 widely used LLM\nevaluators across 2 different evaluation settings to investigate the\nuncertainty in model-based LLM evaluations. We pinpoint that LLM evaluators\nexhibit varying uncertainty based on model families and sizes. With careful\ncomparative analyses, we find that employing special prompting strategies,\nwhether during inference or post-training, can alleviate evaluation uncertainty\nto some extent. By utilizing uncertainty to enhance LLM's reliability and\ndetection capability in Out-Of-Distribution (OOD) data, we further fine-tune an\nuncertainty-aware LLM evaluator named ConfiLM using a human-annotated\nfine-tuning set and assess ConfiLM's OOD evaluation ability on a manually\ndesigned test set sourced from the 2024 Olympics. Experimental results\ndemonstrate that incorporating uncertainty as additional information during the\nfine-tuning phase can largely improve the model's evaluation performance in OOD\nscenarios. The code and data are released at:\nhttps://github.com/hasakiXie123/LLM-Evaluator-Uncertainty.\n","authors":["Qiujie Xie","Qingqiu Li","Zhuohao Yu","Yuejie Zhang","Yue Zhang","Linyi Yang"],"pdf_url":"https://arxiv.org/pdf/2502.10709v2.pdf","comment":"ICLR 2025"},{"id":"http://arxiv.org/abs/2308.11432v7","updated":"2025-03-02T04:04:03Z","published":"2023-08-22T13:30:37Z","title":"A Survey on Large Language Model based Autonomous Agents","summary":"  Autonomous agents have long been a prominent research focus in both academic\nand industry communities. Previous research in this field often focuses on\ntraining agents with limited knowledge within isolated environments, which\ndiverges significantly from human learning processes, and thus makes the agents\nhard to achieve human-like decisions. Recently, through the acquisition of vast\namounts of web knowledge, large language models (LLMs) have demonstrated\nremarkable potential in achieving human-level intelligence. This has sparked an\nupsurge in studies investigating LLM-based autonomous agents. In this paper, we\npresent a comprehensive survey of these studies, delivering a systematic review\nof the field of LLM-based autonomous agents from a holistic perspective. More\nspecifically, we first discuss the construction of LLM-based autonomous agents,\nfor which we propose a unified framework that encompasses a majority of the\nprevious work. Then, we present a comprehensive overview of the diverse\napplications of LLM-based autonomous agents in the fields of social science,\nnatural science, and engineering. Finally, we delve into the evaluation\nstrategies commonly used for LLM-based autonomous agents. Based on the previous\nstudies, we also present several challenges and future directions in this\nfield. To keep track of this field and continuously update our survey, we\nmaintain a repository of relevant references at\nhttps://github.com/Paitesanshi/LLM-Agent-Survey.\n","authors":["Lei Wang","Chen Ma","Xueyang Feng","Zeyu Zhang","Hao Yang","Jingsen Zhang","Zhiyuan Chen","Jiakai Tang","Xu Chen","Yankai Lin","Wayne Xin Zhao","Zhewei Wei","Ji-Rong Wen"],"pdf_url":"https://arxiv.org/pdf/2308.11432v7.pdf","comment":"Correcting several typos, 35 pages, 5 figures, 3 tables"},{"id":"http://arxiv.org/abs/2407.14985v5","updated":"2025-03-02T03:27:58Z","published":"2024-07-20T21:24:40Z","title":"Generalization v.s. Memorization: Tracing Language Models' Capabilities\n  Back to Pretraining Data","summary":"  The impressive capabilities of large language models (LLMs) have sparked\ndebate over whether these models genuinely generalize to unseen tasks or\npredominantly rely on memorizing vast amounts of pretraining data. To explore\nthis issue, we introduce an extended concept of memorization, distributional\nmemorization, which measures the correlation between the LLM output\nprobabilities and the pretraining data frequency. To effectively capture\ntask-specific pretraining data frequency, we propose a novel task-gram language\nmodel, which is built by counting the co-occurrence of semantically related\n$n$-gram pairs from task inputs and outputs in the pretraining corpus. Using\nthe Pythia models trained on the Pile dataset, we evaluate four distinct tasks:\nmachine translation, factual question answering, world knowledge understanding,\nand math reasoning. Our findings reveal varying levels of memorization, with\nthe strongest effect observed in factual question answering. Furthermore, while\nmodel performance improves across all tasks as LLM size increases, only factual\nquestion answering shows an increase in memorization, whereas machine\ntranslation and reasoning tasks exhibit greater generalization, producing more\nnovel outputs. This study demonstrates that memorization plays a larger role in\nsimpler, knowledge-intensive tasks, while generalization is the key for harder,\nreasoning-based tasks, providing a scalable method for analyzing large\npretraining corpora in greater depth.\n","authors":["Xinyi Wang","Antonis Antoniades","Yanai Elazar","Alfonso Amayuelas","Alon Albalak","Kexun Zhang","William Yang Wang"],"pdf_url":"https://arxiv.org/pdf/2407.14985v5.pdf","comment":"Accepted to ICLR 2025"},{"id":"http://arxiv.org/abs/2401.06603v2","updated":"2025-03-02T01:46:57Z","published":"2024-01-12T14:35:57Z","title":"Mutual Enhancement of Large Language and Reinforcement Learning Models\n  through Bi-Directional Feedback Mechanisms: A Planning Case Study","summary":"  Large Language Models (LLMs) have demonstrated remarkable capabilities for\nreinforcement learning (RL) models, such as planning and reasoning\ncapabilities. However, the problems of LLMs and RL model collaboration still\nneed to be solved. In this study, we employ a teacher-student learning\nframework to tackle these problems, specifically by offering feedback for LLMs\nusing RL models and providing high-level information for RL models with LLMs in\na cooperative multi-agent setting. Within this framework, the LLM acts as a\nteacher, while the RL model acts as a student. The two agents cooperatively\nassist each other through a process of recursive help, such as \"I help you help\nI help.\" The LLM agent supplies abstract information to the RL agent, enabling\nefficient exploration and policy improvement. In turn, the RL agent offers\nfeedback to the LLM agent, providing valuable, real-time information that helps\ngenerate more useful tokens. This bi-directional feedback loop promotes\noptimization, exploration, and mutual improvement for both agents, enabling\nthem to accomplish increasingly challenging tasks. Remarkably, we propose a\npractical algorithm to address the problem and conduct empirical experiments to\nevaluate the effectiveness of our method.\n","authors":["Shangding Gu"],"pdf_url":"https://arxiv.org/pdf/2401.06603v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.10594v2","updated":"2025-03-02T01:19:51Z","published":"2024-10-14T15:04:18Z","title":"VisRAG: Vision-based Retrieval-augmented Generation on Multi-modality\n  Documents","summary":"  Retrieval-augmented generation (RAG) is an effective technique that enables\nlarge language models (LLMs) to utilize external knowledge sources for\ngeneration. However, current RAG systems are solely based on text, rendering it\nimpossible to utilize vision information like layout and images that play\ncrucial roles in real-world multi-modality documents. In this paper, we\nintroduce VisRAG, which tackles this issue by establishing a vision-language\nmodel (VLM)-based RAG pipeline. In this pipeline, instead of first parsing the\ndocument to obtain text, the document is directly embedded using a VLM as an\nimage and then retrieved to enhance the generation of a VLM. Compared to\ntraditional text-based RAG, VisRAG maximizes the retention and utilization of\nthe data information in the original documents, eliminating the information\nloss introduced during the parsing process. We collect both open-source and\nsynthetic data to train the retriever in VisRAG and explore a variety of\ngeneration methods. Experiments demonstrate that VisRAG outperforms traditional\nRAG in both the retrieval and generation stages, achieving a 20--40% end-to-end\nperformance gain over traditional text-based RAG pipeline. Further analysis\nreveals that VisRAG is efficient in utilizing training data and demonstrates\nstrong generalization capability, positioning it as a promising solution for\nRAG on multi-modality documents. Our code and data are available at\nhttps://github.com/openbmb/visrag.\n","authors":["Shi Yu","Chaoyue Tang","Bokai Xu","Junbo Cui","Junhao Ran","Yukun Yan","Zhenghao Liu","Shuo Wang","Xu Han","Zhiyuan Liu","Maosong Sun"],"pdf_url":"https://arxiv.org/pdf/2410.10594v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.13629v3","updated":"2025-03-02T00:46:31Z","published":"2024-06-19T15:25:29Z","title":"InstructRAG: Instructing Retrieval-Augmented Generation via\n  Self-Synthesized Rationales","summary":"  Retrieval-augmented generation (RAG) has shown promising potential to enhance\nthe accuracy and factuality of language models (LMs). However, imperfect\nretrievers or noisy corpora can introduce misleading or even erroneous\ninformation to the retrieved contents, posing a significant challenge to the\ngeneration quality. Existing RAG methods typically address this challenge by\ndirectly predicting final answers despite potentially noisy inputs, resulting\nin an implicit denoising process that is difficult to interpret and verify. On\nthe other hand, the acquisition of explicit denoising supervision is often\ncostly, involving significant human efforts. In this work, we propose\nInstructRAG, where LMs explicitly learn the denoising process through\nself-synthesized rationales -- First, we instruct the LM to explain how the\nground-truth answer is derived from retrieved documents. Then, these rationales\ncan be used either as demonstrations for in-context learning of explicit\ndenoising or as supervised fine-tuning data to train the model. Compared to\nstandard RAG approaches, InstructRAG requires no additional supervision, allows\nfor easier verification of the predicted answers, and effectively improves\ngeneration accuracy. Experiments show InstructRAG consistently outperforms\nexisting RAG methods in both training-free and trainable scenarios, achieving a\nrelative improvement of 8.3% over the best baseline method on average across\nfive knowledge-intensive benchmarks. Extensive analysis indicates that\nInstructRAG scales well with increased numbers of retrieved documents and\nconsistently exhibits robust denoising ability even in out-of-domain datasets,\ndemonstrating strong generalizability.\n","authors":["Zhepei Wei","Wei-Lin Chen","Yu Meng"],"pdf_url":"https://arxiv.org/pdf/2406.13629v3.pdf","comment":"ICLR 2025. Code: https://github.com/weizhepei/InstructRAG"}]},"2025-03-01T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2409.01482v2","updated":"2025-03-01T23:34:06Z","published":"2024-09-02T22:17:18Z","title":"Masked Mixers for Language Generation and Retrieval","summary":"  Attention mechanisms that confer selective focus on a strict subset of input\nelements are nearly ubiquitous in language models today. We posit there to be\ndownside to the use of attention: most input information is lost. In support of\nthis idea we observe poor input representation accuracy in transformers and\nmore accurate representation in what we term masked mixers, which replace\nself-attention with masked convolutions. The masked mixer learns causal\nlanguage modeling more efficiently than early transformer implementations and\neven outperforms optimized, current transformers when training on small (<512)\nbut not larger context windows. Evidence is presented for the hypothesis that\ndifferences in transformer and masked mixer training efficiencies for various\ntasks are best predicted by input representation accuracy, or equivalently\nglobal invertibility. We hypothesize that the information loss exhibited by\ntransformers would be more detrimental to retrieval than generation, as the\nformer is more closely approximated by a bijective and thus invertible\nfunction. We find that masked mixers are more effective retrieval models both\nwhen the pretrained embedding model is unchanged as well as when the embedding\nmodel is modified via cosine similarity-based InfoNCE loss minimization. A\nsmall masked mixer is shown to outperform a large and near state-of-the-art\ntransformer-based retrieval model, despite the latter being trained with many\norders of magnitude more data and compute.\n","authors":["Benjamin L. Badger"],"pdf_url":"https://arxiv.org/pdf/2409.01482v2.pdf","comment":"31 pages, 8 figures, 3 tables, 9 supplementary figures, 13\n  supplementary tables"},{"id":"http://arxiv.org/abs/2410.14853v2","updated":"2025-03-01T23:22:15Z","published":"2024-10-18T20:35:28Z","title":"DFlow: Diverse Dialogue Flow Simulation with Large Language Models","summary":"  Developing language model-based dialogue agents requires effective data to\ntrain models that can follow specific task logic. However, most existing data\nsimulation methods focus on increasing diversity in language, topics, or\ndialogue acts at the utterance level, largely neglecting a critical aspect of\ntask logic diversity at the dialogue level. This paper proposes a novel data\nsimulation method designed to enhance the diversity of synthetic dialogues by\nfocusing on task execution logic. Our method uses LLMs to generate decision\ntree-structured task plans, which enables the derivation of diverse dialogue\ntrajectories for a given task. Each trajectory, referred to as a \"dialog flow\",\nguides the generation of a multi-turn dialogue that follows a unique\ntrajectory. We apply this method to generate a task-oriented dialogue dataset\ncomprising 3,886 dialogue flows across 15 different domains. We validate the\neffectiveness of this dataset using the next action prediction task, where\nmodels fine-tuned on our dataset outperform strong baselines, including GPT-4.\nUpon acceptance of this paper, we plan to release the code and data publicly.\n","authors":["Wanyu Du","Song Feng","James Gung","Lijia Sun","Yi Zhang","Saab Mansour","Yanjun Qi"],"pdf_url":"https://arxiv.org/pdf/2410.14853v2.pdf","comment":"16 pages"},{"id":"http://arxiv.org/abs/2408.09632v4","updated":"2025-03-01T23:19:07Z","published":"2024-08-19T01:30:14Z","title":"MoDeGPT: Modular Decomposition for Large Language Model Compression","summary":"  Large Language Models (LLMs) have reshaped the landscape of artificial\nintelligence by demonstrating exceptional performance across various tasks.\nHowever, substantial computational requirements make their deployment\nchallenging on devices with limited resources. Recently, compression methods\nusing low-rank matrix techniques have shown promise, yet these often lead to\ndegraded accuracy or introduce significant overhead in parameters and inference\nlatency. This paper introduces \\textbf{Mo}dular \\textbf{De}composition\n(MoDeGPT), a novel structured compression framework that does not need recovery\nfine-tuning while resolving the above drawbacks. MoDeGPT partitions the\nTransformer block into modules comprised of matrix pairs and reduces the hidden\ndimensions via reconstructing the module-level outputs. MoDeGPT is developed\nbased on a theoretical framework that utilizes three well-established matrix\ndecomposition algorithms -- Nystr\\\"om approximation, CR decomposition, and SVD\n-- and applies them to our redefined transformer modules. Our comprehensive\nexperiments show MoDeGPT, without backward propagation, matches or surpasses\nprevious structured compression methods that rely on gradient information, and\nsaves 98% of compute costs on compressing a 13B model. On \\textsc{Llama}-2/3\nand OPT models, MoDeGPT maintains 90-95% zero-shot performance with 25-30%\ncompression rates. Moreover, the compression can be done on a single GPU within\na few hours and increases the inference throughput by up to 46%.\n","authors":["Chi-Heng Lin","Shangqian Gao","James Seale Smith","Abhishek Patel","Shikhar Tuli","Yilin Shen","Hongxia Jin","Yen-Chang Hsu"],"pdf_url":"https://arxiv.org/pdf/2408.09632v4.pdf","comment":"31 pages, 9 figures"},{"id":"http://arxiv.org/abs/2410.12735v3","updated":"2025-03-01T23:03:21Z","published":"2024-10-16T16:51:01Z","title":"CREAM: Consistency Regularized Self-Rewarding Language Models","summary":"  Recent self-rewarding large language models (LLM) have successfully applied\nLLM-as-a-Judge to iteratively improve the alignment performance without the\nneed of human annotations for preference data. These methods commonly utilize\nthe same LLM to act as both the policy model (which generates responses) and\nthe reward model (which scores and ranks those responses). The ranked responses\nare then used as preference pairs to train the LLM via direct alignment\ntechnologies (e.g. DPO). However, it is noteworthy that throughout this\nprocess, there is no guarantee of accuracy in the rewarding and ranking, which\nis critical for ensuring accurate rewards and high-quality preference data.\nEmpirical results from relatively small LLMs (e.g., 7B parameters) also\nindicate that improvements from self-rewarding may diminish after several\niterations in certain situations, which we hypothesize is due to accumulated\nbias in the reward system. This bias can lead to unreliable preference data for\ntraining the LLM. To address this issue, we first formulate and analyze the\ngeneralized iterative preference fine-tuning framework for self-rewarding\nlanguage model. We then introduce the regularization to this generalized\nframework to mitigate the overconfident preference labeling in the\nself-rewarding process. Based on this theoretical insight, we propose a\nConsistency Regularized sElf-rewarding lAnguage Model (CREAM) that leverages\nthe consistency of rewards across different iterations to regularize the\nself-rewarding training, helping the model to learn from more reliable\npreference data. With this explicit regularization, our empirical results\ndemonstrate the superiority of CREAM in improving both reward consistency and\nalignment performance. The code is publicly available at\nhttps://github.com/Raibows/CREAM.\n","authors":["Zhaoyang Wang","Weilei He","Zhiyuan Liang","Xuchao Zhang","Chetan Bansal","Ying Wei","Weitong Zhang","Huaxiu Yao"],"pdf_url":"https://arxiv.org/pdf/2410.12735v3.pdf","comment":"To appear at ICLR 2025"},{"id":"http://arxiv.org/abs/2406.04604v4","updated":"2025-03-01T20:47:54Z","published":"2024-06-07T03:27:51Z","title":"Learning Task Decomposition to Assist Humans in Competitive Programming","summary":"  When using language models (LMs) to solve complex problems, humans might\nstruggle to understand the LM-generated solutions and repair the flawed ones.\nTo assist humans in repairing them, we propose to automatically decompose\ncomplex solutions into multiple simpler pieces that correspond to specific\nsubtasks. We introduce a novel objective for learning task decomposition,\ntermed assistive value (AssistV), which measures the feasibility and speed for\nhumans to repair the decomposed solution. We collect a dataset of human repair\nexperiences on different decomposed solutions. Utilizing the collected data as\nin-context examples, we then learn to critique, refine, and rank decomposed\nsolutions to improve AssistV. We validate our method under competitive\nprogramming problems: under 177 hours of human study, our method enables\nnon-experts to solve 33.3\\% more problems, speeds them up by 3.3x, and empowers\nthem to match unassisted experts.\n","authors":["Jiaxin Wen","Ruiqi Zhong","Pei Ke","Zhihong Shao","Hongning Wang","Minlie Huang"],"pdf_url":"https://arxiv.org/pdf/2406.04604v4.pdf","comment":"ACL 2024 Main Conference"},{"id":"http://arxiv.org/abs/2407.13193v3","updated":"2025-03-01T20:23:07Z","published":"2024-07-18T06:06:53Z","title":"Retrieval-Augmented Generation for Natural Language Processing: A Survey","summary":"  Large language models (LLMs) have demonstrated great success in various\nfields, benefiting from their huge amount of parameters that store knowledge.\nHowever, LLMs still suffer from several key issues, such as hallucination\nproblems, knowledge update issues, and lacking domain-specific expertise. The\nappearance of retrieval-augmented generation (RAG), which leverages an external\nknowledge database to augment LLMs, makes up those drawbacks of LLMs. This\npaper reviews all significant techniques of RAG, especially in the retriever\nand the retrieval fusions. Besides, tutorial codes are provided for\nimplementing the representative techniques in RAG. This paper further discusses\nthe RAG update, including RAG with/without knowledge update. Then, we introduce\nRAG evaluation and benchmarking, as well as the application of RAG in\nrepresentative NLP tasks and industrial scenarios. Finally, this paper\ndiscusses RAG's future directions and challenges for promoting this field's\ndevelopment.\n","authors":["Shangyu Wu","Ying Xiong","Yufei Cui","Haolun Wu","Can Chen","Ye Yuan","Lianming Huang","Xue Liu","Tei-Wei Kuo","Nan Guan","Chun Jason Xue"],"pdf_url":"https://arxiv.org/pdf/2407.13193v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.19607v2","updated":"2025-03-01T19:33:15Z","published":"2025-02-26T22:45:08Z","title":"Revisiting Word Embeddings in the LLM Era","summary":"  Large Language Models (LLMs) have recently shown remarkable advancement in\nvarious NLP tasks. As such, a popular trend has emerged lately where NLP\nresearchers extract word/sentence/document embeddings from these large\ndecoder-only models and use them for various inference tasks with promising\nresults. However, it is still unclear whether the performance improvement of\nLLM-induced embeddings is merely because of scale or whether underlying\nembeddings they produce significantly differ from classical encoding models\nlike Word2Vec, GloVe, Sentence-BERT (SBERT) or Universal Sentence Encoder\n(USE). This is the central question we investigate in the paper by\nsystematically comparing classical decontextualized and contextualized word\nembeddings with the same for LLM-induced embeddings. Our results show that LLMs\ncluster semantically related words more tightly and perform better on analogy\ntasks in decontextualized settings. However, in contextualized settings,\nclassical models like SimCSE often outperform LLMs in sentence-level similarity\nassessment tasks, highlighting their continued relevance for fine-grained\nsemantics.\n","authors":["Yash Mahajan","Matthew Freestone","Sathyanarayanan Aakur","Santu Karmaker"],"pdf_url":"https://arxiv.org/pdf/2502.19607v2.pdf","comment":"This work was intended as a replacement of the older version,\n  arXiv:2402.11094, and any subsequent updates will appear there"},{"id":"http://arxiv.org/abs/2402.11094v3","updated":"2025-03-01T19:27:41Z","published":"2024-02-16T21:47:30Z","title":"Revisiting Word Embeddings in the LLM Era","summary":"  Large Language Models (LLMs) have recently shown remarkable advancement in\nvarious NLP tasks. As such, a popular trend has emerged lately where NLP\nresearchers extract word/sentence/document embeddings from these large\ndecoder-only models and use them for various inference tasks with promising\nresults. However, it is still unclear whether the performance improvement of\nLLM-induced embeddings is merely because of scale or whether underlying\nembeddings they produce significantly differ from classical encoding models\nlike Word2Vec, GloVe, Sentence-BERT (SBERT) or Universal Sentence Encoder\n(USE). This is the central question we investigate in the paper by\nsystematically comparing classical decontextualized and contextualized word\nembeddings with the same for LLM-induced embeddings. Our results show that LLMs\ncluster semantically related words more tightly and perform better on analogy\ntasks in decontextualized settings. However, in contextualized settings,\nclassical models like SimCSE often outperform LLMs in sentence-level similarity\nassessment tasks, highlighting their continued relevance for fine-grained\nsemantics.\n","authors":["Yash Mahajan","Matthew Freestone","Naman Bansal","Sathyanarayanan Aakur","Shubhra Kanti Karmaker Santu"],"pdf_url":"https://arxiv.org/pdf/2402.11094v3.pdf","comment":"This is an updated version of the older version: 2402.11094. We\n  accidentally submitted this article as a new submission (2502.19607), which\n  we have requested to withdraw. This version has 30 pages and 22 figures"},{"id":"http://arxiv.org/abs/2403.08694v4","updated":"2025-03-01T19:25:49Z","published":"2024-03-13T16:57:57Z","title":"TeaMs-RL: Teaching LLMs to Generate Better Instruction Datasets via\n  Reinforcement Learning","summary":"  The development of Large Language Models (LLMs) often confronts challenges\nstemming from the heavy reliance on human annotators in the reinforcement\nlearning with human feedback (RLHF) framework, or the frequent and costly\nexternal queries tied to the self-instruct paradigm. In this work, we pivot to\nReinforcement Learning (RL) -- but with a twist. Diverging from the typical\nRLHF, which refines LLMs following instruction data training, we use RL to\ndirectly generate the foundational instruction dataset that alone suffices for\nfine-tuning. Our method, TeaMs-RL, uses a suite of textual operations and\nrules, prioritizing the diversification of training datasets. It facilitates\nthe generation of high-quality data without excessive reliance on external\nadvanced models, paving the way for a single fine-tuning step and negating the\nneed for subsequent RLHF stages. Our findings highlight key advantages of our\napproach: reduced need for human involvement and fewer model queries (only\n5.73% of the strong baseline's total), along with enhanced capabilities of LLMs\nin crafting and comprehending complex instructions compared to strong\nbaselines, and substantially improved model privacy protection. Code is\navailable at the link: https://github.com/SafeRL-Lab/TeaMs-RL\n","authors":["Shangding Gu","Alois Knoll","Ming Jin"],"pdf_url":"https://arxiv.org/pdf/2403.08694v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.09102v2","updated":"2025-03-01T19:06:35Z","published":"2024-10-09T12:52:41Z","title":"Instructional Segment Embedding: Improving LLM Safety with Instruction\n  Hierarchy","summary":"  Large Language Models (LLMs) are susceptible to security and safety threats,\nsuch as prompt injection, prompt extraction, and harmful requests. One major\ncause of these vulnerabilities is the lack of an instruction hierarchy. Modern\nLLM architectures treat all inputs equally, failing to distinguish between and\nprioritize various types of instructions, such as system messages, user\nprompts, and data. As a result, lower-priority user prompts may override more\ncritical system instructions, including safety protocols. Existing approaches\nto achieving instruction hierarchy, such as delimiters and instruction-based\ntraining, do not address this issue at the architectural level. We introduce\nthe Instructional Segment Embedding (ISE) technique, inspired by BERT, to\nmodern large language models, which embeds instruction priority information\ndirectly into the model. This approach enables models to explicitly\ndifferentiate and prioritize various instruction types, significantly improving\nsafety against malicious prompts that attempt to override priority rules. Our\nexperiments on the Structured Query and Instruction Hierarchy benchmarks\ndemonstrate an average robust accuracy increase of up to 15.75% and 18.68%,\nrespectively. Furthermore, we observe an improvement in instruction-following\ncapability of up to 4.1% evaluated on AlpacaEval. Overall, our approach offers\na promising direction for enhancing the safety and effectiveness of LLM\narchitectures.\n","authors":["Tong Wu","Shujian Zhang","Kaiqiang Song","Silei Xu","Sanqiang Zhao","Ravi Agrawal","Sathish Reddy Indurthi","Chong Xiang","Prateek Mittal","Wenxuan Zhou"],"pdf_url":"https://arxiv.org/pdf/2410.09102v2.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2410.20513v4","updated":"2025-03-01T18:58:42Z","published":"2024-10-27T16:52:21Z","title":"Self-correction is Not An Innate Capability in Large Language Models: A\n  Case Study of Moral Self-correction","summary":"  Though there has been intensive attention to the self-correction capability\nof Large Language Models (LLMs), conclusions regarding its effectiveness remain\nvaried. In this paper, we investigate a fundamental question: is moral\nself-correction an innate capability in LLMs? To explore this, we conduct (1) a\nmechanistic analysis of how key components of self-correction, such as\nChain-of-Thought (CoT) reasoning and external feedback, interact to enable\nmoral self-correction; and (2) a behavioral analysis of LLMs' ability to\ndistinguish between desired and undesired outputs, introducing a\nself-distinguish framework. Our mechanistic analysis reveals that LLMs struggle\nto effectively leverage helpful feedback, and conflicts can arise between\nfeedback and CoT reasoning. These limitations suggest that LLMs fail to\nidentify useful contextual information, instead prioritizing their own internal\nknowledge. Additionally, our behavioral analysis indicates that LLMs struggle\nto differentiate among their own outputs. Based on these empirical findings\nacross two analytical dimensions, mechanism and behavior, we argue that moral\nself-correction is not an innate capability of LLMs.\n","authors":["Zimo Qi","Guangliang Liu","Kristen Marie Johnson","Lu Cheng"],"pdf_url":"https://arxiv.org/pdf/2410.20513v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.00053v3","updated":"2025-03-01T18:42:10Z","published":"2024-05-28T21:38:20Z","title":"Dual Process Learning: Controlling Use of In-Context vs. In-Weights\n  Strategies with Weight Forgetting","summary":"  Language models have the ability to perform in-context learning (ICL),\nallowing them to flexibly adapt their behavior based on context. This contrasts\nwith in-weights learning (IWL), where memorized information is encoded in model\nparameters after iterated observations of data. An ideal model should be able\nto flexibly deploy both of these abilities. Despite their apparent ability to\nlearn in-context, language models are known to struggle when faced with unseen\nor rarely seen tokens (Land & Bartolo, 2024). Hence, we study\n$\\textbf{structural in-context learning}$, which we define as the ability of a\nmodel to execute in-context learning on arbitrary novel tokens -- so called\nbecause the model must generalize on the basis of e.g. sentence structure or\ntask structure, rather than content encoded in token embeddings. We study\nstructural in-context algorithms on both synthetic and naturalistic tasks using\ntoy models, masked language models, and autoregressive language models. We find\nthat structural ICL appears before quickly disappearing early in LM\npretraining. While it has been shown that ICL can diminish during training\n(Singh et al., 2023), we find that prior work does not account for structural\nICL. Building on Chen et al. (2024) 's active forgetting method, we introduce\npretraining and finetuning methods that can modulate the preference for\nstructural ICL and IWL. Importantly, this allows us to induce a $\\textit{dual\nprocess strategy}$ where in-context and in-weights solutions coexist within a\nsingle model.\n","authors":["Suraj Anand","Michael A. Lepori","Jack Merullo","Ellie Pavlick"],"pdf_url":"https://arxiv.org/pdf/2406.00053v3.pdf","comment":"10 pages, 6 figures"},{"id":"http://arxiv.org/abs/2405.01649v4","updated":"2025-03-01T17:24:49Z","published":"2024-05-02T18:12:08Z","title":"Improving Complex Reasoning over Knowledge Graph with Logic-Aware\n  Curriculum Tuning","summary":"  Answering complex queries over incomplete knowledge graphs (KGs) is a\nchallenging job. Most previous works have focused on learning entity/relation\nembeddings and simulating first-order logic operators with various neural\nnetworks. However, they are bottlenecked by the inability to share world\nknowledge to improve logical reasoning, thus resulting in suboptimal\nperformance. In this paper, we propose a complex reasoning schema over KG upon\nlarge language models (LLMs), containing a curriculum-based logical-aware\ninstruction tuning framework, named LACT. Specifically, we augment the\narbitrary first-order logical queries via binary tree decomposition, to\nstimulate the reasoning capability of LLMs. To address the difficulty gap among\ndifferent types of complex queries, we design a simple and flexible logic-aware\ncurriculum learning framework. Experiments across widely used datasets\ndemonstrate that LACT has substantial improvements~(brings an average +5.5% MRR\nscore) over advanced methods, achieving the new state-of-the-art.\n","authors":["Tianle Xia","Liang Ding","Guojia Wan","Yibing Zhan","Bo Du","Dacheng Tao"],"pdf_url":"https://arxiv.org/pdf/2405.01649v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.17173v2","updated":"2025-03-01T17:23:31Z","published":"2025-02-24T14:09:45Z","title":"Cheems: A Practical Guidance for Building and Evaluating Chinese Reward\n  Models from Scratch","summary":"  Reward models (RMs) are crucial for aligning large language models (LLMs)\nwith human preferences. However, most RM research is centered on English and\nrelies heavily on synthetic resources, which leads to limited and less reliable\ndatasets and benchmarks for Chinese. To address this gap, we introduce\nCheemsBench, a fully human-annotated RM evaluation benchmark within Chinese\ncontexts, and CheemsPreference, a large-scale and diverse preference dataset\nannotated through human-machine collaboration to support Chinese RM training.\nWe systematically evaluate open-source discriminative and generative RMs on\nCheemsBench and observe significant limitations in their ability to capture\nhuman preferences in Chinese scenarios. Additionally, based on\nCheemsPreference, we construct an RM that achieves state-of-the-art performance\non CheemsBench, demonstrating the necessity of human supervision in RM\ntraining. Our findings reveal that scaled AI-generated data struggles to fully\ncapture human preferences, emphasizing the importance of high-quality human\nsupervision in RM development.\n","authors":["Xueru Wen","Jie Lou","Zichao Li","Yaojie Lu","Xing Yu","Yuqiu Ji","Guohai Xu","Hongyu Lin","Ben He","Xianpei Han","Le Sun","Debing Zhang"],"pdf_url":"https://arxiv.org/pdf/2502.17173v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.17631v2","updated":"2025-03-01T17:06:43Z","published":"2023-10-26T17:48:58Z","title":"JudgeLM: Fine-tuned Large Language Models are Scalable Judges","summary":"  Evaluating Large Language Models (LLMs) in open-ended scenarios is\nchallenging because existing benchmarks and metrics can not measure them\ncomprehensively. To address this problem, we propose to fine-tune LLMs as\nscalable judges (JudgeLM) to evaluate LLMs efficiently and effectively in\nopen-ended benchmarks. We first propose a comprehensive, large-scale,\nhigh-quality dataset containing task seeds, LLMs-generated answers, and\nGPT-4-generated judgments for fine-tuning high-performance judges, as well as a\nnew benchmark for evaluating the judges. We train JudgeLM at different scales\nfrom 7B, 13B, to 33B parameters, and conduct a systematic analysis of its\ncapabilities and behaviors. We then analyze the key biases in fine-tuning LLM\nas a judge and consider them as position bias, knowledge bias, and format bias.\nTo address these issues, JudgeLM introduces a bag of techniques including swap\naugmentation, reference support, and reference drop, which clearly enhance the\njudge's performance. JudgeLM obtains the state-of-the-art judge performance on\nboth the existing PandaLM benchmark and our proposed new benchmark. Our JudgeLM\nis efficient and the JudgeLM-7B only needs 3 minutes to judge 5K samples with 8\nA100 GPUs. JudgeLM obtains high agreement with the teacher judge, achieving an\nagreement exceeding 90% that even surpasses human-to-human agreement. JudgeLM\nalso demonstrates extended capabilities in being judges of the single answer,\nmultimodal models, multiple answers, multi-turn chat, etc. Code is available at\nhttps://github.com/baaivision/JudgeLM.\n","authors":["Lianghui Zhu","Xinggang Wang","Xinlong Wang"],"pdf_url":"https://arxiv.org/pdf/2310.17631v2.pdf","comment":"JudgeLM is accepted by ICLR2025. Code is available at\n  https://github.com/baaivision/JudgeLM"},{"id":"http://arxiv.org/abs/2411.17637v2","updated":"2025-03-01T16:07:45Z","published":"2024-11-26T17:55:37Z","title":"On Limitations of LLM as Annotator for Low Resource Languages","summary":"  Low-resource languages face significant challenges due to the lack of\nsufficient linguistic data, resources, and tools for tasks such as supervised\nlearning, annotation, and classification. This shortage hinders the development\nof accurate models and datasets, making it difficult to perform critical NLP\ntasks like sentiment analysis or hate speech detection. To bridge this gap,\nLarge Language Models (LLMs) present an opportunity for potential annotators,\ncapable of generating datasets and resources for these underrepresented\nlanguages. In this paper, we focus on Marathi, a low-resource language, and\nevaluate the performance of both closed-source and open-source LLMs as\nannotators, while also comparing these results with fine-tuned BERT models. We\nassess models such as GPT-4o and Gemini 1.0 Pro, Gemma 2 (2B and 9B), and Llama\n3.1 (8B and 405B) on classification tasks including sentiment analysis, news\nclassification, and hate speech detection. Our findings reveal that while LLMs\nexcel in annotation tasks for high-resource languages like English, they still\nfall short when applied to Marathi. Even advanced models like GPT-4o and Llama\n3.1 405B underperform compared to fine-tuned BERT-based baselines, with GPT-4o\nand Llama 3.1 405B trailing fine-tuned BERT by accuracy margins of 10.2% and\n14.1%, respectively. This highlights the limitations of LLMs as annotators for\nlow-resource languages.\n","authors":["Suramya Jadhav","Abhay Shanbhag","Amogh Thakurdesai","Ridhima Sinare","Raviraj Joshi"],"pdf_url":"https://arxiv.org/pdf/2411.17637v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.16806v3","updated":"2025-03-01T15:17:50Z","published":"2025-02-24T03:30:29Z","title":"CoT2Align: Cross-Chain of Thought Distillation via Optimal Transport\n  Alignment for Language Models with Different Tokenizers","summary":"  Large Language Models (LLMs) achieve state-of-the-art performance across\nvarious NLP tasks but face deployment challenges due to high computational\ncosts and memory constraints. Knowledge distillation (KD) is a promising\nsolution, transferring knowledge from large teacher models to smaller student\nmodels. However, existing KD methods often assume shared vocabularies and\ntokenizers, limiting their flexibility. While approaches like Universal Logit\nDistillation (ULD) and Dual-Space Knowledge Distillation (DSKD) address\nvocabulary mismatches, they overlook the critical \\textbf{reasoning-aware\ndistillation} aspect. To bridge this gap, we propose CoT2Align a universal KD\nframework that integrates Chain-of-Thought (CoT) augmentation and introduces\nCross-CoT Alignment to enhance reasoning transfer. Additionally, we extend\nOptimal Transport beyond token-wise alignment to a sequence-level and\nlayer-wise alignment approach that adapts to varying sequence lengths while\npreserving contextual integrity. Comprehensive experiments demonstrate that\nCoT2Align outperforms existing KD methods across different vocabulary settings,\nimproving reasoning capabilities and robustness in domain-specific tasks.\n","authors":["Anh Duc Le","Tu Vu","Nam Le Hai","Nguyen Thi Ngoc Diep","Linh Ngo Van","Trung Le","Thien Huu Nguyen"],"pdf_url":"https://arxiv.org/pdf/2502.16806v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.20207v2","updated":"2025-03-01T14:39:33Z","published":"2024-07-29T17:39:08Z","title":"QAEA-DR: A Unified Text Augmentation Framework for Dense Retrieval","summary":"  In dense retrieval, embedding long texts into dense vectors can result in\ninformation loss, leading to inaccurate query-text matching. Additionally,\nlow-quality texts with excessive noise or sparse key information are unlikely\nto align well with relevant queries. Recent studies mainly focus on improving\nthe sentence embedding model or retrieval process. In this work, we introduce a\nnovel text augmentation framework for dense retrieval. This framework\ntransforms raw documents into information-dense text formats, which supplement\nthe original texts to effectively address the aforementioned issues without\nmodifying embedding or retrieval methodologies. Two text representations are\ngenerated via large language models (LLMs) zero-shot prompting: question-answer\npairs and element-driven events. We term this approach QAEA-DR: unifying\nquestion-answer generation and event extraction in a text augmentation\nframework for dense retrieval. To further enhance the quality of generated\ntexts, a scoring-based evaluation and regeneration mechanism is introduced in\nLLM prompting. Our QAEA-DR model has a positive impact on dense retrieval,\nsupported by both theoretical analysis and empirical experiments.\n","authors":["Hongming Tan","Shaoxiong Zhan","Hai Lin","Hai-Tao Zheng","Wai Kin Chan"],"pdf_url":"https://arxiv.org/pdf/2407.20207v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.16406v2","updated":"2025-03-01T14:22:20Z","published":"2024-02-26T08:59:05Z","title":"From RAGs to riches: Utilizing large language models to write documents\n  for clinical trials","summary":"  This manuscript has now been published: - Link to article on journal website:\nhttps://journals.sagepub.com/doi/10.1177/17407745251320806 - Pubmed link:\nhttps://pubmed.ncbi.nlm.nih.gov/40013826/\n","authors":["Nigel Markey","Ilyass El-Mansouri","Gaetan Rensonnet","Casper van Langen","Christoph Meier"],"pdf_url":"https://arxiv.org/pdf/2402.16406v2.pdf","comment":"6 pages, 2 figures"},{"id":"http://arxiv.org/abs/2409.06666v2","updated":"2025-03-01T12:59:49Z","published":"2024-09-10T17:34:34Z","title":"LLaMA-Omni: Seamless Speech Interaction with Large Language Models","summary":"  Models like GPT-4o enable real-time interaction with large language models\n(LLMs) through speech, significantly enhancing user experience compared to\ntraditional text-based interaction. However, there is still a lack of\nexploration on how to build speech interaction models based on open-source\nLLMs. To address this, we propose LLaMA-Omni, a novel model architecture\ndesigned for low-latency and high-quality speech interaction with LLMs.\nLLaMA-Omni integrates a pretrained speech encoder, a speech adaptor, an LLM,\nand a streaming speech decoder. It eliminates the need for speech\ntranscription, and can simultaneously generate text and speech responses\ndirectly from speech instructions with extremely low latency. We build our\nmodel based on the latest Llama-3.1-8B-Instruct model. To align the model with\nspeech interaction scenarios, we construct a dataset named InstructS2S-200K,\nwhich includes 200K speech instructions and corresponding speech responses.\nExperimental results show that compared to previous speech-language models,\nLLaMA-Omni provides better responses in both content and style, with a response\nlatency as low as 226ms. Additionally, training LLaMA-Omni takes less than 3\ndays on just 4 GPUs, paving the way for the efficient development of\nspeech-language models in the future.\n","authors":["Qingkai Fang","Shoutao Guo","Yan Zhou","Zhengrui Ma","Shaolei Zhang","Yang Feng"],"pdf_url":"https://arxiv.org/pdf/2409.06666v2.pdf","comment":"ICLR 2025"},{"id":"http://arxiv.org/abs/2402.02611v3","updated":"2025-03-01T12:46:25Z","published":"2024-02-04T20:56:09Z","title":"FCoReBench: Can Large Language Models Solve Challenging First-Order\n  Combinatorial Reasoning Problems?","summary":"  Can the large language models (LLMs) solve challenging first-order\ncombinatorial reasoning problems such as graph coloring, knapsack, and\ncryptarithmetic? By first-order, we mean these problems can be instantiated\nwith potentially an infinite number of problem instances of varying sizes. They\nare also challenging being NP-hard and requiring several reasoning steps to\nreach a solution. While existing work has focused on coming up with datasets\nwith hard benchmarks, there is limited work which exploits the first-order\nnature of the problem structure. To address this challenge, we present\nFCoReBench, a dataset of 40 such challenging problems, along with scripts to\ngenerate problem instances of varying sizes and automatically verify and\ngenerate their solutions. We first observe that LLMs, even when aided by\nsymbolic solvers, perform rather poorly on our dataset, being unable to\nleverage the underlying structure of these problems. We specifically observe a\ndrop in performance with increasing problem size. In response, we propose a new\napproach, SymPro-LM, which combines LLMs with both symbolic solvers and program\ninterpreters, along with feedback from a few solved examples, to achieve huge\nperformance gains. Our proposed approach is robust to changes in the problem\nsize, and has the unique characteristic of not requiring any LLM call during\ninference time, unlike earlier approaches. As an additional experiment, we also\ndemonstrate SymPro-LM's effectiveness on other logical reasoning benchmarks.\n","authors":["Chinmay Mittal","Krishna Kartik"," Mausam","Parag Singla"],"pdf_url":"https://arxiv.org/pdf/2402.02611v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.04909v2","updated":"2025-03-01T12:40:09Z","published":"2024-08-09T07:31:06Z","title":"Surveying the Landscape of Image Captioning Evaluation: A Comprehensive\n  Taxonomy, Trends and Metrics Analysis","summary":"  The task of image captioning has recently been gaining popularity, and with\nit the complex task of evaluating the quality of image captioning models. In\nthis work, we present the first survey and taxonomy of over 70 different image\ncaptioning metrics and their usage in hundreds of papers, specifically designed\nto help users select the most suitable metric for their needs. We find that\ndespite the diversity of proposed metrics, the vast majority of studies rely on\nonly five popular metrics, which we show to be weakly correlated with human\nratings. We hypothesize that combining a diverse set of metrics can enhance\ncorrelation with human ratings. As an initial step, we demonstrate that a\nlinear regression-based ensemble method, which we call EnsembEval, trained on\none human ratings dataset, achieves improved correlation across five additional\ndatasets, showing there is a lot of room for improvement by leveraging a\ndiverse set of metrics.\n","authors":["Uri Berger","Gabriel Stanovsky","Omri Abend","Lea Frermann"],"pdf_url":"https://arxiv.org/pdf/2408.04909v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.04286v2","updated":"2025-03-01T11:19:12Z","published":"2024-05-07T12:57:01Z","title":"Who Wrote This? The Key to Zero-Shot LLM-Generated Text Detection Is\n  GECScore","summary":"  The efficacy of detectors for texts generated by large language models (LLMs)\nsubstantially depends on the availability of large-scale training data.\nHowever, white-box zero-shot detectors, which require no such data, are limited\nby the accessibility of the source model of the LLM-generated text. In this\npaper, we propose a simple yet effective black-box zero-shot detection approach\nbased on the observation that, from the perspective of LLMs, human-written\ntexts typically contain more grammatical errors than LLM-generated texts. This\napproach involves calculating the Grammar Error Correction Score (GECScore) for\nthe given text to differentiate between human-written and LLM-generated text.\nExperimental results show that our method outperforms current state-of-the-art\n(SOTA) zero-shot and supervised methods, achieving an average AUROC of 98.62%\nacross XSum and Writing Prompts dataset. Additionally, our approach\ndemonstrates strong reliability in the wild, exhibiting robust generalization\nand resistance to paraphrasing attacks. Data and code are available at:\nhttps://github.com/NLP2CT/GECScore.\n","authors":["Junchao Wu","Runzhe Zhan","Derek F. Wong","Shu Yang","Xuebo Liu","Lidia S. Chao","Min Zhang"],"pdf_url":"https://arxiv.org/pdf/2405.04286v2.pdf","comment":"COLING 2025"},{"id":"http://arxiv.org/abs/2502.17810v2","updated":"2025-03-01T11:14:44Z","published":"2025-02-25T03:31:48Z","title":"URO-Bench: A Comprehensive Benchmark for End-to-End Spoken Dialogue\n  Models","summary":"  In recent years, with advances in large language models (LLMs), end-to-end\nspoken dialogue models (SDMs) have made significant strides. Compared to\ntext-based LLMs, the evaluation of SDMs needs to take speech-related aspects\ninto account, such as paralinguistic information and speech quality. However,\nthere is still a lack of comprehensive evaluations for SDMs in speech-to-speech\n(S2S) scenarios. To address this gap, we propose URO-Bench, an extensive\nbenchmark for SDMs. Notably, URO-Bench is the first S2S benchmark that covers\nevaluations about multilingualism, multi-round dialogues, and paralinguistics.\nOur benchmark is divided into two difficulty levels: basic track and pro track,\nconsisting of 16 and 20 datasets respectively, evaluating the model's abilities\nin Understanding, Reasoning, and Oral conversation. Evaluations on our proposed\nbenchmark reveal that current open-source SDMs perform rather well in daily QA\ntasks, but lag behind their backbone LLMs in terms of instruction-following\nability and also suffer from catastrophic forgetting. Their performance in\nadvanced evaluations of paralinguistic information and audio understanding\nremains subpar, highlighting the need for further research in this direction.\nWe hope that URO-Bench can effectively facilitate the development of spoken\ndialogue models by providing a multifaceted evaluation of existing models and\nhelping to track progress in this area.\n","authors":["Ruiqi Yan","Xiquan Li","Wenxi Chen","Zhikang Niu","Chen Yang","Ziyang Ma","Kai Yu","Xie Chen"],"pdf_url":"https://arxiv.org/pdf/2502.17810v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.18448v3","updated":"2025-03-01T09:48:15Z","published":"2024-05-28T01:15:21Z","title":"Multi-objective Representation for Numbers in Clinical Narratives: A\n  CamemBERT-Bio-Based Alternative to Large-Scale LLMs","summary":"  The processing of numerical values is a rapidly developing area in the field\nof Language Models (LLMs). Despite numerous advancements achieved by previous\nresearch, significant challenges persist, particularly within the healthcare\ndomain. This paper investigates the limitations of Transformer models in\nunderstanding numerical values. \\textit{Objective:} this research aims to\ncategorize numerical values extracted from medical documents into eight\nspecific physiological categories using CamemBERT-bio. \\textit{Methods:} In a\ncontext where scalable methods and Large Language Models (LLMs) are emphasized,\nwe explore lifting the limitations of transformer-based models. We examine two\nstrategies: fine-tuning CamemBERT-bio on a small medical dataset, integrating\nLabel Embedding for Self-Attention (LESA), and combining LESA with additional\nenhancement techniques such as Xval. Given that CamemBERT-bio is already\npre-trained on a large medical dataset, the first approach aims to update its\nencoder with the newly added label embeddings technique. In contrast, the\nsecond approach seeks to develop multiple representations of numbers\n(contextual and magnitude-based) to achieve more robust number embeddings.\n\\textit{Results:} As anticipated, fine-tuning the standard CamemBERT-bio on our\nsmall medical dataset did not improve F1 scores. However, significant\nimprovements were observed with CamemBERT-bio + LESA, resulting in an over 13\\%\nincrease. Similar enhancements were noted when combining LESA with Xval,\noutperforming conventional methods and giving comparable results to GPT-4\n\\textit{Conclusions and Novelty:} This study introduces two innovative\ntechniques for handling numerical data, which are also applicable to other\nmodalities. We illustrate how these techniques can improve the performance of\nTransformer-based models, achieving more reliable classification results even\nwith small datasets.\n","authors":["Boammani Aser Lompo","Thanh-Dung Le"],"pdf_url":"https://arxiv.org/pdf/2405.18448v3.pdf","comment":"Under the revision. arXiv admin note: substantial text overlap with\n  arXiv:2404.10171"},{"id":"http://arxiv.org/abs/2410.06615v2","updated":"2025-03-01T08:32:14Z","published":"2024-10-09T07:12:24Z","title":"QA-Calibration of Language Model Confidence Scores","summary":"  To use generative question-and-answering (QA) systems for decision-making and\nin any critical application, these systems need to provide well-calibrated\nconfidence scores that reflect the correctness of their answers. Existing\ncalibration methods aim to ensure that the confidence score is, *on average*,\nindicative of the likelihood that the answer is correct. We argue, however,\nthat this standard (average-case) notion of calibration is difficult to\ninterpret for decision-making in generative QA. To address this, we generalize\nthe standard notion of average calibration and introduce QA-calibration, which\nensures calibration holds across different question-and-answer groups. We then\npropose discretized posthoc calibration schemes for achieving QA-calibration.\nWe establish distribution-free guarantees on the performance of this method and\nvalidate our method on confidence scores returned by elicitation prompts across\nmultiple QA benchmarks and large language models (LLMs).\n","authors":["Putra Manggala","Atalanti Mastakouri","Elke Kirschbaum","Shiva Prasad Kasiviswanathan","Aaditya Ramdas"],"pdf_url":"https://arxiv.org/pdf/2410.06615v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.19870v2","updated":"2025-03-01T08:16:00Z","published":"2025-02-27T08:21:28Z","title":"MMKE-Bench: A Multimodal Editing Benchmark for Diverse Visual Knowledge","summary":"  Knowledge editing techniques have emerged as essential tools for updating the\nfactual knowledge of large language models (LLMs) and multimodal models (LMMs),\nallowing them to correct outdated or inaccurate information without retraining\nfrom scratch. However, existing benchmarks for multimodal knowledge editing\nprimarily focus on entity-level knowledge represented as simple triplets, which\nfail to capture the complexity of real-world multimodal information. To address\nthis issue, we introduce MMKE-Bench, a comprehensive MultiModal Knowledge\nEditing Benchmark, designed to evaluate the ability of LMMs to edit diverse\nvisual knowledge in real-world scenarios. MMKE-Bench addresses these\nlimitations by incorporating three types of editing tasks: visual entity\nediting, visual semantic editing, and user-specific editing. Besides,\nMMKE-Bench uses free-form natural language to represent and edit knowledge,\noffering a more flexible and effective format. The benchmark consists of 2,940\npieces of knowledge and 8,363 images across 33 broad categories, with\nevaluation questions automatically generated and human-verified. We assess five\nstate-of-the-art knowledge editing methods on three prominent LMMs, revealing\nthat no method excels across all criteria, and that visual and user-specific\nedits are particularly challenging. MMKE-Bench sets a new standard for\nevaluating the robustness of multimodal knowledge editing techniques, driving\nprogress in this rapidly evolving field.\n","authors":["Yuntao Du","Kailin Jiang","Zhi Gao","Chenrui Shi","Zilong Zheng","Siyuan Qi","Qing Li"],"pdf_url":"https://arxiv.org/pdf/2502.19870v2.pdf","comment":"Accept to ICLR2025. Project Page: https://mmke-bench-iclr.github.io/"},{"id":"http://arxiv.org/abs/2502.13922v3","updated":"2025-03-01T08:02:07Z","published":"2025-02-19T17:59:03Z","title":"LongPO: Long Context Self-Evolution of Large Language Models through\n  Short-to-Long Preference Optimization","summary":"  Large Language Models (LLMs) have demonstrated remarkable capabilities\nthrough pretraining and alignment. However, superior short-context LLMs may\nunderperform in long-context scenarios due to insufficient long-context\nalignment. This alignment process remains challenging due to the impracticality\nof human annotation for extended contexts and the difficulty in balancing\nshort- and long-context performance. To address these challenges, we introduce\nLongPO, that enables short-context LLMs to self-evolve to excel on long-context\ntasks by internally transferring short-context capabilities. LongPO harnesses\nLLMs to learn from self-generated short-to-long preference data, comprising\npaired responses generated for identical instructions with long-context inputs\nand their compressed short-context counterparts, respectively. This preference\nreveals capabilities and potentials of LLMs cultivated during short-context\nalignment that may be diminished in under-aligned long-context scenarios.\nAdditionally, LongPO incorporates a short-to-long KL constraint to mitigate\nshort-context performance decline during long-context alignment. When applied\nto Mistral-7B-Instruct-v0.2 from 128K to 512K context lengths, LongPO fully\nretains short-context performance and largely outperforms naive SFT and DPO in\nboth long- and short-context tasks. Specifically, LongPO-trained models can\nachieve results on long-context benchmarks comparable to, or even surpassing,\nthose of superior LLMs (e.g., GPT-4-128K) that involve extensive long-context\nannotation and larger parameter scales. Our code is available at\nhttps://github.com/DAMO-NLP-SG/LongPO.\n","authors":["Guanzheng Chen","Xin Li","Michael Qizhe Shieh","Lidong Bing"],"pdf_url":"https://arxiv.org/pdf/2502.13922v3.pdf","comment":"ICLR 2025"},{"id":"http://arxiv.org/abs/2502.10596v2","updated":"2025-03-01T06:33:01Z","published":"2025-02-14T23:00:49Z","title":"Post-training an LLM for RAG? Train on Self-Generated Demonstrations","summary":"  Large language models (LLMs) often struggle with knowledge intensive NLP\ntasks, such as answering \"Who won the latest World Cup?\" because the knowledge\nthey learn during training may be insufficient or outdated. Conditioning\ngeneration on retrieved documents -- a technique known as retrieval augmented\ngeneration (RAG) -- mitigates these shortcomings by allowing the model to\nleverage in-context information. Practitioners can improve LLM RAG performance\nby fine-tuning on retrieval-augmented instructions, but must beware that this\ncan cause undesirable model behaviors like hallucinations. We attribute this\ndegradation to the fact that the training data is likely to be\nout-of-distribution for the model and may suffer from quality issues, such as\nmisalignment between retrievals and target responses (since retrievals are\nfrequently added post-hoc). We propose a recipe for training RAG-enabled LLMs\nusing self-generated demonstrations, thereby avoiding training on\nout-of-distribution text and integrating retrievals into the LLM responses. We\nevaluate our method on knowledge intensive question answering (QA) tasks and\nshow that our method teaches LLMs to properly handle in-context retrievals and\nabstain from questions it will likely get wrong. Compared to conventional RA-IT\nmethods, our method prevents model degradation in non-RAG settings while\nexhibiting superior QA performance.\n","authors":["Matthew Finlayson","Ilia Kulikov","Daniel M. Bikel","Barlas Oguz","Xilun Chen","Aasish Pappu"],"pdf_url":"https://arxiv.org/pdf/2502.10596v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.21368v2","updated":"2025-03-01T06:14:00Z","published":"2024-07-31T06:34:38Z","title":"Prompting Medical Large Vision-Language Models to Diagnose Pathologies\n  by Visual Question Answering","summary":"  Large Vision-Language Models (LVLMs) have achieved significant success in\nrecent years, and they have been extended to the medical domain. Although\ndemonstrating satisfactory performance on medical Visual Question Answering\n(VQA) tasks, Medical LVLMs (MLVLMs) suffer from the hallucination problem,\nwhich makes them fail to diagnose complex pathologies. Moreover, they readily\nfail to learn minority pathologies due to imbalanced training data. We propose\ntwo prompting strategies for MLVLMs that reduce hallucination and improve VQA\nperformance. In the first strategy, we provide a detailed explanation of the\nqueried pathology. In the second strategy, we fine-tune a cheap, weak learner\nto achieve high performance on a specific metric, and textually provide its\njudgment to the MLVLM. Tested on the MIMIC-CXR-JPG and Chexpert datasets, our\nmethods significantly improve the diagnostic F1 score, with the highest\nincrease being 0.27. We also demonstrate that our prompting strategies can be\nextended to general LVLM domains. Based on POPE metrics, it effectively\nsuppresses the false negative predictions of existing LVLMs and improves Recall\nby approximately 0.07.\n","authors":["Danfeng Guo","Demetri Terzopoulos"],"pdf_url":"https://arxiv.org/pdf/2407.21368v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.16880v2","updated":"2025-03-01T06:13:45Z","published":"2025-02-24T06:28:26Z","title":"CORAL: Learning Consistent Representations across Multi-step Training\n  with Lighter Speculative Drafter","summary":"  Speculative decoding is a powerful technique that accelerates Large Language\nModel (LLM) inference by leveraging a lightweight speculative draft model.\nHowever, existing designs suffers in performance due to misalignment between\ntraining and inference. Recent methods have tried to solve this issue by\nadopting a multi-step training strategy, but the complex inputs of different\ntraining steps make it harder for the draft model to converge. To address this,\nwe propose CORAL, a novel framework that improves both accuracy and efficiency\nin speculative drafting. CORAL introduces Cross-Step Representation Alignment,\na method that enhances consistency across multiple training steps,\nsignificantly improving speculative drafting performance. Additionally, we\nidentify the LM head as a major bottleneck in the inference speed of the draft\nmodel. We introduce a weight-grouping mechanism that selectively activates a\nsubset of LM head parameters during inference, substantially reducing the\nlatency of the draft model. We evaluate CORAL on three LLM families and three\nbenchmark datasets, achieving speedup ratios of 2.50x-4.07x, outperforming\nstate-of-the-art methods such as EAGLE-2 and HASS. Our results demonstrate that\nCORAL effectively mitigates training-inference misalignment and delivers\nsignificant speedup for modern LLMs with large vocabularies.\n","authors":["Yepeng Weng","Dianwen Mei","Huishi Qiu","Xujie Chen","Li Liu","Jiang Tian","Zhongchao Shi"],"pdf_url":"https://arxiv.org/pdf/2502.16880v2.pdf","comment":"Under Review"},{"id":"http://arxiv.org/abs/2501.19393v3","updated":"2025-03-01T06:07:39Z","published":"2025-01-31T18:48:08Z","title":"s1: Simple test-time scaling","summary":"  Test-time scaling is a promising new approach to language modeling that uses\nextra test-time compute to improve performance. Recently, OpenAI's o1 model\nshowed this capability but did not publicly share its methodology, leading to\nmany replication efforts. We seek the simplest approach to achieve test-time\nscaling and strong reasoning performance. First, we curate a small dataset s1K\nof 1,000 questions paired with reasoning traces relying on three criteria we\nvalidate through ablations: difficulty, diversity, and quality. Second, we\ndevelop budget forcing to control test-time compute by forcefully terminating\nthe model's thinking process or lengthening it by appending \"Wait\" multiple\ntimes to the model's generation when it tries to end. This can lead the model\nto double-check its answer, often fixing incorrect reasoning steps. After\nsupervised finetuning the Qwen2.5-32B-Instruct language model on s1K and\nequipping it with budget forcing, our model s1-32B exceeds o1-preview on\ncompetition math questions by up to 27% (MATH and AIME24). Further, scaling\ns1-32B with budget forcing allows extrapolating beyond its performance without\ntest-time intervention: from 50% to 57% on AIME24. Our model, data, and code\nare open-source at https://github.com/simplescaling/s1\n","authors":["Niklas Muennighoff","Zitong Yang","Weijia Shi","Xiang Lisa Li","Li Fei-Fei","Hannaneh Hajishirzi","Luke Zettlemoyer","Percy Liang","Emmanuel Candès","Tatsunori Hashimoto"],"pdf_url":"https://arxiv.org/pdf/2501.19393v3.pdf","comment":"46 pages (9 main), 10 figures, 15 tables"},{"id":"http://arxiv.org/abs/2406.17972v3","updated":"2025-03-01T04:10:03Z","published":"2024-06-25T23:07:18Z","title":"LABOR-LLM: Language-Based Occupational Representations with Large\n  Language Models","summary":"  Vafa et al. (2024) introduced a transformer-based econometric model, CAREER,\nthat predicts a worker's next job as a function of career history (an\n\"occupation model\"). CAREER was initially estimated (\"pre-trained\") using a\nlarge, unrepresentative resume dataset, which served as a \"foundation model,\"\nand parameter estimation was continued (\"fine-tuned\") using data from a\nrepresentative survey. CAREER had better predictive performance than\nbenchmarks. This paper considers an alternative where the resume-based\nfoundation model is replaced by a large language model (LLM). We convert\ntabular data from the survey into text files that resemble resumes and\nfine-tune the LLMs using these text files with the objective to predict the\nnext token (word). The resulting fine-tuned LLM is used as an input to an\noccupation model. Its predictive performance surpasses all prior models. We\ndemonstrate the value of fine-tuning and further show that by adding more\ncareer data from a different population, fine-tuning smaller LLMs surpasses the\nperformance of fine-tuning larger models.\n","authors":["Susan Athey","Herman Brunborg","Tianyu Du","Ayush Kanodia","Keyon Vafa"],"pdf_url":"https://arxiv.org/pdf/2406.17972v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.06615v2","updated":"2025-03-01T03:07:31Z","published":"2024-06-07T04:25:38Z","title":"Language Guided Skill Discovery","summary":"  Skill discovery methods enable agents to learn diverse emergent behaviors\nwithout explicit rewards. To make learned skills useful for unknown downstream\ntasks, obtaining a semantically diverse repertoire of skills is essential.\nWhile some approaches introduce a discriminator to distinguish skills and\nothers aim to increase state coverage, no existing work directly addresses the\n\"semantic diversity\" of skills. We hypothesize that leveraging the semantic\nknowledge of large language models (LLMs) can lead us to improve semantic\ndiversity of resulting behaviors. In this sense, we introduce Language Guided\nSkill Discovery (LGSD), a skill discovery framework that aims to directly\nmaximize the semantic diversity between skills. LGSD takes user prompts as\ninput and outputs a set of semantically distinctive skills. The prompts serve\nas a means to constrain the search space into a semantically desired subspace,\nand the generated LLM outputs guide the agent to visit semantically diverse\nstates within the subspace. We demonstrate that LGSD enables legged robots to\nvisit different user-intended areas on a plane by simply changing the prompt.\nFurthermore, we show that language guidance aids in discovering more diverse\nskills compared to five existing skill discovery methods in robot-arm\nmanipulation environments. Lastly, LGSD provides a simple way of utilizing\nlearned skills via natural language.\n","authors":["Seungeun Rho","Laura Smith","Tianyu Li","Sergey Levine","Xue Bin Peng","Sehoon Ha"],"pdf_url":"https://arxiv.org/pdf/2406.06615v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.11418v3","updated":"2025-03-01T01:47:51Z","published":"2024-07-16T06:19:14Z","title":"Semantic Operators: A Declarative Model for Rich, AI-based Data\n  Processing","summary":"  The semantic capabilities of large language models (LLMs) have the potential\nto enable rich analytics and reasoning over vast knowledge corpora.\nUnfortunately, existing systems either empirically optimize expensive\nLLM-powered operations with no performance guarantees, or serve a limited set\nof row-wise LLM operations, providing limited robustness, expressiveness and\nusability. We introduce semantic operators, the first formalism for declarative\nand general-purpose AI-based transformations based on natural language\nspecifications (e.g., filtering, sorting, joining or aggregating records using\nnatural language criteria). Each operator opens a rich space for execution\nplans, similar to relational operators. Our model specifies the expected\nbehavior of each operator with a high-quality gold algorithm, and we develop an\noptimization framework that reduces cost, while providing accuracy guarantees\nwith respect to a gold algorithm. Using this approach, we propose several novel\noptimizations to accelerate semantic filtering, joining, group-by and top-k\noperations by up to $1,000\\times$. We implement semantic operators in the LOTUS\nsystem and demonstrate LOTUS' effectiveness on real, bulk-semantic processing\napplications, including fact-checking, biomedical multi-label classification,\nsearch, and topic analysis. We show that the semantic operator model is\nexpressive, capturing state-of-the-art AI pipelines in a few operator calls,\nand making it easy to express new pipelines that match or exceed quality of\nrecent LLM-based analytic systems by up to $170\\%$, while offering accuracy\nguarantees. Overall, LOTUS programs match or exceed the accuracy of\nstate-of-the-art AI pipelines for each task while running up to $3.6\\times$\nfaster than the highest-quality baselines. LOTUS is publicly available at\nhttps://github.com/lotus-data/lotus.\n","authors":["Liana Patel","Siddharth Jha","Melissa Pan","Harshit Gupta","Parth Asawa","Carlos Guestrin","Matei Zaharia"],"pdf_url":"https://arxiv.org/pdf/2407.11418v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.17514v3","updated":"2025-03-01T01:39:28Z","published":"2024-01-31T00:15:34Z","title":"How Useful is Continued Pre-Training for Generative Unsupervised Domain\n  Adaptation?","summary":"  Recent breakthroughs in scale have enabled the emergence of powerful\ngenerative language models, and the ability to fine-tune these models on\nvarious tasks by casting them into prompts or instructions. In this landscape,\nthe problem of Unsupervised Domain Adaptation (UDA), or the problem of\nleveraging knowledge from a labeled source domain to an unlabeled target\ndomain, has been left behind, with recent UDA methods still addressing\ndiscriminative classification. In particular, two popular UDA approaches,\ninvolving Continued Pre-Training (CPT) and learning domain invariant\nrepresentations, have been under-explored in the generative setting, signaling\na gap. In this work, we evaluate the utility of CPT for generative UDA. We\nfirst perform an empirical evaluation to measure the trade-offs between CPT and\nstrong methods promoting domain invariance. We further evaluate how well the\nbenefits of CPT extend to different architectures, tuning methods and data\nregimes. We then motivate the use of CPT by studying to what degree it benefits\nclassification performance on the target domain. Finally, we attempt to\nunderstand the mechanism behind which CPT improves classification performance\non the unlabeled target domain. Our findings suggest that a implicitly learns\nthe downstream task while predicting masked words informative to that task. Our\nwork connects the body of UDA research with that of instruction tuning,\nenabling an initial step towards a wider applicability of modern language\nmodels.\n","authors":["Rheeya Uppaal","Yixuan Li","Junjie Hu"],"pdf_url":"https://arxiv.org/pdf/2401.17514v3.pdf","comment":"Accepted to RepL4NLP at ACL 2024"},{"id":"http://arxiv.org/abs/2405.13967v5","updated":"2025-03-01T01:35:47Z","published":"2024-05-22T20:08:48Z","title":"Model Editing as a Robust and Denoised variant of DPO: A Case Study on\n  Toxicity","summary":"  Recent alignment algorithms such as direct preference optimization (DPO) have\nbeen developed to improve the safety of large language models (LLMs) by\ntraining these models to match human behaviors exemplified by preference data.\nHowever, these methods are both computationally intensive and lacking in\ncontrollability and transparency, inhibiting their widespread use. Furthermore,\nthese tuning-based methods require large-scale preference data for training and\nare susceptible to noisy preference data. In this paper, we introduce a\ntuning-free alignment alternative, ProFS (Projection Filter for Subspaces), and\ndemonstrate its effectiveness under the use case of toxicity reduction.\nGrounded on theory from factor analysis, ProFS is a sample-efficient model\nediting approach that identifies a toxic subspace in the model parameter space\nand reduces model toxicity by projecting away the detected subspace. The toxic\nsubspace is identified by extracting preference data embeddings from the\nlanguage model, and removing non-toxic information from these embeddings. We\nshow that ProFS is more sample-efficient than DPO, further showcasing greater\nrobustness to noisy data. Finally, we attempt to connect tuning based alignment\nwith editing, by establishing both theoretical and empirical connections\nbetween ProFS and DPO, showing that ProFS can be interpreted as a denoised\nversion of a single DPO step.\n","authors":["Rheeya Uppaal","Apratim Dey","Yiting He","Yiqiao Zhong","Junjie Hu"],"pdf_url":"https://arxiv.org/pdf/2405.13967v5.pdf","comment":"Accepted to ICLR 2025"},{"id":"http://arxiv.org/abs/2408.04650v2","updated":"2025-03-01T00:49:53Z","published":"2024-08-03T19:57:49Z","title":"Building Trust in Mental Health Chatbots: Safety Metrics and LLM-Based\n  Evaluation Tools","summary":"  Objective: This study aims to develop and validate an evaluation framework to\nensure the safety and reliability of mental health chatbots, which are\nincreasingly popular due to their accessibility, human-like interactions, and\ncontext-aware support. Materials and Methods: We created an evaluation\nframework with 100 benchmark questions and ideal responses, and five guideline\nquestions for chatbot responses. This framework, validated by mental health\nexperts, was tested on a GPT-3.5-turbo-based chatbot. Automated evaluation\nmethods explored included large language model (LLM)-based scoring, an agentic\napproach using real-time data, and embedding models to compare chatbot\nresponses against ground truth standards. Results: The results highlight the\nimportance of guidelines and ground truth for improving LLM evaluation\naccuracy. The agentic method, dynamically accessing reliable information,\ndemonstrated the best alignment with human assessments. Adherence to a\nstandardized, expert-validated framework significantly enhanced chatbot\nresponse safety and reliability. Discussion: Our findings emphasize the need\nfor comprehensive, expert-tailored safety evaluation metrics for mental health\nchatbots. While LLMs have significant potential, careful implementation is\nnecessary to mitigate risks. The superior performance of the agentic approach\nunderscores the importance of real-time data access in enhancing chatbot\nreliability. Conclusion: The study validated an evaluation framework for mental\nhealth chatbots, proving its effectiveness in improving safety and reliability.\nFuture work should extend evaluations to accuracy, bias, empathy, and privacy\nto ensure holistic assessment and responsible integration into healthcare.\nStandardized evaluations will build trust among users and professionals,\nfacilitating broader adoption and improved mental health support through\ntechnology.\n","authors":["Jung In Park","Mahyar Abbasian","Iman Azimi","Dawn T. Bounds","Angela Jun","Jaesu Han","Robert M. McCarron","Jessica Borelli","Parmida Safavi","Sanaz Mirbaha","Jia Li","Mona Mahmoudi","Carmen Wiedenhoeft","Amir M. Rahmani"],"pdf_url":"https://arxiv.org/pdf/2408.04650v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.21637v2","updated":"2025-03-01T00:12:48Z","published":"2024-10-29T00:46:24Z","title":"Mitigating Paraphrase Attacks on Machine-Text Detectors via Paraphrase\n  Inversion","summary":"  High-quality paraphrases are easy to produce using instruction-tuned language\nmodels or specialized paraphrasing models. Although this capability has a\nvariety of benign applications, paraphrasing\nattacks$\\unicode{x2013}$paraphrases applied to machine-generated\ntexts$\\unicode{x2013}$are known to significantly degrade the performance of\nmachine-text detectors. This motivates us to consider the novel problem of\nparaphrase inversion, where, given paraphrased text, the objective is to\nrecover an approximation of the original text. The closer the approximation is\nto the original text, the better machine-text detectors will perform. We\npropose an approach which frames the problem as translation from paraphrased\ntext back to the original text, which requires examples of texts and\ncorresponding paraphrases to train the inversion model. Fortunately, such\ntraining data can easily be generated, given a corpus of original texts and one\nor more paraphrasing models. We find that language models such as GPT-4 and\nLlama-3 exhibit biases when paraphrasing which an inversion model can learn\nwith a modest amount of data. Perhaps surprisingly, we also find that such\nmodels generalize well, including to paraphrase models unseen at training time.\nFinally, we show that when combined with a paraphrased-text detector, our\ninversion models provide an effective defense against paraphrasing attacks, and\noverall our approach yields an average improvement of +22% AUROC across seven\nmachine-text detectors and three different domains.\n","authors":["Rafael Rivera Soto","Barry Chen","Nicholas Andrews"],"pdf_url":"https://arxiv.org/pdf/2410.21637v2.pdf","comment":null}]}}